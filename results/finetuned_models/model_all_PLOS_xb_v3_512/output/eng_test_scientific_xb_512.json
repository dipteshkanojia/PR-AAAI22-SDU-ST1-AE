[{"text":"The label restricted means the model is restricted to recovering rules that have been seen in training data. LR = labeled recall. LP = labeled","acronyms":[[109,111],[130,132]],"long-forms":[[4,9],[114,128]],"ID":"1"},{"text":"We have followed their methodology as best as we could, using the same WordNet (WN) categories and the same corpora.","acronyms":[[80,82]],"long-forms":[[71,78]],"ID":"2"},{"text":"adaptation scenario. Duan et al (2009) proposed a Domain Adaptation Machine (DAM) method to learn a Least-Squares SVM classifier for target do-","acronyms":[[77,80],[114,117]],"long-forms":[[48,75],[0,1]],"ID":"3"},{"text":" Actes de la 13e Confe?rence sur le Traitement Automatique des Langues Naturelles (TALN), pages 20?42.","acronyms":[[82,86]],"long-forms":[[6,11],[32,80]],"ID":"4"},{"text":"TGTM P=p,pk ,b   TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l   TGTM PW=pw, pkw, bw ","acronyms":[[0,4],[49,51]],"long-forms":[[8,9],[8,9],[8,9],[8,9],[26,27],[-1,9],[55,56],[8,9],[8,9]],"ID":"5"},{"text":"MIRA\/AROW requires selecting the loss function `(w) so that wt can be solved in closed-form, by a quadratic program (QP), or in some other way that is better than linearizing.","acronyms":[[60,62],[117,119]],"long-forms":[[96,115]],"ID":"6"},{"text":" 1 Introduction Information extraction (IE) systems generally consist of multiple interdependent components, e.g., en-","acronyms":[[41,42]],"long-forms":[[27,37]],"ID":"7"},{"text":"117 d?eriv?es) (AP (ADJ thiazidiques) (COORD (PONCT ,) (NP (DET les) (ADV plus) (ADJ accessibles)) (PONCT ,) (AP (ADJ disponibles)))) (PP (P sous forme de) (NP (NC m?edicaments) (AP (ADJ g?en?eriques)))))))))","acronyms":[[16,18],[20,23],[39,44],[46,51],[56,58],[60,63],[70,73],[135,137]],"long-forms":[[-1,12],[24,36],[81,96],[139,154],[-1,13],[-1,19]],"ID":"8"},{"text":"2002. The concaveconvex procedure (CCCP). In Proc.","acronyms":[[39,40]],"long-forms":[[24,33]],"ID":"9"},{"text":"most related words pop-up. Then the documents are re-ranked about their term frequency (TF) values (G. Salton and C. Buckley, 1988) and contextual infor-","acronyms":[[88,90]],"long-forms":[[72,86]],"ID":"10"},{"text":"in part by ONR grant number N00014-95-1-1164,  and has been done in collaboration with the US  Navy's NCCOSC RDT&E Division (NRaD), Ascent  Technologies, Mitre Corp., MRJ Corp., and SRI In- ","acronyms":[[11,14],[102,108],[182,185]],"long-forms":[[-1,6]],"ID":"11"},{"text":"approach.  The Minimum Token Margin (MTM) strategy is a variant of the margin sampling strategy introduced","acronyms":[[37,40]],"long-forms":[[15,35],[0,1]],"ID":"12"},{"text":"food(FOOD) artifact(AFT) article(ART)  location(LOC) psych_feature(PSY)  cognition(COG) feeling(FEEL)  motivation(MOT) abstraction(ABS) ","acronyms":[[5,9],[20,23],[33,36],[48,51],[67,70],[83,86],[96,100],[114,117],[131,134]],"long-forms":[[0,4],[11,19],[25,32],[39,47],[53,66],[73,82],[88,95],[103,113],[119,130]],"ID":"13"},{"text":"m } using standard support vector machine (SVM) training (holding A fixed), and then make a simple","acronyms":[[46,47]],"long-forms":[[0,1],[27,41],[12,13]],"ID":"14"},{"text":"son. One is based on chi-square value and the other is based on Pointwise Mutual Information (PMI). ","acronyms":[[94,97]],"long-forms":[[9,11],[1,3],[9,11],[61,92]],"ID":"15"},{"text":"concerned with video lecture viewing only) before 11 Figure 4: Variation of Average Information Processing Indices(IPI) for Video 4-6 Figure 5: Variation of Average Information Processing Indices(IPI) for the full course","acronyms":[[115,118]],"long-forms":[[84,114],[45,48],[84,114],[45,48]],"ID":"16"},{"text":"Figure 1. Annotated Syntax Tree  (AST) and Phrase Levels (PL). ","acronyms":[[34,37],[58,60]],"long-forms":[[10,31],[43,56]],"ID":"17"},{"text":"Num. of Friendships 265 Average Clustering Coefficient (ACC) 0.42 Diameter 12 Table 1: Statistical information of our Foursquare dataset.","acronyms":[[56,59]],"long-forms":[[24,54]],"ID":"18"},{"text":"number of phrase pairs that can be extracted. We observe that it (OEF) is able to find more than 14% more phrase pairs than heuristic methods and","acronyms":[[66,69]],"long-forms":[],"ID":"19"},{"text":"? Frequency-based: LUHN (Luhn, 1958) score(S) = maxci?{clusters(S)}{csi}, where","acronyms":[[19,23]],"long-forms":[[0,1],[-1,5],[46,47],[-1,2],[54,55],[29,30]],"ID":"20"},{"text":"resource presented here implements formal semantic descriptions of verbs in the Web Ontology Language (OWL) and exploits its reasoning potential based on Description Logics (DL) for the disambiguation of verbs in context, since before the correct sense of a verb can be reliably","acronyms":[[103,106],[174,176]],"long-forms":[[80,101],[60,62],[154,172],[39,40]],"ID":"21"},{"text":"the noun phrase (NP) rule, a top-down parser is delaying making any commitments about the category following the determiner (DT). This delay in predic-","acronyms":[[17,19],[125,127]],"long-forms":[[4,15],[12,13],[53,55]],"ID":"22"},{"text":"for test year 2010 (train on 2009), polarity task.  SemTree combined with FWD (SemTreeFWD) generally gives the best performance in both","acronyms":[],"long-forms":[[52,68],[74,77]],"ID":"23"},{"text":"Marcu (2007) note that none of the tens of papers published over the last five years has shown that significant decreases in alignment error rate (AER) result in significant increases in translation perfor-","acronyms":[[147,150]],"long-forms":[[125,145]],"ID":"24"},{"text":"\"bridge\" between this bilingual word pair. This  leads us to use the term frequency(TF) mea-  sure.","acronyms":[[84,86]],"long-forms":[[69,83]],"ID":"25"},{"text":" 4 Evaluation and Experiments We use the General Inquirer (GI)8 data for evaluation.","acronyms":[[60,61]],"long-forms":[[48,56]],"ID":"26"},{"text":"We ran our experiments with three corpora in different languages and representing different textual typologies: the British National Corpus (BNC), a ? bal-","acronyms":[[141,144]],"long-forms":[[116,139],[4,5]],"ID":"27"},{"text":"the same discourse relation annotation style over different domain corpora: PDTB is annotated on top of Wall Street Journal (WSJ) corpus (financial news-wire domain); and it is aligned with Penn","acronyms":[[76,80],[125,128]],"long-forms":[[104,123]],"ID":"28"},{"text":" 6 Related Work  Boosting is a machine learning (ML) method that  has been well studied in the ML community ","acronyms":[[50,51]],"long-forms":[[5,6],[38,46],[21,23]],"ID":"29"},{"text":"MOVE is a label for complex events that con-  sists o f  maximal ly  three sub-events,  namely  START, CHPOS (CHANGE OF POSITION), and STOP,  where the first and the last sub-event are optional ","acronyms":[],"long-forms":[[117,128]],"ID":"30"},{"text":">puncS Hertz equipment is a major supplier of rental equipment N\/N N S\\N (S\\S)\/N N\/N N (N\\N)\/N N\/N N > >","acronyms":[],"long-forms":[[67,72],[74,77],[78,86],[88,91],[78,86]],"ID":"31"},{"text":" Several algorithms have been evaluated using 80 multiple-choice synonym questions taken from the Test of English as a Foreign Language (TOEFL). An example of","acronyms":[[136,141]],"long-forms":[[97,134]],"ID":"32"},{"text":"halcea and Nataste (2012)). In our framework, we train a Neural Language Model (NLM) on yearly corpora to obtain word vectors for each year","acronyms":[[80,83]],"long-forms":[[55,78]],"ID":"33"},{"text":"summarizing the work of the Corpora and Performance  Evaluation Committee (CPEC) of the DARPA Spoken Language  Systems (SLS) Program, with specific reports from several  working groups which have been dealing with various aspects ","acronyms":[[75,79],[88,93],[120,123]],"long-forms":[[-1,43],[-1,22]],"ID":"34"},{"text":"robust and the failure of matching produces no results.  On the other hand, statistical learning model (SLM) can  deal with unexpected cases during designing and ","acronyms":[[104,107]],"long-forms":[[76,102]],"ID":"35"},{"text":"Statistical model (S) O O O O O O O O  Cooperative(CPR)  O   O O  O  Corrective(COR)   O  O  O O  Self-directing(SFD)    O  O O O ","acronyms":[[51,54]],"long-forms":[[0,1],[69,79],[22,23],[22,23],[98,112],[22,23]],"ID":"36"},{"text":"Among  three state-of-the-art systems we have, the best Fscores of single character location (SCL) and single character person (SCP) are 43.63% and 43.48% ","acronyms":[[94,97],[128,131]],"long-forms":[[67,92],[103,126]],"ID":"37"},{"text":"5. Introducing background knowledge via CCMs [30 min]   We will look at ways in which Constrained Conditional Models (CCMs)can be used to  augment probabilistic models with declarative constraints in order to support decisions ","acronyms":[[40,44]],"long-forms":[[69,71],[11,13],[86,116],[11,13]],"ID":"38"},{"text":"bor.hodoscek@gmail.com Abstract Regarding the construction of an ontology of Japanese lexical properties (JLP-O) as fundamental in terms of establishing a conceptual framework to guide and facilitate the construction of a large-scale","acronyms":[[106,111]],"long-forms":[[77,104],[15,16],[15,16]],"ID":"39"},{"text":"Abstract  This paper presents a new approach  based on Equivalent Pseudowords (EPs)  to tackle Word Sense Disambiguation ","acronyms":[[79,82]],"long-forms":[[5,6],[55,77]],"ID":"40"},{"text":"phrase-based decoder that has been augmented to translate ambiguous input given in the form of a confusion network (CN), a weighted finite state representation of a","acronyms":[[116,118]],"long-forms":[[95,114],[3,4]],"ID":"41"},{"text":"BACKGROUN D The LOLITA (Large-scale, Object-based, Linguistic Interactor, Translator, and Analyser) system is de signed as a general purpose Natural Language Processing (NLP) system and has been under development a t the University of Durham since 1986 .","acronyms":[[16,22],[170,173]],"long-forms":[[25,26],[141,168],[213,216]],"ID":"42"},{"text":"al. ( 2008), on the other hand, include features from the grammar in a maximum entropy (ME) classifier to predict new lexical entries for the","acronyms":[[88,90]],"long-forms":[[69,86]],"ID":"43"},{"text":" select_id_schema(Sign,Sit,Phrase,NewSit) :-  id_schema(ID),  extend~sit (Sit,NewS\\]l), ","acronyms":[[55,57]],"long-forms":[[7,16]],"ID":"44"},{"text":"different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Beginning an Idiomatic chunk), I-I (Inside an Idiomatic chunk), O (Outside a chunk).","acronyms":[[16,19],[52,55],[85,88]],"long-forms":[[21,43],[57,76],[90,112],[126,145],[157,166]],"ID":"45"},{"text":"Besides, other tools such as ELAN2 or Anvil3 are available as well, as are tool kits such as the Annotation Graph Toolkit (AGTK)4 or the NITE XML Toolkit.5 While multimodal annotation","acronyms":[[29,34],[123,127],[137,141]],"long-forms":[[97,121]],"ID":"46"},{"text":"Eparl+NC no 29.28 (0.11) 55.28 (0.13) Eparl+NC yes 29.26 (0.10) 55.44 (0.29) Table 1: Results of the study on number translation (NT) from English to French","acronyms":[[9,11],[130,132]],"long-forms":[[107,128]],"ID":"47"},{"text":"Xue and Palmer, 2005). The present version  PCTB5 (PCTB Version 5), contains 18,782 sentences, 507,222 words, 824,983 Hanzi and 890 ","acronyms":[[44,49],[44,48]],"long-forms":[[19,20]],"ID":"48"},{"text":"Proper noun (PropN): yes when the description has a capitalized initial.  Restrictive postmodification (RPostm): yes if the definite description is modified by relative or associative clauses.","acronyms":[[13,18],[104,110]],"long-forms":[[0,11],[47,48],[74,102],[169,171]],"ID":"49"},{"text":"ply it on substrings of names. In the English to Russian task, we report ACC (Accuracy in top-1) of 0.545,  Mean F-score of 0.917, and MRR (Mean Reciprocal  ","acronyms":[[73,76],[135,138]],"long-forms":[[78,86],[108,112]],"ID":"50"},{"text":"Lexico-syntactic properties of English Non-Deverbal Event Nouns (NDV E N),  Process Nouns (PR-N) and Result Nouns (RESN) and Non Event Nouns (NEN).","acronyms":[[65,68],[91,95],[115,119]],"long-forms":[[39,51],[52,63],[69,72],[76,89],[101,113],[125,140]],"ID":"51"},{"text":"dimensionality reduction. NG = ngrams, E = emoticon replacement, IR = informal register replacement, TL = tweet length, RT = retweet count, SVO = subject-verb-","acronyms":[[26,28],[65,67],[101,103],[120,122],[140,143]],"long-forms":[[31,37],[43,51],[70,99],[106,118],[125,138]],"ID":"52"},{"text":"levels of the discourse tree. Segmented Discourse  Structure Theory (SDRT) is introduced (Asher,  1993) and the predictions of this theory are dis- ","acronyms":[[69,73]],"long-forms":[[-1,35],[15,17]],"ID":"53"},{"text":"We describe two classifiers we have built for relevance. A Naive Bayes classifier (NB) was used as the baseline.","acronyms":[[83,85]],"long-forms":[[59,81]],"ID":"54"},{"text":"On all test sets, and for both the evaluation metrics used, the results achieved by the classifier built from the automatically annotated training set (AA) produces lower error rates (Weighted FPR-FDR)","acronyms":[[152,154]],"long-forms":[[128,150]],"ID":"55"},{"text":"Figure 1: The system combination architecture.  system prior weights and a language model (LM). ","acronyms":[[91,93]],"long-forms":[[73,89]],"ID":"56"},{"text":"and its too slowly paced to be a thriller.  Question Classification (QC)What is the temperature at the center of the earth? numberWhat state did the Battle of Bighorn take place in?","acronyms":[[69,71]],"long-forms":[[0,1],[44,67],[62,64]],"ID":"57"},{"text":"Table 3: AMT evaluation results. Numbers are percentages or counts. BL = baseline, SY = system, N-D = no decision, B=S = same sentence selected by baseline and system","acronyms":[[9,12],[68,70],[83,85],[96,99]],"long-forms":[[73,81],[88,94],[102,113],[121,134],[73,81]],"ID":"58"},{"text":" From the annotation pipeline we extracted as features: the polar words (PolW) and their basic polarity (Pol); the sentiment annotations from LIWC","acronyms":[[76,77]],"long-forms":[[65,70]],"ID":"59"},{"text":"There-fore, the end for the tram was sealed in the 1970s.] As an application example, a small corpus consisting of 21 newspaper articles is analyzed. The corpus belongs to the interdisciplinary pro-ject Future Mobility (FuMob), which is funded by the Excellent Initiative of the German federal and state governments. The methodological ap-proach consists of three steps, which are per-formed iteratively: (1) manual discourse-linguistic argumentation analysis, (2) semi-automatic Text Mining (PoS-tagging and linguis-tic multi-level annotation), and (3) data merge.","acronyms":[[220,225]],"long-forms":[[44,46],[30,31],[203,218],[480,491]],"ID":"60"},{"text":"~21   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 10?18, Sofia, Bulgaria, August 9, 2013.","acronyms":[[71,78]],"long-forms":[[34,69]],"ID":"61"},{"text":"For instance, in (def4) a polar interrogative clauses  (I'olS) is detined as a verb-first clause (V1S), which  in (def3) is deiined as a main clause (MainS), which  in turn is defined as a clause (S).","acronyms":[[22,23]],"long-forms":[[4,6],[8,9],[8,9],[46,52],[4,6],[8,9],[46,52],[4,6],[187,195]],"ID":"62"},{"text":"number of correct matched constituents in proposed parse  number of constituents in treebank parse  3) Crossing Brackets(CBs) ffinumber of constituents which violate constituent boundaries with a  constituent inthe treebank parse.","acronyms":[[121,124]],"long-forms":[[39,41],[39,41],[103,120],[19,20]],"ID":"63"},{"text":" 1 Introduction Word Sense Disambiguation (WSD) is the process of resolving the meaning of a word unambiguously","acronyms":[[45,46]],"long-forms":[[20,40],[27,29],[29,30]],"ID":"64"},{"text":"In order to represent gram-  mar rules distributively, we adopt categorial unifi-  cAtion grammar (CUG) Where eaclh category owns  its functional type.","acronyms":[[102,103]],"long-forms":[[-1,20]],"ID":"65"},{"text":"Translate has achieved very good results on the  Chinese-to-English translation tracks of NIST open  machine translation test (MT)5 and it ranks the first  on most tracks.","acronyms":[[90,94],[127,129]],"long-forms":[[41,43],[68,79],[101,125],[136,138],[41,43]],"ID":"66"},{"text":" In earlier topic modeling work such as latent Dirichlet alocation (LDA) (Blei et al, 2003; Griffiths and Steyvers, 2004), documents are treated as bags of","acronyms":[[67,70]],"long-forms":[[39,65],[53,55]],"ID":"67"},{"text":"Apart from  other features, each modifier should be anno\/atecl  with a pragmatic function feature (PRAGM), which  specifies why a modifier is used it: an NP.","acronyms":[[104,105]],"long-forms":[[2,3],[81,97],[2,3]],"ID":"68"},{"text":"way as the above feature.  The Acoustic Features (AF) were extracted directly from the wave files using SoX: Minimum,","acronyms":[[50,52],[104,107]],"long-forms":[[31,48]],"ID":"69"},{"text":"factfinding? technology, Information Extraction (IE), to determine exactly what happened in each article:","acronyms":[[49,51]],"long-forms":[[25,47]],"ID":"70"},{"text":" Relations between function words and content words (e.g. specifier (SPR), marker complement (CMP), infinitival zu marker (PM)) are frequent and","acronyms":[[68,71],[93,96],[122,124]],"long-forms":[[57,66],[74,91],[99,120]],"ID":"71"},{"text":"name along the path ? LCA (Lowest Common Ancestor) path that is from ORG name to its lowest common ancestor with PRO name","acronyms":[[22,25],[69,72],[113,116]],"long-forms":[[27,49],[46,48]],"ID":"72"},{"text":"1 The following abbreviations are used POSS = possessive prefix\/suffix; LOC = locative suffix; OBV = obviative suffix; DIM = diminutive suffix; NUM = number marking suffix; IN","acronyms":[],"long-forms":[],"ID":"73"},{"text":"the Brill tagger.  NNP = proper noun, CD = cardinal number,  CC = coordinating conjunction, JJ = adjective, VBG = verb,  gerund\/present participle ","acronyms":[[19,22],[38,40],[61,63],[92,94],[108,111]],"long-forms":[[43,51],[66,90],[97,106],[114,118]],"ID":"74"},{"text":"natural_object(NOBJ) substance(SUB)  food(FOOD) artifact(AFT) article(ART)  location(LOC) psych_feature(PSY)  cognition(COG) feeling(FEEL) ","acronyms":[[15,19],[31,34],[42,46],[57,60],[70,73],[85,88],[104,107],[120,123],[133,137]],"long-forms":[[0,14],[21,30],[37,41],[48,56],[62,69],[76,84],[90,103],[110,119],[125,132]],"ID":"75"},{"text":"the increase is.  Table 2 shows the average solve time (ST) for sentences with respect to the number of tokens in","acronyms":[[56,58]],"long-forms":[[36,54]],"ID":"76"},{"text":"language processing. Specifically, stochastic finitestate transducers (SFSTs) have proved to be useful for machine translation tasks within restricted do-","acronyms":[[71,76]],"long-forms":[[35,69],[36,38]],"ID":"77"},{"text":"Inspired by our experience of dealing with different text classification problems, we decide to  employ a linear support vector machine (SVM) in  our NLI2013 system.","acronyms":[[137,140]],"long-forms":[[93,95],[32,33],[113,135],[34,36]],"ID":"78"},{"text":" The transformation phase is done by applying singular value decomposition (SVD) to the initial term-by-sentence matrix defined as A = U?V T .","acronyms":[[78,79]],"long-forms":[[54,73]],"ID":"79"},{"text":"Abstract WordNet, a widely used sense inventory for Word Sense Disambiguation(WSD), is often too fine-grained for many Natural Language","acronyms":[[78,81]],"long-forms":[[5,6],[52,77],[64,66]],"ID":"80"},{"text":"Mihael Arcan and Paul Buitelaar Unit for Natural Language Processing, Digital Enterprise Research Institute (DERI) National University of Ireland Galway (NUIG)","acronyms":[[109,113],[154,158]],"long-forms":[[70,107],[115,152]],"ID":"81"},{"text":" For French, the main problem was to retrieve MWEs (Multi Word Expression) in pred data mode.","acronyms":[],"long-forms":[[-1,16]],"ID":"82"},{"text":"General-type: region Specific type: RCC Spatial value: PP (proper part) Dynamic","acronyms":[[36,39],[55,57]],"long-forms":[[66,70]],"ID":"83"},{"text":"system of Conceptual Dependency (Schank 1975). Some of the  Conceptual Dependency (CD) s t r u c t u r e s  are passed on to a program  which expresses them in E n g l i s h .","acronyms":[[83,85]],"long-forms":[[10,31],[10,31],[89,92],[3,4],[4,5],[18,19],[12,13],[19,20]],"ID":"84"},{"text":"60  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 118?125, Seoul, South Korea, 5-6 July 2012.","acronyms":[[100,107]],"long-forms":[[50,98]],"ID":"85"},{"text":" Chapter 1 has the lowest pi?S score in the table, and also the highest bias (BS). One of the reasons for","acronyms":[[28,29]],"long-forms":[[63,75]],"ID":"86"},{"text":"order to adapt them to process dialects. This paper adopts this general framework: we propose a method to build a lexicon of deverbal nouns for Tunisian (TUN) using MSA tools and resources as starting material.","acronyms":[[154,157],[165,168]],"long-forms":[[9,10],[9,10],[144,152]],"ID":"87"},{"text":"3 Approach Following this intuition, we fit a directed Gaussian graphical model (GGM) that simultaneously considers (i) each word?s embedding (obtained from","acronyms":[[81,84]],"long-forms":[[7,8],[55,79],[17,18],[24,25]],"ID":"88"},{"text":"In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL). ","acronyms":[[86,89]],"long-forms":[[43,84]],"ID":"89"},{"text":"teacher for advice.  ","acronyms":[],"long-forms":[],"ID":"90"},{"text":"models. Among stochastic models, bi-gram and  tri-gram Hidden Markov Model (HMM) are  quite popular.","acronyms":[[76,79]],"long-forms":[[55,74]],"ID":"91"},{"text":"els, which seamlessly incorporates graphbased and more general supervision by extending the posterior regularization (PR) framework.","acronyms":[[118,120]],"long-forms":[[92,116]],"ID":"92"},{"text":"{alexispalmer,ponvert,jbaldrid,carlotasmith}@mail.utexas.edu Abstract Situation entities (SEs) are the events, states, generic statements, and embedded facts and","acronyms":[[90,93]],"long-forms":[[70,88]],"ID":"93"},{"text":" Conf. Computational Linguistics (COLING), pages 89?97.","acronyms":[[33,39]],"long-forms":[[6,31]],"ID":"94"},{"text":"TO-DEATH).  VAg and related NPs\/PPs (VAgRel) This is similar to VPa above, but for VAg.","acronyms":[[12,15],[64,67]],"long-forms":[],"ID":"95"},{"text":"Reduced Sentences 0.121 0.055 4.89 0.027* 1.129 1.01 to 1.26 Constant 5.23 1.18 19.67 <0.000* 187.25 ADAG, n=242; HAG, n = 242; S.E = standard error; OR = Odds ratio or Exp(?); CI = confidence Interval.","acronyms":[[101,105],[114,117],[128,131],[150,152],[177,179]],"long-forms":[[10,11],[134,148],[155,165],[182,201]],"ID":"96"},{"text":"to learn coherent topics. To solve this problem, we build a Markov Random Field (MRF) regularized Latent Dirichlet Allocation (LDA)","acronyms":[[81,84],[127,130]],"long-forms":[[58,79],[98,125]],"ID":"97"},{"text":"mensions. The counts were then transformed into Local Mutual Information (LMI) scores, an association measure that closely approximates the com-","acronyms":[[74,77]],"long-forms":[[48,72]],"ID":"98"},{"text":"bringert@chalmers.se Abstract Grammatical Framework (GF) is a grammar formalism which supports interlingua-","acronyms":[[53,55]],"long-forms":[[30,51],[11,12]],"ID":"99"},{"text":" Conf. on Language Resources and Evaluation (LREC), pages 697?702, Genoa, Italy, May.","acronyms":[[48,49]],"long-forms":[[18,27]],"ID":"100"},{"text":"stantin, Evan Herbst, Moses: Open Source Toolkit for Statistical Machine Translation, Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session, Prague, Czech Republic, June","acronyms":[[151,154]],"long-forms":[[49,52],[108,149]],"ID":"101"},{"text":" In an effort to apply such models to noisy optical character recognition (OCR) text output, we endeavor to understand the effect","acronyms":[[74,77]],"long-forms":[[3,5],[43,72]],"ID":"102"},{"text":"the fragment space that can describe all the trees in Ms. Fragment Mining and Indexing (FMI) In Equation 1 it is possible to isolate the gradient ~w =?","acronyms":[[88,91]],"long-forms":[[51,53],[58,86],[78,80]],"ID":"103"},{"text":"Section 3 for exact criteria), reporting  approximately 40% precision and 45% recall for  transitional probability (TP) and 50% precision and  53% recall for mutual information (MI) on the first ","acronyms":[[116,118],[178,180]],"long-forms":[[10,13],[-1,27],[154,176],[5,7]],"ID":"104"},{"text":"are at least four candidates: less studied (LS) languages, resource scarce (RS) languages, less computerized (LC) languages, and less privileged (LP) languages.","acronyms":[[44,46],[76,78],[110,112],[146,148]],"long-forms":[[0,3],[30,42],[59,74],[91,108],[129,144]],"ID":"105"},{"text":"thesauri: Macquarie, Moby, Oxford and Roget?s.  The inverse rank (InvR) metric allows a comparison to be made between the extracted rank list","acronyms":[[66,70]],"long-forms":[[52,64],[4,5],[60,64]],"ID":"106"},{"text":"  1 Introduction  Word Sense Disambiguation (WSD) is wellknown as one of the more difficult problems in ","acronyms":[[46,47]],"long-forms":[[21,41],[28,30]],"ID":"107"},{"text":"FF-AUTO-NONE Fullform Auto None FF-DEFAULT-GRAM Fullform Default Auto (GRAM) FF-AUTO-GRAM Fullform Auto Auto (GRAM) FF-DEFAULT-SAO* Fullform Default Auto (SAO)","acronyms":[[0,12],[127,130]],"long-forms":[[13,21],[22,26],[48,69],[43,47],[90,108],[48,69]],"ID":"108"},{"text":"The parameters are trained using the 764 Margin Infused Relaxed Algorithm (MIRA) (Crammer et al, 2006).","acronyms":[[79,80]],"long-forms":[[48,73]],"ID":"109"},{"text":" ? Activity Tree (AT): a tree-structure representing the current, past, and planned activities that","acronyms":[[19,20]],"long-forms":[[11,15]],"ID":"110"},{"text":"ging from Merialdo (1994). The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algo-","acronyms":[[90,93],[131,133]],"long-forms":[[14,15],[69,88],[105,129]],"ID":"111"},{"text":"email: {firstname.lastname} @itri.bton.ac.uk  Introduction ~,  WYSIWYM (What You See Is What You Meant) is a user interface technique which allows anauthor to create  and edit in a natural and simple way the knowledge contained in a generated document.","acronyms":[[63,70]],"long-forms":[[72,102],[2,3],[2,3],[2,3]],"ID":"112"},{"text":"and beyond, in several AI applications. Neel and Garzon (2010) show that the quality of a knowledge resource like WN affects the performance in recognizing textual entailment (RTE) and word-sense disambiguation (WSD) tasks.","acronyms":[[23,25],[114,116],[176,179],[212,215]],"long-forms":[[0,1],[156,174],[185,210]],"ID":"113"},{"text":"This measure combines two metrics. The first metric, predicted frequency (PF), estimates the degree to which a word appears to be used consis-","acronyms":[[74,76]],"long-forms":[[53,72],[7,8]],"ID":"114"},{"text":"Table 5: Participating teams and references to system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, CS=Computer Scientist, LI=Linguist, ML=Machine Learning researcher.","acronyms":[],"long-forms":[[116,135],[160,169],[195,203]],"ID":"115"},{"text":"predictive ones among all our features. We used the Correlation based Feature Subset (CFS) selection method in WEKA for this purpose.","acronyms":[[86,89],[111,115]],"long-forms":[[52,84]],"ID":"116"},{"text":"In Ellen M. Voorhees and Donna K. Harman, editors, The Seventh Text Retrieval Conference (TREC-7), volume 7.","acronyms":[[90,96]],"long-forms":[[55,88]],"ID":"117"},{"text":"currently concerns include Chinese personal  names( CN), transliterated foreign personal names( TFN)  and Chinese place names(CPN). They can not be ","acronyms":[[52,54],[96,99],[126,129]],"long-forms":[[-1,21],[72,94],[106,125]],"ID":"118"},{"text":"The results are reported for Same Sentence (SS) and Previous Sentence (PS) models, and the joined results for each of the arguments (ALL) as average","acronyms":[[46,47]],"long-forms":[[34,42],[34,42]],"ID":"119"},{"text":"some structural constraints which are mostly prag-  matic in natnre:  2based on PVM (parallel virtual madfine)  semRnb~ ","acronyms":[[80,83]],"long-forms":[[23,25],[85,109]],"ID":"120"},{"text":" and Iryna Gurevych Ubiquitous Knowledge Processing (UKP) Lab Computer Science Department","acronyms":[[52,55]],"long-forms":[[19,50]],"ID":"121"},{"text":"This paper describes the Universal Decompositional Semantics (Decomp) project, which aims to augments Universal Dependencies (UD) data sets with robust, scalable semantic annotations based in lin-","acronyms":[[35,41],[126,128]],"long-forms":[[25,60],[102,124]],"ID":"122"},{"text":"of National Intelligence (ODNI) and the Intelligence Advanced Research Projects Activity (IARPA) via the Air Force Research Laboratory (AFRL) contract number FA8750-16-C-0114.","acronyms":[[26,30],[90,95],[136,140]],"long-forms":[[3,24],[40,88],[105,134]],"ID":"123"},{"text":"nominal elements. For German, we see confusions with the object functions (accusative OA and dative objects DA), predicates (PD), and the EP function marking expletive pronouns in subject position.","acronyms":[[86,88],[108,110],[125,127],[138,140]],"long-forms":[[57,63],[100,107],[113,123]],"ID":"124"},{"text":"trated in Figure 1. At the outset, the table (T1), the  pump (PU), the apprentice (you) and the compressor  (COMP) are in \"primary focus\".","acronyms":[[62,64],[109,113]],"long-forms":[[56,60],[96,106]],"ID":"125"},{"text":" Thirdly, most PROLOG implementations include a  version of metamorphosis grammars (MGs), a logic-  based formalism useful in particular for describing NL ","acronyms":[[86,87]],"long-forms":[[30,31],[73,81],[30,31]],"ID":"126"},{"text":"  Machine Translation (prototype phase)  The machine translation (MT) sub-component  implements the hybrid MT paradigm, combining ","acronyms":[[64,66]],"long-forms":[[43,62]],"ID":"127"},{"text":"for 4 of the 9 classes, and was usually competitive on the remaining 5 classes. WordNet (W.Net) consistently produced high precision, but with compar-","acronyms":[[78,79]],"long-forms":[[80,87]],"ID":"128"},{"text":"as dependants. Dependency structures are suit-  ably depicted as a directed acyclic graph(DAG),  where arrows direct from dependants to gover- ","acronyms":[[90,93]],"long-forms":[[65,89],[67,73]],"ID":"129"},{"text":"These derivations were induced using a collapsed Gibbs sampler, which sampled from the posterior of a Dirichlet process (DP) defined over the subtree rewrites of each nonterminal.","acronyms":[[121,123]],"long-forms":[[102,119]],"ID":"130"},{"text":" For both these models, we use cost sensitive LibSVM with radial basis kernel function (RBF) as the learning algorithm (Hsu et al.,","acronyms":[[45,51],[87,90]],"long-forms":[[57,85],[65,67]],"ID":"131"},{"text":"is also significant that this model?s paraphraser can be employed not only for MT but also for most natural language processing (NLP) applications.","acronyms":[[79,81],[129,132]],"long-forms":[[1,2],[100,127]],"ID":"132"},{"text":"6.2 Methodology We conducted experiments on MUC-6, ACE-2004, and ACE Phrase-2 (ACE-2). We evaluated our sys-","acronyms":[[44,49]],"long-forms":[[65,77]],"ID":"133"},{"text":"ALCOGRAM. ? P2E5N5S1, C T W D A I Common Logic Controlled English (CLCE) (Sowa 2004) is a language that can be translated into first-order logic with equality in the form of the Conceptual Graph","acronyms":[[12,20]],"long-forms":[[2,3],[34,65]],"ID":"134"},{"text":"text categorization tasks. The newer method of Latent Semantic Indexing (LSI) 3 (Deerwester et al.,","acronyms":[[73,76]],"long-forms":[[47,71]],"ID":"135"},{"text":"non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999). ","acronyms":[[120,123]],"long-forms":[[84,118]],"ID":"136"},{"text":" These models are trained only using negative entities which we refer to as Negative Entity (NE) objective. ","acronyms":[[94,95]],"long-forms":[[84,90]],"ID":"137"},{"text":"main line of the narrative. This move is signaled  by the temporal focus (TF), and the entire deictic  center, returning to an established node in the ","acronyms":[[74,76]],"long-forms":[[58,72]],"ID":"138"},{"text":" 4.1 Task  The Spontaneous Scheduling Task (SST) databases  are a collection of dialogues in which two speak- ","acronyms":[[46,47]],"long-forms":[[26,41],[5,6],[33,35]],"ID":"139"},{"text":"The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). Language models","acronyms":[[58,61],[100,103]],"long-forms":[[35,36],[55,57],[73,98]],"ID":"140"},{"text":"retrieval effectiveness. The following figure  shows the change of average precision (AvgP)  using CDQE (Model 2) along with the change of ","acronyms":[[86,90],[99,103]],"long-forms":[[67,84]],"ID":"141"},{"text":"level-2 domains) yet still did not sufficiently cover relevant subject fields identified by our users, such as IT, medicine and mechanical engineering. The Internal Classification for Standards (ICS) scheme was considered next, as it covers technical subject fields, but it was lacking with respect to legal and","acronyms":[[111,113],[195,198]],"long-forms":[[124,127],[156,193]],"ID":"142"},{"text":"logical subject\/object and its verb governor, General  Event (GE) on who did what when and where and  Predefined Event (PE) such as Management  Succession and Company Acquisition.","acronyms":[[62,64],[120,122]],"long-forms":[[-1,12],[102,118]],"ID":"143"},{"text":" We cover two main thrusts: (i) a black-box evaluation of several NE taggers (commercial and research systems); and (ii) an error analysis of system performance. 2.1 Evaluation data Our evaluation data set contains three distinct sec-tions.  The largest component consists of publicly-available financial reports filed with the Securities and Exchange Commission (SEC), in particular the 2003 forms 10-K filed by eight Fortune 500 com-panies.  These corporate annual reports share the same subject matter as much business news: sales, profits, acquisitions, business strategies and the like.","acronyms":[[65,67],[363,366]],"long-forms":[[15,16],[14,15],[88,91],[88,91],[88,90],[327,361],[88,91]],"ID":"144"},{"text":"@math.canterbury.ac.nz Abstract We introduce Peripheral Diversity (PD) as a knowledge-based approach to achieve multi-","acronyms":[[67,69]],"long-forms":[[45,65],[2,3]],"ID":"145"},{"text":"SS 0.47 5.1 100 times faster than that of Tree Kernel, and the retrieval speed of Subpath Set (SS) is about 1,000 times faster than that of Tree Kernel.","acronyms":[[0,2]],"long-forms":[[82,93]],"ID":"146"},{"text":"grammar (LTAG) (Bangalore and Joshi, 1999) and then extended to other lexicalized grammars, such as combinatory categorial grammar (CCG) (Clark, 2002) and Head-driven phrase structure grammar","acronyms":[[9,13],[132,135]],"long-forms":[[0,7],[61,63],[100,130]],"ID":"147"},{"text":"embeddings from the Neural Language Model of Collobert and Weston [2008] and word representations from random indexing (RI)1. These, however, were","acronyms":[[120,122]],"long-forms":[[110,118]],"ID":"148"},{"text":"Finally, some Wikipages are redirections to other pages, e.g. SODA (SODIUM CARBONATE) redirects to SODIUM CARBONATE.","acronyms":[[62,66]],"long-forms":[[68,84],[68,84]],"ID":"149"},{"text":" : : , .  :~..~. NAT =nat iona =ty .:~:~',,~,.~ . .:.-,~.~;~ SRC~.;ob I .~concrete-:,~ ","acronyms":[[16,19]],"long-forms":[[25,29]],"ID":"150"},{"text":"+ BD 67.4 67.0 67.2 + NEG + BD 67.4 67.1 67.3 Table 1: Results on development corpus: LP = labeled precision, LR = labeled recall, F1 = balanced F-measure","acronyms":[[2,4],[22,25],[86,88],[110,112]],"long-forms":[[63,65],[91,108],[115,129],[136,144]],"ID":"151"},{"text":"Machine Translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 192?202, Sapporo, Japan.","acronyms":[[94,97]],"long-forms":[[51,92]],"ID":"152"},{"text":"against three baselines. The first baseline was based on the minimum overlap (MinOv) of characters in consecutive scenes and corresponds closely to the","acronyms":[[78,83]],"long-forms":[[61,76],[3,5]],"ID":"153"},{"text":"by a rhetorical relation R, Triple=verb pair associated with a relation R in V 2 R, BG = Background, cont.=continuation, elab.=elaboration.","acronyms":[[84,86]],"long-forms":[[3,4],[3,4],[89,99]],"ID":"154"},{"text":"contains two data sets, training and devtest, which were used for training and testing, respectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews).","acronyms":[[218,223]],"long-forms":[[202,216]],"ID":"155"},{"text":"Allocation (LDA), but using linguistic dependency information in place of simple features from bag of words (BOW) representations.","acronyms":[[12,15],[109,112]],"long-forms":[[71,73],[95,107]],"ID":"156"},{"text":"Pseudo-disambiguation results for inverse selectional preferences (BNC as primary and secondary corpus, DISCR weighting). ER = Error rate; Cov = Coverage. ","acronyms":[[67,70],[104,109],[122,124],[139,142]],"long-forms":[[127,137],[145,153]],"ID":"157"},{"text":" Results and Analysis  A finite state machine (FSM) description of user be-  havior was used to analyze session data.","acronyms":[[46,49],[49,50]],"long-forms":[[24,44]],"ID":"158"},{"text":"is used to generate ground truth answers).  The Children?s Book Test (CBT) dataset, created by Hill et al (2016), contains 113,719 cloze-style","acronyms":[[70,73]],"long-forms":[[-1,21]],"ID":"159"},{"text":"ipants represent cognitive scenarios as schematic representations of events, objects, situations, or states of affairs. The participants are called frame elements (FEs) and are described in terms of semantic roles such as AGENT, LOCATION, or MANNER.","acronyms":[[164,167],[222,227],[229,237],[242,248]],"long-forms":[[148,162]],"ID":"160"},{"text":"do you have? for taskswitching (SWT) and poker-playing (PKR) respectively.","acronyms":[[32,35],[56,59]],"long-forms":[],"ID":"161"},{"text":"Conjoined noun phrases are required to all  be members of the same semantic  class, which may be one of the set PERSON, PHYSOB (physical object), LOCNAME  (location name), ATTRNAME (attribute name), or MEASU (measurement unit). ","acronyms":[],"long-forms":[[137,143],[165,169],[-1,5],[221,225]],"ID":"162"},{"text":"mation is likely to be found). This is similar to  tasks such as named entity recognition (NER) or  part-of-speech tagging, where sequence modeling ","acronyms":[[91,94]],"long-forms":[[65,89]],"ID":"163"},{"text":"this assumption.  In a synchronous TAG (STAG) the elementary structures are ordered pairs of TAG trees, with a","acronyms":[],"long-forms":[[21,38],[35,38]],"ID":"164"},{"text":"3.2 Graph-based Approaches Laparra et al(2010) utilize the SSI-Dijkstra+ algorithm to align FN lexical units (LUs) with WN synsets.","acronyms":[[92,94],[110,113],[120,122]],"long-forms":[[38,40],[95,108]],"ID":"165"},{"text":" 2.1 Multilingual Central Repository The Multilingual Central Repository (MCR)2 follows the model proposed by the EuroWordNet","acronyms":[[76,77]],"long-forms":[[17,35],[17,35]],"ID":"166"},{"text":"POL (politics) Belgium elections 2003 16 15107 15.4 SPO (sports) Kim Clijsters 9 9713 11.1 HIS (history) History of Belgium 3 8396 17.9 BUS (business) Belgium Labour Federation 9 4440 11.0","acronyms":[[0,3],[52,55],[91,94],[136,139]],"long-forms":[[5,13],[57,63],[96,103],[141,149]],"ID":"167"},{"text":"study (usually as an elective) or not at all. At the  City University of New York (CUNY)?s Graduate  Center (the primary Ph.","acronyms":[[83,87]],"long-forms":[[54,81]],"ID":"168"},{"text":"1992. Proceedings of the Fourth  Message Understanding Conference (MUC-$). Mor- ","acronyms":[[67,72]],"long-forms":[[-1,38]],"ID":"169"},{"text":"lemma(L)? and ? nonlem(NL)? systems for ran-","acronyms":[[6,7]],"long-forms":[[0,5]],"ID":"170"},{"text":"to analyse the e\u000bects of applying pronominal anaphora resolution to Question Answering (QA) systems. ","acronyms":[[88,90]],"long-forms":[[68,86]],"ID":"171"},{"text":" To evaluate feature effectiveness, we group the features into seven groups: textual features (TX), utterance features (UT), pointing gesture fea-","acronyms":[[94,96],[119,121]],"long-forms":[[76,83],[99,108]],"ID":"172"},{"text":"number of bilingual term pairs)  We compare our model with IBM Model 2  (IBM-2), and IBM Model 4 (IBM-4) implemented by GIZA++ (Och et al, 2003).","acronyms":[[120,126]],"long-forms":[[59,70],[85,96]],"ID":"173"},{"text":"predicted this outcome correctly in 70.37% of the cases (upper left cell). However,  IBL also predicted the outcome penultimate stress (PEN) in 25.26% of the words and  440 ","acronyms":[[85,88],[136,139]],"long-forms":[[116,134]],"ID":"174"},{"text":"It is not useful to exploit latent  semantic analysis directly on the user-topic matrix  UR = UQ * QR , where UR represents how many  times each user is diffused for existing topic R (R ","acronyms":[[89,91]],"long-forms":[[94,96]],"ID":"175"},{"text":" Thus the determination of lexical scopes of  Complex Predicates (CPs) from a long consecutive sequence is indeed a crucial task.","acronyms":[],"long-forms":[],"ID":"176"},{"text":"baseline adapted language model.  The Table 2 shows the word error rates (WERs) of experiments on the code switching lecture","acronyms":[[74,78]],"long-forms":[[56,72]],"ID":"177"},{"text":"This section describes the two evaluation methods we employed ? average precision (AP) and correlation coefficient (CC).","acronyms":[[83,85],[116,118]],"long-forms":[[64,81],[91,114]],"ID":"178"},{"text":"A block ?[] invokes both the inner and outer generations simultaneously in Bracket Model A (BM-A). ","acronyms":[],"long-forms":[[0,1],[83,90]],"ID":"179"},{"text":"In a third stage, they are put in a Multilingual Polyphraz Memory (MPM). A \"polyphrase\" is a structure","acronyms":[[70,71]],"long-forms":[[3,4],[31,35],[49,65],[3,4]],"ID":"180"},{"text":"Semantic Information Retrieval (SIR) AQUA Sentiment Analysis in User Generated Discourse (SentAL) Internet der Dienste (THESEUS) ?","acronyms":[[32,35],[37,41],[90,96],[120,127]],"long-forms":[[0,30],[42,88],[98,118]],"ID":"181"},{"text":"6 Event Ordering TimeML defines three different types of links: subordinate (SLINK), temporal (TLINK), and aspectual (ALINK).","acronyms":[[77,82],[118,123]],"long-forms":[[64,75],[85,93],[107,116]],"ID":"182"},{"text":"Lapata, 2006). All systems were controlled to produce similar compression ratios (CR) for fair comparison.","acronyms":[[82,84]],"long-forms":[[62,80]],"ID":"183"},{"text":"idealistic) practice of balancing and purging quirks.  6.2 Lexicography and Exploratory Data Analysis (EDA)  Statistics can be used for many different purposes.","acronyms":[[103,106]],"long-forms":[[34,37],[59,101]],"ID":"184"},{"text":"4 Experimental Setup 4.1 Corpus and Experimental Expressions We use the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins,","acronyms":[[97,100]],"long-forms":[[25,31],[72,95]],"ID":"185"},{"text":"redefined  While politicians all over the world want to  make Information Society Technologies (IST)  available and accessible in the language and locale ","acronyms":[[99,100]],"long-forms":[[74,94]],"ID":"186"},{"text":"method to train large neural networks. We use mini-batch version RPROP (RMSPROP) (Hinton, 2012) to minimize the loss function.","acronyms":[[65,70]],"long-forms":[],"ID":"187"},{"text":"words not found in the dictionary, a Markov grammar that  computes the optimal ordering of the possible classes of all  words and a Wild Card Parser (WPC), i.e., a deterministic parser  based on a Context Free Grammar.","acronyms":[[150,153]],"long-forms":[[30,31],[130,148],[30,31],[30,31]],"ID":"188"},{"text":" ? Conditional Random Fields (CRF) is the state  of art for named entity extraction, in the ","acronyms":[[32,33]],"long-forms":[[14,27]],"ID":"189"},{"text":"Our Chinese  word segmentation system is based on three models: (a) word boundary token (WBT) model and (b)  triple context matching model for unknown word ","acronyms":[[66,67]],"long-forms":[[25,26],[73,87]],"ID":"190"},{"text":" With the availability of Chinese Gigaword Corpus (CGC) and Word Sketch Engine (WSE) Tools (Kilgarriff, 2004).","acronyms":[[53,54]],"long-forms":[[33,48],[64,77]],"ID":"191"},{"text":"features are chosen due to their effectiveness and availability for on-line detection.  They are independent word probability (IWP), anti-word pair (AWP), word formation analogy Table 8","acronyms":[[127,130],[149,152]],"long-forms":[[109,125],[133,147],[109,113]],"ID":"192"},{"text":"1 Introduction RWTH?s main approach to System Combination (SC) for Machine Translation (MT) is a refined version of the ROVER approach in Automatic","acronyms":[[61,62]],"long-forms":[[20,21],[46,57],[75,86],[23,24],[24,26]],"ID":"193"},{"text":"This is called empirical Bayesian estimation. Our approach differs from maximum a posteriori (MAP) estimation, since we re-estimate the parameters of","acronyms":[[94,97]],"long-forms":[[72,92]],"ID":"194"},{"text":"1 Introduction Statistical machine translation (MT) uses large target language models (LMs) to improve the fluency of generated texts, and it is commonly","acronyms":[[50,51]],"long-forms":[[35,46],[70,85]],"ID":"195"},{"text":"Our approach to this problem is influenced by the named entity annotation in the Automatic Content Extraction (ACE) project (Consortium, 2002), in which ?","acronyms":[[111,114]],"long-forms":[[81,109]],"ID":"196"},{"text":" It is used for many natural language tasks, such as part of speech (POS) and named entity tagging (Toutanova and others, 2003; Carreras et al.,","acronyms":[],"long-forms":[[52,66]],"ID":"197"},{"text":"NC 91.0 99.1 89.5 92.1 99.7 90.7 Table 2: Attachment score for Java and the lexical feature set, where CO = convertible and NC = nonconvertible dependency trees.","acronyms":[[0,2],[103,105]],"long-forms":[[108,119],[129,154]],"ID":"198"},{"text":"precisely the issue we address in this article. We concentrate on the task of automatically classifying NSUs, which we approach using machine learning (ML) techniques. Our aim","acronyms":[[104,108],[152,154]],"long-forms":[[31,33],[134,150]],"ID":"199"},{"text":"ABSTRACT  French auxilliaries and clitics have been analysed  in the flame of U.C.G. (Unification Categorial Grammar). ","acronyms":[[78,84]],"long-forms":[[86,116]],"ID":"200"},{"text":" ? Word translation features (WT): ?","acronyms":[[31,32]],"long-forms":[[7,27]],"ID":"201"},{"text":"(e.g. adjective bivs?i, which means former in both languages), (b) the term partial false friends (PFF) describes pairs that are polysemous and","acronyms":[[99,102]],"long-forms":[[12,13],[76,97]],"ID":"202"},{"text":"2 VIP targeted technologies  Current products for VIP such as screen readers mainly depend on speech synthesis or Braille solutions, e.g. ChromeVox [3], Windows-Eyes [4], or JAWS (Job Access With Speech) [5]. Braille displays ","acronyms":[[2,5],[174,178]],"long-forms":[[180,202]],"ID":"203"},{"text":"Peter Robinson; Philip Tuddenham; 3 Visualisation Scalable Vector Graphics (SVG)1 is a language for describing two-dimensional graphics and graphical","acronyms":[[76,79]],"long-forms":[[50,74],[30,31]],"ID":"204"},{"text":"Subset of significant adjacency pairs CORRECTTASKACTION?CORRECTTASKACTION;??EXTRADOMAINS?EXTRADOMAINT;?GROUNDINGS?GROUNDINGT;?ASSESSINGQUESTIONT?POSITIVEFEEDBACKS;??ASSESSINGQUESTIONS?POSITIVEFEEDBACKT;?QUESTIONT?STATEMENTS;?ASSESSINGQUESTIONT?STATEMENTS;?EXTRADOMAINT?EXTRADOMAINS;?QUESTIONS?STATEMENTT;?NEGATIVEFEEDBACKS?GROUNDINGT;?INCOMPLETETASKACTION?INCOMPLETETASKACTION;?POSITIVEFEEDBACKS?GROUNDINGT;??BUGGYTASKACTION?BUGGYTASKACTION 4 Models We learned three types of models using cross-validation with systematic sampling of training and testing sets.  4.1 First-Order Markov Model The simplest model we discuss is the first-order Markov model (MM), or bigram model (Figure 2). A MM that generates observation (state) sequence o1o2?ot is defined in the following way.","acronyms":[[654,656]],"long-forms":[[578,584],[476,481],[640,652],[476,481]],"ID":"205"},{"text":"We searched for four conditions: depression, bipolar disorder, post traumatic stress disorder (PTSD) and seasonal affective disorder (SAD).","acronyms":[[95,99],[134,137]],"long-forms":[[53,61],[63,93],[105,132]],"ID":"206"},{"text":" ? If the key and response do not match, the category is incorrect (INC) ; if interactively assigned, a tall y appears in both the INC and XIC (interactive incorrect) columns .","acronyms":[[67,70],[138,141]],"long-forms":[[56,65],[13,14],[56,58],[143,164]],"ID":"207"},{"text":"(ARG0) and ? Greenspan? as the object (ARG1) of the noun predicate ?","acronyms":[[1,5],[39,43]],"long-forms":[],"ID":"208"},{"text":"(2014c): ? Italian - Romanian (IT-RO); ?","acronyms":[],"long-forms":[[11,29]],"ID":"209"},{"text":"1 Introduction Despite the advances in natural language processing (NLP), Word Sense Disambiguation (WSD) is still considered one of the most challenging prob-","acronyms":[[71,72]],"long-forms":[[36,38],[47,66],[79,99],[86,88]],"ID":"210"},{"text":" ? Unstressed (US) average: Each feature is normalized by its mean value in the un-","acronyms":[[16,17]],"long-forms":[[40,42]],"ID":"211"},{"text":"and a search procedure. For example, we can build a n-gram word language model (LM)?itself a large weighted FSA.","acronyms":[[80,82],[108,111]],"long-forms":[[0,1],[0,1],[64,78],[0,1]],"ID":"212"},{"text":"The key reason to compute tsim under the equiprobability assumption is that we need not compute the MWBM, but may find just the maximum cardinality bipartite matching (MCBM), since all potential links have the same weight. An O(e","acronyms":[[100,104],[168,172]],"long-forms":[[128,166]],"ID":"213"},{"text":"10http:\/\/rapid-i.com\/ classification problems, we were unable to achieve results with a Support Vector Machine (SVM) learner (libSVMLearner) using the Radial Base","acronyms":[[112,115]],"long-forms":[[62,64],[86,110]],"ID":"214"},{"text":" First, the matching score of the matching two nodes, NMS (Node Match Score) is calculated with their node scores, NS1 and NS2,","acronyms":[[53,56]],"long-forms":[[58,74]],"ID":"215"},{"text":"2007. CRFsuite: A fast implementation of Conditional Random Fields (CRFs), http:\/\/www.chokkan.org\/software\/crfsuite\/.","acronyms":[[6,14]],"long-forms":[[41,66]],"ID":"216"},{"text":"rithms can also be used in discriminative settings (Bellare et al, 2009; Ganchev et al, 2010) specifically for semi-supervised learning (SSL.) ","acronyms":[[137,140]],"long-forms":[[24,26],[111,135]],"ID":"217"},{"text":"It makes sense now that you explained it, but I never used an else if in any of my other programs .04 POSITIVE FEEDBACK (P) Second part complete. .11 QUESTION (Q) Why couldn?t I have said if (i<5) .11 STATEMENT (S) i is my only index .07  REQUEST FOR FEEDBACK (RF) So I need to create a new method that sees how many elements are in my array? .16 RESPONSE (RSP) You mean not the length but the contents .14 UNCERTAIN FEEDBACK WITH ELABORATION (UE) I?m trying to remember how to copy arrays .008 UNCERTAIN FEEDBACK (U) Not quite yet .008  3.2 Task action annotation The tutoring sessions were task-oriented, focusing on a computer programming exercise.","acronyms":[[261,263],[151,153]],"long-forms":[[102,110],[111,119],[102,103],[150,158],[201,210],[104,105],[239,259],[347,355],[407,442],[0,1],[407,425],[151,152],[127,129]],"ID":"218"},{"text":"nodes in their MRs mij . Then, after setting the context ci as the MR of the root node (MR(T ) ? ci),","acronyms":[[15,17],[15,17]],"long-forms":[[77,81]],"ID":"219"},{"text":"until the current sentence (PENT) and the word entropy for the conversation subsequent to the current sentence (SENT). We hypothesize that informative","acronyms":[[28,32],[112,116]],"long-forms":[[18,26],[38,62],[6,9],[18,26]],"ID":"220"},{"text":"  1. PARADISEC (Pacific and Regional Archive for Digital Sources in Endangered Cultures): audio, video,  text and image resources for Australian and Pacific Island languages (Thieberger, Barwick, Billington, & ","acronyms":[],"long-forms":[[-1,64],[22,25],[43,46],[22,25]],"ID":"221"},{"text":"c, include latent variable models that simultaneously capture the semantics of words and sentences, such as latent semantic analysis (LSA) or latent Dirichlet alocation (LDA).","acronyms":[[134,137],[170,173]],"long-forms":[[0,1],[11,17],[108,132],[142,168]],"ID":"222"},{"text":" Abstract Minimum Error Rate Training (MERT) is a method for training the parameters of a log-","acronyms":[[42,43]],"long-forms":[[17,36],[5,6],[5,6]],"ID":"223"},{"text":"A phonetic system represents sound segments as 3Phonemic and phonetic representations are given in the International Phonetic Alphabet (IPA). ","acronyms":[],"long-forms":[[0,1],[117,134]],"ID":"224"},{"text":"word w is defined as the largest connected subgraph that contains w. For each content  9 Other thesauri have been used for WSD, e.g., the German Hallig-Wartburg (see Schmidt \\[1988, 1991\\])  and the Longman Lexicon of Contemporary English (LLOCE) (Chen and Chang, this volume). ","acronyms":[[123,126],[240,245]],"long-forms":[[199,238]],"ID":"225"},{"text":"the Switchboard corpus.1 The standard measure of error used in ASR is word error rate (WER), computed as 100(I + D + S)\/R, where I,D and S are the number of inser-","acronyms":[[63,66],[87,90]],"long-forms":[[49,54],[70,85]],"ID":"226"},{"text":"verbs, adject ives,  and others. Then a  separate  Keyword In Context  (KWIC) Index  was made for each part of speech.","acronyms":[[72,76]],"long-forms":[[51,69]],"ID":"227"},{"text":"company was interested in knowledge discovery  approaches applicable to the data aggregated by its  Emergency Control System (ECS) in the form of  field service tickets.","acronyms":[[126,129]],"long-forms":[[100,124]],"ID":"228"},{"text":"Italian - Romanian (IT-RO); ? Portuguese - Romanian (PT-RO); ?","acronyms":[],"long-forms":[[0,18],[30,51]],"ID":"229"},{"text":"V (verb) 6946 81.9 85.8 PR (preposition) 5302 60.0 79.0 CONJ (conjunction) 2998 76.1 80.7 ADV (adverb) 2855 72.3 83.3","acronyms":[[0,1],[24,26],[56,60]],"long-forms":[[3,7],[28,39],[62,73],[95,101]],"ID":"230"},{"text":"lexicalized Baselines. In Proceedings of the ACL Workshop on Parsing German (PaGe), pages 40?46, Columbus, OH, USA.","acronyms":[[45,48],[77,81],[107,109],[111,114]],"long-forms":[[61,75]],"ID":"231"},{"text":"It consists mainly of four incremental (cas-  caded) processes that work on the blackboard-like  current conceptual structure (CCR). At first sight, ","acronyms":[[127,130]],"long-forms":[[105,125]],"ID":"232"},{"text":"when reconcilable with Bias 1. Whenever the sentence or query has a verb phrase (VP) spanning roughly half of it, annotators seem to chunk be-","acronyms":[[81,83]],"long-forms":[[66,79]],"ID":"233"},{"text":"Abbreviations NE = Named Entity CE = Correlated Entity EP = Entity Profile","acronyms":[],"long-forms":[[25,31],[25,31]],"ID":"234"},{"text":"which can handle the non-projective trees present in the Irish data. In each case we report Labelled Attachment Score (LAS) and Unlabelled Attachment Score (UAS). ","acronyms":[[119,122],[157,160]],"long-forms":[[92,117],[128,155]],"ID":"235"},{"text":"Table 1: A classification of grammar rules for the HPB model. PR = phrasal rule, HR = hierarchical rule, GR = glue rule.","acronyms":[[51,54],[62,64],[81,83],[105,107]],"long-forms":[[67,74],[37,41],[86,103],[110,119]],"ID":"236"},{"text":"senses. Semantic role labeling is achieved using maximum entropy (MaxEnt) model based semantic role classification and integer linear","acronyms":[[66,72]],"long-forms":[[49,64]],"ID":"237"},{"text":"FS = false start  E = echo  ADD = added information  SELF = talking to oneself ","acronyms":[[0,2],[28,31],[53,57]],"long-forms":[[5,16],[18,19],[22,26],[34,39],[40,51]],"ID":"238"},{"text":"tasks or languages.  Amazon?s Mechanical Turk (MTurk) service facilitates inexpensive collection of large amounts of","acronyms":[],"long-forms":[[30,45]],"ID":"239"},{"text":" iii. Simple_Rank (S-Rank): It is computed  based on Rank(i)=tfi*Len(i), which aims ","acronyms":[[18,24]],"long-forms":[[5,16],[12,16]],"ID":"240"},{"text":"The MRF provides the base frame to  combine various statistical information  with maximum entropy (ME) method. ","acronyms":[[101,102]],"long-forms":[[90,97]],"ID":"241"},{"text":"annotation ? the Penn Chinese Treebank (CTB)(Xia et al, 2000), and the People?s Daily News (PDN) corpus from Beijing University.","acronyms":[[40,43],[92,95]],"long-forms":[[11,12],[22,38],[-1,20]],"ID":"242"},{"text":"Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Gary Geunbae Lee Department of Computer Science and Engineering Pohang University of Computer Science and Technology(POSTECH) San 31, Hyoja-Dong, Pohang, 790-784, Korea","acronyms":[[161,168]],"long-forms":[[72,95],[108,160],[108,114]],"ID":"243"},{"text":"6. A rule to convert the Hindi word into its  base form (BF). ","acronyms":[[59,60]],"long-forms":[],"ID":"244"},{"text":"for acquiring high quality non-expert knowledge from on-demand workforce using Amazon Mechanical Turk (MTurk). We show how ","acronyms":[],"long-forms":[[86,101]],"ID":"245"},{"text":"integral to membrane  membrane  The protein encoded by this gene is a receptor for interleukin 20 (IL20), a cytokine that may be involved in epidermal function.","acronyms":[],"long-forms":[[83,97],[0,2]],"ID":"246"},{"text":"We evaluate the lexicons proposed in Section 3 both intrinsically (by comparing their lexicon entries against General Inquirer (GI) lexicon) and extrinsically (by using them in a phrase polarity anno-","acronyms":[[128,130]],"long-forms":[[34,36],[110,126],[5,6]],"ID":"247"},{"text":"out of this phrase. The word with the parent out of  the phrase is called Head of Phrase (HP). The ","acronyms":[[90,92]],"long-forms":[[4,6],[4,6],[74,88]],"ID":"248"},{"text":" 1 Introduction Noun phrase (NP) coreference resolution, the task of determining which NPs in a text or dialogue re-","acronyms":[[30,31]],"long-forms":[[20,26],[23,24]],"ID":"249"},{"text":"Generation of Crisp Descriptions Arguably the most fundamental task in the generation of referring expressions (GRE), content determination (CD) requires finding a set of properties that jointly identify the intended referent.","acronyms":[[112,115],[141,143]],"long-forms":[[11,13],[68,70],[75,110],[118,139],[5,6],[11,13]],"ID":"250"},{"text":"entities presence feature (DIC), numerical entities presence feature (NUM), question specific feature (SPE), and dependency validity feature (DEP). ","acronyms":[[27,30],[70,73],[103,106],[142,145]],"long-forms":[[0,25],[33,68],[76,101],[113,140]],"ID":"251"},{"text":"3.1 Semantic Types In the present task, we use a subset of semantic types from the Brandeis Shallow Ontology (BSO), which is a shallow hierarchy of types developed as a part","acronyms":[[110,113]],"long-forms":[[22,25],[7,8],[79,108],[122,126],[7,8]],"ID":"252"},{"text":"applied to the sentiment analysis problem. Models such as Na??ve Bayes (NB), Maximum Entropy (ME) and Support Vector Machines (SVM) can determine","acronyms":[[72,74],[94,96],[127,130]],"long-forms":[[8,10],[-1,14],[77,92],[102,125]],"ID":"253"},{"text":" Rank Group Lexical Features 1 HM HM1 (head of M1) HM2 (head of M2)","acronyms":[[28,29]],"long-forms":[[38,42],[38,42]],"ID":"254"},{"text":"Cross-lingual textual entailment (CLTE) (Mehdad et al., 2010) is an extension of textual entailment (TE) (Dagan and Glickman, 2004).","acronyms":[[34,38],[36,38]],"long-forms":[[0,32],[14,32]],"ID":"255"},{"text":"1977).  The parameters of the IDCLM model are computed using the variational Bayes EM (VB-EM) procedure by maximizing the marginal distribution of the training data that contains a set of n-gram events","acronyms":[[30,35]],"long-forms":[[65,85],[13,14]],"ID":"256"},{"text":" 2 Related Work Reference resolution (RR), which is the task of resolving referring expressions (REs) to what they are","acronyms":[[39,40]],"long-forms":[[25,35],[73,94]],"ID":"257"},{"text":"or fictional) world. These discourse ntities,  called reference objects (RefOs), are stored  and processed ina net-like structure, called a ","acronyms":[[73,78]],"long-forms":[[54,71]],"ID":"258"},{"text":"then X Y =~ Z  then Y X ::~ Z  Permutation Closure of language L (PermL)  PermL = { s \\[ s' in L and s is a per- ","acronyms":[],"long-forms":[[31,64],[46,47],[63,64],[46,47],[37,38]],"ID":"259"},{"text":" I. Introduction  SABA (\"Semantic Analyser , Backward Ap-  proach\") is an automatic parser of French ","acronyms":[],"long-forms":[[33,43],[65,66],[27,29]],"ID":"260"},{"text":"Proceedings of  the First International Symposium on Compurers and Chinese  Inpuf\/Output Systems, Acadernig Sinico, 983-998  in the FCL (FACOM Composition Language) System, information is punched on paper tape  with a Kanji keyboard.","acronyms":[],"long-forms":[[35,37],[-1,21],[35,37],[32,33]],"ID":"261"},{"text":"In order to answer this question, we propose a new model called the Recursive Neural Tensor Network (RNTN). The main idea is to use","acronyms":[[101,105]],"long-forms":[[12,13],[68,99]],"ID":"262"},{"text":"In Proceedings of the 19th International Conference on Computational Linguistics (COLING), volume I, pages 267?273.","acronyms":[[88,89]],"long-forms":[[36,38],[69,80]],"ID":"263"},{"text":"tating full-text passages that describe the functional relationships between bio-entities summarised in a Molecular Interaction Map (MIM). Our corpus","acronyms":[[133,136]],"long-forms":[[104,131]],"ID":"264"},{"text":"Abstract A number of issues arise when trying to scaleup natural language understanding (NLU) tools designed for relatively simple domains (e.g.,","acronyms":[[89,92]],"long-forms":[[57,87]],"ID":"265"},{"text":"distance (EDIT), which is the Levenshtein distance between generated word string and human reference output, and string accuracy (S-A), which is the proportion of times the word string was identical to the","acronyms":[[10,14],[130,133]],"long-forms":[[74,80],[113,128],[74,80]],"ID":"266"},{"text":"The second column represents three SMT systems, namely: the baseline system adapted to the domain (DA), the same system with a CSLM (DA+CSLM) and the project adapted sys-","acronyms":[[35,38],[99,101],[127,131]],"long-forms":[[91,97]],"ID":"267"},{"text":"(i)(a) If the verb is the last word  of the surface shape of the  sentence (SS), it always be-  longs to the focus.","acronyms":[[2,3]],"long-forms":[[1,2],[4,5],[10,13],[10,13],[37,43],[-1,20],[10,13]],"ID":"268"},{"text":" In Expertise column, C=Computer Scientist, BI=Bioinformatician, B=Biologist, L=Linguist ?","acronyms":[],"long-forms":[[32,41]],"ID":"269"},{"text":"As is common practice for continuous features, we choose this pdf to be a Gaussian mixture model (GMM) since any continuous distribution can be approximated with ar-","acronyms":[[98,101]],"long-forms":[[3,5],[72,96]],"ID":"270"},{"text":"So, for example, the preposition in addition to (krom?) appears altogether in 309 instances in PDT,  within which there are 44 instances in the function of AltLex (automatically looked up). All ","acronyms":[[95,98],[156,162]],"long-forms":[],"ID":"271"},{"text":" Finally, we propose using the beam-search decoder to combine multiple discriminative models such as M3N and multiple generative models such as language models (LM) and perform multiple passes of disfluency detection.","acronyms":[[100,103],[160,162]],"long-forms":[[85,91],[85,91],[143,158]],"ID":"272"},{"text":"The first system, as its name suggests, is very  simple: using the WSD model, it chooses the  most frequent sense (MFS) of the lemma l with  POS p according to WN (that is, the lowest num-","acronyms":[[67,70],[115,118],[141,144],[160,162]],"long-forms":[[40,42],[94,113],[40,42]],"ID":"273"},{"text":" 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in","acronyms":[[53,54]],"long-forms":[[22,47]],"ID":"274"},{"text":" Our approach differs in important ways from the  use of hidden Markov models (HMMs) for class-  based language modeling (Jelinek et al, 1992).","acronyms":[[82,83]],"long-forms":[[63,76]],"ID":"275"},{"text":"the semantics of the head noun of the reference  object. A noun phrase (NP) denoting a place  gives rise to a spatial PP.","acronyms":[[72,74],[118,120]],"long-forms":[[26,30],[59,70],[7,8],[7,8]],"ID":"276"},{"text":"contact(CeNT) motion(MOT)  emoeion(ENO) perception(PER)  possession(POSS) stat ive(STA)  ~eather(WEA) ingestion(ING) ","acronyms":[[8,12],[21,24],[35,38],[51,54],[68,72],[83,86],[97,100],[112,115]],"long-forms":[[0,7],[14,20],[27,34],[40,50],[57,67],[74,82],[102,111]],"ID":"277"},{"text":"tropy modeling. Berger et al (1996) presents an incremental feature selection (IFS) algorithm, which computes the approximate gains","acronyms":[[79,82]],"long-forms":[[26,28],[48,77]],"ID":"278"},{"text":"pre-processed ATB (Table 10). Consequently, this particular Arabic MWE identification experiment is similar to joint parsing and named entity recognition (NER) (Finkel and Manning 2009).","acronyms":[[14,17],[67,70],[155,158]],"long-forms":[[129,153]],"ID":"279"},{"text":"ficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu?noz et","acronyms":[[80,82]],"long-forms":[[67,78]],"ID":"280"},{"text":"Frame abbreviations:  INAN=inanimate NP, ANIM=animate NP, VBZ--inflected  main verb, IS=is, VBG=gerund, PP=prepositional phrase,  TO=to (prep.),","acronyms":[[37,39],[58,61]],"long-forms":[[-1,13],[79,83],[121,127],[-1,5]],"ID":"281"},{"text":"2 where AF = adjusted frequency di = relative size of category i","acronyms":[],"long-forms":[[22,31]],"ID":"282"},{"text":"respective polarities. This new value will be called  Positive Association (PosA). The PosA value is ","acronyms":[[76,80]],"long-forms":[[54,74]],"ID":"283"},{"text":"production strategies. In Proceedings of the 16th International Conference on Computational Linguistics (COLING?96), pages 249?254.","acronyms":[[105,114]],"long-forms":[[64,103]],"ID":"284"},{"text":"paring to BL system (N.S.), the average number of alternative translations of each source phrase (T\/S) and the average source phrase length in the output (A.L.) -1.80 on average TER.","acronyms":[[10,12],[21,25],[98,101]],"long-forms":[[83,96],[119,139]],"ID":"285"},{"text":"and Johnson, 2011), particularly from the DARPA EARS (Effective, Affordable, Reusable Speech-toText) MDE (MetaData Extraction) (DARPA Information Processing Technology Office, 2003) pro-","acronyms":[[42,47],[48,52],[101,104]],"long-forms":[[106,125]],"ID":"286"},{"text":"lion words. From TIPSTER,  we used the Associ-  ated Press (AP), Wall Street Journal (WSJ), and  San Jose Mercury News (SJM) data, yielding 123, ","acronyms":[[60,62],[86,89],[120,123]],"long-forms":[[-1,17],[65,84],[97,118]],"ID":"287"},{"text":"Chinese sentence into a sequence of words. This is  the task of Chinese word segmentation (CWS), an  important and challenging task in Chinese NLP.","acronyms":[[91,94],[143,146]],"long-forms":[[0,7],[22,23],[64,89],[97,99],[132,142]],"ID":"288"},{"text":"Video in sentences out.  In Association for Uncertainty in Artificial Intelligence (UAI). ","acronyms":[[84,87]],"long-forms":[[6,8],[25,27],[40,82]],"ID":"289"},{"text":"BOEING CO(BA) OTC  UTL USA  UTL CORP(UTLC)  BOEING'S ARGOSYSTEMS SUBSIDIARY TO MAKE TENDER OFFER FOR ALL UTL CORE SHARES ","acronyms":[[7,9],[10,12],[14,17],[37,41]],"long-forms":[[0,6],[-1,15],[0,6],[19,22]],"ID":"290"},{"text":" For comparison we re-implemented the probabilistic Visual Objects Algorithm (VOA) of Mitchell et al(2013).","acronyms":[[80,81]],"long-forms":[[58,75]],"ID":"291"},{"text":" 2 Platform Architecture  The Application Generation Platform (AGP), created during the European project GEMINI, is an ","acronyms":[[65,66],[66,67]],"long-forms":[[2,10],[41,60]],"ID":"292"},{"text":"The query-based selection model utilizes Support Vector Regression (SVR) models to predict the mean average precision (MAP) of each query from the ambiguity measures, and to choose an ap-","acronyms":[[71,72]],"long-forms":[[49,66],[52,54],[100,117],[52,54],[97,99]],"ID":"293"},{"text":"will describe our method of automatically creating a training set based on the click-through links and how we build an SVM (Support Vector Machine) classifier with the integration of enriched informa-","acronyms":[[119,122]],"long-forms":[[28,29],[124,146]],"ID":"294"},{"text":"Mophological processing, syntactic parsing and  other useflfl tools have been proposed in the field  of natural language processing(NLP). Many ","acronyms":[[132,135]],"long-forms":[[13,23],[20,22],[104,131]],"ID":"295"},{"text":"Computational Linguistics Volume 23, Number 2  1993c). FUF (Functional Unification Formalism) is a programming language based on  functional unification (Kay 1979).","acronyms":[[55,58]],"long-forms":[[60,92],[94,98],[9,11]],"ID":"296"},{"text":"translation with overall understanding?.  Rhetorical structure theory (RST) (Mann and  Thompson, 1988) provides us with a good per-","acronyms":[[71,74]],"long-forms":[[42,69],[2,3]],"ID":"297"},{"text":"Sentences from TST2-MUC4-0048 Sl : SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIAII CONDEMNED THE TERRORIST KILLING OF ATTORNE Y GENERAL ROBERTO GARCIA ALVARADO AND ACCUSED THE FARABUNDO MARTI NATIONAL LIBERATION FRONT (FMLN ) OF THE CRIME .","acronyms":[[30,32],[216,220]],"long-forms":[[173,214]],"ID":"298"},{"text":"Finally, the intention  translates into a call to UC's expression mechanism,  UCExpress (UCexpressl in the trace), which eventu-  ally calls UCGen to produce the answer.","acronyms":[],"long-forms":[[9,12],[50,52],[89,106],[9,12]],"ID":"299"},{"text":"For 1http:\/\/maltparser.org\/ English?Chinese (EN?ZH) word alignment, we observe that 75.62% of the consecutive Chinese","acronyms":[],"long-forms":[[28,35],[-1,8],[35,36]],"ID":"300"},{"text":"28  Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 39?47, October 25, 2014, Doha, Qatar.","acronyms":[[23,28],[82,86]],"long-forms":[[46,80]],"ID":"301"},{"text":"and  Communications Industry Association (CCIA) filed differing recommendations  with the OMB on Federal automatic data processing (ADP) procurement. CBEW ","acronyms":[[42,46],[90,93],[132,135]],"long-forms":[[5,40],[94,130]],"ID":"302"},{"text":"for many NLP applications including machine translation. In fact, Google Translate (GT)3 translates Examples (1) and (3) as ?","acronyms":[[9,12],[84,86]],"long-forms":[[73,82]],"ID":"303"},{"text":"Economics neighborhood f bank  bank  Subject Code EC = Economics  have it person out ","acronyms":[[50,52]],"long-forms":[[0,9],[-1,13]],"ID":"304"},{"text":" The meaning of complex phrases is represented as  a composed LCS (CLCS). This is constructed \"com- ","acronyms":[],"long-forms":[[61,64]],"ID":"305"},{"text":"which has been developed within the projects K1T-NASEV and its  successor KIT-FAST 2 will be described (for details see \\[Hanen-  1 LP = Linear Precedence; FCR = Feature Co-oecmenee R striction  KIT-FAST (FAST = Functor-Argument Structure for Translation; KIT = ","acronyms":[[45,54],[74,82],[132,134],[156,159],[74,77]],"long-forms":[[104,107],[137,154],[162,183],[78,82],[212,254]],"ID":"306"},{"text":" 2 Universal Networking Language Universal Networking Language (UNL) is an interlingua that represents a sentence in a language inde-","acronyms":[[66,67]],"long-forms":[[12,31],[12,31],[24,26],[9,10],[113,117]],"ID":"307"},{"text":" Ckakraborty, Tanmoy, 2010, Identification of NounNoun (N-N) Collocations as Multi-Word Expressions  in Bengali Corpus.","acronyms":[[55,58]],"long-forms":[[45,53]],"ID":"308"},{"text":"4.2 Dataset and preprocessing To evaluate the proposed approach, we use SemEval-2013 datasets: TW (tweets obtained by merging learn and development data) and SMS, in","acronyms":[[72,84],[95,97],[158,161]],"long-forms":[[99,105]],"ID":"309"},{"text":" The experiments were performed using the  Wall Street Journal (WSJ) corpus of the Uni-  versity of Pennsylvania (Marcus et al, 1993) ","acronyms":[[66,67]],"long-forms":[[47,61],[51,53]],"ID":"310"},{"text":"INIT MED FIN TOTAL 201 (87.8%) 13 (5.7%) 15 (6.5%) 229 Table 1: Distribution of the Position (POS) of Discourse Adverbials","acronyms":[[94,97]],"long-forms":[[64,92],[77,79]],"ID":"311"},{"text":"rada@cs.unt.edu Abstract Amazon Mechanical Turk (MTurk) is a marketplace for so-called ?","acronyms":[],"long-forms":[[32,47],[1,2]],"ID":"312"},{"text":"tions and so on.  3.2 The Greedy Prepend Algorithm (GPA) To learn a decision list from a given set of training","acronyms":[[52,55]],"long-forms":[[26,50],[6,7],[6,7]],"ID":"313"},{"text":"legislatures, councils, \"other government bodies , I 1  and the private sector  should withhold action implementing major proposals for EFTS until the  National Commission on Electronic Fund transfers (NCEFT) has completed its  studies.","acronyms":[[136,140],[202,207]],"long-forms":[[152,200]],"ID":"314"},{"text":" 2. Na?ve Bayesian approach with full vocabulary (NBF). It","acronyms":[[52,53]],"long-forms":[[-1,12]],"ID":"315"},{"text":"correction. ? P3E3N4S2, F W I Controlled Automotive Service Language (CASL) (Means and Godden 1996; Means, Chapman, and Liu 2000) is a controlled language for writing service manuals and bul-","acronyms":[[14,22],[70,74]],"long-forms":[[30,68],[61,62]],"ID":"316"},{"text":"extend Eigenwords, spectral monolingual word embeddings based on canonical correlation analysis (CCA), to crosslingual settings with sentence-alignment.","acronyms":[[97,100]],"long-forms":[[62,95]],"ID":"317"},{"text":"The initial translation outputs from Google Translate (GT) and the results of the targeted paraphrasing translation process (TP) were evaluated according to widely used critera of fluency and adequacy.","acronyms":[[55,57],[125,127]],"long-forms":[[12,23],[44,53],[104,123]],"ID":"318"},{"text":"simultaneous (SML) with another proposition: \"Fred washed the car  while John chased Mary\", Figure 50 A sequenttal rn iering of proposi-  t ions is also found, characterized by a sequence (SEQ) relation. The ","acronyms":[[14,17],[189,192]],"long-forms":[[0,12]],"ID":"319"},{"text":"wards Task 2.  4.1 Wikipedia system (WIKI) In the WIKI data a sentence is marked as uncertain","acronyms":[[37,41]],"long-forms":[[19,35],[1,2]],"ID":"320"},{"text":" 4. Coreference (COR) As mentioned in our discussion of transitional phrases, a strong argument","acronyms":[[19,20]],"long-forms":[],"ID":"321"},{"text":"Bayesian Networks (Samuelsson, 1993), Neural  Networks (Marques and Lopes, 1996) and  Conditional Random Fields (CRF) (Lafferty et  al.,","acronyms":[[113,116]],"long-forms":[[64,67],[-1,28]],"ID":"322"},{"text":"Evaluation Metrics: We evaluate the performance of question retrieval using the following metrics: Mean Average Precision (MAP) and Precision@N (P@N).","acronyms":[[123,126]],"long-forms":[[99,121],[-1,12],[-1,4]],"ID":"323"},{"text":"quen cies Figure 6: Distribution of Ratio of Frequencies(RF) values over the nouns in the corpus","acronyms":[[57,59]],"long-forms":[[33,56]],"ID":"324"},{"text":"system utterances with respect o dialog context.  Utterances can be either appropriate (AP), inappro-  priate (IP), or ambiguous (AM).","acronyms":[[88,90],[111,113],[130,132]],"long-forms":[[31,32],[75,86],[-1,14],[119,128]],"ID":"325"},{"text":" 2.2 Tree substitution grammars Tree substitution grammars (TSGs) allow for complementary analysis.","acronyms":[[63,64]],"long-forms":[[9,30],[9,30]],"ID":"326"},{"text":" 2 Background and Related Work Amazon?s Mechanical Turk (MTurk) is an online marketplace for work that gives employers","acronyms":[],"long-forms":[[50,54],[13,15]],"ID":"327"},{"text":"the points plus 10% of the surrounding area. For this, The Generic Map Tools (GMT)10 were used, in this case via HTTP.11","acronyms":[[43,44],[78,81]],"long-forms":[[59,76]],"ID":"328"},{"text":"entry description to a lexeme. A part-of-speech of the lexeme is set to a common noun (NN ) where the minimum word probability of NN is assigned","acronyms":[[87,89]],"long-forms":[[81,85]],"ID":"329"},{"text":"ary, we assign a default value 3.0.  3.2 Named Entities (NE)  Named Entities are important semantic information ","acronyms":[[57,59]],"long-forms":[[0,1],[41,55],[41,55]],"ID":"330"},{"text":"2.1 Processing definitions  Our algorithms are used in an overall system  called \"onomasiological search system\" (OSS),  whose aim is to allow the user to find terms by ","acronyms":[[114,117]],"long-forms":[[66,72],[82,111],[131,133]],"ID":"331"},{"text":"In MT Summit XIII: the Thirteenth Machine Translation Summit [organized by the] AsiaPacific Association for Machine Translation (AAMT), pages 513-520.","acronyms":[[3,5]],"long-forms":[[34,53],[80,127]],"ID":"332"},{"text":"guished from mentions in text or mentions in other sources. The Terence Annotation Format (TAF) provides a unified framework to annotate events, par-","acronyms":[[91,94]],"long-forms":[[64,89],[77,78]],"ID":"333"},{"text":"Subjacency sub-  sumes, as well as other principles, Ross's lO  Complex Noun Phrase Constraint (CNPC), which  prohibits movements out o f~-NpNPS ~ structures, ","acronyms":[[96,100]],"long-forms":[[24,26],[24,26],[72,94],[35,36]],"ID":"334"},{"text":"We report both the aggregate curves precision\/recall curves and Precision@N (P@N) in our experiments.","acronyms":[],"long-forms":[[-1,2],[-1,4]],"ID":"335"},{"text":"On the source-language side of the  corpus we will automatically generate lists of  frequent multiword expressions (MWEs) and  grammatical constructions using the methodology ","acronyms":[[116,120]],"long-forms":[[93,114]],"ID":"336"},{"text":"al. were entered into Graph Spider using the  metapattern language (MPL) designed by the  Graph Spider authors.","acronyms":[[68,71]],"long-forms":[[46,66]],"ID":"337"},{"text":"In interaction with the user, the system should play the role of an Information Search Assistant (ISA). ","acronyms":[[101,102]],"long-forms":[[0,2],[80,96]],"ID":"338"},{"text":"ducted experiments on the same dataset for sentence identification using interaction patterns generated by another pattern generating algorithm (PGA) (Huang et al.,","acronyms":[[145,148]],"long-forms":[[115,133]],"ID":"339"},{"text":"{zhaosq,xlan,tliu,lisheng}@ir.hit.edu.cn Abstract Paraphrase generation (PG) is important in plenty of NLP applications.","acronyms":[[73,75],[103,106]],"long-forms":[[50,71]],"ID":"340"},{"text":"1. Introduction Word segmentation is an important task in natural language processing (NLP) for languages without word delimiters (e.g., Chinese).","acronyms":[[87,90]],"long-forms":[[37,39],[55,85]],"ID":"341"},{"text":"of derivations. Each such derivation is realized in  PROVERB by a proof communicative act (PEA),  following the viewpoint hat language utterances are ","acronyms":[[53,60],[91,94]],"long-forms":[[64,89]],"ID":"342"},{"text":" Introduction The Penn Chinese Treebank (CTB) is an ongoing project, with its objective being to","acronyms":[[43,44]],"long-forms":[[30,38],[35,37]],"ID":"343"},{"text":"News stories typically describe real-world events.  Topic detection and tracking (TDT) aims to detect stories that discuss identical or directly related","acronyms":[[82,85]],"long-forms":[[52,80]],"ID":"344"},{"text":"For Arabic, morphological segmentation is performed by MADA 3.2 (Habash et al, 2009), using the Penn Arabic Treebank (PATB) segmentation scheme as recommended by El Kholy and Habash","acronyms":[[55,59],[118,122]],"long-forms":[[4,10],[96,116]],"ID":"345"},{"text":"In ear l ier  papers devoted to interpersonal  interaction iFrank,1981; Levinson,1981\\] much atten-  tion is paid to studying the role of speecb act (SA)  i n  d ia logue  s t ructure .","acronyms":[[150,152]],"long-forms":[[138,148],[19,20]],"ID":"346"},{"text":"we can use these annotations to measure an average precision across the precision-recall curve, and an aggregate mean average precision (MAP) across all relations.","acronyms":[[137,140]],"long-forms":[[40,60],[100,135]],"ID":"347"},{"text":" ? BLEU (bilingual evalutation understudy) score: This score measures the precision of unigrams, bigrams, trigrams, and 4-grams with respect to a","acronyms":[],"long-forms":[[-1,23]],"ID":"348"},{"text":"2007 task into four sub-tasks: (1) target word frame disambiguation (TWFD); (2) FE boundary detection (FEBD); (3) GF label classification (GFLC) and (4) FE label classification (FELC).","acronyms":[[69,73],[103,107]],"long-forms":[[35,67],[80,101],[114,137],[117,137]],"ID":"349"},{"text":" Table 2 shows part of a decision list for the target noun chicken that was learned from a subset of the BNC (British National Corpus) [17]. Note that the","acronyms":[[104,107]],"long-forms":[[1,2],[1,2],[109,132]],"ID":"350"},{"text":"The table shows percentage of phrases that we have to retain. ES=Spanish, EN=English, FR=French, CS=Czech, DE=German. ","acronyms":[],"long-forms":[],"ID":"351"},{"text":"josefr@coli.uni-sb.de Abstract Active Learning (AL) has been proposed as a technique to reduce the amount of annotated","acronyms":[[48,50]],"long-forms":[[31,46],[27,28]],"ID":"352"},{"text":" 2.1 Weighted regular tree grammars A weighted regular tree grammar (WRTG) is a 4tuple G = (S,L,R, s`), where S and L are two","acronyms":[[72,73]],"long-forms":[[13,25],[13,33],[18,19],[33,34]],"ID":"353"},{"text":"(i) identify the scope of coordinations regardless of phrase types, and (ii) detect noun phrase (NP) coordinations and identify their scopes.","acronyms":[[97,99]],"long-forms":[[54,60],[84,95]],"ID":"354"},{"text":" 1 Introduction Among many natural language processing (NLP) tasks, such as text classification, question answer-","acronyms":[[58,59]],"long-forms":[[34,53]],"ID":"355"},{"text":"(01) AT (Singular Article)  (03) BED (were)  (05) BEG (being)  (07) BER (are, 're) ","acronyms":[[5,7],[33,36],[50,53],[68,71]],"long-forms":[[55,60]],"ID":"356"},{"text":"In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI?77), page 67=76. ","acronyms":[[81,89]],"long-forms":[[0,2],[22,79]],"ID":"357"},{"text":"In Proc. of the Human Language Technologies (HLT): The Annual Conf.","acronyms":[[45,48]],"long-forms":[[16,43]],"ID":"358"},{"text":"We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RST-DTB) (Carlson et al.,","acronyms":[],"long-forms":[[99,121]],"ID":"359"},{"text":"implementation of SVM.  Lexical Classifier (LC): This method calculates the number of positive words and negative words","acronyms":[[18,21],[44,46]],"long-forms":[[24,42]],"ID":"360"},{"text":" (Undecided) Meronym(MER)  (a) IF x=ANT ","acronyms":[[10,11]],"long-forms":[],"ID":"361"},{"text":"organizations in knowledge mining approaches to  master this information for quality assurance or  Customer Relationship Management (CRM) purposes.","acronyms":[[133,136]],"long-forms":[[99,131]],"ID":"362"},{"text":"12 Interac Figure 2: Upper chart: Turn-wise Interaction Quality (IQ) annotation from 3 raters. The final label is the median of","acronyms":[[65,67]],"long-forms":[[44,63]],"ID":"363"},{"text":"We annotate the relation node in the path with the exact relation word (as a lexical constraint) and the POS (postag constraint). We create a re-","acronyms":[[105,108]],"long-forms":[[30,32],[3,4],[85,95],[110,127],[3,4]],"ID":"364"},{"text":"(#classes) with respect o each part of speech.  Table 1 Outline of Bunruigoihy3 (BGH)  POS noun I verb adj other total ","acronyms":[[81,84],[87,90]],"long-forms":[[67,79]],"ID":"365"},{"text":" Conditional Random Field  A conditional random field (CRF)[5] can be seen  as an undirected graph model in which the nodes ","acronyms":[[54,57]],"long-forms":[[28,52],[13,15]],"ID":"366"},{"text":"VIOLATED EXPECTATION (Ho) NONVOLITIONAL-RESULT (M&T) EXPLANATION (Ho) ( CAUSAL \u0004 ADDITIVE ) - RESULT (A&L) ( SEMANTIC \u0004 PRAGMATIC ) - EXPLANATION (A&L)","acronyms":[[22,24]],"long-forms":[[48,49],[5,6],[53,64],[72,80],[39,40],[21,22],[107,117],[24,25],[53,64],[4,5]],"ID":"367"},{"text":" 5 Conclusion Our approach is akin to so-called semantic role labelling (SRL) approaches [CM05] and to several rewriting approaches developed to modify parsing output in RTE systems [Ass07].","acronyms":[[169,172]],"long-forms":[[56,70],[31,33]],"ID":"368"},{"text":"   (5) Flattened CPT (FCPT): the CPT with the  single in and out arcs of non-terminal nodes (ex-","acronyms":[[2,3]],"long-forms":[[14,17],[14,17]],"ID":"369"},{"text":"two requirements. Since it has to be transformed  into context?free grammar (CFG) for recognition,  features must have a finite number of values, as ","acronyms":[[77,80]],"long-forms":[[55,62],[63,75],[28,29]],"ID":"370"},{"text":"capture various relationships related to the predicate, we assign function label ? ADT (adjunct)? for","acronyms":[[83,86]],"long-forms":[[88,95]],"ID":"371"},{"text":"SN  where CN = common oun  PN = proper name  SN = Sa-inflection oun (nominal verb) ","acronyms":[[0,2],[10,12],[27,29]],"long-forms":[[15,21],[22,25],[22,25]],"ID":"372"},{"text":" They solve this by formulating the problem as a quadratic assignment problem (QAP). But, even","acronyms":[[78,81]],"long-forms":[[35,42],[46,76]],"ID":"373"},{"text":"on its n?1 previous tokens, i.e. we directly model the following conditional probability (in practice, we choose n = 3, Tri-gram (TRI) ): p(w","acronyms":[[130,133]],"long-forms":[[120,128]],"ID":"374"},{"text":"on adjunction (Joshi, 1987):  ? Null adjunction (NA): disallow any adjunc-  tion on the given node.","acronyms":[[49,51]],"long-forms":[[0,13],[32,47],[76,83]],"ID":"375"},{"text":"It takes an examplebased approach to recognize IV words and follows description length gain (DLG) to infer OOV words in terms of their text compression effect.","acronyms":[[47,49],[93,96]],"long-forms":[[68,79],[80,91],[89,91]],"ID":"376"},{"text":"8. Strong forms of pronouns not preceded by a preposition (unless they carry IC) t Table 1: Annotation guidelines; IC = Intonation Center. ","acronyms":[[77,79]],"long-forms":[[44,45],[4,5],[120,137]],"ID":"377"},{"text":" 1 Introduction Topical Text Categorization (TC), the task of classifying documents by pre-defined topics, is most","acronyms":[[46,47]],"long-forms":[[23,27],[28,42]],"ID":"378"},{"text":"As an example, the following arrow property says that within an interrogative phrase (Pint), an interrogative chunk (IntC) with an interrogative pronoun inside (pint) ar-","acronyms":[[86,90],[117,121]],"long-forms":[[64,84],[64,77],[64,77]],"ID":"379"},{"text":"Controlled English at Douglas SMART Controlled English ASD Simplified Technical English (ASD-STE) AECMA Simplified English (AECMA-SE)","acronyms":[[30,35]],"long-forms":[[11,18],[47,87],[98,122]],"ID":"380"},{"text":"16: end while 17: return builtPPs 3.3 Extended GNPPA (E-GNPPA) The GNPPA described in section 3.1 assumes that","acronyms":[],"long-forms":[[38,52],[47,52]],"ID":"381"},{"text":"  3.3 Optimality Theory  Optimality Theory (OT) is a theory of language  and grammar, developed by Alan Prince and Paul ","acronyms":[[44,45]],"long-forms":[[15,21],[15,21],[9,10]],"ID":"382"},{"text":"1001   Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 844?853, October 25-29, 2014, Doha, Qatar.","acronyms":[[95,100]],"long-forms":[[45,93]],"ID":"383"},{"text":"2 tMi The two parameter classes for generating modifying nonterminals that are children of base NPs (NPB nodes), PM,NPB and PMw,NPB, have the following back-off structures. ","acronyms":[[101,104],[113,115]],"long-forms":[],"ID":"384"},{"text":"being developed as an Apache incubator project.  UIMA?s Common Analysis System (CAS) is used to describe typed objects (annotations) associated","acronyms":[[49,54],[80,83]],"long-forms":[[19,21],[54,78],[69,71]],"ID":"385"},{"text":"Sane: ..PERJANTAINA (pltkanl iper jantalna)  PER3ANTAI FRIDAY Noun $8 Eaa  Sane: PITK&KSI.. (p i tk ika iper Janta iks i )   PITK& LI3NG Ad ject ive  $8 Trane l  ","acronyms":[[81,91]],"long-forms":[[21,22]],"ID":"386"},{"text":"115  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 566?576, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]],"ID":"387"},{"text":"831 . . . demonstrated that HOIL-1L interacting protein (HOIP), a ubiquitin ligase that can catalyze the assembly of linear polyubiquitin chains, is recruited to DC40 in a TRAF2-dependent manner following engagement of CD40 . . .","acronyms":[[61,62]],"long-forms":[[36,55],[36,38]],"ID":"388"},{"text":"tasks. While having the same model structure as Hidden Markov Models (HMMs), CRFs are trained discriminatively and can use large numbers of corre-","acronyms":[[70,74],[77,81]],"long-forms":[[48,68]],"ID":"389"},{"text":"derstanding our approach.  2.1.1 The Conversat ional  Roles Model  (COR)   In the field of information retrieval (IR) the interactive ","acronyms":[[68,71],[114,116]],"long-forms":[[-1,26],[91,112]],"ID":"390"},{"text":"evt EVENT lfr LIVING THING sub SUBSTANCE fod FOOD lme LINEAR MEASURE tme TIME Table 1: The 39 CoreLex basic types (BTs) and their WordNet anchor nodes Basic type WordNet anchor Examples","acronyms":[[50,53],[94,101],[115,118]],"long-forms":[[102,113],[108,112]],"ID":"391"},{"text":"The first observation is that the task is quite difficult as evidenced by extremely poor performance  of the bag of words approach (BOW). The correct ","acronyms":[[132,135]],"long-forms":[[116,130]],"ID":"392"},{"text":"Table 3 describes the used data sets.  Assault Weapons (AW) 4","acronyms":[[56,58]],"long-forms":[[39,54]],"ID":"393"},{"text":" Definition 1 A character string ABC is called an overlap ambiguity string (OAS) if it can be segmented into two words either as AB\/C or A\/BC (not both), depending on context.","acronyms":[[13,14]],"long-forms":[[25,31],[57,73],[5,7]],"ID":"394"},{"text":" Irish students do not receive any instruction in  Modern Foreign Languages (MFL) up until this  point (Irish is not considered a MFL).","acronyms":[[79,80]],"long-forms":[[57,74],[30,31]],"ID":"395"},{"text":"resulting grammar. We cast the minimization as an integer linear program (ILP). Let V be the set of","acronyms":[[74,77]],"long-forms":[[50,72]],"ID":"396"},{"text":"L J  = lea:-ncd j o u r n a l s   PJ - 1 journals  NR = newt;pc?pcr reportasc  F = fictlon ","acronyms":[],"long-forms":[[24,29]],"ID":"397"},{"text":"icoglu and Bergler, 2009). The second group uses a  machine learning (ML)-based approach which exploits various specific features and learning algo-","acronyms":[[70,72]],"long-forms":[[-1,17],[60,68]],"ID":"398"},{"text":"1 The  SCAN System  SCAN was developed for the TREC-96 SDR task,  a known item information retrieval (IR) task from  approximately 47hours of the NIST\/DARPA HUB4 ","acronyms":[[55,58],[102,104]],"long-forms":[[39,42],[26,27],[79,100]],"ID":"399"},{"text":"Department of Linguistics, ? Center for the Preservation of Ancient Religious Texts (CPART) Brigham Young University","acronyms":[[85,90]],"long-forms":[[11,13],[29,83]],"ID":"400"},{"text":"145  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1092?1103, October 25-29, 2014, Doha, Qatar.","acronyms":[[93,98]],"long-forms":[[43,91]],"ID":"401"},{"text":"Specifically, we investigate dialectal language in publicly available Twitter data, focusing on AfricanAmerican English (AAE), a dialect of Standard American English (SAE) spoken by millions of peo-","acronyms":[[121,124],[167,170]],"long-forms":[[96,111],[112,119],[8,9],[140,165]],"ID":"402"},{"text":"? ?  Table 4: Results for the best baseline (B5) and the learning to rank method (LTR), using all entity pairs in the dataset, including those without any relevant sentences.","acronyms":[[45,47],[82,85]],"long-forms":[[35,43],[57,65],[66,73],[40,42]],"ID":"403"},{"text":"active (ACT) and passive (PASS) 3. causative (CAUS) 4.","acronyms":[[8,11],[26,30],[46,50]],"long-forms":[[0,6],[17,24],[35,44]],"ID":"404"},{"text":"problems. Informally, a CRF bears resemblance to a Hidden Markov Model (HMM) in which, for each input position in a sequence, there is an observed","acronyms":[[24,27],[72,75]],"long-forms":[[16,17],[49,70],[16,17]],"ID":"405"},{"text":"Gambling neighborhood f bank  bank  Subject Code GB = Gambling  person use money piece ","acronyms":[[49,51]],"long-forms":[[0,8],[0,8]],"ID":"406"},{"text":"He poured wine from the barrel into the bottle The semantic description of (20) appeals to an intermediate locus IME(LOC), which is not specified here.","acronyms":[[113,116],[117,120]],"long-forms":[[107,112]],"ID":"407"},{"text":"UC 3 NLP, 1 BioNLP ML (Weka SVM) Table 2: Participation. UU = UofU, UZ = UZH, CU=ConcordU, UT = UTurku, UZ = UZH, US =","acronyms":[[0,2],[5,8],[12,18],[19,21],[28,31],[57,59],[68,70],[91,93],[114,116]],"long-forms":[[62,66],[73,76],[96,102]],"ID":"408"},{"text":"ratio of the number of completely corrected generated MIUs over the number of all MIUs, and character accuracy (Ch-Acc), but the sentence accuracy (S-Acc) will also be reported in evaluation","acronyms":[[112,118],[148,153]],"long-forms":[[92,110],[129,146]],"ID":"409"},{"text":"They are Na?ve  Bayes (NB), Support Vector Machine (SVM),  Maximum Entropy (MaxEnt) (Kamal Nigam et al  1999) and standard chain CRFs (Fei et al 2003).","acronyms":[[23,25],[52,56]],"long-forms":[[-1,12],[36,50],[67,74]],"ID":"410"},{"text":"The similarity of decisions can be evaluated by calculating the proportion of identical decisions (PID)when comparing the test results with those of the gold stan-","acronyms":[[102,103]],"long-forms":[[18,27],[18,27]],"ID":"411"},{"text":" ? HybFSum (Hybrid Flat Summarizer): To investigate the performance of hierarchical topic","acronyms":[[10,11]],"long-forms":[[-1,16]],"ID":"412"},{"text":"been proposed in \\[Uszkoreit 87\\]: p.145 in his German  grammar. It makes the adverbial phrase (AdvP) a sister  node of the verb and its arguments: ","acronyms":[[96,100]],"long-forms":[[78,94],[52,53],[80,84]],"ID":"413"},{"text":"(Zhao et al, 2014), The Meaning Factory (Bjerva et al, 2014), UNAL-NLP (Jimenez et al, 2014), and Illinois-LH (Lai and Hockenmaier, 2014). ","acronyms":[[62,70]],"long-forms":[],"ID":"414"},{"text":"sentence, Multiple Linear Regression is used to  build a quantitative model relating the content  tags of the source language (SL) sentence to the  response, which is assumed to be the sum of the ","acronyms":[[127,129]],"long-forms":[[23,24],[110,125]],"ID":"415"},{"text":"Commentary We distinguish three types of events in the domain: identification (ID) events trigger the system to name the street the car is on, turn events fire","acronyms":[[79,81]],"long-forms":[[63,77],[75,77]],"ID":"416"},{"text":"For biomedical terms other than genes\/gene products, the Unified Medical Language System (UMLS) meta-thesaurus (Lindberg et al, 1993) is a large","acronyms":[[90,94]],"long-forms":[[57,88],[12,14],[12,13]],"ID":"417"},{"text":"snippets with a frequency higher than three. Then we calculate the inverse sentence frequency (ISF) for these phrases using the entire ICSI meeting corpus.","acronyms":[[95,98],[135,139]],"long-forms":[[14,25],[67,93]],"ID":"418"},{"text":" 1 Introduction In recognizing textual entailment (RTE), automated systems assess whether a human reader","acronyms":[[53,54]],"long-forms":[[38,48],[35,36]],"ID":"419"},{"text":"A decade ago, students interested in natural language processing arrived at universities having been exposed to the idea of machine translation (MT) primarily through science fiction.","acronyms":[[145,147]],"long-forms":[[23,25],[38,40],[124,143]],"ID":"420"},{"text":"man annotators identified in the texts. TP (true positives) is |A?G|, FP (false positives) is |A\\G|, FN (false negatives) is |G\\A|, and precision (P ),","acronyms":[[40,42],[70,72],[101,103]],"long-forms":[[44,48],[49,58],[74,89],[60,62],[105,120]],"ID":"421"},{"text":"We report results for the ATAS versions (ATAS-TC, ATAS-CRF) and for the baselines (Z-CRF, C-value, FRTC) as well as for using supervised (S-SEL) and unsupervised feature selection (U-SEL) in system setting (S) and gold boundary setting (G).","acronyms":[[26,30],[83,88],[99,103],[138,143],[181,186]],"long-forms":[[126,136],[149,179]],"ID":"422"},{"text":"amjbara@umich.edu Abstract The ACL Anthology Network (AAN)1 is a comprehensive manually curated networked","acronyms":[[54,57]],"long-forms":[[31,34],[35,52],[0,1]],"ID":"423"},{"text":"where Q(w,w?) is proportional to the integral term in Equation (IX). The term PC(w) corresponds","acronyms":[[78,80]],"long-forms":[],"ID":"424"},{"text":"Their by-country breakdown is as follows: 3.99M (61%) from Saudi Arabia (SA), 880K (13%) from Egypt (EG), 707K (11%) from Kuwait (KW), 302K (5%) from United Arab Emi-","acronyms":[[73,75],[101,103],[130,132]],"long-forms":[[59,71],[122,128]],"ID":"425"},{"text":"section 8.1). Then, the grammar underlying the parser is provided with a specific attachment heuristic that uses corequirement (CR) information from the lexicon. ","acronyms":[[128,130]],"long-forms":[[113,126]],"ID":"426"},{"text":" In line 4, G91 provides an Acknowledge type of evidence, and Moves On to the next task item: identifying the Target Location - Grid (TL-GR) of the CFF. The Acknowledge and Move On, referring to the CGU","acronyms":[[147,150]],"long-forms":[[109,131]],"ID":"427"},{"text":" With basic CG there are just two rules for combining categories: the forward (FA) and backward (BA) functional application rules.","acronyms":[[11,13],[78,80],[96,98]],"long-forms":[[69,76],[86,94]],"ID":"428"},{"text":"To overcome this problem, Shen et al (2008) proposed a dependency language model (DLM) to exploit longdistance word relations for SMT.","acronyms":[[42,43]],"long-forms":[[34,35],[66,80]],"ID":"429"},{"text":"Abstract In this paper, we explore the possibility of leveraging Residual Networks (ResNet), a powerful structure in constructing extremely","acronyms":[[84,90]],"long-forms":[[65,82],[5,6]],"ID":"430"},{"text":"the end we build two NGCMs: NGCMP  (NGCM according to preceding context) and  NGCMS (NGCM according to succeeding  context).","acronyms":[[21,26],[28,33],[78,83]],"long-forms":[[21,25]],"ID":"431"},{"text":"Ures uniformly (Dadam et al, 1986). Our LDB  rmat and Lexical Query l_anguage (LQL) sup-  port the hierarchical model for dictionary data; ","acronyms":[[40,43],[79,82]],"long-forms":[[25,27],[54,77]],"ID":"432"},{"text":"The Office. Television series, the National Broadcasting Company (NBC). ","acronyms":[[66,69]],"long-forms":[[35,64]],"ID":"433"},{"text":" 1 Introduction Question Answering (QA) is a challenging task that draws upon many aspects of NLP.","acronyms":[[37,38]],"long-forms":[[24,33]],"ID":"434"},{"text":"opment and sentence generation. Report, German National Center for Information Technology (GMD),  Institute for integrated publication and information systems (IPSI), Darmstadt, Germany, January 1997. ","acronyms":[[91,94],[160,164]],"long-forms":[[7,10],[40,89],[98,158],[178,185]],"ID":"435"},{"text":"knowledge, mnong others in l:'ei~onal or DBMT systems.  Such Discovery Assistants (DA) slmuld certainly be  highly cooperative, namely show sensible interactivity ","acronyms":[[41,45],[83,85]],"long-forms":[[61,81]],"ID":"436"},{"text":"pact of syllabification on the L2P problem in English. Their Syllabification by Analogy (SbA) algorithm is a data-driven, lazy learning approach.","acronyms":[[31,34],[89,92]],"long-forms":[[21,23],[61,87],[1,2]],"ID":"437"},{"text":"1 Introduction Resolving the ambiguity of person names in web search results is a challenging problem becoming an area of interest for Natural Language Processing (NLP) and Information Retrieval (IR) communities. ","acronyms":[[164,167],[196,198]],"long-forms":[[21,23],[29,30],[111,113],[131,162],[173,194]],"ID":"438"},{"text":"In Proceedings  from the 16th International Conference on  Computational Linguistics (COLING-96), pages  592-597.","acronyms":[[86,95]],"long-forms":[[-1,27]],"ID":"439"},{"text":"e .g .  M C ~  --7 SUM + PRED* + IOBJ + WBJ + PREP P  *PRED = predicator - not predicate  1-1  detailed analysis of first occurring element of ","acronyms":[[19,22],[33,37],[40,43],[-1,4]],"long-forms":[[0,1],[62,72]],"ID":"440"},{"text":"We think, based on the explicit sentences, several Support Vector Machine (SVM) classifiers can be established to do this task.","acronyms":[[78,79]],"long-forms":[[59,73],[62,64]],"ID":"441"},{"text":"average values. Figure 6 shows the average pitch of the phrase do you have in task interruption (INT) and poker-playing (PKR) of each player, with the actual values displayed in the columns below.","acronyms":[[97,100],[121,124]],"long-forms":[[75,77],[83,95],[106,119],[75,77]],"ID":"442"},{"text":"enizes, tags, lemmatizes and parses the input sentences, outputting syntactic trees and then adding grammatical relations (GR) as described by (Buttery and Korhonen, 2005).","acronyms":[[123,125]],"long-forms":[[100,121]],"ID":"443"},{"text":" We discuss the available functions of the  prototype Chinese Sketch Engine (CSE) as well  as the robustness of language-independent ","acronyms":[[76,79]],"long-forms":[[53,74]],"ID":"444"},{"text":"tionary. In (Zingarelli, 2008), we found 33 different types of prepositional phrases (PPs), which we grouped into 21 classes (for instance, all of the 48","acronyms":[[86,89]],"long-forms":[[63,84]],"ID":"445"},{"text":"for Statistical Machine Translation Shixiang Lu, Zhenbiao Chen, Bo Xu Interactive Digital Media Technology Research Center (IDMTech) Institute of Automation, Chinese Academy of Sciences, Beijing, China","acronyms":[[124,131]],"long-forms":[[70,122]],"ID":"446"},{"text":"overall scores considering all metrics. To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004).","acronyms":[[86,89]],"long-forms":[[66,84]],"ID":"447"},{"text":"(see also Sima?an, 1999, 2003). We haven?t implemented the max rule product (MRP) where posteriors are multiplied instead of added (Petrov and","acronyms":[[77,80]],"long-forms":[[41,42],[59,75]],"ID":"448"},{"text":"of the intuition behind the inclusion of Tree Adjoining Lan-  gages (TAL) in the class of languages generated by a variant  of HG's called Modified Head Grammars (MHG's). In the ","acronyms":[[69,72],[127,129],[129,131]],"long-forms":[[41,55],[7,9],[57,58],[139,161]],"ID":"449"},{"text":"If a goal is not accomplished before worst-time  timeout value,  ACCUMVALUE = ACCUM.VALUE - \\[response-  complexity(punishment) * sub-goal(worst-ease timeout punis- ","acronyms":[[65,75]],"long-forms":[],"ID":"450"},{"text":"Each of these sets is further divided by three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). ","acronyms":[[114,119]],"long-forms":[[98,112]],"ID":"451"},{"text":"The set of ncs in ? are selected from all the possibilities in the hyponym hierarchy according to the minimum description length (MDL) principle (Rissanen 1978) as used by Li and Abe (1995, 1998).","acronyms":[[130,133]],"long-forms":[[15,17],[15,17],[102,128]],"ID":"452"},{"text":"However, we are interesting in the potential power of our model by incorporating lexical reordering (LR) models and comparing it with syntax-based models.","acronyms":[[101,103]],"long-forms":[[16,18],[81,99]],"ID":"453"},{"text":" In the next sections, we briefly introduce the kernel trick and describe the subtree (ST) kernel devised in Vishwanathan and Smola (2002), the subset tree (SST) kernel defined in Collins and Duffy (2002), and the partial tree (PT) kernel proposed in","acronyms":[[86,88],[156,159],[227,229]],"long-forms":[[77,84],[143,149],[80,84],[213,225]],"ID":"454"},{"text":"*****- TRANSFOREATIONS * l f**   SCAN CALLED AT 1 I  ANTEST CALLED FOR 12?F ALAT 3 (AACC) ,SO= 13. RES= 0, TOP= 1:s ","acronyms":[[84,88]],"long-forms":[[38,47],[53,70],[74,80]],"ID":"455"},{"text":" ? Shallow Syntactic Similarity (SP) SP-Op-?.","acronyms":[[34,35]],"long-forms":[[20,30]],"ID":"456"},{"text":"using only a single, probable alignment.\" The single most probable assignment Ama~  is the maximum a posteriori (MAP) assignment:  Amax = ar~maxPr(U,A, VIO ) (22) -- AE~4 ","acronyms":[[113,116],[152,155]],"long-forms":[[11,12],[91,111]],"ID":"457"},{"text":"Sentence). The dependency labels are NK (Noun Kernel), SB (Subject), AO (Object Accusative), HD (Head), MO (Modifier), AC (Adpositional Case Marker), CJ (Conjunct), and OC (Clausal Object).","acronyms":[[37,39],[55,57],[69,71],[93,95],[104,106],[119,121],[150,152],[169,171]],"long-forms":[[41,52],[59,66],[73,90],[97,101],[108,116],[123,147],[154,162],[173,187]],"ID":"458"},{"text":" Semantic relatedness of two given terms (text fragments, phrases or words) can be obtained by calculating the correlation between two high dimensional vectors of a Distributional Semantic Model (DSM), which is based on the assumption that semantic meaning of a text can be inferred from its usage in context","acronyms":[[195,198]],"long-forms":[[0,8],[162,193],[119,121],[3,4]],"ID":"459"},{"text":"An integrated, conditional model of information extraction and coreference with application to citation graph construction. In 20th Conference on Uncertainty in Artificial Intelligence (UAI). ","acronyms":[[186,189]],"long-forms":[[124,126],[146,184]],"ID":"460"},{"text":"4 Experiments In this section, we evaluate performance of different methods on the Relation Schema Induction (RSI) task.","acronyms":[[110,113]],"long-forms":[[14,16],[27,29],[83,108]],"ID":"461"},{"text":"We developed and tested our system on 30 full  length UK archaeological reports archived by the  Arts and Humanities Data Service (AHDS)4. ","acronyms":[[54,56],[131,135]],"long-forms":[[13,16],[97,129]],"ID":"462"},{"text":"formance. They are the One-error Loss (O-Loss) function, the Symmetric Loss (S-Loss) function, and the Hierarchical Loss (H-Loss) function:","acronyms":[],"long-forms":[[23,37],[61,75],[103,120]],"ID":"463"},{"text":"Instead of simple web page counts and complex  web page collection, we propose a novel model,  a Web Search with Double Checking (WSDC), to  analyze snippets.","acronyms":[[130,134]],"long-forms":[[97,128]],"ID":"464"},{"text":"hedge words Table 3: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, SDE=Software Development Engineer, CoreNLP=Stanford CoreNLP, Porter=Porter","acronyms":[],"long-forms":[[118,137],[163,183],[185,192]],"ID":"465"},{"text":" 4 Dimensionality Reduction In this section, the Sparse Projection (SP) algorithm is described (see also Algorithm 1). SP is the core","acronyms":[[69,70]],"long-forms":[[55,65]],"ID":"466"},{"text":"2.2 Hierarchical Agglomerative Clustering After discovering sense clusters of paths, we employ hierarchical agglomerative clustering (HAC) to discover semantic relations from these sense clusters.","acronyms":[[134,137]],"long-forms":[[95,132]],"ID":"467"},{"text":"extremely limited in this domain. Thus, language 1KEY: COM=completive aspect, DEM=demonstrative, DIR=directional","acronyms":[],"long-forms":[[70,76]],"ID":"468"},{"text":" 2 Background: MaxEnt Models Maximum Entropy (MaxEnt) models are widely used in Natural Language Processing (Berger et","acronyms":[],"long-forms":[[36,43]],"ID":"469"},{"text":"is set to 0.95 and threshold_f is set to 1;  Step 3. Use TCT (triple context template) matching  model to extract 2-char, 3-char and 4-char ","acronyms":[[57,60]],"long-forms":[[0,2],[62,85]],"ID":"470"},{"text":"trigger in the Trigger Rule Factbase.  Consistency Checkers ( CC' s): Report  inconsistencies within and between factbases and, ","acronyms":[[62,65]],"long-forms":[[39,59],[34,35]],"ID":"471"},{"text":"task: Argumentative Zoning (Teufel and Moens, 2002). Argumentative Zoning (AZ) is the task of applying one of seven discourse level tags (Fig-","acronyms":[[75,77]],"long-forms":[[6,26],[6,26]],"ID":"472"},{"text":"ture of I end set to 1.  Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word ","acronyms":[],"long-forms":[[25,52]],"ID":"473"},{"text":"examples in (5). This change in beliefs about the past is treated as an error identification signal (EIS). ","acronyms":[[101,104]],"long-forms":[[72,99]],"ID":"474"},{"text":"non-terminal symbols to characterize linguistic objects allow us to use much richer statistical means such as ME (maximum entropy model), etc.","acronyms":[[110,112]],"long-forms":[[114,129]],"ID":"475"},{"text":"An example of a comma rule is the following: SX=S X ; ? SX=S X (18)","acronyms":[],"long-forms":[[46,47],[46,47]],"ID":"476"},{"text":"recognition. In Proceedings of the 26th Conference on Artificial Intelligence (AAAI). ","acronyms":[[79,83]],"long-forms":[[13,15],[54,77]],"ID":"477"},{"text":"nication after speech and email.4 Millions of users of instant messaging (IM) services and short message service (SMS) generate electronic content in a dialect that does not adhere to conventional gram-","acronyms":[[74,76],[114,117]],"long-forms":[[55,72],[91,112],[147,151]],"ID":"478"},{"text":"speech recognition (ASR), dialog management (DM), database access (DB Access), data storage (DB) and oral response generation (RG). In ad-","acronyms":[[20,23],[45,47],[67,69],[127,129]],"long-forms":[[0,18],[26,43],[79,91],[101,125]],"ID":"479"},{"text":"classifier.  6.1   Language Model (LM)  As language model has already been used in question classification [7], it is taken as ","acronyms":[[35,37]],"long-forms":[[19,33]],"ID":"480"},{"text":"argument facet inducer. We introduce a new task of ARGUMENT FACET SIMILARITY (AFS). We discuss","acronyms":[[78,81]],"long-forms":[[51,76]],"ID":"481"},{"text":"Question reformulation ? Information Extraction (IE) ?","acronyms":[[49,51]],"long-forms":[[25,47]],"ID":"482"},{"text":"results training on Multi-Domain Sentiment Dataset and testing on citation dataset (CITD). The horizontal line","acronyms":[[84,88]],"long-forms":[[17,19],[63,82]],"ID":"483"},{"text":" In this paper, we propose to disambiguate NEs using a Personalized PageRank (PPR)-based random walk algorithm.","acronyms":[[42,45],[77,80]],"long-forms":[[52,75]],"ID":"484"},{"text":"coverage of the course material.  The quality estimation task (QET) (CallisonBurch et al 2012) aims to develop quality indica-","acronyms":[[63,66]],"long-forms":[[38,61],[38,45]],"ID":"485"},{"text":"4.2 Evaluation of Different Representation Learning Methods Experiment Setup and Dataset We conduct sentiment classification of items in two traditional sentiment lexicons, HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005), to evaluate the effective of the","acronyms":[[173,175],[199,203]],"long-forms":[[77,80],[177,187],[77,80]],"ID":"486"},{"text":"match number, SM (Short Match) is the continuous match number which is no more than 4, and LM (Long Match) is the continuous match number which is more than 4.","acronyms":[[14,16],[91,93]],"long-forms":[[18,29],[95,105]],"ID":"487"},{"text":"are written in the original language.  Direct orthographical mapping (DOM), which performs the transliteration between two lan-","acronyms":[[70,73]],"long-forms":[[12,14],[39,68]],"ID":"488"},{"text":"for each mention, four pieces of information: 1. the mention type: person (PER), organization (ORG), location (LOC), geopolitical en-","acronyms":[[75,78],[95,98],[111,114]],"long-forms":[[67,73],[81,93],[101,109]],"ID":"489"},{"text":"degree of semantic equivalence between a pair of texts. Natural Language Processing (NLP) applications such as Question Answering (Lin and","acronyms":[[85,88]],"long-forms":[[13,14],[56,83]],"ID":"490"},{"text":"from comparable in-domain corpora. We used the AFP (Agence France Presse) and APW (Associated Press Worldstream Service) news texts since there","acronyms":[[47,50],[78,81]],"long-forms":[[52,72],[83,119]],"ID":"491"},{"text":"additional computation costs, and can be applied to  several different learners, such as Naive Bayes  (NB), Maximum Entropy (ME), and Support  Vector Machines (SVMs) models.","acronyms":[[103,105],[125,127],[160,164]],"long-forms":[[49,51],[89,100],[108,123],[-1,22]],"ID":"492"},{"text":"train statistical models that rely on annotated data.8 In this paper, we test automatic annotation using Conditional Random Fields (CRFs) (Lafferty et al, 2001) which have achieved high performance for in-","acronyms":[[132,136]],"long-forms":[[35,37],[105,130],[15,17]],"ID":"493"},{"text":"measured on separate grammatical and ungrammatical data: Gr = Grammatical, AG = Agreement, RW = Real-Word, EW = Extra Word, MW = Missing Word","acronyms":[[75,77],[91,93],[107,109],[124,126]],"long-forms":[[57,59],[62,73],[80,89],[96,105],[112,122]],"ID":"494"},{"text":"Schu?tze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et","acronyms":[[94,97]],"long-forms":[[64,92]],"ID":"495"},{"text":"is a.t least cubic in t, ime, this fl)llows trivially  fronl the inequality  A a+B a <A a+:C4 ~B+aAB=+B a =(A+B)  a  for A,B positive - length of strings) ","acronyms":[],"long-forms":[[3,4],[81,82]],"ID":"496"},{"text":"present specialized knowledge, since both the writer and readers are experts. Medical texts include the abstracts of all medical articles written in Basque in the Gaceta M?edica de Bilbao (GMB) ? Medical","acronyms":[],"long-forms":[[-1,25],[171,172]],"ID":"497"},{"text":"2014). We note the linguistic rules included in the Lease, Johnson & Charniak (2006) tree adjoining grammar (TAG) noisy-channel model ? lexical,","acronyms":[[109,112]],"long-forms":[[20,22],[85,107]],"ID":"498"}]