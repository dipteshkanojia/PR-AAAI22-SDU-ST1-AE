ID	text	acronyms	long-forms	acronyms-text	long-forms-text	AN_Pred	LF_Pred	AN_Pred_idxs	LF_Pred_idxs
1	2 Related Work The availability of emotion-rich text has helped to promote studies of sentiments from a boutique science into the mainstream of Text Data Mining (TDM). The ?	[[162, 165]]	[[144, 160]]	['TDM']	['Text Data Mining']	['TDM']	['a', 'Text Data Mining']	[[162, 165]]	[[144, 160]]
2	WebDict 0.2919 Backoff 0.3282 Table 1: Mean Average Precision (MAP), averaged over 34 topics	[[63, 66]]	[[39, 61]]	['MAP']	['Mean Average Precision']	['MAP']	['Mean Average Precision']	[[63, 66]]	[[39, 61]]
3	Baselines are a unigram query likelihood (QL) model (bag of words) and a highly effective sequential dependence (SD) variant of the Markov random field (MRF) model (Metzler and Croft,	[[113, 115], [42, 45], [153, 156]]	[[90, 111], [132, 151]]	['SD', 'QL)', 'MRF']	['sequential dependence', 'Markov random field']	['QL', 'SD', 'MRF']	['a', 'query likelihood', 'and a', 'sequential dependence', 'Markov random field', 'and']	[[113, 115], [153, 156]]	[[90, 111], [132, 151]]
4	tion. Then, we extract expansion terms from these clusters using pseudo relevance feedback (PRF) as implemented in Terrier.	[[92, 95]]	[[65, 90]]	['PRF']	['pseudo relevance feedback']	['PRF']	['pseudo relevance feedback', 'in']	[[92, 95]]	[[65, 90]]
5	person, mood, voice and case, CATiB uses 6 POS tags: NOM (non-proper nominals including nouns, pronouns, adjectives and adverbs), PROP (proper nouns), VRB (verbs), VRB-PASS (passive-voice	[[130, 134], [30, 35], [43, 46], [53, 56], [151, 154], [164, 172]]	[[136, 142], [156, 161]]	['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PASS']	['proper', 'verbs']	['CATiB', 'POS', 'NOM', 'PROP', 'VRB']	['non-proper', 'nouns', 'proper nouns', 'verbs']	[[30, 35], [43, 46], [53, 56], [130, 134], [151, 154]]	[[156, 161]]
6	(LDA), Maximum Likelihood Linear Transform (MLLT), Boosted Maximum Mutual Information (BMMI), Minimum Phone Error (MPE). It pro-	[[115, 118], [44, 48], [87, 91]]	[[94, 113], [7, 42], [51, 85]]	['MPE', 'MLLT', 'BMMI']	['Minimum Phone Error', 'Maximum Likelihood Linear Transform', 'Boosted Maximum Mutual Information']	['LDA', 'MLLT', 'BMMI', 'MPE']	['Maximum Likelihood Linear Transform', 'Boosted Maximum Mutual Information', 'Minimum Phone Error']	[[44, 48], [87, 91], [115, 118]]	[[7, 42], [51, 85], [94, 113]]
7	gold label (Section 3). Our algorithm is called The Structured Weighted Violations Perceptron (SWVP) as its update rule is based on a weighted sum of up-	[[95, 99]]	[[52, 93]]	['SWVP']	['Structured Weighted Violations Perceptron']	['SWVP']	['Structured Weighted Violations Perceptron', 'on a']	[[95, 99]]	[[52, 93]]
8	phrase markers or words. For simplicity of manipulation but without loss of general-  ity, we will limit the productions to the Chomsky Normal Form (CNF). That is, only 	[[149, 152]]	[[128, 147]]	['CNF']	['Chomsky Normal Form']	['CNF']	['or', 'Chomsky Normal Form']	[[149, 152]]	[[128, 147]]
9	 When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely	[[85, 89], [8, 11]]	[[66, 83]]	['PCFG', 'CFG']	['Probabilistic CFG']	[]	['a CFG is', 'a Probabilistic CFG', 'n']	[]	[]
10	1] proposed a language-neutral framework for representing semantic tense. This framework is called the Language Neutral Syntax (LNS). Based on 	[[128, 131]]	[[103, 126]]	['LNS']	['Language Neutral Syntax']	['LNS']	['a', 'Language Neutral Syntax']	[[128, 131]]	[[103, 126]]
11	12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3 64.2 53.5 60.0 56.6 51.1 57.9 54.3 13 Base+Verb Pairs (VP) 62.1 72.2 66.8 60.1 69.3 64.4 54.4 60.1 57.1 51.9 58.2 54.9 14 Base+Appositives (AP) 63.1 71.7 67.1 60.5 69.4 64.6 54.1 60.1 56.9 51.9 57.8 54.7 Table 1: Results obtained by applying different types of features in isolation to the Baseline system.	[[187, 189], [18, 20], [102, 104]]	[[169, 185], [8, 16], [90, 99]]	['AP', 'FN', 'VP']	['Base+Appositives', 'FrameNet', 'Verb Pair']	['FN', 'VP', 'AP']	['Base+FrameNet', 'Base+Verb Pairs', 'Base+Appositives']	[[18, 20], [102, 104], [187, 189]]	[[169, 185]]
12	Model 3 (Figure 3) illustrates how the source language text and MT component may be replaced by a natural language generation (NLG) system, given a rich enough semantic representation.	[[127, 130], [64, 66]]	[[98, 125]]	['NLG', 'MT']	['natural language generation']	['MT', 'NLG']	['language', 'a natural language generation', 'a']	[[64, 66], [127, 130]]	[]
13	in the V column indicates that the verb conditions were used in the distance measure. LR = labeled recall; LP = labeled precision.	[[86, 88], [107, 109]]	[[91, 105], [112, 129]]	['LR', 'LP']	['labeled recall', 'labeled precision']	['LR', 'LP']	['labeled recall', 'labeled precision']	[[86, 88], [107, 109]]	[[91, 105], [112, 129]]
14	The big blue door.?  In this case, the GrM asks  the Response Planner (RP) to provide an elaboration for the current UU; the RP generates this 	[[71, 73], [39, 42], [117, 119], [125, 127]]	[[53, 69]]	['RP', 'GrM', 'UU', 'RP']	['Response Planner']	['GrM', 'RP', 'UU']	['Response Planner', 'an']	[[39, 42], [71, 73], [125, 127], [117, 119]]	[[53, 69]]
15	TI = terse information  INT = interrupted  TRUN = truncated  TRANS = transposed sentence (discussed in 	[[43, 47], [0, 2], [24, 27], [61, 66]]	[[50, 59], [5, 22], [30, 41], [69, 79]]	['TRUN', 'TI', 'INT', 'TRANS']	['truncated', 'terse information', 'interrupted', 'transposed']	['TI', 'INT', 'TRUN', 'TRANS']	['terse information', 'interrupted', 'truncated', 'transposed sentence']	[[0, 2], [24, 27], [43, 47], [61, 66]]	[[5, 22], [30, 41], [50, 59]]
16	classes in both original text and main text corpora.  chose Support Vector Machines (SVM) because it has been shown by other researchers in AGI	[[85, 88], [140, 143]]	[[60, 83]]	['SVM', 'AGI']	['Support Vector Machines']	['SVM']	['in', 'Support Vector Machines', 'in']	[[85, 88]]	[[60, 83]]
17	as in figure 3.  It is parsed as an adverb (AA), whereas it should be a verb group (VG).	[[44, 46], [84, 86]]	[[33, 42], [72, 82]]	['AA', 'VG']	['an adverb', 'verb group']	['AA', 'VG']	['adverb', 'a verb group']	[[44, 46], [84, 86]]	[]
18	can develop after exposure to a terrifying event.  Q-based Union PTSD (posttraumatic stress disorder) is a psychological disorder caused by a mental trauma (also called psychotrauma) that can develop after exposure to a terrifying event.	[[65, 69]]	[[71, 100]]	['PTSD']	['posttraumatic stress disorder']	['PTSD']	['a', 'posttraumatic stress disorder', 'is a', 'disorder', 'a', 'trauma', 'a']	[[65, 69]]	[[71, 100]]
19	It allows for testing interaction scenarios that employ one or more Language Technology Components (LTC). 	[[100, 103]]	[[68, 98]]	['LTC']	['Language Technology Components']	['LTC']	['Language Technology Components']	[[100, 103]]	[[68, 98]]
20	Feature Description lexical the words of the product attribute(PA) the POS for each word of the PA	[[63, 65], [71, 74], [96, 98]]	[[45, 61]]	['PA', 'POS', 'PA']	['product attribut']	['PA']	[]	[[63, 65], [96, 98]]	[]
21	" The tagging has been done using a GUI-based tool  called the Discourse Tagging Tool (DTTool) ac-  cording to ""The Discourse Tagging Guidelines"" we "	[[86, 92], [35, 38]]	[[62, 84]]	['DTTool', 'GUI']	['Discourse Tagging Tool']	[]	['a', 'Discourse Tagging Tool', 'Discourse Tagging']	[]	[[62, 84]]
22	Och and Ney (2003) show that for larger corpora, using word classes leads to lower Alignment Error Rate (AER). This is not	[[105, 108]]	[[83, 103]]	['AER']	['Alignment Error Rate']	['AER']	['Alignment Error Rate']	[[105, 108]]	[[83, 103]]
23	2 Relation Extraction System In this section, we describe the features used in our basic relation extraction (RE) system. Given	[[110, 112]]	[[89, 108]]	['RE']	['relation extraction']	['RE']	['relation extraction']	[[110, 112]]	[[89, 108]]
24	target question, or zero if the target question was not found. The Mean Reciprocal Rank (MRR) is the mean of the reciprocal ranks over all the input ques-	[[89, 92]]	[[67, 87]]	['MRR']	['Mean Reciprocal Rank']	['MRR']	['Mean Reciprocal Rank']	[[89, 92]]	[[67, 87]]
25	discussion in Ritchie(1984).  Functional unification (FU) grammar is a  grammatical formalism which allows descriptions of 	[[54, 56]]	[[30, 52]]	['FU']	['Functional unification']	['FU']	['Functional unification', 'a']	[[54, 56]]	[[30, 52]]
26	Bigram Perplex. ( PP) MDI Missed Samples (MS) Bigram Missed Samples (MS)	[[42, 44], [18, 20], [22, 25], [69, 71]]	[[26, 40], [7, 14], [53, 67]]	['MS', 'PP', 'MDI', 'MS']	['Missed Samples', 'Perplex', 'Missed Samples']	['PP', 'MDI', 'MS']	['Missed Samples', 'Missed Samples']	[[18, 20], [22, 25], [42, 44], [69, 71]]	[[26, 40], [53, 67], [26, 40], [53, 67]]
27	networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 	[[92, 96], [32, 36]]	[[51, 90]]	['CVPR', 'IEEE']	['Computer Vision and Pattern Recognition']	['IEEE', 'CVPR']	['Conference on Computer Vision and Pattern Recognition']	[[32, 36], [92, 96]]	[]
28	Therefore, identification methods like Tsuchiya et al (2006) which uses Support Vector Machines(SVM) have been proposed to solve this problem.	[[96, 99]]	[[72, 94]]	['SVM']	['Support Vector Machine']	['SVM']	['Support Vector Machines', 'to']	[[96, 99]]	[]
29	domain-independent mpirical induction algorithm. We test this idea by examining the  machine learning of simple Sound Pattern of English (SPE)-style phonological rules  (Chomsky and Halle 1968), beginning by representing phonological rules as finite- 	[[138, 141]]	[[112, 136]]	['SPE']	['Sound Pattern of English']	['SPE']	['of', 'Sound Pattern of English']	[[138, 141]]	[[112, 136]]
30	ents) *100%  ? F1-score = 2*P*R / (P+R)  Two correctness criteria are used for constitu-	[[35, 38]]	[[26, 31]]	['P+R']	['2*P*R']	[]	[]	[]	[]
31	 4 Corpus description The GRN corpus is a set of 201 sentences selected from PubMed abstracts, which are  mainly about the sporulation phenomenon in Bacillus subtilis. This corpus is an extended version of the LLL and BI (BioNLP-ST?11) corpora. The additional sentences ensure a better coverage of the description of the sporulation.	[[218, 220], [210, 213], [26, 29], [77, 83]]	[[222, 228]]	['BI', 'LLL', 'GRN', 'PubMed']	['BioNLP']	['GRN', 'LLL', 'BI']	[]	[[26, 29], [210, 213], [218, 220]]	[]
32	The DlmSum Summarization Clientprovldes a sum-  mary of a document in multiple dlmeuslons through  a graphical user interface (GUI) to smt dflferent  users' needs In contrast o a static view of a doc- 	[[127, 130]]	[[101, 125]]	['GUI']	['graphical user interface']	['GUI']	['a', 'a', 'a graphical user interface', 'users', 'o a', 'a']	[[127, 130]]	[]
33	is encoded in the attributes MODALITY and POLARITY. Modality at the syntactic level is encoded as an attribute of the tag SLINK (Subordination Link), which can have several values: factive, counterfactive, evidential, negative evidential, modal,	[[122, 127]]	[[129, 147]]	['SLINK']	['Subordination Link']	['SLINK']	['in', 'at', 'Subordination Link']	[[122, 127]]	[[129, 147]]
34	So, the  first matrix (with the data from IBL trained with Encoding 1) shows in its upper row  the classification of the words that have final stress (FIN). It appears that the classifier 	[[151, 154], [42, 45]]	[[137, 142]]	['FIN', 'IBL']	['final']	['IBL', 'FIN']	['final stress']	[[42, 45], [151, 154]]	[]
35	stream. In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI), 20?29. 	[[89, 92]]	[[49, 87]]	['UAI']	['Uncertainty in Artificial Intelligence']	['UAI']	['In', 'Uncertainty in Artificial Intelligence']	[[89, 92]]	[[49, 87]]
36	We propose a method to solve this problem, which also results in a new topic model, called AKL (Automated Knowledge LDA), whose inference can exploit the automatically learned	[[91, 94]]	[[96, 119]]	['AKL']	['Automated Knowledge LDA']	['AKL']	['a', 'a', 'Automated Knowledge LDA']	[[91, 94]]	[[96, 119]]
37	lated work. We then introduce Markov logic and our Markov Logic Network (MLN) for joint bio-event extraction.	[[73, 76]]	[[51, 71]]	['MLN']	['Markov Logic Network']	['MLN']	['Markov', 'Markov Logic Network']	[[73, 76]]	[[51, 71]]
38	 We explore how to utilize the source-language test corpus for adapting the language model (LM) and the translation model (TM).	[[92, 94], [123, 125]]	[[76, 90], [104, 121]]	['LM', 'TM']	['language model', 'translation model']	['LM', 'TM']	['language model', 'translation model']	[[92, 94], [123, 125]]	[[76, 90], [104, 121]]
39	INAN=inanimate NP, ANIM=animate NP, VBZ--inflected  main verb, IS=is, VBG=gerund, PP=prepositional phrase,  TO=to (prep.), ONmon (prep.).	[[108, 110], [0, 4], [15, 17], [19, 23], [32, 34], [36, 39], [63, 65], [70, 73], [82, 84]]	[[111, 113], [5, 14], [24, 31], [66, 68], [74, 80], [85, 105]]	['TO', 'INAN', 'NP', 'ANIM', 'NP', 'VBZ', 'IS', 'VBG', 'PP']	['to', 'inanimate', 'animate', 'is', 'gerund', 'prepositional phrase']	['NP']	['NP', 'verb', 'phrase', 'prep', 'prep']	[[15, 17], [32, 34]]	[]
40	and WLLR in this paper.  3.3 Information Gain (IG)  IG measures the number of bits of information 	[[47, 49], [4, 8], [52, 54]]	[[29, 45]]	['IG', 'WLLR', 'IG']	['Information Gain']	['WLLR', 'IG']	['in', 'Information Gain']	[[4, 8], [47, 49], [52, 54]]	[[29, 45]]
41	Three query tasks were defined in TREC-2003 and TREC-2004 web track, which are home page finding (HP), named page finding (NP) and topic distillation (TD) (Voorhees, 2003; Voorhees, 2004).	[[123, 125], [34, 38], [48, 52], [98, 100], [151, 153]]	[[103, 113], [79, 88], [131, 149]]	['NP', 'TREC', 'TREC', 'HP', 'TD']	['named page', 'home page', 'topic distillation']	['HP', 'NP', 'TD']	['in', 'home page finding', 'page finding', 'topic distillation']	[[98, 100], [123, 125], [151, 153]]	[[131, 149]]
42	329 Figure 1: The different structures (left: constituency trees, right: predicate argument structure) derived from Sentence (1) for the opinion holder candidate Malaysia used as input for convolution kernels (CK). 	[[210, 212]]	[[189, 208]]	['CK']	['convolution kernels']	['CK']	['convolution kernels']	[[210, 212]]	[[189, 208]]
43	There are two other threads of research literature relevant to our work. Named entity (NE) extraction attempts to identify entities of interest	[[87, 89]]	[[73, 85]]	['NE']	['Named entity']	['NE']	['Named entity']	[[87, 89]]	[[73, 85]]
44	Eat_On_Time? Theme 3.3 Planning for Linguistic Variation Sentences generated by Picture Books vary in length and word complexity according to the user?s age. The variation in length is handled by specificity character goals (SCG) that are ap-pended as supporting details to their respective character goals. SCGs are designed so that their existence will give more detail to the preceding character goal while ensuring consistency, and that their non-existence will still make the story complete.	[[225, 228], [308, 312]]	[[196, 223]]	['SCG', 'SCGs']	['specificity character goals']	['SCG']	['s', 'is', 'specificity character goals', 'character goals', 'character goal']	[[225, 228]]	[[196, 223]]
45	correct class of an item from its context. The Maximum Entropy (MaxEnt) framework is especially suited for integrating evidence from var-	[[64, 70]]	[[47, 62]]	['MaxEnt']	['Maximum Entropy']	['MaxEnt']	['Maximum Entropy']	[[64, 70]]	[[47, 62]]
46	 and  As for perceptron criterion, we employ the  average perceptron (AvgP) (Freund and  Sc	[[70, 74]]	[[50, 68]]	['AvgP']	['average perceptron']	['AvgP']	['perceptron', 'average perceptron']	[[70, 74]]	[[50, 68]]
47	               XOAJC + LC*Ave(XOAJC)/Ave(XOAR)  The second strategy consists in building a prediction model for BLAST bit score (BBS) using the  XOA score and the log-cosine LC as predictors 	[[129, 132], [41, 45], [30, 35], [15, 20], [23, 25], [145, 148], [174, 176]]	[[112, 127]]	['BBS', 'XOAR', 'XOAJC', 'XOAJC', 'LC', 'XOA', 'LC']	['BLAST bit score']	['XOAJC', 'LC', 'XOAR', 'BBS']	['a', 'BLAST bit score', 'score']	[[30, 35], [15, 20], [23, 25], [174, 176], [41, 45], [129, 132]]	[[112, 127]]
48	22M 35.88 27.16 30.20 36.21 27.26 30.48 -0.33 -0.10 -0.28 Table 2: BLEU Score results for the Spanish Treelet Penalty experiments EX Treelet Phrasal Diff (T-P) Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010	[[155, 158], [67, 71], [130, 132], [177, 180], [194, 197], [203, 206], [220, 223], [229, 232]]	[[133, 148]]	['T-P', 'BLEU', 'EX', 'WMT', 'WMT', 'WMT', 'WMT', 'WMT']	['Treelet Phrasal']	['EX', 'T-P', 'WMT', 'WMT']	['Treelet', 'Treelet Phrasal Diff']	[[130, 132], [155, 158], [177, 180], [194, 197], [203, 206], [220, 223], [229, 232], [177, 180], [194, 197], [203, 206], [220, 223], [229, 232]]	[]
49	Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs	[[84, 86], [49, 51], [130, 132]]	[[65, 82], [32, 48], [100, 129]]	['BK', 'QK', '8K']	['Biquadratic Kerne', 'Quadratic Kernel', '8-th Degree Polynomial Kernel']	['QK', 'BK']	['Quadratic Kernel', 'Biquadratic Kernel', 'Kernel']	[[49, 51], [84, 86]]	[[32, 48]]
50	mented in song sentiment classification, i.e. audiobased (AB) approach, knowledge-based (KB) approach and machine learning (ML) approach, in  which the latter two approaches are also referred to 	[[124, 126], [58, 60], [89, 91]]	[[106, 122], [46, 56], [72, 87]]	['ML', 'AB', 'KB']	['machine learning', 'audiobased', 'knowledge-based']	['AB', 'KB', 'ML']	['in', 'audiobased', 'knowledge-based', 'machine learning', 'in']	[[58, 60], [89, 91], [124, 126]]	[[46, 56], [72, 87], [106, 122]]
51	standard measures such as AUC values. For each positive+unlabeled (PU) corpus used in our evaluation we randomly selected x% of the positive ex-	[[67, 69], [26, 29]]	[[47, 65]]	['PU', 'AUC']	['positive+unlabeled']	['AUC', 'PU']	['positive+unlabeled', 'positive']	[[26, 29], [67, 69]]	[[47, 65]]
52	3 Experimental Results and Discussion We conducted closed track experiments on the Hong Kong City University (CityU) corpus in The Second International Chinese Word Segmen-	[[110, 115]]	[[93, 108]]	['CityU']	['City University']	[]	['City University']	[]	[[93, 108]]
53	related to the segments bi and bj .  We obtain Dbest = argmaxD P (D|B) taking into all the combination of these probabilities.	[[66, 69]]	[[47, 52]]	['D|B']	['Dbest']	['D|B']	['bi', 'Dbest', 'argmaxD P']	[[66, 69]]	[[47, 52]]
54	DOC system, and they observe a close correspondence.  We have employed Functional Grammar (FG) (c.f \[6\])  as a principal analysis tool to developing representations 	[[91, 93], [0, 3]]	[[71, 89]]	['FG', 'DOC']	['Functional Grammar']	['DOC', 'FG']	['a', 'Functional Grammar', 'a']	[[0, 3], [91, 93]]	[[71, 89]]
55	to generate these features from the training data.  In the NE (named entities) feature ? PERSON?	[[59, 61], [89, 96]]	[[63, 77]]	['NE', 'PERSON?']	['named entities']	['NE']	['named entities']	[[59, 61]]	[[63, 77]]
56	WSD, which rely on knowledge represented as  attribute-value vectors: C4.5 (decision-trees),  Naive Bayes and Support Vector Machine (SVM)1. 	[[134, 137], [0, 3]]	[[110, 132]]	['SVM', 'WSD']	['Support Vector Machine']	['WSD', 'SVM']	['Support Vector Machine']	[[0, 3], [134, 137]]	[[110, 132]]
57	Intensional Allan Ramsay University of Manchester (UK) email: allan.ramsay@manchester.ac.uk	[[51, 53]]	[[25, 49]]	['UK']	['University of Manchester']	[')']	['University of Manchester']	[]	[[25, 49]]
58	 6 BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire;	[[3, 5], [33, 35], [54, 57], [93, 95]]	[[8, 31], [38, 52], [60, 91], [98, 106]]	['BC', 'BN', 'CTS', 'NW']	['Broadcast Conversations', 'Broadcast News', 'Conversational Telephone Speech', 'Newswire']	['BC', 'BN', 'CTS', 'NW']	['Broadcast Conversations', 'Broadcast', 'News', 'Conversational Telephone', 'Newswire']	[[3, 5], [33, 35], [54, 57], [93, 95]]	[[8, 31], [98, 106]]
59	3.1 Overview   Graph-based Representation  Attribute Relation Graph (ARG) (Tsai and Fu, 1979)  is used to represent information in our approach.	[[69, 72]]	[[43, 67]]	['ARG']	['Attribute Relation Graph']	[')']	['Representation Attribute Relation Graph']	[]	[]
60	 Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a linguistically informed definition of the search space	[[83, 85]]	[[65, 81]]	['DL']	['distortion limit']	['DL']	['a', 'low distortion limit', 'a']	[[83, 85]]	[]
61	 3.2 HL-MRFs for Tweet Stance Classification Finding the maximum a posteriori (MAP) state is a difficult discrete optimization problem and, in gen-	[[79, 82], [5, 11]]	[[57, 77]]	['MAP', 'HL-MRF']	['maximum a posteriori']	['HL-MRFs', 'MAP']	['maximum a posteriori', 'a']	[[79, 82]]	[[57, 77]]
62	1 Introduction Part-Of-Speech(POS) tagging is the essential basis of Natural language processing(NLP). It is the pro-	[[97, 100], [30, 33]]	[[69, 95], [15, 29]]	['NLP', 'POS']	['Natural language processin', 'Part-Of-Speech']	[]	['of', 'Natural language processing']	[]	[]
63	We therefore use a different type of filter in order to detect these errors, which we call the Verb Arity Sampling Test (VAST). 	[[121, 125]]	[[95, 119]]	['VAST']	['Verb Arity Sampling Test']	['VAST']	['a', 'in', 'Verb Arity Sampling Test']	[[121, 125]]	[[95, 119]]
64	that is as close as possible to the gold standard C. Most work on verb clustering has used the Fmeasure or the Rand Index (RI) (Rand, 1971) for evaluation, which rely on counting pairwise	[[123, 125]]	[[111, 121]]	['RI']	['Rand Index']	['RI']	['or', 'Rand Index', 'Rand']	[[123, 125]]	[[111, 121]]
65	and test splits, and bias the results.  We trained a Support Vector Machine (SVM) for regression with RBF kernel using scikit-learn (Pe-	[[77, 80], [102, 105]]	[[53, 75]]	['SVM', 'RBF']	['Support Vector Machine']	['SVM', 'RBF']	['a Support Vector Machine']	[[77, 80], [102, 105]]	[]
66	Output (0 < x < 1)  Figure 3  Neural network architecture (DA = descriptor array of 20 items). 	[[59, 61]]	[[64, 80]]	['DA']	['descriptor array']	['DA']	['Neural network architecture', 'descriptor array']	[[59, 61]]	[[64, 80]]
67	Extraction (from now on AVE) is performed with two approaches: a) Rule-based approaches apply Regular Expressions (RE) to map the words realizing a concept into a normalized value.	[[115, 117], [24, 27]]	[[94, 113]]	['RE', 'AVE']	['Regular Expressions']	['AVE', 'RE']	['on', 'a', 'Regular Expressions', 'a', 'a']	[[24, 27], [115, 117]]	[[94, 113]]
68	to-fine n-Best Parsing and MaxEnt Discriminative Reranking Proceedings of the 43th Annual Meeting on Association for Computational Linguistics(ACL) 173?180.	[[143, 146]]	[[101, 141]]	['ACL']	['Association for Computational Linguistic']	['ACL']	['on Association for Computational Linguistics']	[[143, 146]]	[]
69	We annotate  the semantic roles for the 50 most frequent verbs in  the Quranic Arabic Dependency Treebank (QATB)  (Dukes and Buckwalter 2010).	[[107, 111]]	[[71, 105]]	['QATB']	['Quranic Arabic Dependency Treebank']	['QATB']	['Quranic Arabic Dependency Treebank']	[[107, 111]]	[[71, 105]]
70	We parse each corpus sentence pair using the OpenCCG parser to yield a logical form (LF) as a semantic dependency graph with the gold-standard alignments projected	[[85, 87], [45, 52]]	[[71, 83]]	['LF', 'OpenCCG']	['logical form']	['LF']	['a logical form', 'a']	[[85, 87]]	[]
71	WDS 0.931 [0.905, 0.958] 0.813 [0.738, 0.887] 0.872 R=Random; LR=LexRank; DR=DivRank; DR(p)=DivRank with Priors; CLR=C-LexRank; WDS=Word Distributional Similarity; C.I.=Confidence Interval	[[128, 131], [0, 3], [62, 64], [74, 76], [86, 91], [113, 116], [164, 167]]	[[132, 162], [54, 60], [65, 72], [77, 84], [92, 111], [117, 126], [169, 188]]	['WDS', 'WDS', 'LR', 'DR', 'DR(p)', 'CLR', 'C.I']	['Word Distributional Similarity', 'Random', 'LexRank', 'DivRank', 'DivRank with Priors', 'C-LexRank', 'Confidence Interval']	['WDS', 'DR']	[';', 'p', 'with Priors', 'Distributional Similarity']	[[128, 131], [0, 3], [74, 76]]	[]
72	(PART+), (e.g., + s+ will [future]). Most shallow is the class of conjunctions (CONJ+), (e.g., +  w+	[[81, 86], [1, 6]]	[[67, 79]]	['CONJ+', 'PART+']	['conjunctions']	['CONJ+']	['conjunctions']	[[81, 86]]	[[67, 79]]
73	1 In t roduct ion   This paper deals with the discovery, representation,  and use of lexical rules (LRs) in the process of large-  scale semi-automatic computational lexicon acqui- 	[[100, 103]]	[[85, 98]]	['LRs']	['lexical rules']	['LRs']	['lexical rules']	[[100, 103]]	[[85, 98]]
74	).  Rand Index (RI) (Rand, 1971) measures the percentage of decisions that are correct, penalizing false pos-	[[16, 18]]	[[4, 14]]	['RI']	['Rand Index']	['RI']	['Rand Index', 'Rand']	[[16, 18]]	[[4, 14]]
75	uew stngc in the development toward a grammar and  style chc~ker can emerge: the organization of an  algorithmic on(foiled grammar (ALCOGRAM). The 	[[132, 140]]	[[101, 130]]	['ALCOGRAM']	['algorithmic on(foiled grammar']	['ALCOGRAM']	['a grammar', '( foiled grammar']	[[132, 140]]	[]
76	Since many responses in ETLA are expected to  follow certain patterns, it is intuitive to construct  limited regular expressions (RegEx) to match gold  standard responses for candidates with high profi-	[[130, 135], [24, 28]]	[[109, 128]]	['RegEx', 'ETLA']	['regular expressions']	['ETLA', 'RegEx']	['regular expressions']	[[24, 28], [130, 135]]	[[109, 128]]
77	advP Adverb phrase(ADVP)  punct Punctuation(,)  adjP Adjective phrase(ADJP)  OP  advP, np and/or pp 	[[70, 74], [19, 23], [48, 52], [77, 79], [81, 85], [97, 99], [87, 89]]	[[53, 68], [5, 18]]	['ADJP', 'ADVP', 'adjP', 'OP', 'advP', 'pp', 'np']	['Adjective phras', 'Adverb phrase']	['advP', 'ADVP', 'ADJP']	['Adverb phrase', 'Adjective phrase']	[[81, 85], [19, 23], [70, 74]]	[[5, 18]]
78	composition algorithms have been proposed in the meantime. Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version	[[90, 93]]	[[65, 88]]	['MET']	['Memory Efficient Tucker']	['MET']	['Memory Efficient Tucker']	[[90, 93]]	[[65, 88]]
79	tion probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). 	[[149, 151]]	[[133, 147]]	['LM']	['language model']	['LM']	['model', 'a', 'a', 'a', 'language model']	[[149, 151]]	[[133, 147]]
80	AST = Adjectival Sta~ VST = Verb Stem  DET = Determine~ N-FLEX = Nominal Inflexion  NST = Noun Stem V-FLEX = Verbal Inflsxion  PRN = Pronoun A-FLEX = Adjectival Inflexion 	[[84, 87], [0, 3], [22, 25], [39, 42], [56, 62], [100, 106], [127, 130], [141, 147]]	[[90, 99], [6, 20], [28, 37], [45, 54], [65, 82], [109, 125], [133, 140], [150, 170]]	['NST', 'AST', 'VST', 'DET', 'N-FLEX', 'V-FLEX', 'PRN', 'A-FLEX']	['Noun Stem', 'Adjectival Sta', 'Verb Stem', 'Determine', 'Nominal Inflexion', 'Verbal Inflsxion', 'Pronoun', 'Adjectival Inflexion']	['AST', 'VST', 'DET', 'N-FLEX', 'PRN', 'A-FLEX']	['Adjectival Sta~', 'Verb Stem', 'Determine~', 'Nominal', 'Inflexion', 'Noun Stem', 'Verbal Inflsxion', 'Pronoun']	[[0, 3], [22, 25], [39, 42], [56, 62], [127, 130], [141, 147]]	[[28, 37], [90, 99], [109, 125], [133, 140]]
81	 1 Introduction The use of Support Vector Machines (SVMs) in supervised learning frameworks is spreading	[[52, 56]]	[[27, 50]]	['SVMs']	['Support Vector Machines']	[')']	['Vector Machines', 'in']	[]	[]
82	We have tested our algorithms on both the handcoded tag set used in (Chen et al, 1999) and supertags extracted for Penn Treebank(PTB). On the	[[129, 132]]	[[115, 128]]	['PTB']	['Penn Treebank']	['PTB']	['Penn Treebank']	[[129, 132]]	[[115, 128]]
83	non-standard token?s formation process.  Machine translation (MT) is another commonly chosen method for text normalization.	[[62, 64]]	[[41, 60]]	['MT']	['Machine translation']	['MT']	['Machine translation']	[[62, 64]]	[[41, 60]]
84	y?i / _ + [+ANY] Features: VWL = vowel ANY = any char.	[[27, 30], [39, 42]]	[[33, 38], [45, 53]]	['VWL', 'ANY']	['vowel', 'any char']	['VWL', 'ANY']	['y', 'vowel', 'any char']	[[27, 30], [39, 42]]	[[33, 38], [45, 53]]
85	The research systems were:  ? CANDIDE (IBM Research: French - English(FE)),  produced both FA and human-assisted (HA) outputs.	[[70, 72], [39, 42], [91, 93], [114, 116], [30, 37]]	[[53, 68], [98, 112]]	['FE', 'IBM', 'FA', 'HA', 'CANDIDE']	['French - Englis', 'human-assisted']	['CANDIDE', 'IBM', 'FE', 'FA', 'HA']	['French - English', 'human-assisted']	[[30, 37], [39, 42], [70, 72], [91, 93], [114, 116]]	[[98, 112]]
86	"An Organization for a Dictionary of Word Senses  3. A ""Sense Data Item"" (SDI) represents one distinct sense common to a set of wordq  and/or phrases."	[[73, 76]]	[[55, 70]]	['SDI']	['Sense Data Item']	['SDI']	['a', 'Sense Data Item']	[[73, 76]]	[[55, 70]]
87	tion (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our focus is specifically	[[125, 128]]	[[93, 123]]	['CCG']	['combinatory categorial grammar']	['CCG']	['a combinatory categorial grammar']	[[125, 128]]	[]
88	we annotate ? Chen? as the Agent (ARG0)  of the full predicate ?[	[[34, 38]]	[[20, 32]]	['ARG0']	['as the Agent']	['ARG0']	['Agent']	[[34, 38]]	[]
89	Table 3: The configurations of our systems. The abbreviations in the last column mean  training set(TS) and validating set(VS) explaining in section 5.1. 	[[100, 102], [123, 125]]	[[87, 99], [108, 122]]	['TS', 'VS']	['training set', 'validating set']	['TS', 'VS']	['in', 'mean training set', 'validating set', 'in']	[[100, 102], [123, 125]]	[[108, 122]]
90	  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1804?1809, October 25-29, 2014, Doha, Qatar.	[[90, 95]]	[[40, 88]]	['EMNLP']	['Empirical Methods in Natural Language Processing']	['),']	['Methods in Natural Language Processing (']	[]	[]
91	1 Introduction Many text understanding applications, such as Question Answering (QA) and Information Extraction (IE), need to infer a target textual mean-	[[81, 83], [113, 115]]	[[61, 79], [89, 111]]	['QA', 'IE']	['Question Answering', 'Information Extraction']	['QA', 'IE']	['Question Answering', 'Information Extraction', 'a']	[[81, 83], [113, 115]]	[[61, 79], [89, 111]]
92	Given query Q = (q1, ? ? ? , qL), for each document D, expected term frequencies (ETF) of all sub-strings Q[i,j] = (qi, ? ? ? ,	[[82, 85]]	[[55, 80]]	['ETF']	['expected term frequencies']	['ETF']	['expected term frequencies', 'i']	[[82, 85]]	[[55, 80]]
93	whole of MARY = KMary>.  referent of MARY = Mary. 	[[37, 41], [9, 13]]	[[44, 48], [17, 21]]	['MARY', 'MARY']	['Mary', 'Mary']	['MARY']	['KMary', 'Mary']	[[37, 41], [9, 13]]	[[44, 48], [17, 21]]
94	Abbreviations: Trig./Arg./Group./Modif.=event trigger detection/argument detection/argument grouping/modification detection, BI=Bioinformatician, NLP=Natural Language Processing researcher, CS=Computer scientist, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snowball=Snowball stemmer McCCJ=McClosky-Charniak-Johnson parser, LGP=Link Grammar Parser, SD=Stanford De-	[[190, 192], [353, 355], [328, 331], [288, 293], [213, 220], [230, 237]]	[[193, 211], [356, 364], [332, 351], [294, 319]]	['CS', 'SD', 'LGP', 'McCCJ', 'CoreNLP', 'CoreNLP']	['Computer scientist', 'Stanford', 'Link Grammar Parser', 'McClosky-Charniak-Johnson']	[]	['Language Processing', 'scientist', 'CoreNLP', 'stemmer', 'stemmer', 'Grammar']	[]	[]
95	data are used to train a classifier, using any cost-sensitive classification (CSC) method (line 15). New pi	[[78, 81]]	[[47, 76]]	['CSC']	['cost-sensitive classification']	['CSC']	['a', 'cost-sensitive classification']	[[78, 81]]	[[47, 76]]
96	Learning Summary Content Units with Topic Modeling Leonhard Hennig Ernesto William De Luca Distributed Artificial Intelligence Laboratory (DAI-Lab) Technische Universita?t Berlin	[[139, 146]]	[[91, 137]]	['DAI-Lab']	['Distributed Artificial Intelligence Laboratory']	['DAI-Lab']	['Artificial Intelligence Laboratory', 't']	[[139, 146]]	[]
97	analysis as discussed in the next section.  Analysis of Variance (ANOVA) tests were performed on the full 5 years for each sector, to com-	[[66, 71]]	[[44, 64]]	['ANOVA']	['Analysis of Variance']	['ANOVA']	['Analysis of Variance']	[[66, 71]]	[[44, 64]]
98	 Connectivity Strength Features provide two scores, Source Connectivity Strength (SCS) and Target Connectivity Strength (TCS).	[[82, 85], [121, 124]]	[[52, 80], [91, 119]]	['SCS', 'TCS']	['Source Connectivity Strength', 'Target Connectivity Strength']	['SCS', 'TCS']	['Connectivity', 'Strength', 'Source Connectivity Strength', 'Target Connectivity Strength']	[[82, 85], [121, 124]]	[[52, 80], [91, 119]]
99	Since finding the optimal set of edges respecting transitivity is NP-hard, they employed Integer Linear Programming (ILP) to find the exact solution.	[[117, 120]]	[[89, 115]]	['ILP']	['Integer Linear Programming']	['ILP']	['Integer Linear Programming']	[[117, 120]]	[[89, 115]]
100	The e-rater system TM ~ is an operational  automated essay scoring system, developed  at Educational Testing Service (ETS). The 	[[118, 121], [19, 21]]	[[89, 116]]	['ETS', 'TM']	['Educational Testing Service']	['TM', 'ETS']	['an', 'at Educational Testing Service']	[[19, 21], [118, 121]]	[]
101	mensionality reduction, such as Latent Semantic Analysis (LSA) in (Pad?o and Lapata, 2007) or Non-negative Matrix Factorization (NMF) (Zheng et al.,	[[129, 132], [58, 61]]	[[98, 127], [32, 56]]	['NMF', 'LSA']	['negative Matrix Factorization', 'Latent Semantic Analysis']	['LSA', 'NMF']	['Latent Semantic Analysis', 'o', 'or Non-negative Matrix Factorization']	[[58, 61], [129, 132]]	[[32, 56]]
102	of the adjacency pair involving speaker B, we use four categories of features: structural, durational, lexical, and dialog act (DA) information. For the	[[128, 130]]	[[116, 126]]	['DA']	['dialog act']	['DA']	['dialog act']	[[128, 130]]	[[116, 126]]
103	 3 Latent Semantic Analysis Latent Semantic Analysis (LSA) (Deerwester et al 1990) is a widely used continuous vector space	[[54, 57]]	[[28, 52]]	['LSA']	['Latent Semantic Analysis']	[')']	['Semantic Analysis', 'Semantic Analysis', 'is a']	[]	[]
104	2. Corpus Resource  This study uses the Switchboard Dialog Act (SWBD-DA)  Corpus as the corpus resource, which is available online 	[[64, 71]]	[[40, 62]]	['SWBD-DA']	['Switchboard Dialog Act']	['SWBD-DA']	['Switchboard Dialog Act']	[[64, 71]]	[[40, 62]]
105	reflecting the distribution of the genres in the MTC (Zeyrek et al 2009). The main objective of the project is to annotate discourse connectives with their two arguments, modifiers and supplementary text spans. Following the Penn Discourse Tree Bank (PDTB), we take discourse connectives as discourse-level predicates taking two (and only  two) arguments, called Arg1 and Arg2, which may span one or more clauses and sentences that are adjacent or nonadjacent to the connective (Prasad et al 2007, Webber, 2004).	[[251, 255], [49, 52]]	[[225, 249]]	['PDTB', 'MTC']	['Penn Discourse Tree Bank']	['MTC', 'PDTB']	['is', 'Penn Discourse Tree Bank']	[[49, 52], [251, 255]]	[[225, 249]]
106	data. The first is a parser trained on the standard training sections of the PennTreebank (PTB) and the second is a parser trained on the training por-	[[91, 94]]	[[77, 89]]	['PTB']	['PennTreebank']	['PTB']	['a', 'PennTreebank', 'a']	[[91, 94]]	[[77, 89]]
107	query large amounts of data is a key requirement for data-driven dialog systems, in which the data is generated by the spoken dialog system (SDS) components (spoken language understanding (SLU), di-	[[141, 144], [189, 192]]	[[119, 139], [158, 187]]	['SDS', 'SLU']	['spoken dialog system', 'spoken language understanding']	['SDS', 'SLU']	['a', 'in', 'spoken dialog system', 'spoken language understanding']	[[141, 144], [189, 192]]	[[119, 139], [158, 187]]
108	Ah receptor recognizes the B cell transcription factor, BSAP (b) Grf40 binds to linker for activation of T cells (LAT) (c)	[[114, 117], [56, 60]]	[[80, 106]]	['LAT', 'BSAP']	['linker for activation of T']	['Grf40']	['B', 'cell', 'activation of T cells', 'c']	[]	[]
109	pendencies. Hochreiter and Schmidhuber (1997), thus proposed long short term memory (LSTMs), a variant of recurrent neural networks.	[[85, 90]]	[[61, 83]]	['LSTMs']	['long short term memory']	['LSTMs']	['long short term memory', 'a']	[[85, 90]]	[[61, 83]]
110	the affect projection rules are useful. However, when we use automated coreference (ACoref), recall goes down and precision goes up.	[[84, 90]]	[[61, 82]]	['ACoref']	['automated coreference']	['ACoref']	['automated coreference']	[[84, 90]]	[[61, 82]]
111	tion) strategies. All systems are evaluated according to their Mean Average Precision 6 (MAP) as computed by the trec eval software on the pre-	[[89, 92]]	[[63, 85]]	['MAP']	['Mean Average Precision']	['MAP']	['tion', 'Mean Average Precision', 'on']	[[89, 92]]	[[63, 85]]
112	HN=highly negative terms, N=negative, P=positive, HP=highly positive, INV=invertors, DIM=diminishers, INV=invertors. 	[[85, 88], [0, 2], [50, 52], [70, 73], [102, 105]]	[[89, 100], [3, 18], [28, 36], [40, 48], [53, 68], [74, 83], [106, 115]]	['DIM', 'HN', 'HP', 'INV', 'INV']	['diminishers', 'highly negative', 'negative', 'positive', 'highly positive', 'invertors', 'invertors']	[]	['negative', 'positive']	[]	[[28, 36], [40, 48]]
113	4.1 Test Dataset The dataset used in our experiments comes from the Automated Student Assessment Prize (ASAP)1, which is sponsored by the William and Flora	[[104, 108]]	[[68, 102]]	['ASAP']	['Automated Student Assessment Prize']	['ASAP']	['Automated Student Assessment Prize']	[[104, 108]]	[[68, 102]]
114	 These are typically expressed in simple syntactic frames called subcategorization frames (SCFs). 	[[91, 95]]	[[65, 89]]	['SCFs']	['subcategorization frames']	['SCFs']	['frames', 'subcategorization frames']	[[91, 95]]	[[65, 89]]
115	 1 Introduction Assigning each word its most frequent sense (MFS) is commonly used as a baseline in Word Sense Dis-	[[61, 64]]	[[40, 59]]	['MFS']	['most frequent sense']	[')']	['frequent sense', 'is']	[]	[]
116	of CoNLL-2000 and LLL-2000, Lisbon, Portugal.  Tageszeitung (TAZ) Corpus. Contrapress Media GmbH.	[[61, 64], [3, 13], [18, 26], [92, 96]]	[[47, 59]]	['TAZ', 'CoNLL-2000', 'LLL-2000', 'GmbH']	['Tageszeitung']	['CoNLL-2000', 'LLL-2000', 'TAZ']	['Tageszeitung']	[[3, 13], [18, 26], [61, 64]]	[[47, 59]]
117	wisdom of crowds. Our approach is based on the Latent Mixture of Discriminative Experts (LMDE) model originally introduced for multimodal fu-	[[89, 93]]	[[47, 87]]	['LMDE']	['Latent Mixture of Discriminative Experts']	['LMDE']	['of', 'Latent Mixture of Discriminative Experts']	[[89, 93]]	[[47, 87]]
118	In Proceedings of the 22nd International Conference on World Wide Web (WWW), pp. 1009-1020, 2013.	[[71, 74]]	[[55, 69]]	['WWW']	['World Wide Web']	['WWW', 'pp']	['World Wide Web']	[[71, 74]]	[[55, 69]]
119	When the morphophonology returns multiple parse candidates, the system employs an N -gram language model (LM) 21	[[106, 108], [82, 89]]	[[90, 104]]	['LM', 'N -gram']	['language model']	['LM']	['an', 'language model']	[[106, 108]]	[[90, 104]]
120	fore and after the SVD improved results slightly.  For the accumulated tag counts (ACT) we annotate the data with our baseline model and extract word-	[[83, 86]]	[[59, 81]]	['ACT']	['accumulated tag counts']	['SVD', 'ACT']	['accumulated tag counts']	[[83, 86]]	[[59, 81]]
121	cal markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting. The	[[108, 110]]	[[89, 106]]	['OT']	['Optimality Theory']	['OT']	['Optimality Theory']	[[108, 110]]	[[89, 106]]
122	sense as a group of similar contexts of target word.  The context group discrimination (CGD) algorithm presented in (Schu?tze, 1998) adopted this strategy.	[[88, 91]]	[[58, 86]]	['CGD']	['context group discrimination']	['CGD']	['a group', 'context group discrimination']	[[88, 91]]	[[58, 86]]
123	on these intuitions, we define the following features: Document frequency of constituents (DF): We use the document frequency of a constituent as	[[91, 93]]	[[55, 73]]	['DF']	['Document frequency']	['DF']	['Document frequency of constituents', 'frequency of a constituent']	[[91, 93]]	[]
124	  To address the issues in transliteration, we  propose a direct orthographic mapping (DOM)  framework through a joint source-channel model 	[[87, 90]]	[[58, 85]]	['DOM']	['direct orthographic mapping']	[')']	['in', 'a', 'mapping', 'a']	[]	[]
125	parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).13 Statistical significance is checked using Dan Bikel?s	[[127, 130], [90, 93]]	[[99, 125], [64, 88]]	['UAS', 'LAS']	['unlabeled attachment score', 'labeled attachment score']	['LAS', 'UAS']	['labeled attachment score', 'unlabeled attachment score']	[[90, 93], [127, 130]]	[[64, 88], [99, 125]]
126	veloped for machine learning and data mining, to  determine the data classification performance of  support vector machine (SVM) learning on the                                                   	[[124, 127]]	[[100, 122]]	['SVM']	['support vector machine']	['SVM']	['machine', 'to', 'support vector machine']	[[124, 127]]	[[100, 122]]
127	the work of language specialists. They often need to perform extensive corpus research, including Natural Language Processing (NLP), statistical modelling and data visualisation. Our 	[[127, 130]]	[[98, 125]]	['NLP']	['Natural Language Processing']	['NLP']	['Natural Language Processing']	[[127, 130]]	[[98, 125]]
128	are shown in Table 4. Not surprisingly, using all gazetteer features (AllG) boosts the F1 score from 85.14 % to 88.30%, confirming the power	[[70, 74]]	[[46, 59]]	['AllG']	['all gazetteer']	['AllG']	['all gazetteer']	[[70, 74]]	[[46, 59]]
129	 A simple approach is presented in (Cardie and  Pierce, 1998) called Treebank Apl)roach (TA). This 	[[89, 91]]	[[69, 87]]	['TA']	['Treebank Apl)roach']	['TA']	['A', 'Treebank', 'roach']	[[89, 91]]	[]
130	respectively. Hacioglu et al (2004) showed that  tagging phrase by phrase (P-by-P) is better than  word by word (W-by-W).	[[75, 81], [113, 119]]	[[57, 73], [99, 111]]	['P-by-P', 'W-by-W']	['phrase by phrase', 'word by word']	[')']	['phrase by phrase', 'word by word']	[]	[[57, 73], [99, 111]]
131	 Since precision is measured as the proportion of true positives (TP) to the sum of true positives and false positives (FP): 1748	[[120, 122], [66, 68]]	[[103, 118], [50, 64]]	['FP', 'TP']	['false positives', 'true positives']	['TP', 'FP']	['true positives', 'true positives', 'false positives']	[[66, 68], [120, 122]]	[[50, 64], [50, 64], [103, 118]]
132	tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, Massachusetts, USA.	[[89, 93]]	[[48, 87]]	['CVPR']	['Computer Vision and Pattern Recognition']	['IEEE', 'CVPR', 'USA']	['Conference', 'on Computer Vision and Pattern Recognition']	[[89, 93]]	[]
133	sponsored by the U.S. government. ACE 2004  defined 7 major entity types: PER (Person), ORG  (Organization), FAC (Facility), GPE (Geo-Political 	[[74, 77], [17, 20], [34, 37], [88, 91], [109, 112], [125, 128]]	[[79, 85], [94, 106], [114, 122], [130, 143]]	['PER', 'U.S', 'ACE', 'ORG', 'FAC', 'GPE']	['Person', 'Organization', 'Facility', 'Geo-Political']	['U.S.', 'ACE', 'PER', 'ORG', 'FAC', 'GPE']	['Person', 'Organization', 'Facility']	[[34, 37], [74, 77], [88, 91], [109, 112], [125, 128]]	[[79, 85], [94, 106], [114, 122]]
134	Semantic Role Labeling (SRL) has been used successfully in several stages of automated Question Answering (QA) systems but its inherent slow procedures make it	[[107, 109], [24, 27]]	[[87, 105], [0, 22]]	['QA', 'SRL']	['Question Answering', 'Semantic Role Labeling']	['SRL', 'QA']	['Semantic Role Labeling', 'in', 'Question Answering']	[[24, 27], [107, 109]]	[[0, 22], [87, 105]]
135	 1 Introduction Named Entity Recognition (NER) is usually solved by a supervised learning approach, where	[[42, 45]]	[[16, 40]]	['NER']	['Named Entity Recognition']	[')']	['Entity Recognition', 'a']	[]	[]
136	+Valency = Adding valency filtering to the setting in the preceding row. SUC = Subset of Stockholm-Umea? 	[[73, 76]]	[[79, 98]]	['SUC']	['Subset of Stockholm']	['SUC']	['Adding valency', 'Subset of']	[[73, 76]]	[]
137	tying the feature parameters. In particular, we perform maximum entropy (MaxEnt) estimation over the conditional distribution using second-order gra-	[[73, 79]]	[[56, 71]]	['MaxEnt']	['maximum entropy']	['MaxEnt']	['maximum entropy']	[[73, 79]]	[[56, 71]]
138	due to French proper names, which are left untranslated in the English parallel text.  15 CLEF = Cross Language Evaluation Forum, ? www.clef-campaign.org?.	[[90, 94]]	[[97, 128]]	['CLEF']	['Cross Language Evaluation Forum']	['CLEF']	['Cross Language Evaluation Forum']	[[90, 94]]	[[97, 128]]
139	For our simulations, we built two subcorpora by filtering out entity annotations: the PENNBIOIE gene corpus (PBgene), including the three gene entity subtypes generic, protein, and rna,	[[109, 115]]	[[86, 100]]	['PBgene']	['PENNBIOIE gene']	['PENNBIOIE']	['gene corpus', 'gene']	[]	[]
140	Our participation in the STS task is inspired by previous work on paraphrase recognition, in which machine translation (MT) evaluation metrics are used to identify whether a pair of sentences are	[[120, 122], [25, 28]]	[[99, 118]]	['MT', 'STS']	['machine translation']	['STS', 'MT']	['in', 'on', 'in', 'machine translation', 'a']	[[25, 28], [120, 122]]	[[99, 118]]
141	"Efficient Algorithms for Parsing the DOP  Model"", Proceedings Empirical Methods in Natural Language  Processing, Philadelphia (PA),  J. Goodman, 1998."	[[127, 129], [37, 40]]	[[113, 125]]	['PA', 'DOP']	['Philadelphia']	['DOP', 'PA']	['Philadelphia']	[[37, 40], [127, 129]]	[[113, 125]]
142	case.  We choose Support Vector Machines (SVMs) as our learning algorithm for their widely acclaimed	[[42, 46]]	[[17, 40]]	['SVMs']	['Support Vector Machines']	['SVMs']	['Support Vector Machines']	[[42, 46]]	[[17, 40]]
143	clue words characteristic to the nine relation types.  In order to measure the semantic relatedness (SR) of targets and clues, we used the Explicit Seman-	[[101, 103]]	[[79, 99]]	['SR']	['semantic relatedness']	['SR']	['semantic relatedness']	[[101, 103]]	[[79, 99]]
144	 Consider, for example, the tags DT (determiner) and NN (noun), and the four possible ordered tagpairs.	[[53, 55], [33, 35]]	[[57, 61], [37, 47]]	['NN', 'DT']	['noun', 'determiner']	[]	[')', ')']	[]	[]
145	Measures for Semantic Relations Extraction Alexander Panchenko Center for Natural Language Processing (CENTAL) Universite?	[[103, 109]]	[[63, 90]]	['CENTAL']	['Center for Natural Language']	['CENTAL']	['for', 'Center for Natural Language Processing']	[[103, 109]]	[]
146	tilogue, but rather a number of parallel dialogues.  The Mission Rehearsal Exercise (MRE) Project (Traum and Rickel, 2002), one of the largest multilogue	[[85, 88]]	[[57, 83]]	['MRE']	['Mission Rehearsal Exercise']	['MRE']	['a', 'Mission Rehearsal Exercise']	[[85, 88]]	[[57, 83]]
147	tween anaphor and antecedent, the feature ddist captures the distance in sentences, the feature mdist the number of markables (NPs) between anaphor and antecedent.	[[127, 130]]	[[106, 125]]	['NPs']	['number of markables']	['NPs']	['number of']	[[127, 130]]	[]
148	6 , Guardian Weekly and its manually simplified versions for language learners (Allen, 2009), and the FIRST corpus of various texts simplified for people with autism spectrum disorder (ASD) 7	[[185, 188], [102, 107]]	[[159, 183]]	['ASD', 'FIRST']	['autism spectrum disorder']	['ASD']	['autism spectrum disorder']	[[185, 188]]	[[159, 183]]
149	that word in a sentence. Another covering  grammar is the so called null grammar (NG), in  which a word can follow any other word.	[[82, 84]]	[[68, 80]]	['NG']	['null grammar']	['NG']	['a', 'grammar', 'null grammar', 'a']	[[82, 84]]	[[68, 80]]
150	Table 1: Categorization of suitability-labels.  3.1.2 Beneficial (BENEF) While SUIT only states that the consumption of	[[66, 71]]	[[54, 64]]	['BENEF']	['Beneficial']	['BENEF', 'SUIT']	['Beneficial']	[[66, 71]]	[[54, 64]]
151	 The  noun phrases we ex-  tract start with an optional determiner (DT)  or  possessive pronoun (PRP$) ,  followed by  a se-  quence of cardinal numbers  (CDs),  adjectives 	[[97, 101], [68, 70], [155, 158]]	[[77, 95], [56, 66], [136, 144]]	['PRP$', 'DT', 'CDs']	['possessive pronoun', 'determiner', 'cardinal']	['DT', 'PRP$', 'CDs']	['determiner', 'a', 'cardinal numbers']	[[68, 70], [97, 101], [155, 158]]	[[56, 66]]
152	In Proceedings of the 24th International Conference on Computational Linguistics (COLING),Mumbai, India.	[[82, 88]]	[[55, 80]]	['COLING']	['Computational Linguistics']	[]	['on', 'Linguistics']	[]	[]
153	To train models using this information we use 870 generalized expectation (GE) criteria. GE criteria	[[75, 77], [89, 91]]	[[50, 73]]	['GE', 'GE']	['generalized expectation']	[')']	['expectation']	[]	[]
154	 Quasi-valency complementations: 	[[35, 39]]	[[41, 51]]	['DIFF']	['difference']	[]	[]	[]	[]
155	and the other for the target phrase. We call the unit of the two RAEs the Leaf Node Network (LNN). 	[[93, 96], [65, 69]]	[[74, 91]]	['LNN', 'RAEs']	['Leaf Node Network']	['RAEs', 'LNN']	['Leaf Node Network']	[[65, 69], [93, 96]]	[[74, 91]]
156	 ? Implicit Attitude (IA) - n ? t dimensions are ex-	[[22, 24]]	[[3, 20]]	['IA']	['Implicit Attitude']	[')']	['Attitude', 'n', 't']	[]	[]
157	mt. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING) - Volume 1, pages 1145?1152.	[[86, 92]]	[[59, 84]]	['COLING']	['Computational Linguistics']	['COLING']	['on Computational Linguistics']	[[86, 92]]	[]
158	The lattice representing a union of several confusion networks can then be directly rescored with an n-gram language model (LM). A transforma-	[[124, 126]]	[[108, 122]]	['LM']	['language model']	['LM']	['a', 'an', 'language model']	[[124, 126]]	[[108, 122]]
159	cial type of MWEs, for which we are mainly interested in its type, rather than individual lexias, during the annotation: named entities (NE).3 Treatment of NEs together with other MWEs is important, be-	[[137, 139]]	[[121, 135]]	['NE']	['named entities']	['MWEs', 'NE']	['named entities']	[[137, 139]]	[[121, 135]]
160	Document d1 Document d2 Customization Requirement : City, County or State names within sports articles may refer to a sports team  or to the location itself. Customization Solution (CS) :Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location Organization Location	[[182, 184], [9, 11], [21, 23]]	[[158, 180]]	['CS', 'd1', 'd2']	['Customization Solution']	['CS']	['Customization', 'a', 'Customization Solution', 'a']	[[182, 184]]	[[158, 180]]
161	There have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment.	[[147, 149]]	[[125, 145]]	['PP']	['prepositional phrase']	['PP']	['prepositional phrase']	[[147, 149]]	[[125, 145]]
162	alignment problems. In Annual Meeting of the Association for Computational Linguistics (ACL), Columbus, Ohio, June.	[[88, 91]]	[[45, 86]]	['ACL']	['Association for Computational Linguistics']	['ACL']	['Association for Computational Linguistics']	[[88, 91]]	[[45, 86]]
163	the style of Shriberg (1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair.	[[57, 59], [74, 76], [89, 91]]	[[60, 72], [77, 87], [92, 103]]	['FP', 'RM', 'IM']	['Filled Pause', 'Reparandum', 'Interregnum']	[]	['Pause']	[]	[]
164	The final reward is calculated as follows: TaskCompletionReward(TCR) = 1000 TurnCost(TC) = 10 TotalTurnCost(TTC) = #(Turns) ?	[[85, 87], [64, 68], [108, 111]]	[[76, 83], [43, 63], [94, 107]]	['TC', 'TCR)', 'TTC']	['TurnCos', 'TaskCompletionReward', 'TotalTurnCost']	['TCR', 'TC']	['TaskCompletionReward', 'TurnCost', 'TotalTurnCost']	[[85, 87]]	[[43, 63], [94, 107]]
165	operation!  On the other hand, we did start working on machine translation (MT) in 1987. As	[[76, 78]]	[[55, 74]]	['MT']	['machine translation']	['MT']	['on machine translation', 'in']	[[76, 78]]	[]
166	following table shows the results of U-DOP on the WSJ40 using 10 different 90-10 splits, compared to a  supervised binarized PCFG (S-PCFG) and a supervised binarized DOP model (S-DOP) on the	[[131, 137], [37, 42], [50, 55], [177, 182]]	[[104, 129], [145, 169]]	['S-PCFG', 'U-DOP', 'WSJ40', 'S-DOP']	['supervised binarized PCFG', 'supervised binarized DOP']	['U-DOP', 'WSJ40']	['a supervised binarized PCFG', 'a supervised binarized DOP']	[[37, 42], [50, 55]]	[]
167	of the above three neural classifiers.  Recursive Autoencoder (RAE) has proven to be an effective model to compose words vectors in	[[63, 66]]	[[40, 61]]	['RAE']	['Recursive Autoencoder']	['RAE']	['Recursive Autoencoder']	[[63, 66]]	[[40, 61]]
168	guishes tile outputs of these two phases by the  data types l:l.hetRep (rhetorical representation)  and DocRep (document representation). 	[[104, 110]]	[[112, 135]]	['DocRep']	['document representation']	['DocRep']	['l', 'rhetorical representation', 'document representation']	[[104, 110]]	[[112, 135]]
169	m a t i c a l l y  related s t r u c t u r e  is termed a prpposll t ion. -  Four r e l a t i o n s  of p a r t i c i p a t i o n  are dis t ingu ished :  agent  (AGT),  ins t rumenta l  (INS), objective (ORJ), and experiencer  (EXP) .	[[163, 166], [188, 192], [205, 208], [229, 232]]	[[155, 160], [170, 185], [194, 203], [215, 226]]	['AGT', 'INS)', 'ORJ', 'EXP']	['agent', 'ins t rumenta l', 'objective', 'experiencer']	['AGT', 'INS', 'ORJ', 'EXP']	['m', 'a t i', 'a l l', 't r u', 't u r e', 'a', 't', 'r e l a t i o n', 'p a r t i', 'i p a t i o n', 't', 'agent', 'ins t rumenta l', 'objective', 'experiencer']	[[163, 166], [205, 208], [229, 232]]	[[155, 160], [170, 185], [194, 203], [215, 226]]
170	an~ (AN)  art~fact (AR)  attribute (AT)  body (BO) 	[[36, 38], [5, 7], [20, 22], [47, 49]]	[[25, 34], [10, 18], [41, 45], [0, 3]]	['AT', 'AN', 'AR', 'BO']	['attribute', 'art~fact', 'body', 'an~']	['AN', 'AR', 'AT', 'BO']	['an~', 'art~fact', 'attribute', 'body']	[[5, 7], [20, 22], [36, 38], [47, 49]]	[[0, 3], [10, 18], [25, 34], [41, 45]]
171	respect o silence and noise words are removed from  the Nbest lists 7, next the word stream is tagged with  Brill's part of speech (POS) tagger (Brill, 1994),  Version 1.14, adapted to the SWITCHBOARD Cor- 	[[132, 135]]	[[116, 130]]	['POS']	['part of speech']	['POS']	['o', 'part of speech']	[[132, 135]]	[[116, 130]]
172	(such as Noun, Verb, Adjective etc). De-lexicalized text representations through POS tagging were first considered for native language identification (NLI), where they were used as a proxy for syntax in order to capture certain types of grammatical errors (Wong and Dras, 2009).	[[151, 154], [81, 84]]	[[119, 149]]	['NLI', 'POS']	['native language identification']	['POS', 'NLI']	['native language identification', 'a']	[[81, 84], [151, 154]]	[[119, 149]]
173	 The weight selection is performed by using the  minimum error rate training (MERT) for log-linear  model parameter estimation (Och, 2003).	[[78, 82]]	[[49, 76]]	['MERT']	['minimum error rate training']	[')']	['error rate training']	[]	[]
174	  NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 1?4, Montre?al, Canada, June 7?8, 2012.	[[88, 93], [2, 11]]	[[29, 86]]	['SLPAT', 'NAACL-HLT']	['Speech and Language Processing for Assistive Technologies']	['SLPAT']	['Speech and Language Processing for Assistive Technologies']	[[88, 93]]	[[29, 86]]
175	 In the RST framework, a text is first divided into several elementary discourse units (EDUs). Each	[[88, 92], [8, 11]]	[[60, 86]]	['EDUs', 'RST']	['elementary discourse units']	['RST', 'EDUs']	['a', 'elementary discourse units']	[[8, 11], [88, 92]]	[[60, 86]]
176	2008) and a corpus annotated for the entities Finding, Substance and Body was used for training a conditional random fields (CRF) system (Wang, 2009) as well as for training an en-	[[125, 128]]	[[98, 123]]	['CRF']	['conditional random fields']	['CRF']	['and a', 'and', 'a conditional random fields', 'an']	[[125, 128]]	[]
177	plex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR).	[[118, 120], [92, 94], [150, 152]]	[[97, 116], [72, 90], [127, 148]]	['MT', 'QA', 'IR']	['Machine Translation', 'Question Answering', 'Information Retrieval']	['QA', 'MT', 'IR']	['Question Answering', 'Machine Translation', 'Information Retrieval']	[[92, 94], [118, 120], [150, 152]]	[[72, 90], [97, 116], [127, 148]]
178	ation for Computational Linguistics (NAACL) from 2011?2013, and he has served on the editorial boards of the journals Transactions of the ACL (TACL) and Computational Linguistics.	[[143, 147]]	[[118, 141]]	['TACL']	['Transactions of the ACL']	['NAACL']	['ation for Computational Linguistics', 'he', 'on the', 'of the', 'Transactions of the ACL', 'Computational Linguistics']	[]	[[118, 141]]
179	Next we consider the recurrent neural network (RNN) based architecture called the Dual Encoder (DE) model (Lowe et al, 2015). The	[[96, 98], [47, 50]]	[[82, 94], [21, 45]]	['DE', 'RNN']	['Dual Encoder', 'recurrent neural network']	['RNN', 'DE']	['recurrent neural network', 'Dual Encoder', 'et']	[[47, 50], [96, 98]]	[[21, 45], [82, 94]]
180	"- meaningful expression (ME): Any physical act  carrying a non-contextual meaning;  - communicative act (CAct): An instance of ME  issued by a specific ""issuer"" and received by a "	[[105, 109], [25, 27], [127, 129]]	[[86, 103], [2, 23]]	['CAct', 'ME', 'ME']	['communicative act', 'meaningful expression']	['ME', 'CAct']	['meaningful expression', 'act', 'a', 'meaning', 'communicative act', 'a']	[[25, 27], [127, 129], [105, 109]]	[[2, 23], [86, 103]]
181	 For an annotation project, text pieces from a source database (DB) are often copied in a local storage and annotations are attached to them.	[[64, 66]]	[[54, 62]]	['DB']	['database']	[')']	['a', 'database', 'a']	[]	[[54, 62]]
182	consist of two verbs. The first  verb is termed as Full Verb (FV) that is present  at surface level either as conjunctive participial 	[[62, 64]]	[[51, 60]]	['FV']	['Full Verb']	['FV']	['Full Verb']	[[62, 64]]	[[51, 60]]
183	work of KH and FG was supported by the Academy of Finland, and of SVL by the Research Foundation Flanders (FWO). YVdP and SVL ac-	[[107, 110], [8, 10], [15, 17], [66, 69], [113, 117], [122, 125]]	[[86, 96]]	['FWO', 'KH', 'FG', 'SVL', 'YVdP', 'SVL']	['Foundation']	['KH', 'FG', 'SVL', 'FWO', 'YVdP']	['Research Foundation']	[[8, 10], [15, 17], [66, 69], [122, 125], [107, 110], [113, 117]]	[]
184	2007. Identifying authorship by byte-level n-grams: The source code author profile (SCAP) method. Journal of Digital Evidence, 6(1).	[[84, 88]]	[[56, 82]]	['SCAP']	['source code author profile']	['SCAP']	['source code author profile']	[[84, 88]]	[[56, 82]]
185	 A related task was explored at the Document Understanding Conference (DUC) in 2007.5 Here the goal was to find new information with respect to a	[[71, 74]]	[[36, 69]]	['DUC']	['Document Understanding Conference']	[')']	['Understanding Conference', 'in']	[]	[]
186	? and possibly split up ? into True Positive (TP) and False Positive (FP) entities.	[[46, 48], [70, 72]]	[[31, 44], [54, 68]]	['TP', 'FP']	['True Positive', 'False Positive']	['TP', 'FP']	['True Positive', 'False Positive']	[[46, 48], [70, 72]]	[[31, 44], [54, 68]]
187	One possibility is the use of semantic annotation, using sentence-level propositional Logical Forms (LF). It seems more cogni-	[[101, 103]]	[[86, 99]]	['LF']	['Logical Forms']	['LF']	['Logical Forms']	[[101, 103]]	[[86, 99]]
188	basic word-level edit operations (insertion, deletion and substitution) to transform one text into the other: Levenshtein with substitution penalty (LEV2): This feature is a variant of LEV1 in which substi-	[[149, 153], [185, 189]]	[[110, 147]]	['LEV2', 'LEV1']	['Levenshtein with substitution penalty']	['LEV2']	['substitution', 'Levenshtein with substitution penalty', 'a', 'in']	[[149, 153]]	[[110, 147]]
189	matrix of the IBM Model 1. However, there is a signficant out of vocabulary (OOV) issue in the model since training data is limited.	[[77, 80], [14, 17]]	[[58, 75]]	['OOV', 'IBM']	['out of vocabulary']	['IBM', 'OOV']	['of', 'a', 'out of vocabulary']	[[14, 17], [77, 80]]	[[58, 75]]
190	of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP), pages 342?350, Singapore.	[[99, 109], [7, 10], [92, 97]]	[[23, 84]]	['ACL-IJCNLP', 'ACL', 'AFNLP']	['International Joint Conference on Natural Language Processing']	['ACL', 'AFNLP']	[]	[[7, 10], [92, 97]]	[]
191	gories Auxiliary-final VP For auxiliary verbs parsed as verb phrases (VP), this feature checks if the final element in the VP	[[70, 72], [23, 25], [123, 125]]	[[56, 68]]	['VP', 'VP', 'VP']	['verb phrases']	['VP']	['as verb']	[[70, 72], [23, 25], [123, 125]]	[]
192	Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.	[[124, 128], [26, 31], [73, 77]]	[[93, 122], [0, 24], [33, 71]]	['AFRL', 'DARPA', 'DEFT']	['Air Force Research Laboratory', 'Research Projects Agency', 'Deep Exploration and Filtering of Text']	['DARPA', 'DEFT', 'AFRL']	['Research Projects Agency', 'Deep Exploration and Filtering of Text', 'Air Force Research Laboratory']	[[26, 31], [73, 77], [124, 128]]	[[0, 24], [33, 71], [93, 122]]
193	2 ? Lemma, PoS, and Dependency relation (DepRel) for the node itself, the parent, and the left and right sibling	[[41, 47], [11, 14], [4, 9]]	[[20, 39]]	['DepRel', 'PoS', 'Lemma']	['Dependency relation']	[')']	['relation']	[]	[]
194	erol (DAG)?). Note that the position of the long form (LF) and short form (SF) is interchangeable. To dis-	[[75, 77], [6, 9], [55, 57]]	[[63, 73], [44, 53]]	['SF', 'DAG', 'LF']	['short form', 'long form']	['DAG', 'LF', 'SF']	['the', 'of the long', 'short']	[[6, 9], [55, 57], [75, 77]]	[]
195	M27 vp move past, vp pass, vp pass by, vp walk past (all AS) Actions correspond to expansions of lexemes (FV = filling status): 0=unfilled, 1=filled. ( SV = shared variables): the variables np actor (FV), relatum (FV), sentence (FV) and information need (0=low, 1=high) are shared by several subagents; the same applies to their	[[152, 154], [57, 59], [106, 108], [200, 202], [214, 216], [229, 231]]	[[157, 173], [111, 125]]	['SV', 'AS', 'FV', 'FV', 'FV', 'FV']	['shared variables', 'filling status']	['M27', 'vp', 'AS', 'FV', 'SV']	['lexemes', 'filling status', 'shared variables', 'variables', 'sentence']	[[57, 59], [106, 108], [200, 202], [214, 216], [229, 231], [152, 154]]	[[111, 125], [157, 173]]
196	x, y) where t(x) is the true label of the observation sequence x. We can get a quadratic program (QP) using a standard transformation to eliminate ?	[[98, 100]]	[[79, 96]]	['QP']	['quadratic program']	['QP']	['t', 'a quadratic program', 'a']	[[98, 100]]	[]
197	46 date the current hypothesis after each observation; b) Confidence Weighted (CW) learning?a probabilistic large margin online learning algorithm (Dredze et	[[79, 81]]	[[58, 77]]	['CW']	['Confidence Weighted']	['CW']	['Confidence Weighted']	[[79, 81]]	[[58, 77]]
198	(see details below).  pronoun, PUNC = punctuation, PRT = particle, and X = residual (a category for language-specific cat-	[[31, 35], [51, 54]]	[[38, 49], [57, 65]]	['PUNC', 'PRT']	['punctuation', 'particle']	['PUNC', 'PRT']	['punctuation', 'particle', 'residual', 'a']	[[31, 35], [51, 54]]	[[38, 49], [57, 65]]
199	In this paper we restrict our attention to the  translation from the English-oriented level (EL)  to the domain model level (DML) since this is  where CEs are disambiguated bychoosing unam- 	[[125, 128], [93, 95], [151, 154]]	[[105, 123], [69, 91]]	['DML', 'EL', 'CEs']	['domain model level', 'English-oriented level']	['EL', 'DML', 'CEs']	['English-oriented', 'level', 'domain model level', 'is']	[[93, 95], [125, 128], [151, 154]]	[[105, 123]]
200	In Addition to the visualhaptic interface, iDrive includes a speech dialogue system (SDS) as well. The SDS allows the driver	[[85, 88]]	[[77, 83]]	['SDS']	['system']	['SDS']	['a speech dialogue system']	[[85, 88]]	[]
201	phrase (BADVP), base noun phrase (BNP),  73  base temporal phrase (BTN), base location  phrase (BNS), base verb phrase (BVP) and 	[[67, 70], [8, 13], [34, 37], [96, 99], [120, 123]]	[[45, 65], [16, 32], [102, 118], [73, 94]]	['BTN', 'BADVP', 'BNP', 'BNS', 'BVP']	['base temporal phrase', 'base noun phrase', 'base verb phrase', 'base location  phrase']	['BADVP', 'BNP', 'BTN', 'BNS']	['phrase', 'base noun phrase', 'base temporal phrase', 'base location phrase', 'base verb phrase']	[[8, 13], [34, 37], [67, 70], [96, 99]]	[[16, 32], [45, 65], [102, 118]]
202	7 Conclusions We introduce a Laplacian structured sparsity model for computational branding analytics (CBA). In the	[[103, 106]]	[[69, 101]]	['CBA']	['computational branding analytics']	['CBA']	['a', 'computational branding analytics']	[[103, 106]]	[[69, 101]]
203	of modals and auxilliaries is deterministic.  Syntactic Chunk (CHUNK): This feature explicitly models the syntactic phrases in which our	[[63, 68]]	[[56, 61]]	['CHUNK']	['Chunk']	['CHUNK']	['Chunk']	[[63, 68]]	[[56, 61]]
204	In order to capture the compositional effects with higher accuracy, we propose a new model called the Recursive Neural Tensor Network (RNTN). Recur-	[[135, 139]]	[[102, 133]]	['RNTN']	['Recursive Neural Tensor Network']	['RNTN']	['to', 'a', 'Recursive Neural Tensor Network']	[[135, 139]]	[[102, 133]]
205	called CPDB. We used these clues to build a Small Dataset (SD) and a Large Dataset (LD) for reranking.	[[84, 86], [7, 11], [59, 61]]	[[69, 82], [44, 57]]	['LD', 'CPDB', 'SD']	['Large Dataset', 'Small Dataset']	['CPDB', 'SD', 'LD']	['a Small Dataset', 'a Large Dataset']	[[7, 11], [59, 61], [84, 86]]	[]
206	Nu' (a) ~ Nu (a) AP (a)  OH  Nu' (a) = Nu (a) A(P (a)~ P' (a))  (3) la propri~t~ P contredit le noyau ; on a ~ la lois 	[[25, 27], [17, 19]]	[]	['OH', 'AP']	[]	['AP']	"[""Nu ' ("", ')', 'Nu (', ')', '(', ')', ""Nu ' ("", ')', 'Nu (', ')', '(', '(', ')', ""' ("", ') ) (', ')']"	[[17, 19]]	[]
207	data are summarized in Table 6. We also report the Wordnet first sense baseline (WFS). 	[[81, 84]]	[[51, 79]]	['WFS']	['Wordnet first sense baseline']	['WFS']	['in', 'Wordnet first sense']	[[81, 84]]	[]
208	We note there also exist various multilingual or cross-lingual semantic processing works. Most of such works focus on semantic role labeling(SRL), the task of recovery of shallow meaning. Examples	[[141, 144]]	[[118, 140]]	['SRL']	['semantic role labeling']	['SRL']	['semantic', 'semantic role labeling']	[[141, 144]]	[[118, 140]]
209	5th Conference of the Association for Machine Translation in the Americas (AMTA). Boston, Massachusetts.	[[75, 79]]	[[22, 73]]	['AMTA']	['Association for Machine Translation in the Americas']	[')']	['the', 'for Machine Translation in the Americas']	[]	[]
210	 In this paper we deal with automatic acquisition of verbal selectional preferences (SPs) for Latin, i. e. the semantic preferences of verbs on their ar-	[[85, 88]]	[[60, 83]]	['SPs']	['selectional preferences']	[')']	['selectional preferences', 'preferences', 'on']	[]	[[60, 83]]
211	= primary source; C06?C09 = CoNLL 2006?2009; I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip experiment described in Section 5.	[[191, 194], [28, 33], [62, 64], [84, 86], [106, 108], [122, 125], [150, 153], [183, 185]]	[[197, 223], [67, 82], [89, 97]]	['UAS', 'CoNLL', 'SM', 'CJ', 'CS', 'CSs', 'CSs', 'CS']	['unlabeled attachment score', 'shared modifier', 'conjunct']	['SM', 'CJ', 'CS', 'RT', 'UAS']	['shared modifier', 'conjunct']	[[62, 64], [84, 86], [106, 108], [183, 185], [191, 194]]	[[67, 82], [89, 97]]
212	Computational Linguistics Volume 21, Number 4  Table 6  Growth of Hypothesis Space: S = sentence; TL = Total number of links; RL= Relevant Links;  AC = Number of Active Chains; G = Growth rate 	[[98, 100], [126, 128], [147, 149]]	[[103, 108], [88, 96], [130, 144], [162, 174], [181, 192]]	['TL', 'RL', 'AC']	['Total', 'sentence', 'Relevant Links', 'Active Chain', 'Growth rate']	['TL', 'AC']	['Number', 'Growth', 'of', 'sentence', 'of', 'Relevant Links', 'Number of Active Chains']	[[98, 100], [147, 149]]	[[88, 96], [130, 144]]
213	In our experiments, we used the Penn Treebank (PTB) (Marcus et al 1993) for English and the Chinese Treebank version 5.1 (CTB5) (Xue et al 2005) for Chinese.	[[122, 126], [47, 50]]	[[92, 120], [32, 45]]	['CTB5', 'PTB']	['Chinese Treebank version 5.1', 'Penn Treebank']	['PTB', 'CTB5']	['Penn', 'Treebank', 'Chinese Treebank version 5.1', 'Chinese']	[[47, 50], [122, 126]]	[[92, 120]]
214	3 MaxEnt Model and Features  3.1 MaxEnt Model for NOR  The principle of maximum entropy (MaxEnt)  model is that given a collection of facts, choose a 	[[89, 95], [2, 8], [33, 39], [50, 53]]	[[72, 87]]	['MaxEnt', 'MaxEnt', 'MaxEnt', 'NOR']	['maximum entropy']	['MaxEnt']	['maximum entropy', 'a']	[[89, 95], [2, 8], [33, 39]]	[[72, 87]]
215	the following.  The Basic Additive model (BA) (introduced in (Mitchell and Lapata, 2008)) computes the disti-	[[42, 44]]	[[20, 34]]	['BA']	['Basic Additive']	['BA']	['Basic Additive']	[[42, 44]]	[[20, 34]]
216	Two-liners generated by three different algorithms were evaluated by each subject: Script model + Concept clustering (SM+CC) Both script opposition and incongruity are	[[118, 123]]	[[83, 116]]	['SM+CC']	['Script model + Concept clustering']	['+', 'SM+CC']	['Concept clustering']	[[118, 123]]	[]
217	All-before 8 0.0313 88.03 Table 1. Instance classification accuracy (CA) using  different feature sets.	[[69, 71]]	[[44, 67]]	['CA']	['classification accuracy']	['CA']	['classification accuracy']	[[69, 71]]	[[44, 67]]
218	tim+ fo ! lowiug  se lnant i t :  d i~ iens iens~ dependeucy re la t ion   (\]IR), coordieatien ned apposition constructions (CA) and  i :op ic . .	[[126, 128], [78, 80]]	[[83, 110]]	['CA', 'IR']	['coordieatien ned apposition']	['CA']	['i', 't', 'd', 't', 'coordieatien ned apposition constructions', 'i']	[[126, 128]]	[]
219	for our experiment: Telecommunication Services (TS, the sector with the smallest number of companies), Information Technology (IT), and Consumer Staples (CS), due to our familiarity with the	[[127, 129], [48, 50], [154, 156]]	[[103, 125], [20, 46], [136, 152]]	['IT', 'TS', 'CS']	['Information Technology', 'Telecommunication Services', 'Consumer Staples']	['TS', 'IT', 'CS']	['for', 'Information Technology', 'Consumer Staples']	[[48, 50], [127, 129], [154, 156]]	[[103, 125], [136, 152]]
220	and there are many local minima on the error surface.  Therefore, we use an alternative loss function, minimum squared error (MSE) in equation (5), where Score(.)	[[126, 129]]	[[103, 124]]	['MSE']	['minimum squared error']	['MSE']	['error', 'minimum squared error', 'in']	[[126, 129]]	[[103, 124]]
221	ment data time constraints, we were not able to test which of our modules leads to false negative (FN) instances. 	[[99, 101]]	[[83, 97]]	['FN']	['false negative']	['FN']	['false negative']	[[99, 101]]	[[83, 97]]
222	? Rule One. For the first noun phrase (NP) encountered by the system, if 1) this NP has a name entity	[[39, 41]]	[[26, 37]]	['NP']	['noun phrase']	['NP']	['noun phrase', 'a']	[[39, 41]]	[[26, 37]]
223	(ARTG, PR.OC)I ^  (SUBS, SUSU, VT)2  , ARTG = article g~n&al  SUBS = substantif  compl~ment  VT  = verbe conjugu6 	[[62, 66], [1, 5], [7, 12], [19, 23], [25, 29], [31, 33], [39, 43], [93, 95]]	[[69, 79], [46, 60], [99, 104]]	['SUBS', 'ARTG', 'PR.OC', 'SUBS', 'SUSU', 'VT', 'ARTG', 'VT']	['substantif', 'article g~n&al', 'verbe']	['ARTG', 'PR.OC', 'SUBS', 'SUSU', 'VT']	['article g~n & al', 'substantif', 'verbe']	[[1, 5], [39, 43], [7, 12], [62, 66], [19, 23], [25, 29], [31, 33], [93, 95]]	[[69, 79], [99, 104]]
224	tions. The CSR techniques are based on a continuous-  observation Hidden Markov Model (HMM) approach. 	[[87, 90], [11, 14]]	[[66, 85]]	['HMM', 'CSR']	['Hidden Markov Model']	['CSR', 'HMM']	['a', 'Hidden Markov Model']	[[11, 14], [87, 90]]	[[66, 85]]
225	word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 27(7):1063?1074.	[[91, 95], [27, 31]]	[[48, 89]]	['PAMI', 'IEEE']	['Pattern Analysis and Machine Intelligence']	['IEEE', 'PAMI']	['Transactions on Pattern Analysis and Machine Intelligence']	[[27, 31], [91, 95]]	[]
226	lingual interpersonal communication. The improvements in automatic speech recognition (ASR), statistical machine translation (MT), and, text-to-speech	[[87, 90], [126, 128]]	[[57, 85], [105, 124]]	['ASR', 'MT']	['automatic speech recognition', 'machine translation']	['ASR', 'MT']	['in automatic speech recognition', 'statistical machine translation']	[[87, 90], [126, 128]]	[]
227	Table 1: Comparison of knowledge acquisition strategies. Interactive query expansion (IQE)?s poor task completion indicates keywords can?t bridge the knowledge	[[86, 89]]	[[57, 84]]	['IQE']	['Interactive query expansion']	['IQE']	['Interactive query expansion', 't']	[[86, 89]]	[[57, 84]]
228	We experimented with the following four machine learning algorithms: Support Vector Machine (SVM), Multilayer Perceptron(MLP),  Decision Trees(DT) and AdaBoost(AB).	[[121, 124], [93, 96], [143, 145], [160, 162]]	[[99, 119], [69, 91], [128, 141], [151, 159]]	['MLP', 'SVM', 'DT', 'AB']	['Multilayer Perceptro', 'Support Vector Machine', 'Decision Tree', 'AdaBoost']	['SVM', 'MLP', 'DT', 'AB']	['Support Vector Machine', 'Multilayer Perceptron', 'Decision Trees', 'AdaBoost']	[[93, 96], [121, 124], [143, 145], [160, 162]]	[[69, 91], [151, 159]]
229	parsed as the specified category.  The 'target condition(TCND)' represents  conditions on variables in tile 'target pattern.'	[[57, 61]]	[[40, 56]]	['TCND']	['target condition']	['TCND']	['condition', 'on']	[[57, 61]]	[]
230	Probabilistic ID/LP grammars  The idea of separating simple context-free rules into two, orthogunal rule sets, immediate  dominance(ID) rules, and linear precedence(LP) rules, gives a notation for writing grammars called  ID/LP.	[[165, 167], [14, 19], [132, 134], [222, 227]]	[[147, 163], [111, 131]]	['LP', 'ID/LP', 'ID', 'ID/LP']	['linear precedenc', 'immediate  dominance']	['ID/LP']	['immediate dominance', 'linear precedence', 'a']	[[14, 19], [222, 227]]	[]
231	 2 Related Work Locality sensitive hashing (LSH) (Indyk and Motwani, 1998) is an example of an approximate	[[44, 47]]	[[16, 42]]	['LSH']	['Locality sensitive hashing']	[')']	['sensitive hashing', 'is']	[]	[]
232	 2 Methodologies The Hidden Vector State (HVS) model (He and Young, 2005) is a discrete Hidden Markov Model	[[42, 45]]	[[21, 40]]	['HVS']	['Hidden Vector State']	[')']	['Vector State', 'a']	[]	[]
233	The output of our experiments was evaluated using two metrics, (1) BLEU (Papineni et al, 2002), and (2) Lexical Accuracy (LexAcc). Lexical ac-	[[122, 128], [67, 71]]	[[104, 120]]	['LexAcc', 'BLEU']	['Lexical Accuracy']	['BLEU', 'LexAcc']	['al', 'Lexical Accuracy', 'Lexical']	[[67, 71], [122, 128]]	[[104, 120]]
234	corrccmess as follows:  ? Translation Correctness (TA). This is tile percentage of	[[51, 53]]	[[26, 37]]	['TA']	['Translation']	['TA']	['Translation Correctness']	[[51, 53]]	[]
235	We consider two common ways of calculating the translation probability: using the maximum likelihood estimator (MLE) and smoothing the MLE using lexical weighting.	[[112, 115], [135, 138]]	[[82, 110]]	['MLE', 'MLE']	['maximum likelihood estimator']	['MLE']	['maximum likelihood estimator']	[[112, 115], [135, 138]]	[[82, 110]]
236	studied for text categorization task (Forman, 2003).  Information gain (IG) is one of state of the art criteria for feature selection, which measures the de-	[[72, 74]]	[[54, 70]]	['IG']	['Information gain']	['IG']	['for', 'Information gain', 'for']	[[72, 74]]	[[54, 70]]
237	They are Independent Word Probability (IWP), Anti-Word Pair  (AWP), and Word Formation Analogy (WFA). 	[[96, 99], [39, 42], [62, 65]]	[[72, 94], [9, 37], [45, 59]]	['WFA', 'IWP', 'AWP']	['Word Formation Analogy', 'Independent Word Probability', 'Anti-Word Pair']	['IWP', 'AWP', 'WFA']	['Independent', 'Word Probability', 'Anti-Word Pair', 'Word Formation Analogy']	[[39, 42], [62, 65], [96, 99]]	[[45, 59], [72, 94]]
238	freely available framenets for a number of languages (Boas, 2009), among these the Swedish FrameNet (SweFN) (Borin et al, 2010).5	[[101, 106]]	[[83, 99]]	['SweFN']	['Swedish FrameNet']	[]	['a', 'Swedish FrameNet', 'et']	[]	[[83, 99]]
239	1. the mention type: person (PER), organization (ORG), location (LOC), geopolitical entity (GPE), facility (FAC), vehicle (VEH), and	[[65, 68]]	[[55, 63]]	['LOC']	['location']	['PER', 'ORG', 'LOC', 'GPE', 'FAC', 'VEH']	['person', 'organization', 'location', 'geopolitical entity', 'facility', 'vehicle']	[[65, 68]]	[[55, 63]]
240	6 Related Work Two most prevalent discourse parsing treebanks are RST Discourse Treebank (RST-DT) (Carlson et al.,	[[90, 96]]	[[66, 88]]	['RST-DT']	['RST Discourse Treebank']	[]	['RST Discourse Treebank']	[]	[[66, 88]]
241	ranging from generating weather forecasts to summarizing medical information (Reiter and Dale 2000). Of all the subtasks of NLG, Referring Expression Generation (REG) is among those that have received most scholarly attention.	[[162, 165], [124, 127]]	[[129, 160]]	['REG', 'NLG']	['Referring Expression Generation']	['NLG', 'REG']	['Referring Expression Generation']	[[124, 127], [162, 165]]	[[129, 160]]
242	Table 1. Judgment count for the sample instances (HA=high attachment; LA=low attachment; and A=Ambiguous)   	[[70, 72]]	[[73, 87]]	['LA']	['low attachment']	[]	['attachment', 'attachment']	[]	[]
243	8 Conclusion We proposed a framework to generate characteristicrich questions for question answering (QA) evaluation.	[[102, 104]]	[[82, 100]]	['QA']	['question answering']	['QA']	['a', 'question answering']	[[102, 104]]	[[82, 100]]
244	    Two additional general features were used.  The preposition feature (PREP) captures the  most indicative preposition among connected 	[[73, 77]]	[[52, 63]]	['PREP']	['preposition']	['PREP']	['preposition feature', 'preposition']	[[73, 77]]	[[52, 63]]
245	In this paper we present methods for reducing the compu-  tation time of joint segmentation a d recognition of phones  using the Stochastic Segment Model (SSM). Our approach 	[[155, 158]]	[[129, 153]]	['SSM']	['Stochastic Segment Model']	['SSM']	['a d', 'Stochastic Segment Model']	[[155, 158]]	[[129, 153]]
246	Baselines We use the following baselines. The first is the Homogenous Poisson Process (HPP) trained on the training set of the rumour.	[[87, 90]]	[[59, 85]]	['HPP']	['Homogenous Poisson Process']	['HPP']	['Homogenous Poisson Process', 'on']	[[87, 90]]	[[59, 85]]
247	features extracted from the LFG parse.  Lexical Functional Grammar (LFG) (Bresnan, 2000) is a constraint-based theory of grammar.	[[68, 71], [28, 31]]	[[40, 66]]	['LFG', 'LFG']	['Lexical Functional Grammar']	['LFG']	['Lexical Functional Grammar', 'a']	[[68, 71], [28, 31]]	[[40, 66]]
248	to learn a lexicon. The theory of the TAD states is Universal Theory (UT); a UT metalanguage enables an abstract characteriza-	[[70, 72], [38, 41], [77, 79]]	[[52, 68]]	['UT', 'TAD', 'UT']	['Universal Theory']	['TAD', 'UT']	['a', 'The', 'Universal Theory', 'a']	[[38, 41], [70, 72], [77, 79]]	[[52, 68]]
249	appficability o new tasks, such as indications/warn-  ings, text tagging, and document detection support  \[1\] The Text REtrieval Conferences (TREC's) are described  in the Document Detecfien section.	[[144, 150]]	[[116, 142]]	"[""TREC's""]"	['Text REtrieval Conferences']	"['TREC', ""'s""]"	['o', 'Text REtrieval Conferences']	[]	[[116, 142]]
250	two aspects: how well the generated text reflects the source data, whether it be text in another language for machine translation (MT), a natural language generation (NLG) input representation, a doc-	[[131, 133], [167, 170]]	[[110, 129], [138, 165]]	['MT', 'NLG']	['machine translation', 'natural language generation']	['MT', 'NLG']	['it', 'in', 'language', 'machine translation', 'a natural language generation', 'a']	[[131, 133], [167, 170]]	[[110, 129]]
251	pris VPE (participe assE)  la AR  TD  par PREP (prEposition)  main SUBS (substamif) 	[[42, 46]]	[[48, 59]]	['PREP']	['prEposition']	['VPE', 'assE', 'AR', 'TD', 'PREP', 'SUBS']	['participe', 'prEposition', 'substamif']	[[42, 46]]	[[48, 59]]
252	The UMLS includes the Metathesaurus (MT),  which contains over one million biomedical concepts and the Semantic Network (SN), which  represents a high-level abstraction from the UMLS 	[[121, 123], [37, 39], [4, 8], [178, 182]]	[[103, 119], [22, 35]]	['SN', 'MT', 'UMLS', 'UMLS']	['Semantic Network', 'Metathesaurus']	[')']	['the', 'the', 'Network', 'a', 'the']	[]	[]
253	H and T . This set of features is later used by two support vector machine (SVM) classifiers for detecting CLTE separately in both directions (T ?	[[76, 79], [107, 111]]	[[52, 74], [81, 106]]	['SVM', 'CLTE']	['support vector machine', 'classifiers for detecting']	['H', 'T', 'SVM']	['support vector machine', 'in']	[[76, 79]]	[[52, 74]]
254	Conceptual structures (ConcSs);  ? Parsed syntactic structures (PSyntSs). 	[[64, 71], [23, 29]]	[[35, 62], [0, 21]]	['PSyntSs', 'ConcSs']	['Parsed syntactic structures', 'Conceptual structures']	['ConcSs']	['Conceptual structures', 'syntactic structures']	[[23, 29]]	[[0, 21]]
255	To this  end, more and more information extraction (IE)  systems using natural language processing (NLP)  have been developed for use in the biomedical 	[[100, 103], [52, 54]]	[[71, 98], [28, 50]]	['NLP', 'IE']	['natural language processing', 'information extraction']	['IE', 'NLP']	['information extraction', 'natural language processing', 'for', 'in']	[[52, 54], [100, 103]]	[[28, 50], [71, 98]]
256	elements and punctuation. We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as defined in Klein (2005: 21-	[[87, 89], [113, 115]]	[[66, 85], [95, 111]]	['UP', 'UR']	['unlabeled precision', 'unlabeled recall']	['UP', 'UR']	['unlabeled precision', 'unlabeled recall']	[[87, 89], [113, 115]]	[[66, 85], [95, 111]]
257	of the Spanish language. Our experimental project is a collaboration  of the Royal Spanish Academy (R.A.E.) and the Computer Center of  the University of Madrid (CCVM).	[[100, 106], [162, 166]]	[[77, 98], [116, 160]]	['R.A.E.', 'CCVM']	['Royal Spanish Academy', 'Computer Center of  the University of Madrid']	['.', 'CCVM']	['of the Spanish', 'a', 'of the Royal Spanish Academy', 'the Computer Center of the University of Madrid']	[[162, 166]]	[]
258	2008.  Deciding strictly local (SL) languages. In Jon Breit-	[[32, 34]]	[[16, 30]]	['SL']	['strictly local']	[]	['strictly local']	[]	[[16, 30]]
259	an all time high.  	[[21, 24]]	[[26, 34]]	['HER']	['heritage']	[]	[]	[]	[]
260	tion algorithm to chase after undecidable cases. For example, consider prepositional phrases (PPs) with in as the head. These PPs occur frequently, and about half of them	[[94, 97], [126, 129]]	[[85, 92]]	['PPs', 'PPs']	['phrases']	['PPs']	['tion', 'prepositional phrases', 'as']	[[94, 97], [126, 129]]	[]
261	consists of following elements: Topic  Number(NUM),Topic Title(TITLE),Topic  question(DESC),Topic Narrative(NARR) and  Topic Concepts(CONC).	[[108, 112], [63, 68], [46, 49], [86, 90], [134, 138]]	[[98, 106], [57, 62], [39, 45], [125, 133]]	['NARR', 'TITLE', 'NUM', 'DESC', 'CONC']	['Narrativ', 'Title', 'Number', 'Concepts']	['NUM', 'TITLE', 'DESC', 'NARR', 'CONC']	['Topic', 'Topic', 'Title', 'Topic', 'Topic Narrative', 'Topic']	[[46, 49], [63, 68], [86, 90], [108, 112], [134, 138]]	[[57, 62]]
262	MR = MN/length (5) ? The Continuous Match Value(CMV): Continuous match should be better than the	[[48, 51], [0, 2], [5, 7]]	[[25, 46]]	['CMV', 'MR', 'MN']	['Continuous Match Valu']	['MR', 'CMV']	['MN/length', 'Continuous Match Value', 'Continuous']	[[0, 2], [48, 51]]	[]
263	Improvements in a simulated speech-recognition example. Nine versions of a phonemically  identical oronym, ordered by weighted average (W.A.) probability (x 10-20). The W.A. 	[[136, 140], [169, 173]]	[[118, 134]]	['W.A.', 'W.A.']	['weighted average']	['.']	['a', 'a', 'weighted average']	[]	[[118, 134]]
264	The central components of our non-parametric Bayesian models are the Chinese Restaurant Processes (CRPs) and the closely related Dirichlet Processes (DPs) (Ferguson, 1973).	[[99, 103], [150, 153]]	[[69, 97], [129, 148]]	['CRPs', 'DPs']	['Chinese Restaurant Processes', 'Dirichlet Processes']	['CRPs', 'DPs']	['Chinese Restaurant Processes', 'Dirichlet Processes']	[[99, 103], [150, 153]]	[[69, 97], [129, 148]]
265	L&L is suggestive of a particular approach to supervised learning ? maximum entropy (MaxEnt) ? in	[[85, 91], [0, 3]]	[[68, 83]]	['MaxEnt', 'L&L']	['maximum entropy']	['L&']	['a', 'maximum entropy']	[]	[[68, 83]]
266	read speech and two-party dialogue, multi-party dialogues typically exhibit a considerably higher word error rate (WER) (Morgan et al, 2003). 	[[115, 118]]	[[98, 113]]	['WER']	['word error rate']	['WER']	['a', 'word error rate']	[[115, 118]]	[[98, 113]]
267	junct verbs (ConjVs), the Non-MonoClausal  Verbs (NMCV) and Auxiliary Construction  (AC) occur as conjunct verbs (ConjVs).  The 	[[114, 120], [13, 19], [50, 54], [85, 87]]	[[98, 112], [0, 11], [26, 48], [60, 82]]	['ConjVs', 'ConjVs', 'NMCV', 'AC']	['conjunct verbs', 'junct verbs', 'Non-MonoClausal  Verbs', 'Auxiliary Construction']	['ConjVs', 'NMCV', 'AC']	['junct', 'verbs', 'Non-MonoClausal Verbs', 'Auxiliary Construction', 'conjunct verbs']	[[114, 120], [13, 19], [50, 54], [85, 87]]	[[60, 82], [98, 112]]
268	 bo und Figure 1: Accuracy for Na??veBayes classifier (NBC) and Majority Rule (MR) 4 Experimental results	[[79, 81], [55, 58]]	[[64, 77], [31, 53]]	['MR', 'NBC']	['Majority Rule', 'Na??veBayes classifier']	['NBC', 'MR']	['Na ? ? veBayes', 'Majority Rule']	[[55, 58], [79, 81]]	[[64, 77]]
269	3.3 Adapting the POS tagset (STTS) To account for important differences between modern and Early Modern German (EMG), and to facilitate more accurate searches, we adapted the STTS	[[112, 115], [17, 20], [29, 33], [175, 179]]	[[91, 110]]	['EMG', 'POS', 'STTS', 'STTS']	['Early Modern German']	['POS', 'STTS', 'EMG']	['Early Modern German']	[[17, 20], [29, 33], [175, 179], [112, 115]]	[[91, 110]]
270	The rules  in an AG have a considerably different formal character as compared  to the 'rewrite rule' in a general phrase structure grmmmar (PSG). 	[[141, 144], [17, 19]]	[[115, 139]]	['PSG', 'AG']	['phrase structure grmmmar']	['AG', 'PSG']	['a', 'as', 'a', 'phrase structure grmmmar']	[[17, 19], [141, 144]]	[[115, 139]]
271	Previous shared tasks for grammar error correction, such as the HOO shared task of 2012 (HOO-2012) and the CoNLL-2013 shared task(CoNLL-2013),	[[89, 97], [130, 140], [107, 117]]	[[64, 87]]	['HOO-2012', 'CoNLL-2013', 'CoNLL-2013']	['HOO shared task of 2012']	[]	['shared', 'HOO shared task of 2012', 'CoNLL-2013 shared task']	[]	[[64, 87]]
272	languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis). 	[[93, 97], [39, 42]]	[[99, 137], [44, 74]]	['PLSA', 'CCA']	['Probabilistic Latent Semantic Analysis', 'Canonical Correlation Analysis']	['CCA', 'PLSA']	['on', 'Canonical Correlation Analysis', 'Probabilistic Latent Semantic Analysis']	[[39, 42], [93, 97]]	[[44, 74], [99, 137]]
273	For example, an infinitival clause  (INFCL) may contain a noun phrase with an  embedded relative clause (RELCL). Elimination 	[[105, 110], [37, 42]]	[[88, 103], [16, 34]]	['RELCL', 'INFCL']	['relative clause', 'infinitival clause']	['INFCL', 'RELCL']	['infinitival', 'clause', 'a', 'embedded relative clause']	[[37, 42], [105, 110]]	[]
274	natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields (CRFs) and hidden Markov models (HMMs).	[[138, 142], [104, 108], [170, 174]]	[[111, 136], [79, 102], [148, 168]]	['CRFs', 'SVMs', 'HMMs']	['conditional random fields', 'support vector machines', 'hidden Markov models']	['SVMs', 'CRFs', 'HMMs']	['and', 'models', 'support vector machines', 'conditional random fields', 'and hidden Markov models']	[[104, 108], [138, 142], [170, 174]]	[[79, 102], [111, 136]]
275	by the organizers). The results are compared to the best system and the MFS (Most Frequent Sense) baseline.	[[72, 75]]	[[77, 96]]	['MFS']	['Most Frequent Sense']	['MFS']	['Most Frequent Sense']	[[72, 75]]	[[77, 96]]
276	Abstract This paper reports about the development of a Named Entity Recognition (NER) system for Bengali using the statistical Conditional	[[81, 84]]	[[55, 79]]	['NER']	['Named Entity Recognition']	['NER']	['a Named Entity Recognition']	[[81, 84]]	[]
277	David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing  As a more intuitive example of refinement, consider  an English-like language with categories term (TE) and  intransitive verb phrase (IV) that both include singular 	[[183, 185], [218, 220]]	[[177, 181], [192, 209]]	['TE', 'IV']	['term', 'intransitive verb']	['TE', 'IV']	['a', 'verb phrase']	[[183, 185], [218, 220]]	[]
278	 The absence of reliable single-sentence estimates points to a gap in natural language processing (NLP) research.	[[99, 102]]	[[70, 97]]	['NLP']	['natural language processing']	['NLP']	['a', 'in natural language processing']	[[99, 102]]	[]
279	Evaluation of Online Dialogue Policy Learning Techniques, Proceedings of the 8th Conference on Language Resources and Evaluation (LREC) 2012, to appear.	[[130, 134]]	[[95, 113]]	['LREC']	['Language Resources']	['LREC']	['Language Resources']	[[130, 134]]	[[95, 113]]
280	 The ACE 2005 data set alo contains a set of ariticles from the broadcast news (BN) source which is written entirely in lower case.	[[80, 82], [5, 8]]	[[64, 78]]	['BN', 'ACE']	['broadcast news']	['ACE', 'BN']	['a', 'broadcast news']	[[5, 8], [80, 82]]	[[64, 78]]
281	mosque, . . .   Detai l izat ion (DET) - a subst i tut ion of a  detai led description Y1 of a thing, s i tuat ion 	[[34, 37]]	[[16, 32]]	['DET']	['Detai l izat ion']	['DET']	['Detai l izat ion', 'a', 'i', 'ion', 'a', 'a', 's i']	[[34, 37]]	[[16, 32]]
282	"(RA (NULL))))))  (NPOS (NULL)))  (NVAR (N='BYPASS':(SINGULAR) "" ('BYPASS')))  (RN (NULL))))) "	[[34, 38], [18, 22], [1, 3], [5, 9], [24, 28], [79, 81], [83, 87]]	[[40, 60]]	['NVAR', 'NPOS', 'RA', 'NULL', 'NULL', 'RN', 'NULL']	"[""N='BYPASS':(SINGULAR""]"	['RA', 'NULL', 'NVAR']	['NPOS']	[[1, 3], [5, 9], [24, 28], [83, 87], [34, 38]]	[]
283	http://wordnet.princeton.edu/glosstag.shtml 1462 tive baseline, based on Greedy String Tiling (GST) (Wise, 1996).	[[95, 98]]	[[73, 93]]	['GST']	['Greedy String Tiling']	['GST']	['Greedy String Tiling']	[[95, 98]]	[[73, 93]]
284	. . . ( 7) To shorten notation, we use state abbreviations (e.g., CA = California :state). 	[[66, 68]]	[[71, 81]]	['CA']	['California']	['CA']	['state', 'California : state']	[[66, 68]]	[]
285	 3 Tools and Component Combination   We use the support vector machine (SVM) multi-class classifier (Crammer and Singer (2002),  	[[72, 75]]	[[48, 70]]	['SVM']	['support vector machine']	['SVM']	['support vector machine']	[[72, 75]]	[[48, 70]]
286	servations.  Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been de-	[[47, 51]]	[[13, 45]]	['PLSR']	['Partial Least Squares Regression']	['PLSR']	['Partial Least Squares Regression', 'a']	[[47, 51]]	[[13, 45]]
287	making valuable data resources publicly available . The author is a member of the Institute for Robotics and Intelligent Systems (IRIS) and wishes to acknowledge the support of the Networks of Centres of Excellenc e Program of the Government of Canada, the Natural Sciences and Engineering Research Council (NSERC) ,	[[130, 134], [308, 313]]	[[82, 128], [257, 306]]	['IRIS', 'NSERC']	['Institute for Robotics and Intelligent Systems', 'Natural Sciences and Engineering Research Council']	['IRIS', 'NSERC']	['is', 'a', 'Institute for Robotics and Intelligent Systems', 'and', 'e', 'Natural Sciences and Engineering Research']	[[130, 134], [308, 313]]	[[82, 128]]
288	NP re-annotation very helpful for the performance. We think it is because of the annotation style of the Upenn Chinese Treebank (CTB). According to Xue et al	[[129, 132], [0, 2]]	[[111, 127]]	['CTB', 'NP']	['Chinese Treebank']	['NP', 'CTB']	['Chinese Treebank']	[[0, 2], [129, 132]]	[[111, 127]]
289	mender systems using a multidimensional approach.  ACM Transactions on Information Systems (TOIS), 23(1):103?145.	[[92, 96], [51, 54]]	[[55, 90]]	['TOIS', 'ACM']	['Transactions on Information Systems']	['ACM', 'TOIS']	['a', 'Transactions on Information Systems']	[[51, 54], [92, 96]]	[[55, 90]]
290	tracted from a corpus of parsed sentences.  A super abstract role value (SuperARV) is an abstraction of the joint assignment of dependencies for	[[73, 81]]	[[46, 71]]	['SuperARV']	['super abstract role value']	['A']	['a', 'super abstract role value']	[]	[[46, 71]]
291	"duction Factor, and the ""expected utility"" of a PA-  SSF is estimated as the Global Reduction Factor:  Reduct ion  Factor  The Reduction Factor (RF)  of  a given SSFssf is RF(ss f )  = n(ssf)  - 1, where "	[[145, 147], [48, 56], [172, 174], [162, 168]]	[[127, 143]]	['RF', 'PA-  SSF', 'RF', 'SSFssf']	['Reduction Factor']	['PA-', 'SSF', 'RF']	['duction Factor', 'a', 'Reduction Factor', 'ion Factor', 'Reduction Factor', 'a', '( ss f', 'n (']	[[145, 147], [172, 174]]	[[127, 143], [127, 143]]
292	most frequent sense of the training corpus (TRAIN-MFS). However, all of them are far below to the Topic Signatures acquired using the training corpus (TRAIN). 	[[151, 156], [44, 53]]	[[134, 142], [27, 35]]	['TRAIN', 'TRAIN-MFS']	['training', 'training']	['TRAIN-MFS']	['training corpus', 'training corpus']	[[44, 53]]	[]
293	"O. Introduction.  Since 1971 the research group ""Maschinelle Syntaxanalyse"" (MasA)  has been working as a part of the project ""Linguistische Datenverar- "	[[77, 81]]	[[49, 74]]	['MasA']	['Maschinelle Syntaxanalyse']	['MasA']	['Maschinelle Syntaxanalyse', 'a']	[[77, 81]]	[[49, 74]]
294	as given in (6)b, the v-hon type with the -(u)si suffix adds the information that its subject (the first element in the ARG-ST (argument structure)) is [HON +], in addition to the information that the	[[120, 126], [153, 156]]	[[128, 146]]	['ARG-ST', 'HON']	['argument structure']	['-', 'HON+']	['argument structure']	[]	[[128, 146]]
295	second indicator of effective query is the recall at R document retrieved (Recall at R).  The last indicator measures the human effort (HE) in finding the answer. HE is 	[[136, 138], [163, 165]]	[[122, 134]]	['HE', 'HE']	['human effort']	['R']	['human effort']	[]	[[122, 134]]
296	Figure 1 Examples of contextual phenomena,  ellipsis depends on the context and means 'credit card'; it  is both a focus and an object (OBJ). 	[[136, 139]]	[[128, 134]]	['OBJ']	['object']	['OBJ']	['focus']	[[136, 139]]	[]
297	6.7 Presence of  four digits 8 Evaluation  If the token is a four digit number, it is likelier to  be a NETI (Named Entity Time). For example, 	[[104, 108]]	[[110, 127]]	['NETI']	['Named Entity Time']	['NETI']	['a', 'it', 'a', 'Named Entity Time']	[[104, 108]]	[[110, 127]]
298	 1 Introduction Data-driven machine translation (MT) relies on models that can be efficiently estimated from par-	[[49, 51]]	[[28, 47]]	['MT']	['machine translation']	[')']	['translation', 'on']	[]	[]
299	TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al, 2009a). In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data. More broadly, TM is a character mapping problem.	[[389, 391], [68, 70], [105, 108], [155, 157], [159, 161], [394, 396]]	[[368, 387], [45, 66], [76, 103]]	['MT', 'IR', 'NLP', 'IR', 'TM', 'TM']	['Machine Translation', 'Information Retrieval', 'Natural Language Processing']	['TM', 'IR', 'NLP']	['in', 'Information Retrieval', 'Natural Language Processing', 'in', 'in', 'n', 'al', 'In Machine Translation', 'at', 'in', 'a']	[[159, 161], [394, 396], [68, 70], [155, 157], [105, 108]]	[[45, 66], [76, 103]]
300	One team participated in the GRO task, and their results were compared with those of a preliminary system prepared by the task organizers. An analysis of the evaluation results leads us to study issues such as the need to consider the ontology structure and the need for semantic analysis, which are not seriously dealt with by current approaches to event extraction. 6 Organization of the workshop The BioNLP Shared Task 2013 (BioNLP-ST) workshop was organized as part of the ACL BioNLP 2013 workshop. After submission of their system results, participants were invited to submit a paper on their systems to the workshop.	[[428, 437], [29, 32], [477, 480], [481, 487]]	[[403, 421]]	['BioNLP-ST', 'GRO', 'ACL', 'BioNLP']	['BioNLP Shared Task']	['GRO', 'ACL']	['a', 'as', 'BioNLP Shared Task 2013', 'BioNLP 2013', 'a']	[[29, 32], [477, 480]]	[]
301	A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th European Conference on Information Retrieval (ECIR), pages 557?564. 	[[145, 149]]	[[99, 143]]	['ECIR']	['European Conference on Information Retrieval']	['ECIR']	['In', 'European Conference on Information Retrieval']	[[145, 149]]	[[99, 143]]
302	sentence, but many equally good translation options.  Often, machine translation (MT) systems are only evaluated quantitatively, e.g. by the use of automatic	[[82, 84]]	[[61, 80]]	['MT']	['machine translation']	['MT']	['translation', 'machine translation']	[[82, 84]]	[[61, 80]]
303	syntactic tree fragments (STFs) and partial tree fragments (PTFs) 2.2.1 Syntactic Tree Kernels (STK) An STF is a connected subset of the nodes and	[[96, 99], [26, 30], [60, 64], [104, 107]]	[[72, 94], [0, 24], [36, 58]]	['STK', 'STFs', 'PTFs', 'STF']	['Syntactic Tree Kernels', 'syntactic tree fragments', 'partial tree fragments']	['STFs', 'STK']	['syntactic', 'tree fragments', 'partial tree fragments', 'Syntactic Tree Kernels', 'a']	[[26, 30], [96, 99]]	[[36, 58], [72, 94]]
304	PAST (- +)  Finite verbs are specified as (PAST +) if they are in  the past tense, and as (PAST -) otherwise. 	[[91, 97], [0, 4], [43, 47]]	[[71, 81]]	['PAST -', 'PAST', 'PAST']	['past tense']	['PAST']	['Finite', 'in']	[[0, 4], [43, 47]]	[]
305	cal and clinical applications. Early work relied on the Gene Ontology (GO)3, which is a hierarchy of terms used to describe genomic information.	[[71, 73]]	[[56, 69]]	['GO']	['Gene Ontology']	['GO']	['Gene Ontology', 'to']	[[71, 73]]	[[56, 69]]
306	cke et al, 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing al-	[[62, 65]]	[[42, 60]]	['MBR']	['Minimum Bayes Risk']	['MBR']	['Minimum Bayes Risk', 'in', 'in', 'a']	[[62, 65]]	[[42, 60]]
307	these instances. We treat the language ID task as a coreference resolution (CoRef) problem: a mention is an IGT or a language name appearing in a	[[76, 81], [39, 41], [108, 111]]	[[52, 74]]	['CoRef', 'ID', 'IGT']	['coreference resolution']	['ID', 'CoRef', 'IGT']	['coreference resolution']	[[39, 41], [76, 81], [108, 111]]	[[52, 74]]
308	Figure 1: Vocabulary growth rates for English,  Spanish, German and Korean for the Spontaneous  Scheduling Task (SST). 	[[113, 116]]	[[83, 111]]	['SST']	['Spontaneous  Scheduling Task']	['SST']	['Spontaneous Scheduling Task']	[[113, 116]]	[]
309	Sellers algorithms  2.1 Algorithm Principle  The Wagner & Fischer (W&F) dynamic  programming algorithm in Figure 3 gives tile 	[[67, 70]]	[[49, 65]]	['W&F']	['Wagner & Fischer']	[]	['Wagner & Fischer', '& F']	[]	[[49, 65]]
310	sponding to each of the cluster. Fairly intuitively we computed the Longest Common Subsequence(LCS) between the sentences in each cluster which we then	[[95, 98]]	[[68, 93]]	['LCS']	['Longest Common Subsequenc']	['LCS']	['Longest Common Subsequence']	[[95, 98]]	[]
311	(selected expansion  terms are in italic)  OR(locator, finder, location,  directory) 	[[43, 45]]	[[47, 53]]	['OR']	['ocator']	['OR']	['locator', ', finder , location ,']	[[43, 45]]	[]
312	Coarse-to-fine n-best parsing and MaxEnt discriminative reranking Eugene Charniak and Mark Johnson Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University	[[155, 160], [34, 40]]	[[99, 153]]	['BLLIP', 'MaxEnt']	['Brown Laboratory for Linguistic Information Processing']	['BLLIP']	['Laboratory for Linguistic Information Processing']	[[155, 160]]	[]
313	In the table, P is precision; R is  recall; P&R is the harmonic mean of precision and  recall  (P&R = (2*P*R) / (P+R), corresponding to a  F-measure with a ?	[[96, 99], [44, 47]]	[[103, 108], [19, 28], [36, 42]]	['P&R', 'P&R']	['2*P*R', 'precision', 'recall']	['R']	['is precision', 'is recall', 'is', 'precision', 'a', 'a']	[]	[[19, 28]]
314	Similarity  task  at  SEM  2013.  The  Semantic  Textual Similarity Core task  (STS)  computes the  degree  of  semantic  equivalence  between  two 	[[80, 83], [22, 25]]	[[57, 77]]	['STS', 'SEM']	['Similarity Core task']	['SEM', 'STS']	['Similarity', 'Semantic Textual Similarity Core']	[[22, 25], [80, 83]]	[]
315	tion, corpora from seven different genres are used: the MSNBC broadcasting conversation (BC), the CNN broadcasting news (BN), the Sinorama news magazine (MZ), the WSJ newswire (NW), and the	[[121, 123], [56, 61], [89, 91], [98, 101], [154, 156], [163, 166], [177, 179]]	[[102, 119], [62, 87], [144, 152], [167, 175]]	['BN', 'MSNBC', 'BC', 'CNN', 'MZ', 'WSJ', 'NW']	['broadcasting news', 'broadcasting conversation', 'magazine', 'newswire']	['MSNBC', 'BC', 'CNN', 'BN', 'MZ', 'WSJ', 'NW']	['broadcasting conversation', 'broadcasting', 'news', 'Sinorama news magazine', 'newswire']	[[56, 61], [89, 91], [98, 101], [121, 123], [154, 156], [163, 166], [177, 179]]	[[62, 87], [167, 175]]
316	Using the XTAG Tree Adjoining Grammar [Gro01], we start by listing these variations. Indeed a Tree Adjoining Grammar (TAG) lists the set of all possible syntactic configurations for basic clauses and groups them into so-called (tree) families.	[[118, 121], [10, 14], [39, 44]]	[[94, 116]]	['TAG', 'XTAG', 'Gro01']	['Tree Adjoining Grammar']	['XTAG']	['Tree Adjoining Grammar', 'a Tree Adjoining Grammar']	[[10, 14]]	[[94, 116]]
317	In order to quantify the level of noise in the collected SMS data, we built a character-level language model(LM)13 using the questions in the FAQ data-set (vocabulary	[[109, 111], [57, 60], [142, 145]]	[[94, 107]]	['LM', 'SMS', 'FAQ']	['language mode']	['SMS', 'LM', 'FAQ']	['a', 'language model']	[[57, 60], [109, 111], [142, 145]]	[]
318	Chinese research and applications. We refer to it  as CLO (Chinese Lexical Ontology). In addition 	[[54, 57]]	[[59, 83]]	['CLO']	['Chinese Lexical Ontology']	['CLO']	['Chinese', 'to', 'Chinese Lexical Ontology']	[[54, 57]]	[[59, 83]]
319	Stochastic Gradient Descent (SGD) (Tsuruoka et al., 2009) `2 regularization, hinge loss Bernoulli Naive Bayes (B-NB) (McCallum et al., 1998) ?	[[111, 115], [29, 32]]	[[88, 109], [0, 27]]	['B-NB', 'SGD']	['Bernoulli Naive Bayes', 'Stochastic Gradient Descent']	['SGD', 'B-NB']	['Stochastic Gradient Descent', 'hinge loss Bernoulli Naive Bayes']	[[29, 32], [111, 115]]	[[0, 27]]
320	issue. Send membership applications and  address changes to Betty Walker (ACL),  Bellcore, 445 South Street, MRE 2A379, 	[[74, 77], [109, 112]]	[[41, 72]]	['ACL', 'MRE']	['address changes to Betty Walker']	['ACL', 'MRE']	['Betty Walker']	[[74, 77], [109, 112]]	[]
321	training (Blum and Mitchell, 1998) has been successfully applied to English named entity recognition (NER) (Collins & Singer [henceforth C&S] (1999)).	[[102, 105], [137, 140]]	[[94, 100]]	['NER', 'C&S']	['nition']	['NER']	['named entity recognition']	[[102, 105]]	[]
322	Center who will send the artillery fire when given the appropriate information.  In line 1, G19?s utterance is interpreted as a Warning Order - Method of Fire (WO-MOF), describing the kind of artillery fire requested, whose value is ?	[[160, 166]]	[[128, 158]]	['WO-MOF']	['Warning Order - Method of Fire']	[]	['s', 'a Warning Order - Method of Fire', 'of']	[]	[]
323	REPRESENTATION OF INTENSIONAI,  CON'FEXTS  The main extension to the work reported in \[Di Eugenio &  Lesnro II'/l consists in the introduction of CONTEXT SPACES (CS),  which enable us to treat the intensional contexts along the lines pro.. 	[[163, 165]]	[[147, 161]]	['CS']	['CONTEXT SPACES']	['CS']	['CONTEXT SPACES']	[[163, 165]]	[[147, 161]]
324	machine learning algorithms from the Python scikit-learn4 package: Na??ve Bayes (NB), Maximum Entropy (MaxEnt), and Support Vector Machines (SVM).	[[103, 109], [81, 83], [141, 144]]	[[86, 101], [67, 79], [116, 139]]	['MaxEnt', 'NB', 'SVM']	['Maximum Entropy', 'Na??ve Bayes', 'Support Vector Machines']	['NB', 'MaxEnt', 'SVM']	['Na ? ? ve Bayes', 'Maximum Entropy', 'Support Vector Machines']	[[81, 83], [103, 109], [141, 144]]	[[86, 101], [116, 139]]
325	In TAC 2008 Summarization track, all submitted runs were scored with the ROUGE (Lin, 2004) and Basic Elements (BE) metrics (Hovy et al.,	[[111, 113], [3, 6]]	[[95, 109]]	['BE', 'TAC']	['Basic Elements']	['TAC', 'ROUGE', 'BE']	['Basic Elements']	[[3, 6], [111, 113]]	[[95, 109]]
326	on specific characteristics of the signs, have appeared in the international community: HamNoSys  (Prillwitz et al 1989), SEA (Sistema de Escritura  Alfab?tica) (Herrero, A., 2004) and SignWriting 	[[122, 125], [88, 96]]	[[127, 134]]	['SEA', 'HamNoSys']	['Sistema']	['SEA']	['Sistema de Escritura Alfab ? tica']	[[122, 125]]	[]
327	take advantage of the capabilities of a vectofized  concurrent processor (an Alliant FX/80) which consists  of a cluster of up to 8 computing elements (CE's) that  can execute code in vector concurrent mode.	[[152, 156], [85, 90]]	[[132, 150]]	"[""CE's"", 'FX/80']"	['computing elements']	"['FX/80', ""CE's""]"	['a', 'a', '8 computing elements', 'in']	[[85, 90], [152, 156]]	[]
328	Concerning the second objective, we will devise our new measure, known as the Odds of Unithood (OU), which are derived using Bayes Theorem and founded on a few elementary probabil-	[[96, 98]]	[[83, 94]]	['OU']	['of Unithood']	['OU']	['Odds of Unithood', 'a']	[[96, 98]]	[]
329	 1 Introduction  Word sense disambiguation (WSD) is perhaps the  great open problem at the lexical level of natural 	[[44, 47]]	[[17, 42]]	['WSD']	['Word sense disambiguation']	[')']	['sense disambiguation', 'is', 'at']	[]	[]
330	Introduction  In a large natural language processing system,  such as a machine translation system (MTS), am-  biguity resolution is a critical problem.	[[100, 103]]	[[72, 98]]	['MTS']	['machine translation system']	['MTS']	['a', 'system', 'a machine translation system', 'a']	[[100, 103]]	[]
331	rules are learned from the alignment of manuallytranscribed text (T ) with automatically-generated transcripts (TASR) of training data, ranked according to a scoring function (S) and applied to the	[[112, 116]]	[[99, 110], [60, 64]]	['TASR']	['transcripts', 'text']	['TASR']	['text', 'a scoring function']	[[112, 116]]	[[60, 64]]
332	resented by an order domain (DOM), which is a list  of domain objects, whose relative order must satisfy  a set of linear precedence (LP) constraints. The or- 	[[134, 136], [29, 32]]	[[115, 132], [21, 27]]	['LP', 'DOM']	['linear precedence', 'domain']	['DOM', 'LP']	['a', 'a', 'linear precedence']	[[29, 32], [134, 136]]	[[115, 132]]
333	Donnell, 1982).  Def in i t ion 1 A deterministic tree automaton (DTA)  is a 5-tuple M = (Q, ~, ~, qo, F), where Q is a finite 	[[66, 69]]	[[36, 64]]	['DTA']	['deterministic tree automaton']	['A']	['i', 't ion', 'deterministic tree automaton', 'is a', 'is a']	[]	[[36, 64]]
334	Cui et al experimented with two approaches to rank the candidate answers, called Bigram Model and Profile Hidden Markov Model (PHMM). Both	[[127, 131]]	[[98, 125]]	['PHMM']	['Profile Hidden Markov Model']	['PHMM']	['Model', 'Profile Hidden Markov Model']	[[127, 131]]	[[98, 125]]
335	"\canonical"" dependency direction under welldened conditions, distinguishing between ordre lineaire (linear precedence(LP)) and ordre structural (immediate dominance(ID))."	[[120, 122]]	[[102, 118]]	['LP']	['linear precedenc']	['LP', 'ID']	['linear precedence', 'immediate dominance']	[[120, 122]]	[]
336	both tasks together would be desirable. In this section, we propose the use of factorial CRF (F-CRF) (Sutton et al, 2007), which has previously been	[[94, 99]]	[[79, 92]]	['F-CRF']	['factorial CRF']	[]	['CRF']	[]	[]
337	 1 Introduction Word sense disambiguation (WSD) is a key enabling technology.	[[43, 46]]	[[16, 41]]	['WSD']	['Word sense disambiguation']	[')']	['sense disambiguation', 'is a']	[]	[]
338	Turning to emerging structure, PTT assumes that participants perform (often fragmentary) contributions, discourse units (DUs), which are dynamic propositions (DRSs in the	[[121, 124], [31, 34], [159, 163]]	[[104, 119]]	['DUs', 'PTT', 'DRSs']	['discourse units']	['PTT', 'DUs', 'DRSs']	['discourse units']	[[31, 34], [121, 124], [159, 163]]	[[104, 119]]
339	languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of colloca-	[[82, 85]]	[[68, 80]]	['BoW']	['bag of words']	['BoW']	['a bag of words', 'of', 'of']	[[82, 85]]	[]
340	In this paper we have systematically studied these complex relations involving SVC and VopC for BP, which constitute a challenge to Natural Language Processing (NLP) systems, and have been often ignored in related work.	[[161, 164], [79, 82], [96, 98], [87, 91]]	[[132, 159]]	['NLP', 'SVC', 'BP', 'VopC']	['Natural Language Processing']	['SVC', 'VopC', 'BP', 'NLP']	['a', 'Natural Language Processing', 'in']	[[79, 82], [87, 91], [96, 98], [161, 164]]	[[132, 159]]
341	specification whose precise informational content and form is in turn defined by an appropriate Document Type Definition (DTD). 	[[122, 125]]	[[96, 120]]	['DTD']	['Document Type Definition']	['DTD']	['in', 'Document Type Definition']	[[122, 125]]	[[96, 120]]
342	general idea widely used in this kind of systems: if  an input sentence is syntactically ill-formed, i.e. it  cannot be assigned a syntactic structure (SyntS),  the system considers minimal changes that enable it 	[[152, 157]]	[[131, 150]]	['SyntS']	['syntactic structure']	['SyntS']	['a syntactic structure']	[[152, 157]]	[]
343	1203 Figure 1: Bootstrapped Learning. ( HT = hashtag; HP = hashtag pattern) dov et al.,	[[54, 56], [40, 42]]	[[59, 74], [45, 52]]	['HP', 'HT']	['hashtag pattern', 'hashtag']	['HT', 'HP']	['hashtag', 'hashtag pattern']	[[40, 42], [54, 56]]	[[45, 52], [59, 74]]
344	2003) on the term-sentence matrix of human model summaries used in the Document Understanding Conference (DUC) 2007 Pyramid evaluation1.	[[106, 109]]	[[71, 104]]	['DUC']	['Document Understanding Conference']	['DUC']	['on', 'in', 'Document Understanding Conference']	[[106, 109]]	[[71, 104]]
345	1 Introduction Visualizing Web search results remains an open problem in Information Retrieval (IR). For exam-	[[96, 98]]	[[73, 94]]	['IR']	['Information Retrieval']	['IR']	['Information Retrieval']	[[96, 98]]	[[73, 94]]
346	Here, we describe a naive baseline approach to arrange nearest neighbors next to each other by using Independent Random Projections (IRP). In this	[[133, 136]]	[[101, 131]]	['IRP']	['Independent Random Projections']	['IRP']	['a', 'Independent Random Projections', 'In']	[[133, 136]]	[[101, 131]]
347	Figure 2: Algorithm for scope detection by MRS crawling a Formally: If an EP shares its label with the negation cue, or is a quantifier whose restriction (RSTR) is = q	[[155, 159], [43, 46], [74, 76]]	[[142, 153]]	['RSTR', 'MRS', 'EP']	['restriction']	['MRS', 'EP', 'RSTR']	['a', 'or is a quantifier whose', 'is']	[[43, 46], [74, 76], [155, 159]]	[]
348	x?D(X) P (X = x|W1 = w1, . . . , WN = wN ). ( 1)	[[33, 35]]	[[38, 40]]	['WN']	['wN']	['WN']	[]	[[33, 35]]	[]
349	"fr, veronis@fraixll  .univ-aix. fr  Abstract, MULTEXT (Multilingual Text ""Fools and  Corpora) is the largest project funded in the Commission "	[[46, 53], [22, 30]]	[[55, 72]]	['MULTEXT', 'univ-aix']	['Multilingual Text']	['MULTEXT']	['Multilingual Text', 'Fools and Corpora', 'in']	[[46, 53]]	[[55, 72]]
350	In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96). 	[[82, 91]]	[[55, 80]]	['COLING-96']	['Computational Linguistics']	['COLING-96']	['on Computational Linguistics']	[[82, 91]]	[]
351	(located at lower levels) in a taxonomy.  In the Information Retrieval (IR) community, browsing taxonomies.	[[72, 74]]	[[49, 70]]	['IR']	['Information Retrieval']	['IR']	['at', 'a', 'In', 'Information Retrieval']	[[72, 74]]	[[49, 70]]
352	 11 4 Lexical Analysis and Named Entity Recognition (NE ) The lexicon we used contains only syntactic information such as parts of speech and subcategorization frames .	[[53, 55]]	[[28, 39]]	['NE']	['amed Entity']	[')']	['Entity Recognition']	[]	[]
353	 2) Prediction precision(PP) =  number of words with correct BPs(CortBP)  total word number (TWN) 	[[65, 71]]	[[53, 63]]	['CortBP']	['correct BP']	[')', ')']	['precision']	[]	[]
354	We used a  3-pass decoding strategy, in which the first pass uses the  speaker independent (SI) vowelized system, the second  pass uses the speaker adaptive (SA) non-vowelized 	[[92, 94], [158, 160]]	[[71, 90], [140, 156]]	['SI', 'SA']	['speaker independent', 'speaker adaptive']	['SI', 'SA']	['a', 'in', 'speaker independent', 'speaker adaptive']	[[92, 94], [158, 160]]	[[71, 90], [140, 156]]
355	To perform this relocation quickly, Yata et al. ( 2009) introduced two additional one-dimensional arrays, called NLINK (node link) and BLOCK. For each node, NLINK stores the label needed to reach its	[[113, 118], [157, 162]]	[[120, 129]]	['NLINK', 'NLINK']	['node link']	['NLINK', 'BLOCK']	['node link', 'node']	[[113, 118], [157, 162]]	[[120, 129]]
356	Our translation method performs sense-based translation and pronunciation-based translation on the basis of statistical machine translation (SMT) methods.	[[141, 144]]	[[108, 139]]	['SMT']	['statistical machine translation']	['SMT']	['translation', 'translation', 'translation on', 'statistical machine translation']	[[141, 144]]	[[108, 139]]
357	from dictionaries and hand-tailored heuristics. It applies statistical named entity recognition (NER) methods to the more challenging task of deidenti-	[[97, 100]]	[[71, 95]]	['NER']	['named entity recognition']	['NER']	['named entity recognition']	[[97, 100]]	[[71, 95]]
358	(SO) weighting in the spirit of (Zhang et al, 2007; Li et al, 2007), and finally (f) the same system but with the target order (TO) weighting. 	[[128, 130], [1, 3]]	[[114, 126], [73, 80]]	['TO', 'SO']	['target order', 'finally']	['SO', 'TO']	['target order']	[[1, 3], [128, 130]]	[[114, 126]]
359	::= RL RL ::= prep rel loc word(S, object word) RL=rel loc word |	[[48, 50], [4, 6], [7, 9]]	[[51, 58]]	['RL', 'RL', 'RL']	['rel loc']	['RL', 'RL']	['prep rel', 'loc word', 'word', 'loc word']	[[48, 50], [4, 6], [7, 9], [48, 50], [4, 6], [7, 9]]	[]
360	Figure 4: Weight of links and category selection  3 .1  Representat ion  o f  OPED  The Oxford Pictorial English Dictionary(OPED) h,~s  very simple form of text and picture (Fig.3).	[[124, 128], [78, 82]]	[[88, 122]]	['OPED', 'OPED']	['Oxford Pictorial English Dictionar']	['OPED']	['o', 'Oxford Pictorial English Dictionary']	[[124, 128], [78, 82]]	[]
361	Furthermore, if we can rank preferred verbs by domains, inferred verbs can be more useful to applications that focus on specific domains. Hence we defined Domain Verb Association (DVA) to measure how frequently inferred verbs are used with domain instances that can be used as subjects or	[[180, 183]]	[[155, 178]]	['DVA']	['Domain Verb Association']	['DVA']	['on', 'Domain Verb Association']	[[180, 183]]	[[155, 178]]
362	follow-up study, Le Calvez (2007) compared KL to other indicators, namely the Jensen?Shannon divergence (JS) and the Bhattacharyya coefficient (BC).3 2.2 Lexical indicators	[[144, 146], [43, 45], [105, 107]]	[[117, 142]]	['BC', 'KL', 'JS']	['Bhattacharyya coefficient']	['KL', 'JS', 'BC']	['Shannon divergence', 'Bhattacharyya coefficient']	[[43, 45], [105, 107], [144, 146]]	[[117, 142]]
363	that symbol?). The categories are:  1) Words as Words (WW): Within the context of  the sentence, the candidate phrase is used to 	[[55, 57]]	[[39, 53]]	['WW']	['Words as Words']	['WW']	['Words as Words']	[[55, 57]]	[[39, 53]]
364	200 Acknowledgments This research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Qatar Foundation in collaboration with MIT.	[[84, 87], [134, 138]]	[[54, 82], [98, 132]]	['ALT', 'QCRI']	['Arabic Language Technologies', 'Qatar Computing Research Institute']	['ALT', 'QCRI', 'MIT']	['Arabic Language Technologies', 'at Qatar Computing Research Institute', 'Qatar', 'in']	[[84, 87], [134, 138]]	[[54, 82]]
365	Net?s ontological concepts and the correct sense of each word assigned by a Word Sense Disambiguation(WSD) module to extract three sets of pat-	[[102, 105]]	[[76, 100]]	['WSD']	['Word Sense Disambiguatio']	['WSD']	['s', 'a Word Sense Disambiguation']	[[102, 105]]	[]
366	guage model. Instead, with information gathered  from interviews of subject matter experts (SME's),  we developed a handwritten grammar using Gemini 	[[92, 97]]	[[68, 90]]	"[""SME's""]"	['subject matter experts']	"[""SME's""]"	['subject matter experts', 'a']	[[92, 97]]	[[68, 90]]
367	The next most confident 600 tuples (i.e., those numbered 1201?1800) were used to build a development set (DEV1) and the next most confident 600 (those numbered 1801?2400) were used	[[106, 110]]	[[89, 104]]	['DEV1']	['development set']	[]	['development']	[]	[]
368	 INTRODUCTION  Compound Nouns: Compound nouns (CNs) are a  commonly occurring construction in language 	[[47, 50]]	[[31, 45]]	['CNs']	['Compound nouns']	['CNs']	['Compound', 'Compound nouns']	[[47, 50]]	[[31, 45]]
369	more details of this at the end of the next section.  4 Compiling the DiCo (dictionary-like) database into a lexical system	[[70, 74]]	[[76, 86]]	['DiCo']	['dictionary']	['DiCo']	[]	[[70, 74]]	[]
370	in this paper. We choose the classical optimization algorithm limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989).	[[83, 89]]	[[62, 81]]	['L-BFGS']	['limited memory BFGS']	[]	['limited memory BFGS']	[]	[[62, 81]]
371	provide an exhaustive list of connectives in the                                                    4 S=Subject; IO=Indirect Object; DO=Direct Object;  V=Verb; ERG=Ergative; DAT=Dative 	[[113, 115], [133, 135], [160, 163], [174, 177]]	[[116, 131], [136, 149], [104, 111], [154, 158], [164, 172], [178, 184]]	['IO', 'DO', 'ERG', 'DAT']	['Indirect Object', 'Direct Object', 'Subject', 'Verb', 'Ergative', 'Dative']	[]	['Object', 'Object']	[]	[]
372	Non Verbal Behaviors.   Sources abbreviated as: SPKR = Speaker; F = Friendship; V = Visibility; Rte = Route,  	[[48, 52], [96, 99]]	[[55, 62], [68, 78], [84, 94], [102, 107]]	['SPKR', 'Rte']	['Speaker', 'Friendship', 'Visibility', 'Route']	['SPKR']	['Speaker', 'Visibility', 'Route']	[[48, 52]]	[[55, 62], [84, 94], [102, 107]]
373	Supertagging involves assigning words lexical entries based on a lexicalized grammatical theory, such as Combinatory Categorial Grammar (CCG) (Steedman, 2000) Tree-adjoining Grammar (Joshi,	[[137, 140]]	[[105, 135]]	['CCG']	['Combinatory Categorial Grammar']	['CCG']	['a', 'Combinatory Categorial Grammar', 'Grammar']	[[137, 140]]	[[105, 135]]
374	In Proc, of the lOth Pacific Asia  Conference on Language, Information and Com-  putation (PACLING), pages 163-172. 	[[91, 98], [3, 7]]	[[21, 89]]	['PACLING', 'Proc']	['Pacific Asia  Conference on Language, Information and Com-  putation']	['PACLING']	['In', ',', 'on', ', Information and Com- putation', ',']	[[91, 98]]	[]
375	Beth Bryson). In addition, Robert Moore and Eric Jack-  son (SRI) provided critical help in designing and debug-  ging the code to support the minimal/maximal nswer 	[[61, 64]]	[]	['SRI']	[]	['SRI']	[]	[[61, 64]]	[]
376	plies that 58% of the time, our approach has improved the question relevance compared to that of the original candidate list (OList). 	[[126, 131]]	[[101, 124]]	['OList']	['original candidate list']	['OList']	['original candidate list']	[[126, 131]]	[[101, 124]]
377	In the hierarchy of relations, Arguments (ARG) include Subject (SUBJ), Object (OBJ), Indirect Object (INDOBJ), Indirect Complement (IN-	[[64, 68], [79, 82], [42, 45], [102, 108]]	[[55, 62], [71, 77], [31, 40], [85, 100]]	['SUBJ', 'OBJ', 'ARG', 'INDOBJ']	['Subject', 'Object', 'Arguments', 'Indirect Object']	['ARG', 'SUBJ', 'OBJ', 'INDOBJ']	['In', 'Arguments', 'Subject', 'Object', 'Indirect Object', 'Indirect']	[[42, 45], [64, 68], [79, 82], [102, 108]]	[[31, 40], [55, 62], [71, 77], [85, 100]]
378	Higher log-likelihood corresponds to improved model fit. However, typi-cally it is desirable to penalize a higher number of hidden states, since increasing the model complexi-ty results in tradeoffs that may not be fully warrant-ed by the improvement in model fit. In this work, we utilize the Akaike Information Criterion (AIC), a standard penalized log-likelihood metric (Akaike, 1976).     	[[324, 327]]	[[294, 322]]	['AIC']	['Akaike Information Criterion']	['AIC']	['a', 'In', 'Akaike Information Criterion', 'a', 'Akaike']	[[324, 327]]	[[294, 322]]
379	and van Zaanen, 2006).  A Logical Graph (LG) is a directed, bipartite graph with two types of vertices, concepts and re-	[[41, 43]]	[[26, 39]]	['LG']	['Logical Graph']	['LG']	['Logical Graph', 'a']	[[41, 43]]	[[26, 39]]
380	 ? Abbreviation features (ABB): For every term in the noslang dictionary, we checked whether	[[26, 29]]	[[3, 15]]	['ABB']	['Abbreviation']	[')']	[]	[]	[]
381	tion.   In our decoders, language model(LM) is used  for translating edus in Formula(5),(6),(7),(8), but 	[[40, 42]]	[[25, 38]]	['LM']	['language mode']	['LM']	['language model']	[[40, 42]]	[]
382	 The two subgraphs are a parse structure subgraph (PSS) and a linear order subgraph (LOS). 	[[85, 88], [51, 54]]	[[62, 83], [25, 49]]	['LOS', 'PSS']	['linear order subgraph', 'parse structure subgraph']	[')']	['a', 'structure subgraph', 'a', 'order subgraph']	[]	[]
383	The proposals directly affect the organization o f   groups w i t h i n  EOP including the Office of Telecommmicatians Policy  (UP) ; the 0fiice of Science and Technology Policy (OSTP) ; the Intergovernmental  Science, Engineering, and Technology Advisory Panel (ISETAP) ; the President s 	[[179, 183], [73, 76], [128, 130], [263, 269]]	[[138, 177], [191, 260]]	['OSTP', 'EOP', 'UP', 'ISETAP']	['0fiice of Science and Technology Policy', 'Intergovernmental  Science, Engineering, and Technology Advisory Pane']	[')']	['o f', 'w i', 't h i n', 'of Telecommmicatians Policy', 'of Science and Technology Policy', 'Science , Engineering , and Technology Advisory Panel']	[]	[]
384	Machine learning in automated text categorization. In ACM computing surveys (CSUR). 	[[77, 81], [54, 57]]	[[58, 75]]	['CSUR', 'ACM']	['computing surveys']	['ACM', 'CSUR']	['in', 'computing surveys']	[[54, 57], [77, 81]]	[[58, 75]]
385	4.1 Base Extraction As first step of the mention selection stage, we extracted all the noun phrases (NP), pronouns (PRP), and possessive pronouns (PRP$) for English and	[[101, 103]]	[[87, 99]]	['NP']	['noun phrases']	['NP', 'PRP']	['noun phrases', 'pronouns', 'pronouns']	[[101, 103]]	[[87, 99]]
386	strict our attention to documents on this topic.  Thomson Reuter?s Web of Science (WOS), a database of scientific journal and conference arti-	[[83, 86]]	[[67, 81]]	['WOS']	['Web of Science']	['WOS']	['s', 'Web of Science', 'of']	[[83, 86]]	[[67, 81]]
387	It has emerged in the context of information extraction (IE) and text mining (TM). The automatic recog-	[[78, 80], [57, 59]]	[[65, 76], [33, 55]]	['TM', 'IE']	['text mining', 'information extraction']	[')']	['in', 'extraction', 'mining']	[]	[]
388	Format: type_of_word TAG type_of_word TAG ...  NN = Noun, NN-PL = Plural Noun  DET = Determiner, PREP = Preposition  POS = Possessive, J J = Adjective 	[[79, 82], [97, 101], [21, 24], [38, 41], [47, 49], [58, 63], [117, 120], [135, 138]]	[[85, 95], [104, 115], [52, 56], [66, 77], [123, 133], [141, 150]]	['DET', 'PREP', 'TAG', 'TAG', 'NN', 'NN-PL', 'POS', 'J J']	['Determiner', 'Preposition', 'Noun', 'Plural Noun', 'Possessive', 'Adjective']	['TAG', 'NN', 'DET', 'PREP', 'POS', 'JJ']	['Noun', 'Plural Noun', 'Determiner', 'Preposition', 'Possessive']	[[21, 24], [38, 41], [47, 49], [79, 82], [97, 101], [117, 120]]	[[52, 56], [66, 77], [85, 95], [104, 115], [123, 133]]
389	age caption generator. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.	[[90, 94], [30, 34]]	[[49, 88]]	['CVPR', 'IEEE']	['Computer Vision and Pattern Recognition']	['IEEE', 'CVPR']	['Conference', 'on Computer Vision and Pattern Recognition']	[[30, 34], [90, 94]]	[]
390	F = F-Measure with Recall and Precision Weighted Equally  J = Japanese S = Spanish  ME = Microelectronics  Mul t i l ingua l  	[[84, 86]]	[[89, 105], [4, 13], [62, 70], [75, 82]]	['ME']	['Microelectronics', 'F-Measure', 'Japanese', 'Spanish']	['ME']	['F', 'with Recall', 'Weighted Equally', 'Japanese', 'S', 'Spanish', 'Microelectronics', 't i l']	[[84, 86]]	[[62, 70], [75, 82], [89, 105]]
391	 Generation Challenges 2010 brought together three sets of STECs: the three GREC Challenges, GREC Named Entity Generation (GREC-NEG), Named Entity Reference Detection (GREC-NER), and Named Entity Reference Regeneration	[[123, 131], [59, 64], [76, 80], [168, 176]]	[[93, 121], [183, 218]]	['GREC-NEG', 'STECs', 'GREC', 'GREC-NER']	['GREC Named Entity Generation', 'Named Entity Reference Regeneration']	['STECs']	['Generation', 'GREC', 'GREC Named Entity Generation', 'Named Entity Reference Detection', 'Named Entity Reference']	[[59, 64]]	[[93, 121]]
392	(the concept <food,nutrient>).  The aim of using Selectional Preferences (SP) in SRL is to generalize from the argument heads in	[[74, 76], [81, 84]]	[[49, 72]]	['SP', 'SRL']	['Selectional Preferences']	['SP', 'SRL']	['Selectional Preferences', 'in']	[[74, 76], [81, 84]]	[[49, 72]]
393	the use of lexical cohesion devices in each version of MT and HT in terms of the following two ratios, LC = lexical cohesion devices / content words, RC = repetition / content words.	[[103, 105], [55, 57], [62, 64], [150, 152]]	[[108, 124], [155, 175]]	['LC', 'MT', 'HT', 'RC']	['lexical cohesion', 'repetition / content']	['MT', 'HT', 'LC', 'RC']	['lexical cohesion', 'lexical cohesion', '/ content', 'repetition / content']	[[55, 57], [62, 64], [103, 105], [150, 152]]	[[108, 124], [108, 124], [155, 175]]
394	  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177?182, October 25-29, 2014, Doha, Qatar.	[[90, 95]]	[[40, 88]]	['EMNLP']	['Empirical Methods in Natural Language Processing']	['),']	['Methods in Natural Language Processing (']	[]	[]
395	pus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC). 	[[128, 131], [5, 8]]	[[108, 126]]	['WTC', 'EBC']	['Wathctower Society']	['EBC', 'WTC']	['Wathctower Society']	[[5, 8], [128, 131]]	[[108, 126]]
396	utes of training time on an average laptop computer.  This model, the deep averaging network (DAN), works in three simple steps:	[[94, 97]]	[[70, 92]]	['DAN']	['deep averaging network']	['DAN']	['an', 'deep averaging network', 'in']	[[94, 97]]	[[70, 92]]
397	pose augmenting queries in the style of relevance  feedback (Salton and Buckley 1990), Kalashnikov  (2007) treat Web Person Search (WePS) as a disambiguation problem whose objective is to distin-	[[132, 136]]	[[113, 130]]	['WePS']	['Web Person Search']	['WePS']	['Web Person Search']	[[132, 136]]	[[113, 130]]
398	derstand what is being tested. In projects where independent verification and validation (IVV) is required this might be a problem, as most stake-	[[90, 93]]	[[49, 88]]	['IVV']	['independent verification and validation']	['IVV']	['In', 'independent verification and validation', 'a']	[[90, 93]]	[[49, 88]]
399	ken varieties of Arabic, are still lacking. We present ELISSA, a machine translation (MT) system for DA to MSA.	[[86, 88]]	[[65, 84]]	['MT']	['machine translation']	['ELISSA', 'MT', 'DA', 'MSA']	['a machine translation']	[[86, 88]]	[]
400	 CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority	[[81, 84], [1, 6]]	[[51, 79]]	['NRF', 'CSIDM']	['National Research Foundation']	[')']	['a', 'Research Foundation']	[]	[]
401	corpora. For the preposition and determiner errors, we adopt a statistical machine translation (SMT)based approach, aiming at correcting errors in con-	[[96, 99]]	[[63, 94]]	['SMT']	['statistical machine translation']	['SMT']	['a statistical machine translation', 'at', 'in']	[[96, 99]]	[]
402	umn.  Log frequency (LF) and global entropy (GE) are correlated.	[[21, 23], [45, 47]]	[[6, 19], [29, 43]]	['LF', 'GE']	['Log frequency', 'global entropy']	['LF', 'GE']	['Log frequency', 'global entropy']	[[21, 23], [45, 47]]	[[6, 19], [29, 43]]
403	2002. DAML agent semantic communications service (ASCS) http://oak.teknowledge.com:8080/daml/damlquery.jsp	[[50, 54], [6, 10]]	[[11, 48]]	['ASCS', 'DAML']	['agent semantic communications service']	['ASCS']	['agent semantic communications service']	[[50, 54]]	[[11, 48]]
404	The most common approach to document classification is to fit a linear model (e.g., Logistic Regression) over bag of words (BoW) features. To	[[124, 127]]	[[110, 122]]	['BoW']	['bag of words']	['BoW']	['is', 'a', 'bag of words']	[[124, 127]]	[[110, 122]]
405	tences, or the NP is an exophora, this is NULL.  Speaker Binary Whether the speakers of the predicate and the NP are the same (SAME) or not (OTHER).	[[127, 131], [42, 46], [110, 112]]	[[121, 125]]	['SAME', 'NULL', 'NP']	['same']	['NP', 'NULL', 'SAME', 'OTHER']	['the', 'the', 'the', 'the', 'the']	[[110, 112], [42, 46], [127, 131]]	[]
406	ton (within the Perseus Digital Library) based on  texts of the Classical era (Bamman, 2006), and  the Index Thomisticus Treebank (IT-TB) at the  Catholic University of the Sacred Heart in Milan, 	[[131, 136]]	[[103, 129]]	['IT-TB']	['Index Thomisticus Treebank']	['IT-TB']	['Index Thomisticus Treebank']	[[131, 136]]	[[103, 129]]
407	 University of Chicago Given a constraint set with k constraints in the framework of Optimality Theory (OT), what is its capacity as a classification scheme for linguistic data?	[[104, 106]]	[[85, 102]]	['OT']	['Optimality Theory']	['OT']	['a', 'Optimality Theory', 'a']	[[104, 106]]	[[85, 102]]
408	 Figure 4: Algorithm for calculating the F-measure confusion matrix of True Positives (T.P.), False Positives (F.P.), True Negatives (T.N.), and False Negatives (F.N.). The ranking technique described in this paper creates a list of	[[134, 138], [162, 166], [87, 92], [111, 115]]	[[118, 132], [145, 160], [71, 85], [94, 109]]	['T.N.', 'F.N.', 'T.P.)', 'F.P.']	['True Negatives', 'False Negatives', 'True Positives', 'False Positives']	['T.P', '.']	['True Positives', 'False Positives', 'True Negatives', 'False Negatives', 'a']	[]	[[71, 85], [94, 109], [118, 132], [145, 160]]
409	scopes are not necessarily contiguous.  Conditional Random Field (CRF) sequence tag? 	[[66, 69]]	[[40, 64]]	['CRF']	['Conditional Random Field']	['CRF']	['Conditional Random Field']	[[66, 69]]	[[40, 64]]
410	evaluating work in speech recognition (SR), but until  recently no community-wide methodology existed for either  natural language (NL) researchers or speech understanding  (SU) researchers for evaluating the systems they developed.	[[132, 134], [39, 41], [174, 176]]	[[114, 130], [19, 37], [151, 171]]	['NL', 'SR', 'SU']	['natural language', 'speech recognition', 'speech understanding']	['SR', 'NL', 'SU']	['in', 'speech recognition', 'natural language', 'or speech understanding']	[[39, 41], [132, 134], [174, 176]]	[[19, 37], [114, 130]]
411	man evaluators 5.2 Mean Absolute Difference Analysis Here we calculated the mean absolute difference(MAD) between a human rater?s evaluation and SELSA (LSA)	[[101, 104], [145, 150], [152, 155]]	[[76, 100]]	['MAD', 'SELSA', 'LSA']	['mean absolute difference']	['MAD', 'SELSA']	['man', 'mean absolute difference', 'a']	[[101, 104], [145, 150]]	[[76, 100]]
412	question answering over RDF data. In World Wide Web (WWW), pages 639?648. 	[[53, 56], [24, 27]]	[[37, 51]]	['WWW', 'RDF']	['World Wide Web']	['RDF', 'WWW']	['World Wide Web']	[[24, 27], [53, 56]]	[[37, 51]]
413	2 mark up tool in Shakti Standard Format (SSF) (Bharati et al, 2005). For	[[42, 45]]	[[18, 40]]	['SSF']	['Shakti Standard Format']	[')']	['Standard Format']	[]	[]
414	word in context. In this article we present a WSD algorithm based on random walks over large Lexical Knowledge Bases (LKB). We show that our algorithm performs better than other graph-	[[118, 121], [46, 49]]	[[93, 116]]	['LKB', 'WSD']	['Lexical Knowledge Bases']	['WSD', 'LKB']	['a', 'Lexical Knowledge Bases']	[[46, 49], [118, 121]]	[[93, 116]]
415	 1.1 Elman and RCC networks  Simple recurrent networks (SRN's) of the Elman type  are similar to three-layer perceptrons but with recurrent 	[[56, 61]]	[[29, 54]]	"[""SRN's""]"	['Simple recurrent networks']	"[""'s)""]"	['networks', 'recurrent networks']	[]	[]
416	methods.  For example, consider the ways in which evaluation of machine translation (MT) systems is carried out.	[[85, 87]]	[[64, 83]]	['MT']	['machine translation']	['MT']	['in', 'machine translation']	[[85, 87]]	[[64, 83]]
417	 4.1. Document Input (DI)  The DI process is the interface between ADEPT 	[[22, 24], [67, 72], [31, 33]]	[[6, 20]]	['DI', 'ADEPT', 'DI']	['Document Input']	[')']	['Input']	[]	[]
418	method does not select any dimension.  Median Selection (MSel). As a further method	[[57, 61]]	[[39, 55]]	['MSel']	['Median Selection']	['MSel']	['Median Selection', 'a']	[[57, 61]]	[[39, 55]]
419	1 Introduction Many algorithms in speech and language processing can be viewed as instances of dynamic programming (DP) (Bellman, 1957). The basic idea of	[[116, 118]]	[[95, 114]]	['DP']	['dynamic programming']	['DP']	['in', 'dynamic programming']	[[116, 118]]	[[95, 114]]
420	reasoning? In International Conference on Learning Representations (ICLR). 	[[68, 72]]	[[14, 66]]	['ICLR']	['International Conference on Learning Representations']	['ICLR']	['In International Conference on Learning Representations']	[[68, 72]]	[]
421	 2.2 DIRT data The DIRT (Discovering Inference Rules from Text) method is based on extending Harris Distributional	[[19, 23], [5, 9]]	[[25, 62]]	['DIRT', 'DIRT']	['Discovering Inference Rules from Text']	[]	['Inference Rules from Text )', 'on']	[]	[]
422	2 Classification Algorithms  2.1 Conditional Random Fields  Conditional random field (CRF) was an extension  of both Maximum Entropy Model (MEMs) and 	[[86, 89], [140, 144]]	[[60, 84], [117, 138]]	['CRF', 'MEMs']	['Conditional random field', 'Maximum Entropy Model']	['CRF', 'MEMs']	['Conditional', 'random field', 'an', 'Maximum Entropy Model']	[[86, 89], [140, 144]]	[[117, 138]]
423	7 (Ogren and Bethard, 2009) can be used to design and execute pipelines made up of a sequence of AEs (and potentially some more complex flows), and UIMA-AS 8	[[97, 100], [148, 155]]	[[102, 122]]	['AEs', 'UIMA-AS']	['and potentially some']	['AEs', 'UIMA-AS']	[]	[[97, 100], [148, 155]]	[]
424	LM. This approach has been successfully applied in automatic speech recognition (ASR) (Tam and Schultz, 2006) using the Latent Dirichlet Alloca-	[[81, 84], [0, 2]]	[[51, 79]]	['ASR', 'LM']	['automatic speech recognition']	['LM', 'ASR']	['in automatic speech recognition']	[[0, 2], [81, 84]]	[]
425	in all kind of NLP applications. As a particular case, Semantic Role Labeling (SRL) is currently a welldefined task with a substantial body of work and	[[79, 82], [15, 18]]	[[55, 77]]	['SRL', 'NLP']	['Semantic Role Labeling']	['NLP', 'SRL']	['in', 'a', 'Semantic Role Labeling', 'a', 'a']	[[15, 18], [79, 82]]	[[55, 77]]
426	and answer sentences that emphasizes three types  of metadata:   (i) Main Verbs (MVerb), identified by the link  parser (Sleator and Temperley 1993);  	[[81, 86]]	[[69, 79]]	['MVerb']	['Main Verbs']	['MVerb']	['i', 'Main Verbs']	[[81, 86]]	[[69, 79]]
427	controlled vocabulary. As an application of the  Resource Description Framework (RDF), SKOS  allows concepts to be composed and published on the 	[[81, 84], [87, 91]]	[[49, 79]]	['RDF', 'SKOS']	['Resource Description Framework']	['RDF', 'SKOS']	['Resource Description Framework']	[[81, 84], [87, 91]]	[[49, 79]]
428	scheme category in the corpus: Results (RES) is by far the most frequent zone (accounting for 40% of the corpus), while Background (BKG), Objective (OBJ), Method (METH) and Conclusion (CON) cover	[[132, 135], [40, 43], [149, 152], [163, 167], [185, 188]]	[[120, 130], [31, 37], [138, 147], [155, 161], [173, 183]]	['BKG', 'RES', 'OBJ', 'METH', 'CON']	['Background', 'Result', 'Objective', 'Method', 'Conclusion']	['RES', 'BKG', 'OBJ', 'METH', 'CON']	['Results', 'is', 'Background', 'Objective', 'Method', 'Conclusion']	[[40, 43], [132, 135], [149, 152], [163, 167], [185, 188]]	[[120, 130], [138, 147], [155, 161], [173, 183]]
429	1994). Following (Collins, 2002), we used sections 0-18 of the Wall Street Journal (WSJ) corpus for training, sections 19-21 for development, and	[[84, 87]]	[[63, 82]]	['WSJ']	['Wall Street Journal']	['WSJ']	['Wall Street Journal']	[[84, 87]]	[[63, 82]]
430	For our experiments, we collect debate posts from four popular domains, Abortion (ABO), Gay Rights (GAY), Obama (OBA), and Marijuana (MAR), from an online debate forum1.	[[100, 103], [113, 116], [82, 85], [134, 137]]	[[88, 91], [106, 111], [72, 80], [123, 132]]	['GAY', 'OBA', 'ABO', 'MAR']	['Gay', 'Obama', 'Abortion', 'Marijuana']	['ABO', 'GAY', 'OBA', 'MAR']	['Abortion', 'Gay Rights', 'Obama', 'Marijuana']	[[82, 85], [100, 103], [113, 116], [134, 137]]	[[72, 80], [106, 111], [123, 132]]
431	rlt*s*rle TR AN SFORHATIONS *S***   SCAN CALLED AT 1 I  ANTEST CALLED FOR 4l'I NG l1 (AACC) ,SD= 5. RES= 0.	[[70, 73], [86, 90], [93, 95], [100, 103]]	[[56, 69]]	['FOR', 'AACC', 'SD', 'RES']	['ANTEST CALLED']	['AACC']	"['S', 'CALLED', ""1 I ANTEST CALLED FOR 4l ' I""]"	[[86, 90]]	[]
432	P,,nnsylvania (UPenn) Treebank. We plan to include  the parsed ICE-GB (Great Britain component of ICE)  and the BNC (British National Corpus) in the project 	[[67, 69], [98, 101], [112, 115], [15, 20]]	[[71, 84], [117, 140], [0, 13]]	['GB', 'ICE', 'BNC', 'UPenn']	['Great Britain', 'British National Corpus', 'P,,nnsylvania']	['UPenn', 'ICE-GB', 'ICE', 'BNC']	['Great Britain component', 'British National Corpus', 'in']	[[15, 20], [98, 101], [112, 115]]	[[117, 140]]
433	lief chunk), B-NCB (Beginning of non committed belief chunk), I-NCB (Inside of a non committed belief chunk), B-NA (Beginning of a not applicable chunk), I-NA (Inside a not applicable	[[110, 114], [13, 18], [62, 67], [154, 158]]	[[116, 130], [20, 59], [69, 107], [160, 183]]	['B-NA', 'B-NCB', 'I-NCB', 'I-NA']	['Beginning of a', 'Beginning of non committed belief chunk', 'Inside of a non committed belief chunk', 'Inside a not applicable']	['B-NCB', 'I-NCB', 'B-NA', 'I-NA']	['Beginning of non committed belief', 'Inside of a non committed belief', 'Beginning of a not applicable']	[[13, 18], [62, 67], [110, 114], [154, 158]]	[]
434	5 Experiments and Results 5.1 Data We use the British National Corpus (BNC),3 which contains 100M words, because it draws its	[[71, 74]]	[[46, 69]]	['BNC']	['British National Corpus']	['BNC']	['British National Corpus']	[[71, 74]]	[[46, 69]]
435	  An Underspecified Segmented Discourse Representation Theory (USDRT)  Frank Schilder 	[[63, 68]]	[[5, 61]]	['USDRT']	['Underspecified Segmented Discourse Representation Theory']	[')']	['Segmented Discourse Representation Theory']	[]	[]
436	gel Balaban,  ? hsan Yal??nkaya Middle East Technical University, Ankara, Turkey and   ? mit Deniz Turan Anadolu University, Eski?ehir, Turkey  Corresponding author: dezeyrek@metu.edu.tr  Abstract  In this paper, we report on the annotation procedures we developed for annotating the Turkish Discourse Bank (TDB), an effort that extends the Penn Discourse Tree Bank (PDTB) annotation style by using it for annotating Turkish discourse. After a brief introduction to the TDB, we describe the annotation cycle and the annotation scheme we developed, defining which parts of the scheme are an extension of the PDTB and which parts are different.	[[308, 311], [367, 371], [470, 473], [607, 611]]	[[284, 306], [341, 365]]	['TDB', 'PDTB', 'TDB', 'PDTB']	['Turkish Discourse Bank', 'Penn Discourse Tree Bank']	['TDB', 'PDTB']	['Turkish Discourse Bank', 'Penn Discourse Tree Bank', 'Turkish', 'a']	[[308, 311], [470, 473], [367, 371], [607, 611]]	[[284, 306], [341, 365]]
437	siderable interest in robust knowledge extraction, both as an end in itself and as an intermediate step in a variety of Natural Language Processing (NLP) applications.	[[149, 152]]	[[120, 147]]	['NLP']	['Natural Language Processing']	['NLP']	['in', 'in', 'an', 'in a', 'Natural Language Processing']	[[149, 152]]	[[120, 147]]
438	features. Stolcke et al (1998) then expanded the prosodic tree model with a hidden event language model (LM) to identify sentence boundaries, filled pauses and IPs in	[[105, 107]]	[[89, 103]]	['LM']	['language model']	['LM', 'IPs']	['model', 'a', 'language model']	[[105, 107]]	[[89, 103]]
439	"The stated  goal for this language model adaptation spoke was ""to  evaluate an incremental supervised language model (LM)  adaptation algorithm on a problem of sublanguage "	[[118, 120]]	[[102, 116]]	['LM']	['language model']	['LM']	['language model', 'an', 'language model', 'a']	[[118, 120]]	[[102, 116], [102, 116]]
440	Synthesis) is a learning system \[12,13,14\] which  consists of a learning element (Meta-XMAS), a  knowledge base (KB), and two inference ngines  of a morphological nalyzer (MOA) and a mor- 	[[115, 117]]	[[99, 113]]	['KB']	['knowledge base']	['KB', 'MOA']	['a', 'a', 'a knowledge base', 'a morphological nalyzer', 'a']	[[115, 117]]	[]
441	errors; English had three, and German one.   While it is worth noting that (II) is not without  counterexamples, it is significant that true 	[[76, 78]]	[[54, 69]]	['II']	['is worth noting']	['II']	['worth noting']	[[76, 78]]	[]
442	"We need to ""cross"" an instrument the_phone to reach another person, thus we also refer to the intermediate locus IME(LOC) in the domain of communication."	[[117, 120], [113, 116]]	[[107, 112]]	['LOC', 'IME']	['locus']	['LOC']	['locus']	[[117, 120]]	[[107, 112]]
443	5 Tools for the development of the prototype experiment For the practical development of our prototype experiments we are considering to use the upper cate-gories of the NIFSTD ontology and wikis as a collaborative tool. The availability and suitability for our research of the former has been considered in Maroto (2013).  NIFSTD (NIF Standard) ontology stands out as the most comprehensive ontology of the neurosci-ences available on the web. Its wide coverage and its degree of normalisation and reusability make this ontology particularly suitable for our research purposes.	[[324, 330], [170, 176]]	[[332, 344]]	['NIFSTD', 'NIFSTD']	['NIF Standard']	['NIFSTD']	['and', 'a', 'and', 'NIF Standard', 'and', 'and']	[[324, 330], [170, 176]]	[[332, 344]]
444	results are reported together with the error propagation from argument position classification for Same Sentence (SS), Previous Sentence (PS) models and joined results (ALL) as precision (P), recall	[[114, 116], [138, 140], [169, 172], [188, 190]]	[[99, 112], [119, 136], [177, 186]]	['SS', 'PS', 'ALL', 'P)']	['Same Sentence', 'Previous Sentence', 'precision']	['SS', 'PS']	['Same', 'Sentence', 'Previous Sentence', 'precision']	[[114, 116], [138, 140]]	[[119, 136], [177, 186]]
445	354  Table 1: Precision and recMI tables for experiments starting with words-only queries (Words) through phrase (Del l )   and word (Del2) deletion to proper noun (Caps) and noun phrase (NP) grouping. The queries were evaluated on 	[[188, 190], [134, 138], [114, 119]]	[[175, 186]]	['NP', 'Del2', 'Del l']	['noun phrase']	['NP']	['phrase', 'noun', 'noun phrase']	[[188, 190]]	[[175, 186]]
446	 1 Introduction Minimum error rate training (MERT)?also known as direct loss minimization in machine learning?is a	[[45, 49]]	[[16, 43]]	['MERT']	['Minimum error rate training']	[')']	['error rate training', 'in']	[]	[]
447	All conditions were assigned a section and are  thereby excluded. TE = temporal expression; TT = trigger term; V = scoped by verb.	[[66, 68], [92, 94]]	[[71, 90], [97, 109], [125, 129]]	['TE', 'TT']	['temporal expression', 'trigger term', 'verb']	['TE', 'TT']	['a', 'temporal expression', 'trigger term']	[[66, 68], [92, 94]]	[[71, 90], [97, 109]]
448	   1 Introduction  Named Entities (NEs) play a critical role in many  Natural Language Processing and Information 	[[35, 38]]	[[19, 33]]	['NEs']	['Named Entities']	[')']	['Entities', 'a']	[]	[]
449	recognition) which describes a relationship between an input signal sequence and a word, the other is a language model (LM) which measures the likelihood of a sequence of words as a sen-	[[120, 122]]	[[104, 118]]	['LM']	['language model']	['LM']	['a', 'an', 'a', 'a language model', 'a', 'a']	[[120, 122]]	[]
450	jansche.1@osu.edu 1 Introduction Our approach to multilingual named entity (NE) recognition in the context of the CoNLL Shared	[[76, 78]]	[[62, 74], [114, 119]]	['NE']	['named entity', 'CoNLL']	['NE', 'CoNLL']	['named entity']	[[76, 78]]	[[62, 74]]
451	  The word sea is ambiguous and has three senses as  given in the Princeton Wordnet (PWN):  S1: (n) sea (a division of an ocean or a large body 	[[85, 88]]	[[66, 83]]	['PWN']	['Princeton Wordnet']	['PWN']	['in', 'Princeton Wordnet', 'n', 'or']	[[85, 88]]	[[66, 83]]
452	Other language technology applications, such as Question Answering (QA) systems or information retrieval (IR) systems, also suffer from the poor contextual disambiguation of word senses.	[[106, 108], [68, 70]]	[[83, 104], [48, 66]]	['IR', 'QA']	['information retrieval', 'Question Answering']	['QA', 'IR']	['Question Answering', 'or', 'information retrieval']	[[68, 70], [106, 108]]	[[48, 66], [83, 104]]
453	Contextual relationship attributes:  1. Prefix Counted Rule (PRC): The selected  sense is the most commonly appended sense by the 	[[61, 64]]	[[40, 54]]	['PRC']	['Prefix Counted']	['PRC']	['Prefix Counted Rule']	[[61, 64]]	[]
454	metric TF*S~, since we base the importance of a  ? SF = Segment frequency (How many segments does  the term occur in) 	[[51, 53], [7, 11]]	[[56, 73]]	['SF', 'TF*S']	['Segment frequency']	['TF', 'SF']	['a', 'Segment frequency']	[[51, 53]]	[[56, 73]]
455	 {UK, US, AU}.   Figure 1: Proposed architecture      Figure 2: Baseline GMM based dialect classification 5.2 Latent Semantic Analysis for Dialect ID One approach used to address topic classification problems has been latent semantic analysis (LSA), which was first explored for document indexing in (Deerwester et al, 1990). This addresses the issues of synonymy - many ways to refer to the same idea and polysemy ?	[[244, 247], [2, 4], [6, 8], [10, 12], [73, 76], [147, 149]]	[[218, 242]]	['LSA', 'UK', 'US', 'AU', 'GMM', 'ID']	['latent semantic analysis']	['UK', 'US', 'AU', 'GMM', 'ID', 'LSA']	['latent semantic analysis', 'al']	[[2, 4], [6, 8], [10, 12], [73, 76], [147, 149], [244, 247]]	[[218, 242]]
456	Tu?r, Oflazer, and Tu?r 2002; Oflazer 2003; Oflazer et al 2003; Eryig?it and Oflazer 2006) has represented the morphological structure of Turkish words by splitting them into inflectional groups (IGs). The root and derivational elements of a word are represented	[[196, 199]]	[[175, 194]]	['IGs']	['inflectional groups']	['IGs']	['r', 'r', 'al', 'inflectional groups', 'a']	[[196, 199]]	[[175, 194]]
457	to match portions of a PAS.  3.2 The Shallow Semantic Tree Kernel (SSTK) The SSTK is based on two ideas: first, we change	[[67, 71], [23, 26], [77, 81]]	[[37, 65]]	['SSTK', 'PAS', 'SSTK']	['Shallow Semantic Tree Kernel']	['SSTK']	['a', 'Shallow Semantic Tree Kernel']	[[67, 71], [77, 81]]	[[37, 65]]
458	However, this hypothesis is reasonable if the  monolingual wordnets are reliable and correctly  linked to the interlingual index (ILI). Quality 	[[130, 133]]	[[110, 128]]	['ILI']	['interlingual index']	['ILI']	['interlingual index']	[[130, 133]]	[[110, 128]]
459	Main verb of main clause (MV)F8. Boolean feature for MV (BMV)F9. Previous sentence feature (PREV)Additional feature used only for Arg1F10. Arg2 Labels	[[92, 96], [26, 28], [57, 60], [130, 137], [139, 143]]	[[65, 73], [0, 9], [33, 55]]	['PREV', 'MV', 'BMV', 'Arg1F10', 'Arg2']	['Previous', 'Main verb', 'Boolean feature for MV']	['MV', 'PREV']	['Boolean', 'for MV', 'Previous', 'sentence', 'for']	[[26, 28], [92, 96]]	[[65, 73]]
460	projected expectations from English. To this end, we adopt the Generalized Expectation (GE) Criteria framework introduced by Mann and McCallum	[[88, 90]]	[[63, 86]]	['GE']	['Generalized Expectation']	['GE']	['Generalized Expectation']	[[88, 90]]	[[63, 86]]
461	"SOl,licit(co+ t{ach of these phrases conlpriscs a contigtlous  sequence o1 tags that satisfies a strut+h: gral,illilar, l""or  example, a II(,itlll pluase eltil be simi)ly a plonoull  t~ig or (,in  all:l itlaly setitlellce (:,I lie(It1 lind ad.iective lags, pms ib ly  "	[[137, 139]]	[[141, 159]]	['II']	['itlll pluase eltil']	['SOl']	[]	[]	[]
462	(Person Names) by prometheus e. V.16, and Getty ULAN (United List of Artist Names)17 There are two modes of use for name authorities	[[48, 52]]	[[54, 84]]	['ULAN']	['United List of Artist Names)17']	['ULAN']	['United List of Artist', 'of']	[[48, 52]]	[]
463	"procedures"" (ReP). The RePs work on a memory structure which is adequate for the  representation of knowledge about objects, the ""referential net"" (RefN) 4*. A RefN "	[[148, 152], [13, 16], [23, 27], [160, 164]]	[[130, 145]]	['RefN', 'ReP', 'RePs', 'RefN']	['referential net']	['ReP', 'RefN']	['a', 'referential net']	[[13, 16], [148, 152], [160, 164]]	[[130, 145]]
464	rates of sentences (46.1%) (53.9%) (40.4%) (59.6%) (70.7%) (29.3%) (54.3%) (45.7%) (64.3%) (35.7%) supervised CRF (baseline) 46.78 60.99 48.57 60.01 56.92 67.91 79.60 97.35 75.69 91.03 JESS-CM (CRF/HMM) 49.02 62.60 50.79 61.24 62.47 71.30 85.87 97.47 80.84 92.85 (gain from supervised CRF) (+2.24) (+1.61) (+2.22) (+1.23) (+5.55) (+3.40) (+6.27) (+0.12) (+5.15) (+1.82)	[[194, 201], [185, 192]]	[]	['CRF/HMM', 'JESS-CM']	[]	['CRF', 'JESS-CM']	[]	[[185, 192]]	[]
465	[byline : SAN SALVADOR, 19 APR 89 (ACAN-EFE) --] [bracket : [TEXT]] [fullname : SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIIII] CONDEMNED THE TERRORIST KILLING OF [fullname : ATTORIEY GENERAL ROBERTO GARCI A ALVARADO] AND (comp : ACCUSED THE FARABUIDO MARTI NATIONAL LIBERATION FROIT [bracket : (FMLN)) OF) THE CRIME . 	[[294, 298]]	[[240, 275]]	['FMLN']	['FARABUIDO MARTI NATIONAL LIBERATION']	['FMLN']	['A', 'MARTI NATIONAL']	[[294, 298]]	[]
466	BLEU uses 2 reference translations. WER=word error rate, PER=position independent WER. 	[[82, 85]]	[[61, 81]]	['WER']	['position independent']	['BLEU', 'WER']	['error rate', 'independent']	[[82, 85]]	[]
467	 In (Speriosu et al., 2011), a label propagation (LProp) approach is proposed, while Go et al. ( 2009)	[[50, 55]]	[[31, 48]]	['LProp']	['label propagation']	['LProp']	['a label propagation']	[[50, 55]]	[]
468	eugene@mathcs.emory.edu Abstract Community question answering (CQA) websites contain millions of question and answer	[[63, 66]]	[[33, 61]]	['CQA']	['Community question answering']	['CQA']	['question answering', 'question']	[[63, 66]]	[]
469	 means a type of source ? newswire (NW), broadcast news (BN), broadcast conversation (BC), mag-	[[36, 38], [57, 59], [86, 88]]	[[26, 34], [41, 55], [62, 84]]	['NW', 'BN', 'BC']	['newswire', 'broadcast news', 'broadcast conversation']	['NW', 'BN', 'BC']	['a', 'newswire', 'broadcast news', 'broadcast conversation']	[[36, 38], [57, 59], [86, 88]]	[[26, 34], [41, 55], [62, 84]]
470	either lexically encoded, or depends on the intrinsic properties of G, or coincides with a salient VPT (viewpoint). In striking contrast with	[[99, 102]]	[[104, 113]]	['VPT']	['viewpoint']	['VPT']	['a', 'viewpoint']	[[99, 102]]	[[104, 113]]
471	Abstract  This paper attempts to use an off-the-shelf  anaphora resolution (AR) system for Bengali. 	[[76, 78]]	[[55, 74]]	['AR']	['anaphora resolution']	['AR']	['an', 'anaphora resolution']	[[76, 78]]	[[55, 74]]
472	The SemEval?2007 task for extracting frame semantic structures relies on the human annotated data available in the FrameNet (FN) database. The	[[125, 127], [4, 11]]	[[115, 123]]	['FN', 'SemEval']	['FrameNet']	['FN']	['FrameNet']	[[125, 127]]	[[115, 123]]
473	hauer, haver, haber) and the corpus does not contain any other linguistic information, such as lemma and part of speech (PoS). 	[[121, 124]]	[[105, 119]]	['PoS']	['part of speech']	['PoS']	['part of speech']	[[121, 124]]	[[105, 119]]
474	  The ungrammatical distracter, e.g., are in Figure 1,  has a different part of speech (POS) than the correct answer germs.	[[88, 91]]	[[72, 86]]	['POS']	['part of speech']	['POS']	['a', 'part of speech']	[[88, 91]]	[[72, 86]]
475	Words/Phrases as Themselves (WD)  Symbols/Nonliteral Marks (SY)  Phonetic/Sound (PH)  Spelling (SP) 	[[81, 83], [29, 31], [60, 62], [96, 98]]	[[65, 73], [34, 41], [86, 94]]	['PH', 'WD', 'SY', 'SP']	['Phonetic', 'Symbols', 'Spelling']	['WD', 'SY', 'PH', 'SP']	['Phonetic/Sound', 'Spelling']	[[29, 31], [60, 62], [81, 83], [96, 98]]	[[86, 94]]
476	DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department,  GI = Operative Gastrointestinal, RAD = Radiology and  SP = Surgical Pathology. (%)	[[109, 112], [0, 2], [25, 29], [48, 50], [76, 78], [130, 132]]	[[115, 128], [5, 22], [32, 46], [53, 73], [91, 107], [135, 153]]	['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']	['Radiology and', 'Discharge Summary', 'Echocardiogram', 'Emergency Department', 'Gastrointestinal', 'Surgical Pathology']	['DS', 'ED', 'GI', 'RAD', 'SP']	['Discharge Summary', 'Echo', 'Echocardiogram', 'Emergency Department', 'Operative', 'Gastrointestinal', 'Radiology', 'Surgical Pathology']	[[0, 2], [48, 50], [76, 78], [109, 112], [130, 132]]	[[5, 22], [32, 46], [53, 73], [91, 107], [135, 153]]
477	mdiab@ccls.columbia.edu Abstract We analyze overt displays of power (ODPs) in written dialogs.	[[69, 73]]	[[44, 58]]	['ODPs']	['overt displays']	['ODPs']	['overt displays of power', 'in']	[[69, 73]]	[]
478	(PERS), organization (ORG), geo-political entity (GPE), weapon (WEA), vehicle (VEH), location (LOC), and facility (FAC). Since the person type	[[115, 118], [1, 5], [22, 25], [50, 53], [64, 67], [79, 82], [95, 98]]	[[105, 113], [8, 20], [28, 48], [56, 62], [70, 77], [85, 93]]	['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'LOC']	['facility', 'organization', 'geo-political entity', 'weapon', 'vehicle', 'location']	['PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'LOC', 'FAC']	['organization', 'geo-political entity', 'weapon', 'vehicle', 'location', 'facility']	[[1, 5], [22, 25], [50, 53], [64, 67], [79, 82], [95, 98], [115, 118]]	[[8, 20], [28, 48], [56, 62], [70, 77], [85, 93], [105, 113]]
479	Abstract In this paper, we propose a new syntaxbased machine translation (MT) approach based on reducing the MT task to a tree-	[[74, 76], [109, 111]]	[[53, 72]]	['MT', 'MT']	['machine translation']	['MT']	['a', 'machine translation', 'on', 'a']	[[74, 76], [109, 111]]	[[53, 72]]
480	  The second one is a variant that we named  Double Levenshtein?s Edit Distance (DLED)  (see Table 9 for detail).	[[81, 85]]	[[45, 79]]	['DLED']	['Double Levenshtein?s Edit Distance']	[]	['is a', 'Levenshtein ? s Edit Distance']	[]	[]
481	by HG's (HL). In particular, we show that HL's are included in TAL's andthat TAG's are equivalent toa modification ofHG:s  called Modified Head Grammars (MHG's). The inclusion of MHL in HI.,,	[[154, 159], [3, 5], [9, 11], [42, 44], [63, 66], [77, 80], [179, 182], [117, 119]]	[[130, 152]]	"[""MHG's"", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL', 'HG']"	['Modified Head Grammars']	"[""HG's"", 'HL', ""TAL's"", ""TAG's""]"	['s', 'Modified Head Grammars']	[[9, 11], [42, 44]]	[[130, 152]]
482	TEMPLATE GENERATO R Template Generation Algorithm The memory-based parser generates one or several concept sequence instances (CSI's) for each sentence . 	[[127, 132]]	[[99, 125]]	"[""CSI's""]"	['concept sequence instances']	"[""CSI's""]"	['concept sequence instances']	[[127, 132]]	[[99, 125]]
483	 (Ramshaw and Marcus, 1995) approached chucking by using Transformation Based Learning(TBL). 	[[87, 90]]	[[57, 86]]	['TBL']	['Transformation Based Learning']	['TBL']	['Transformation Based Learning']	[[87, 90]]	[[57, 86]]
484	demonstrate such dependencies.  The Maximum Entropy (MaxEnt) model (Berger et al, 1996) estimates the probability of a time-bin	[[53, 59]]	[[36, 51]]	['MaxEnt']	['Maximum Entropy']	['MaxEnt']	['Maximum Entropy', 'a']	[[53, 59]]	[[36, 51]]
485	dialogues categorized into multiple domains, we create a particular type of hidden Markov model (HMM) called Class Speaker HMM (CSHMM) to model operator/caller utterance sequences.	[[128, 133]]	[[109, 126]]	['CSHMM']	['Class Speaker HMM']	[]	['a', 'hidden Markov', 'HMM', 'Class Speaker HMM']	[]	[[109, 126]]
486	shared task, namely FLORIAN (Florian et al.,  2003) and CHIEU-NG (Chieu and Ng, 2003). 	[[56, 64], [20, 27]]	[[66, 78], [29, 36]]	['CHIEU-NG', 'FLORIAN']	['Chieu and Ng', 'Florian']	['FLORIAN', 'CHIEU-NG']	['Florian', 'and', 'Chieu and Ng']	[[20, 27], [56, 64]]	[[29, 36], [66, 78]]
487	 1 Introduction Open-domain Question Answering (QA) systems are concerned with the problem of trying	[[48, 50]]	[[28, 46]]	['QA']	['Question Answering']	[')']	['Answering']	[]	[]
488	229  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1810?1815, October 25-29, 2014, Doha, Qatar.	[[93, 98]]	[[43, 91]]	['EMNLP']	['Empirical Methods in Natural Language Processing']	['EMNLP']	['Empirical Methods in Natural Language Processing']	[[93, 98]]	[[43, 91]]
489	1 Introduction Large-scale open-domain question answering from structured Knowledge Base (KB) provides a good balance of precision and recall in everyday QA	[[90, 92], [154, 156]]	[[74, 88]]	['KB', 'QA']	['Knowledge Base']	['KB']	['Knowledge Base', 'a']	[[90, 92]]	[[74, 88]]
490	th fo frture representations abstracts (AbT), titles (ArT),  authors (Aut), Journals (Jou), and Mesh Headings	[[40, 43], [54, 57], [70, 73], [86, 89]]	[[29, 38], [46, 52], [61, 68], [76, 84]]	['AbT', 'ArT', 'Aut', 'Jou']	['abstracts', 'titles', 'authors', 'Journals']	['AbT', 'ArT', 'Aut']	['th', 'representations abstracts', 'titles', 'authors', 'Journals']	[[40, 43], [54, 57], [70, 73]]	[[46, 52], [61, 68], [76, 84]]
491	787 After labeling the reference BINet, we train a learning to rank (L2R) model2 using the following features for scoring nodes in the target BINet	[[69, 72]]	[[51, 67]]	['L2R']	['learning to rank']	[',', ')']	['a', 'to rank', 'in']	[]	[]
492	for the annotation process.  Topic models (TMs) are a suite of unsuper992	[[43, 46]]	[[29, 41]]	['TMs']	['Topic models']	['TMs']	['Topic models', 'a']	[[43, 46]]	[[29, 41]]
493	Evidence for a text?s topic and genre comes, in part, from its lexical and syntactic features?features used in both Automatic Topic Classification and Automatic Genre Classification (AGC). Because an ideal AGC system should	[[183, 186], [206, 209]]	[[151, 181]]	['AGC', 'AGC']	['Automatic Genre Classification']	['AGC']	['a', 'Automatic Topic Classification', 'Automatic Genre Classification']	[[183, 186], [206, 209]]	[[151, 181]]
494	0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co	[[40, 42]]	[[25, 38]]	['OR']	['Omission Rate']	['OR']	['Omission Rate']	[[40, 42]]	[[25, 38]]
495	The query (Figure 4a) will match adjectives (ADJA) adjacent to a following noun (NN) which must not have another dependent that is either a modifying noun or name (NE).	[[81, 83], [45, 49], [164, 166]]	[[75, 79], [33, 43], [158, 162]]	['NN', 'ADJA', 'NE']	['noun', 'adjectives', 'name']	['ADJA', 'NN']	['match', 'noun', 'noun', 'name']	[[45, 49], [81, 83]]	[[75, 79], [75, 79], [158, 162]]
496	the middle (Baxendale, 1958).  Sentence Position Yield (SPY) is obtained separately for both types of documents.	[[56, 59]]	[[31, 54]]	['SPY']	['Sentence Position Yield']	['SPY']	['Sentence Position Yield']	[[56, 59]]	[[31, 54]]
497	   By contrast, our approach operates at the level  of inflectional property sets (IPS), or more  properly, at the level of inflectional paradigms.	[[83, 86]]	[[55, 81]]	['IPS']	['inflectional property sets']	[]	['inflectional property sets', 'inflectional']	[]	[[55, 81]]
