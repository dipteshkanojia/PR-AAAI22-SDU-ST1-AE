ID,text,acronyms,long-forms,acronyms-text,long-forms-text,AN_Pred,LF_Pred,AN_Pred_idxs,LF_Pred_idxs
1,2 . 2 . 3 . Dịch mã và Protein Dịch mã là hoạt động tổng hợp protein từ RNA được tạo ra trong quá trình phiên mã .,"[[72, 75]]",[],['RNA'],[],['RNA'],[],"[[72, 75]]",[]
2,"Learning ( PIRL ) [ 10 ] học biểu diễn ảnh sao cho biểu diễn đó bất biến với phép biến đổi ảnh dựa vào trò chơi xếp hình jigsaw puzzle . Gần đây , những tiến bộ","[[11, 15]]",[],['PIRL'],[],['PIRL'],[],"[[11, 15]]",[]
3,"làm trước đó và nhóm nghiên cứu hi vọng , phương pháp LCL sẽ đem lại những hiệu ứng và kết quả tốt đẹp , có triển vọng trong tương lai . 4.3","[[54, 57]]",[],['LCL'],[],['LCL'],[],"[[54, 57]]",[]
4,Điều này mở ra một hướng có thể áp dụng VBD trong các phương pháp thuộc lớp tiếp cận dựa trên mở rộng kiến trúc đã có . 41,"[[40, 43]]",[],['VBD'],[],['VBD'],[],"[[40, 43]]",[]
5,"Hà Nội , ngày 18 tháng 06 năm 2021 Giảng viên hướng dẫn PGS. TS Thân Quang Khoát","[[56, 60], [61, 63]]",[],"['PGS.', 'TS']",[],"['PGS', 'TS']",[],"[[56, 59], [61, 63]]",[]
6,"Câu 2 : Two weeks ago , viewers of several NBC daytime consumer segment s started calling a 900 number for advice on various life - style issues .",[],[],[],[],['NBC'],[],"[[43, 46]]",[]
7,Mô hình Neural Collaborative Filtering ( NCF ) do Xiangnan He và các đồng nghiệp đề xuất [ 3 ] . Model này gồm có hai nhánh hợp thành là Generalized Matrix,"[[41, 44]]",[],['NCF'],[],['NCF'],[],"[[41, 44]]",[]
8,độ overfit càng giảm Soft parameter sharing Hình dưới đây mô tả phương pháp soft parameter sharing trong MTL .,"[[105, 108]]",[],['MTL'],[],['MTL'],[],"[[105, 108]]",[]
9,Tác giả ĐATN Nguyễn Thế Linh 5 . Xác nhận của giáo viên hướng dẫn về mức độ hoàn thành của ĐATN và cho phép bảo vệ :,"[[8, 12], [91, 95]]",[],"['ĐATN', 'ĐATN']",[],['ĐATN'],[],"[[8, 12]]",[]
10,"Đưa chuỗi này qua K mạng tích chập Conv1 , Conv2 , … , Convk và max pooling ( để đơn giản ta set K = 2 ) Với mỗi mạng tích chập i ta thu được vector :",[],[],[],[],[],[],[],[]
11,Phương pháp tiếp cận dựa trên Support Vector Machine ( SVM ) .. 17 2.2.,"[[55, 58]]",[],['SVM'],[],['SVM'],[],"[[55, 58]]",[]
12,"trí , âm nhạc … ) trong một khoảng thời gian nhất định . Input : Person ( P ) , Page ( Pg ) , Comment ( C ) , Status ( S ) Output : G ( N , E )",[],[],[],[],[],['Status'],[],"[[110, 116]]"
13,"Trong đó , V là số chiều từ điển được xây dựng trên toàn bộ các văn bản hoặc được sử dụng từ bộ từ điển có sẵn . Tham số 𝛽𝑘𝑣 được minh họa bằng hình vẽ",[],[],[],[],[],[],[],[]
14,"Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 20 2 KIẾN THỨC CƠ SỞ","[[63, 74]]",[],['KSTN - CNTT'],[],['KSTN-CNTT'],[],"[[-1, 8]]",[]
15,"việc học của CL được quyết định nhiều bởi các cặp positive và negative , tuy nhiên Đồ án tốt nghiệp 30","[[13, 15]]",[],['CL'],[],['CL'],[],"[[13, 15]]",[]
16,"cách ghép khác nhau : sum , concat , mul , … Cấu trúc này cho phép các mạng có cả thông tin lùi và tiến về chuỗi ở mỗi bước . Hình 3 Mạng nơ-ron hồi quy mở rộng RNN hai chiều","[[161, 164]]","[[133, 152]]",['RNN'],['Mạng nơ-ron hồi quy'],['RNN'],[],"[[161, 164]]",[]
17,"Đại học Bách Khoa Hà Nội Viện Công nghệ Thông tin và Truyền thông Trong các phương pháp thuộc CL , bên cạnh cặp positive và negative , người ta còn","[[94, 96]]",[],['CL'],[],['CL'],[],"[[94, 96]]",[]
18,hình 25 : ACD ca sử dụng xem thông tin bài đăng 3 . 4 . 5 . Tìm kiếm việc làm theo từ khóa,"[[10, 13]]",[],['ACD'],[],['ACD'],[],"[[10, 13]]",[]
19,Em - Lê Thế Nam - cam kết đồ án nghiên cứu ( ĐANC ) là công trình nghiên cứu của riêng em dưới sự hướng dẫn của ThS. Ngô Văn Linh . Các kết quả có,"[[45, 49], [112, 116]]","[[26, 42]]","['ĐANC', 'ThS.']",['đồ án nghiên cứu'],"['ĐANC', 'ThS.']","['cam kết đồ án nghiên cứu', 'nghiên cứu', 'em']","[[45, 49], [112, 116]]","[[18, 42], [32, 42], [87, 89]]"
20,"trong việc kết hợp các thuật toán của Manifold Learning vào hướng nghiên cứu CL truyền thống . Bên cạnh việc xây dựng một nền tảng lí thuyết cho LCL , các thực","[[77, 79], [145, 148]]",[],"['CL', 'LCL']",[],['CL'],[],"[[77, 79]]",[]
21,Có tổng cộng 60000 ảnh cho tập huấn luyện và 10000 ảnh cho tập kiểm thử . Chia MNIST thành cách nhóm,"[[79, 84]]",[],['MNIST'],[],['MNIST'],[],"[[79, 84]]",[]
22,Viện Công nghệ Thông tin và Truyền thông Hình 11 : Kiến trúc tổng quan của mạng để giải bài toán xếp hình ( Nguồn : [ NF16 ] ),[],[],[],[],[],[],[],[]
23,"network ) , giúp các mô hình này giảm được overfitting một cách hiệu quả . Ý Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT","[[137, 148]]",[],['KSTN - CNTT'],[],[],[],[],[]
24,"Trong khi đó , nhìn chung xu hướng càng tăng C thì kết quả biểu diễn ẩn tìm được Đồ án tốt nghiệp 55",[],[],[],[],[],[],[],[]
25,toán này là chia các quan sát vào K cụm ( K biết trước ) sao cho mỗi quan sát chỉ thuộc về một cụm duy nhất . Khoảng cách giữa các quan sát trong cùng một cụm nhỏ hơn so với,[],[],[],[],[],[],[],[]
26,Hình 8 : Mô tả cách xây dựng dữ liệu huấn luyện . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03,"[[317, 326]]",[],['CNTT – TT'],[],['–'],[],"[[294, 295]]",[]
27,Hình 16 : So sánh kiến trúc MOCO với các kĩ thuật trước đây ( Nguồn [ He+20 ] ) mometum 0,"[[28, 32]]",[],['MOCO'],[],['MOCO'],[],"[[28, 32]]",[]
28,"xác suất tương ứng p ( z = k|d ) = DOC [ d, k ] và p ( z = k|j ) = T OP IC [ k, j ] . Lúc này , hai ma trận DOC và T OP IC được gọi là các tham số của mô hình , được xác",[],[],[],[],[],[],[],[]
29,"Variational Bayes ( AEVB ) là phương pháp sử dụng mạng suy diễn có khả năng ánh xạ trực tiếp một văn bản tới một phân phối hậu nghiệm xấp xỉ , có hiệu quả tính toán tương đối cao mà chi phí so với các phương pháp khác lại thấp .","[[20, 24]]",[],['AEVB'],[],['AEVB'],[],"[[20, 24]]",[]
30,"bằng việc quan sát sự tác động khác nhau của các siêu tham số lên mô hình mà đồ án đề xuất , em nhận thấy kết quả đạt được không hề thua kém so với các kết quả SOTA hiện nay trong CL và hướng nghiên cứu này sẽ còn có thể phát triển","[[160, 164], [180, 182]]",[],"['SOTA', 'CL']",[],"['SOTA', 'CL']",[],"[[160, 164], [180, 182]]",[]
31,"với các phương pháp trong CL truyền thống chính là nằm ở chỗ , các tri thức về cấu trúc cục bộ của không gian gốc được đưa vào để mô hình có thể học được . Và","[[26, 28]]",[],['CL'],[],['CL'],[],"[[26, 28]]",[]
32,"( 20 ) Ưu điểm lớn nhất của FCM là nó có thể phân các quan sát về nhiều nhóm cùng lúc . Trong tự nhiên điều này hợp lí hơn là mỗi đối tượng chỉ thuộc về một nhóm , ví dụ một","[[28, 31]]",[],['FCM'],[],['FCM'],[],"[[28, 31]]",[]
33,"Khi đó , nếu trên thực tế rank ( H ) > k , phép phân rã trên 57 Đồ án tốt nghiệp kĩ sư hệ thống thông tin",[],[],[],[],[],[],[],[]
34,"CIFAR100 . Điều này giúp cho mô hình tránh được hiện tượng học quá khớp , do lượng dữ liệu của CIFAR10 lớn hơn rất nhiều so với dữ liệu của các tác vụ được","[[0, 8], [95, 102]]",[],"['CIFAR100', 'CIFAR10']",[],['CIFAR100'],[],"[[0, 8]]",[]
35,( 17 ) j=1 Với W thỏa mãn điều kiện :,[],[],[],[],[],[],[],[]
36,"Nhưng ở chiều dịch Việt - Anh , kết quả LSTM lại cao hơn kết quả ở chiều dịch Anh - Việt và trái ngược hẳn với hai mô hình Transformer và ConvS2S .","[[40, 44], [138, 145]]",[],"['LSTM', 'ConvS2S']",[],"['LSTM', 'ConvS2S']",[],"[[40, 44], [138, 145]]",[]
37,"liệu "" views "" và "" wants "" tương ứng . Các tham số của mô hình được ước lượng sử dụng kỹ thuật stochastic gradient descent ( SGD ) truyền thống .","[[126, 129]]",[],['SGD'],[],['SGD'],[],"[[126, 129]]",[]
38,q ( Θ | λ̂t ) ∝ exp ( λ̂t . T (Θ ) ) Ở đây giá trị tiên nghiệm η được chuyển thành tham số tự nhiên λ0 của một,[],[],[],[],[],[],[],[]
39,". . . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 12","[[69, 80]]",[],['KSTN - CNTT'],[],['KSTN-CNTT'],[],"[[-1, 8]]",[]
40,"pu ( x ) trong đó , pu ( x) là một phân phối xác suất không đổi của x . Như vậy chúng ta mô hình hóa bài toán thành xấp xỉ p ( Θt | D1 : t−1 ) bởi một",[],[],[],[],[],[],[],[]
41,Bảng 0 .3 Tham số tốt nhất cho Split CIFAR100 EWC,"[[37, 45], [46, 49]]",[],"['CIFAR100', 'EWC']",[],['CIFAR100'],[],"[[37, 45]]",[]
42,"Ở đây đại lượng phân phối 𝑞𝑡 ( θ ) cần được chọn để ước lượng , VCL đề xuất sử 𝑘 dụng xấp xỉ bằng GMF , ở đó chiều thứ 𝑗 ′ của tham số θ = { 𝜃𝑗 } 𝑗=1 sẽ tuân theo","[[64, 67], [98, 101]]",[],"['VCL', 'GMF']",[],"['VCL', 'GMF']",[],"[[64, 67], [98, 101]]",[]
43,"Phương pháp tối ưu là stochastic gradient descent ( SGD ) . Mô hình ITE ( Implicit to Explicit ) Như đã đề cập ở phần trước , trong một số hệ thống mà dữ liệu hành vi tiềm ẩn và","[[52, 55], [68, 71]]",[],"['SGD', 'ITE']",[],"['SGD', 'ITE']",[],"[[52, 55], [68, 71]]",[]
44,"mua nó ) . Em gọi nhóm các mô hình loại này là Implicit To Explicit ( ITE ) . Sau đây , em sẽ giới thiệu về sáu mô hình cụ thể trong nhóm các mô hình","[[70, 73]]",[],['ITE'],[],['ITE'],[],"[[70, 73]]",[]
45,3 . 4 . 1 Generalized Matrix Factorization ( GMF ) Hình 18 . Mô hình Gereralized Matrix Factorization,"[[45, 48]]",[],['GMF'],[],['GMF'],[],"[[45, 48]]",[]
46,∆T ransf ormer − base Anh - Việt Việt - Anh,[],[],[],[],[],[],[],[]
47,"• Cũng giống như các phương pháp thuần lọc cộng tác khác , mô hình NeuMF gặp vấn đề cold start . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT","[[67, 72], [160, 171]]",[],"['NeuMF', 'KSTN - CNTT']",[],['NeuMF'],[],"[[67, 72]]",[]
48,"′ Theo công thức PT 3. 15 , ta thấy nếu như thành phần 𝐶𝑜𝑣 [ 𝐿 ( 𝑚 ) , 𝐿 ( 𝑚 ) ] ≠ 0 , khi đó phương sai của khả năng xảy ra trên toàn bộ dữ liệu sẽ lớn , dẫn đến việc tối ưu","[[17, 19]]",[],['PT'],[],['PT'],[],"[[17, 19]]",[]
49,2 . 2 Mô hình Gợi ý tin tức dựa trên cơ chế Học chú ý đa góc nhìn ( NAML ),"[[68, 72]]","[[6, 65]]",['NAML'],['Mô hình Gợi ý tin tức dựa trên cơ chế Học chú ý đa góc nhìn'],['NAML'],"['ý', 'Học chú ý đa góc nhìn']","[[68, 72]]","[[18, 19], [44, 65]]"
50,"𝑁 ( 𝜇𝑚 , 𝜎𝑚 , trong đó { 𝑚",[],[],[],[],[],[],[],[]
51,Stochastic Optimization Tối ưu hóa ngẫu nhiên LSA,"[[46, 49]]",[],['LSA'],[],[],[],[],[]
52,"CIFAR10 / 100 Trong thử nghiệm này , qua Hình 4.9 ta thấy được các kết quả đạt được trên các phương pháp đều cao hơn so với thử nghiệm trên Split Cifar 100 , mặc dù các tác","[[0, 13], [146, 151]]",[],"['CIFAR10 / 100', 'Cifar']",[],[],[],[],[]
53,"Normalized discounted cumulative gain NDCG Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT","[[106, 117], [38, 42]]",[],"['KSTN - CNTT', 'NDCG']",[],['NDCG'],[],"[[38, 42]]",[]
54,"Phần này sẽ trình bày về kỹ thuật Dropout Bayes biến phân ( VBD ) cũng như kiến thức liên quan . Để đơn giản , ta làm việc với một tầng kết nối đầy đủ ( Fullyconnected layer ) với K nơ-ron đầu vào , D nơ-ron đầu ra trước hàm kích hoạt .","[[60, 63]]","[[34, 57]]",['VBD'],['Dropout Bayes biến phân'],['VBD'],['biến phân'],"[[60, 63]]","[[48, 57]]"
55,"Kiến trúc của mô hình hoàn Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 35","[[90, 101]]",[],['KSTN - CNTT'],[],['KSTN-CNTT'],[],"[[-1, 8]]",[]
56,biểu diễn ẩn hi = f ( xi ) 3 . Một mạng MLP g ( . ) đơn giản hay còn gọi là projection head để map từ không,"[[40, 43]]",[],['MLP'],[],['MLP'],[],"[[40, 43]]",[]
57,en và train . vi . Mô hình ngôn ngữ sử dụng mô hình ngôn ngữ dựa trên LSTM,"[[70, 74], [0, 2], [14, 16]]",[],"['LSTM', 'en', 'vi']",[],[],['Mô hình ngôn ngữ sử dụng mô hình ngôn ngữ dựa'],[],"[[19, 64]]"
58,dụng kết hợp dữ liệu hành vi tiềm ẩn và hành vi rõ ràng được công bố gần đây hay không ? • Mô hình ITE - item _ pcat và ITE - user _ item _ pcat có hiệu quả hơn khi có,"[[99, 116], [120, 144]]",[],"['ITE - item _ pcat', 'ITE - user _ item _ pcat']",[],['ITE-item_pcat'],[],"[[-1, 12]]",[]
59,"sự hướng dẫn của THS. Ngô Văn Linh . Các kết quả nêu trong ĐATN là trung thực , không phải sao chép của bất cứ công","[[17, 21], [59, 63]]",[],"['THS.', 'ĐATN']",[],"['THS.', 'ĐATN']",[],"[[17, 21], [59, 63]]",[]
60,"Sentence Prediction . Masked Language Model ( MLM ) Đầu tiên , 15 % số từ trong một chuỗi sẽ được thay thế bằng [ MASK ] , sau đó được","[[46, 49]]",[],['MLM'],[],"['MLM', 'MASK']",[],"[[46, 49], [114, 118]]",[]
61,"Kết hợp manifold learning vào Contrastive Learning : Như đã nói từ trước , việc kết hợp manifold learning vào CL dựa trên ý tưởng thuật toán LLE . Cách","[[110, 112], [141, 144]]",[],"['CL', 'LLE']",[],"['CL', 'LLE']",[],"[[110, 112], [141, 144]]",[]
62,"hiểu về phân phối population : Giả sử có một tập gồm D văn bản được đánh số từ 1 đến D , thực hiện D lần lấy mẫu có hoàn lại trong tập này , kết quả thu được",[],[],[],[],[],[],[],[]
63,Nội dung Impression ID,"[[20, 22]]",[],['ID'],[],[],[],[],[]
64,"trong quá trình huấn luyện , hàm Attention được tính bởi : 𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 ( 𝑄 , 𝐾 , 𝑉 ) = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 ( 𝑄𝐾 𝑇",[],[],[],[],[],[],[],[]
65,"a . Kiến trúc cho thử nghiệm Split MNIST , b . Kiến trúc cho thử nghiệm Split CIFAR100 và Split CIFAR10 / 100 , trong đó k là kích thước bộ lọc , c là số kênh đầu ra , s","[[35, 40], [78, 86], [96, 109]]",[],"['MNIST', 'CIFAR100', 'CIFAR10 / 100']",[],['MNIST'],[],"[[35, 40]]",[]
66,"gốc trong cả quá trình học , và khi kết quả đến hội tụ sau 500 epoch , các phương pháp LCL cho kết quả nhỉnh hơn so với SimCLR khoảng 1 % độ chính xác trên tập Đồ án tốt nghiệp","[[87, 90], [120, 126]]",[],"['LCL', 'SimCLR']",[],['LCL'],[],"[[87, 90]]",[]
67,"Kết luận Những đóng góp chính của đồ án Xuất phát từ bài toán thực tế từ dữ liệu , SSL là một hướng nghiên cứu đang","[[83, 86]]",[],['SSL'],[],['SSL'],[],"[[83, 86]]",[]
68,Phiên mã là bước đầu tiên trong việc chuyển thông tin di truyền từ DNA thành tính trạng . Các DNA vốn ở dạng mạch xoắn kép sẽ được tháo xoắn và tách thành hai mạch,"[[67, 70], [94, 97]]",[],"['DNA', 'DNA']",[],['DNA'],[],"[[67, 70]]",[]
69,"Đối với từng tác vụ riêng biệt , ALV cũng cho thấy khả năng vượt trội của mình hơn hai trường hợp cùng được đánh giá .","[[33, 36]]",[],['ALV'],[],['ALV'],[],"[[33, 36]]",[]
70,"Đầu ra : Tập các tâm cụm { 𝜇1 , 𝜇2 , … , 𝜇𝐾 } ; 𝜇𝑘 ∈ ℝ𝑀 ; tập nhãn ứng với từng quan 1 nếu 𝑥𝑛 thuộc cụm 𝑘",[],[],[],[],[],[],[],[]
71,"bản d và fj là số văn bản chứa từ j . Khi đó LSI sử dụng phương pháp phân tích giá trị riêng ( cụ thể hơn là truncated Single Value Decomposition ) , để tìm ra","[[45, 48]]",[],['LSI'],[],['LSI'],[],"[[45, 48]]",[]
72,"tập dữ liệu đầu vào , ANN thường sẽ cần học lại từ đầu để có thể thích nghi với dữ liệu mới đó và tránh hiện tượng quên nghiêm trọng ( Catastrophic Forgetting ) [ 3 ] . Hiện tượng này xảy ra khi tham số học được từ dữ liệu cũ của mạng không phù hợp","[[22, 25]]",[],['ANN'],[],['ANN'],[],"[[22, 25]]",[]
73,thức PT 2.21 28,"[[5, 7]]",[],['PT'],[],['PT'],[],"[[5, 7]]",[]
74,quên của mô hình trên tác vụ 1 sau khi học xong tác vụ 2 : F1 = L1 ( w2 ∗ ) − L1 ( w1∗ ) 1,[],[],[],[],[],[],[],[]
75,"khác nhau và cập nhật một cách riêng biệt ( hình 16a ) hoặc không cần đến hai mạng encoder mà chỉ cần sample từ một memory bank để lấy negative ( hình 16b ) . Tuy nhiên , cách làm của MOCO là sử dụng một từ điển hoạt động như hàng đợi","[[184, 188]]",[],['MOCO'],[],['MOCO'],[],"[[184, 188]]",[]
76,"Giữ lại thông tin là chưa đủ : Tuy nhiên , việc chỉ giữ nhiều thông tin nhất từ đầu vào có thể dẫn đến những điểm yếu làm cho biểu diễn ẩn mà mạng AE học ra","[[147, 149]]",[],['AE'],[],['AE'],[],"[[147, 149]]",[]
77,"lớn về mặt tính toán , cụ thể các mô hình CL thường muốn có kích thước batch lớn để tăng được số mẫu negative . Trong một số phương pháp khác , việc phải","[[42, 44]]",[],['CL'],[],['CL'],[],"[[42, 44]]",[]
78,30 Hình 4 . 1 Biểu đồ ROC Trong đó :,"[[22, 25]]",[],['ROC'],[],['ROC'],[],"[[22, 25]]",[]
79,"lý . Đặc biệt là loại văn bản trên web có lẫn các HTML tag , code JS , đó chính là nhiễu . Tiền xử lý được xây dựng trên 4 bước :","[[50, 54], [66, 68]]",[],"['HTML', 'JS']",[],[],[],[],[]
80,"Từ tập các vector thu được ở bước một ta đưa qua các CNN block đưa ra vector biểu diễn cho câu . Module 3 ( pair module ) : Sử dụng các cơ chế attention , label embeddings , connective","[[53, 56]]",[],['CNN'],[],['CNN'],[],"[[53, 56]]",[]
81,áp dụng hướng nghiên cứu SSL trong các lĩnh vực trên mà sẽ chỉ hướng đến việc đi sâu vào trọng tâm một vài phương pháp rất mới trong SSL và tìm cách để cải thiện các phương pháp đó theo một tư duy mới .,"[[25, 28], [133, 136]]",[],"['SSL', 'SSL']",[],['SSL'],[],"[[25, 28]]",[]
82,"thay đổi bất thường của dữ liệu dòng , đặc biệt là concept drift . Tuy nhiên chúng ta có thể thấy phương pháp học của HPP là khá phức tạp , đặc biệt với những","[[118, 121]]",[],['HPP'],[],['HPP'],[],"[[118, 121]]",[]
83,và mở rộng sâu hơn nữa cho nhiều mô hình khác của CL . Đồ án tốt nghiệp 7,"[[50, 52]]",[],['CL'],[],['CL'],[],"[[50, 52]]",[]
84,t này được đưa qua một hàm chuyển đổi Θ̃ = f ( Θt π t ) . Hàm chuyển đổi f này,[],[],[],[],[],[],[],[]
85,"4 . Các thuật toán regularization : Ridge Regression , Least Absolute Shrinkage and Selection Operator ( LASSO ) , Least-Angle Regression ( LARS ) , . . . 12","[[105, 110], [140, 144]]",[],"['LASSO', 'LARS']",[],"['LASSO', 'LARS']",['Least-Angle'],"[[105, 110], [140, 144]]","[[115, 126]]"
86,các thử nghiệm . Các thử nghiệm được thực hiện trên bộ dữ liệu chuẩn Penn Discourse Treebank ( PDTB ) .,"[[95, 99]]",[],['PDTB'],[],['PDTB'],[],"[[95, 99]]",[]
87,"– Tất nhiên , mô hình vẫn chưa giải quyết được vấn đề cold start Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 34","[[128, 139]]",[],['KSTN - CNTT'],[],['KSTN-CNTT'],[],"[[-1, 8]]",[]
88,4 . 2 Độ đo WER Độ đo được sử dụng cho bước nhận dạng văn bản là Word Error Rate ( WER ) .,"[[12, 15], [83, 86]]",[],"['WER', 'WER']",[],['WER'],[],"[[12, 15]]",[]
89,"dữ liệu khi đã biết tham số mô hình được định nghĩa : L ( θ ) = p ( D | θ ) . Khi đó ước lượng hợp lý cực đại ( Maximum Likelihood estimator , MLE ) tìm ra tham số θM LE","[[143, 146]]","[[85, 109]]",['MLE'],['ước lượng hợp lý cực đại'],[],[],[],[]
90,i=1 Phương pháp đề xuất bởi đồ án được đặt tên là VBD - CL . VBD - CL được so sánh,"[[50, 58], [61, 69]]",[],"['VBD - CL', 'VBD - CL']",[],['VBD-CL'],[],"[[-1, 5]]",[]
91,"điểm mạnh và hạn chế riêng . Ở phần này , em xin tiến hành thử nghiệm , đánh giá với những mục tiêu sau :",[],[],[],[],[],[],[],[]
92,Bảng 0 .5 Tham số tốt nhất cho Split Omniglot EWC,"[[46, 49]]",[],['EWC'],[],[],[],[],[]
93,"- Cung cấp một nền tảng lý tưởng , cho phép cung cấp tích hợp các kỹ thuật hiện đại như mô hình thiết kế hướng đối tượng , hệ chuyên gia , hệ thông tin địa lý ( GIS ) .","[[161, 164]]","[[139, 158]]",['GIS'],['hệ thông tin địa lý'],['GIS'],"['hệ', 'hệ thông tin địa lý']","[[161, 164]]","[[123, 125], [139, 158]]"
94,hình 31 : ACD ca sử dụng đăng bài tuyển dụng 3.4 .11 . Xem các bài tuyển dụng đã đăng,"[[10, 13]]",[],['ACD'],[],['ACD'],[],"[[10, 13]]",[]
95,"• Ảnh màu : Là ảnh được tổ hợp từ 3 màu cơ bản đỏ ( R ) , lục ( G ) , lơ ( B ) theo lý thuyết của Thomas . Ảnh màu thường được thu nhận trên các dải băng tần khác",[],[],[],[],[],[],[],[]
96,Tác v 10 VBD - CL UCL,"[[9, 17], [18, 21]]",[],"['VBD - CL', 'UCL']",[],[],[],[],[]
97,"Bayes , ta cần xấp xỉ phân phối hậu nghiệm p ( E | D , γ ) bằng phân phối biến phân qφ ( E ) . Một lựa chọn phổ biến cho phân phối biến phân là tuân theo phân phối",[],[],[],[],[],[],[],[]
98,ẩn có dạng : k X,[],[],[],[],[],[],[],[]
99,"( 𝜃𝑗 − 𝜃𝑡−1,𝑗 2 Đối với việc áp dụng cho EWC do chỉ sử dụng mạng nơ-ron nhân tạo kết nối","[[41, 44]]",[],['EWC'],[],['EWC'],[],"[[41, 44]]",[]
100,viên tiềm năng 2 – Hệ thống sẽ gửi lại thông tin của ứng viên 3 – NTD chọn vào mục thao tác trên 1 hồ sơ,"[[66, 69]]",[],['NTD'],[],['NTD'],[],"[[66, 69]]",[]
101,"bằng cách lấy kì vọng của phân phối biến phân tương ứng ( với phân phối Dir , kỳ vọng chính là giá trị trung bình của tham số ) , cụ thể : • Với mỗi văn bản d = 1 , ... , D : θ̂dk ∝ γdk ( chuẩn hóa vectơ K chiều γd )","[[72, 75]]",[],['Dir'],[],[],[],[],[]
102,trên toàn bộ dữ liệu sẽ được xấp xỉ bằng ước lượng Monte Carlo trên tập con gồm 𝑁 ( 𝑚 ),[],[],[],[],[],[],[],[]
103,công việc của VD [ 11 ] . Công việc của đồ án sẽ mượn ý tưởng của VD và áp dụng cho bài toán Học liên tục bằng việc bổ sung thành phần tham số cục bộ tương tự như ý tưởng thêm,"[[14, 16], [66, 68]]",[],"['VD', 'VD']",[],['VD'],[],"[[14, 16]]",[]
104,"Lấy ví dụ một banner nằm ở vị trí zone 2 có thể hay được click hơn so với khi đặt ở vị trí Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT","[[154, 165]]",[],['KSTN - CNTT'],[],[],[],[],[]
105,"để xấp xỉ cho phân phối thực của 𝑠 khi đã biết dữ liệu . Trong công việc hiện tại để thuận tiện tính toán , phương pháp GMF được sử dụng để lựa chọn cho 𝑞 ( 𝑠 ) , trong","[[120, 123]]",[],['GMF'],[],['GMF'],[],"[[120, 123]]",[]
106,Augmentation ( DA ) vào Decoder của mô hình DVAE và thay thế phép thêm nhiễu vào dữ liệu trước khi đưa vào mạng Encoder của mô hình . Các kết quả cho thấy sự,"[[15, 17], [44, 48]]",[],"['DA', 'DVAE']",[],"['DA', 'DVAE']",[],"[[15, 17], [44, 48]]",[]
107,"Cụ thể , trong bài Contrastive Multiview Coding ( ở [ TKI19 ] ) , bài báo đã nhìn một dữ liệu theo nhiều hướng chứ không phải hai hướng như thông thường . Tác",[],[],[],[],[],[],[],[]
108,"Sử dụng một cách làm đơn giản , làm nhiễu dữ liệu : Chỉ với hàm mất mát của AE truyền thông , bài báo đã chỉ ra rằng biểu diễn ẩn tìm ra không đủ khả","[[76, 78]]",[],['AE'],[],['AE'],[],"[[76, 78]]",[]
109,"Như thể hiện ở hình 5 . 6 , VBD - CL nhìn chung có kết quả tốt hơn so với UCL và AGS - CL mặc dù có dao động lớn hơn ở một vài tác vụ . Điều này có thể do tri thức của tác vụ đó được lưu giữ ở các nơ-ron mà có độ quan","[[28, 36], [74, 77], [81, 89]]",[],"['VBD - CL', 'UCL', 'AGS - CL']",[],"['VBD', '-CL']",[],"[[28, 31], [-1, 2]]",[]
110,"Sau khi quan sát được dữ liệu , các tham số này sẽ được học sao cho phù hợp với dữ liệu đó thông qua biểu diễn của xác suất hậu nghiệm 𝑝 ( 𝜃 | 𝐷 ) . Theo định lý Bayes , xác suất hậu nghiệm của tham số 𝜃 sau khi có dữ liệu D :",[],[],[],[],[],[],[],[]
111,được một phân phối FD trên tập D văn bản này với : D 1 X,[],[],[],[],[],[],[],[]
112,o Convolution Neural Network ( CNN ) : Xây dựng mô hình học CNN bậc từ tối ưu hiệu quả huấn luyện mô hình sử dụng Dropout o LSTM : Mô hình học triển khai LSTM hai chiều và một tầng Attention,"[[31, 34], [60, 63], [124, 128], [154, 158]]",[],"['CNN', 'CNN', 'LSTM', 'LSTM']",[],"['CNN', 'LSTM']",[],"[[31, 34], [124, 128]]",[]
113,"3.2 Mô hình đề xuất Thay vì sử dụng mô hình dịch dựa trên LSTM giống như nghiên cứu gốc [ 19 ] , chúng tôi","[[58, 62]]",[],['LSTM'],[],['LSTM'],[],"[[58, 62]]",[]
114,Tham số 𝜆 trong công thức PT 2.12,"[[26, 28]]",[],['PT'],[],['PT'],[],"[[26, 28]]",[]
115,truyền tiến được biểu diễn như sau : zi = W i hi−1 + bi hi = σ ( z i ),[],[],[],[],[],[],[],[]
116,"UCL , AGS - CL và VBD - CL . AGS - CL có xu hướng học tương tự như ở tập Split CIFAR100 , còn UCL mềm dẻo hơn khi có có những tác vụ cải thiện được độ chính","[[0, 3], [6, 14], [18, 26], [29, 37], [79, 87], [94, 97]]",[],"['UCL', 'AGS - CL', 'VBD - CL', 'AGS - CL', 'CIFAR100', 'UCL']",[],"['UCL', 'AGS-CL']",[],"[[0, 3], [-1, 5]]",[]
117,"Hình 7 : Tăng kích thước đầu vào sử dụng Sub-pixel Convolutional Layer ( nguồn : [ 10 ] ) Trong hình 7 , xét tại lớp thứ ba , dữ liệu S đang có kích thước W × H × ( C × r 2 ) . f ( S ) có kích",[],[],[],[],[],[],[],[]
118,"xác suất , ta có hàm mất mát như sau : L=− R",[],[],[],[],[],[],[],[]
119,Sinh viên thực hiện : Nguyễn Thế Linh - 20142585 Lớp CNTT 2. 03 – K59,"[[53, 57]]",[],['CNTT'],[],['CNTT'],[],"[[53, 57]]",[]
120,"chủ đề kinh doanh , thảm họa , tội phạm và chính trị và được lấy từ nhiều nguồn khác nhau bao gồm NYT ( USA ) , Reuters ( Europe ) , Xinhua ( China ) và tập KBP 2015 [ 2 ] . 32","[[98, 101], [104, 107], [157, 160]]",[],"['NYT', 'USA', 'KBP']",[],"['NYT', 'KBP']",[],"[[98, 101], [157, 160]]",[]
121,Chuyên ngành : Hệ thống thông tin Giảng viên hướng dẫn : PGS. TS. Thân Quang Khoát,"[[57, 61], [62, 65]]",[],"['PGS.', 'TS.']",[],"['PGS', '.TS.']",[],"[[57, 60], [-1, 3]]",[]
122,"danh của người dùng và item làm đầu vào của mô hình , ITE - onehot cũng gặp phải vấn đề cold start . 4 . 2 Mô hình ITE - item _ pcat","[[115, 132], [54, 66]]",[],"['ITE - item _ pcat', 'ITE - onehot']",[],"['item', 'ITE-onehot']",[],"[[23, 27], [-1, 9]]",[]
123,Tương tự các nghiên cứu trước em sử dụng phương pháp chia dữ liệu PDTB - Ji . Với cài đặt này em sử dụng 5,"[[66, 70]]",[],['PDTB'],[],['PDTB'],[],"[[66, 70]]",[]
124,"Hình 2 . 2 Các dạng bài toán RNN [ 5 ] Hình 2 . 2 cho thấy các dạng bài toán RNN như sau :  One to one : mẫu bài toán cho mạng nơ-ron thần kinh ( NN ) và mạng nơron tích chập ( CNN ) , 1 đầu vào và 1 đầu ra , ví dụ với bài toán phân loại","[[29, 32], [77, 80], [147, 149], [178, 181]]","[[123, 144], [155, 175]]","['RNN', 'RNN', 'NN', 'CNN']","['mạng nơ-ron thần kinh', 'mạng nơron tích chập']","['RNN', 'CNN']","['mạng nơ-ron thần kinh', 'mạng nơron tích chập']","[[29, 32], [178, 181]]","[[123, 144], [155, 175]]"
125,2 . Mục đích nội dung của ĐATN Tìm hiểu áp dụng các phương pháp và kỹ thuật về xử lý ngôn ngữ tự nhiên và học sâu cho bài toán nhận dạng quan hệ ẩn giữa hai câu với dữ liệu Penn Discourse Treebank nhằm cải thiện,"[[26, 30]]",[],['ĐATN'],[],['ĐATN'],[],"[[26, 30]]",[]
126,"In Advances in neural information processing systems , pages 1153 – 1161 , 2015 . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT","[[142, 153]]",[],['KSTN - CNTT'],[],['–'],[],"[[66, 67]]",[]
127,"Khi cố định Q , việc tối ưu P được đưa về bài toán tối ưu hàm N 1 X X",[],[],[],[],[],[],[],[]
128,hàm Digamma Γ hàm Gamma,[],[],[],[],[],[],[],[]
129,"Hàm mất mát dùng để thực hiện nhiệm vụ phân loại . Với một batch N dữ liệu đầu vào ta được 2N view sau khi đi qua các phép DA , thông qua hàm mất","[[123, 125]]",[],['DA'],[],['DA'],[],"[[123, 125]]",[]
130,"Tuy nhiên , các mô hình này lại bỏ qua sự liên kết giữa các từ nối với quan hệ giữa hai câu . Cụ thể , với tập dữ liệu PDTB","[[119, 123]]",[],['PDTB'],[],[],[],[],[]
131,"hình sao cho xác suất dự đoán các Positive Samples là cao hơn . Cụ thể , xác suất mỗi bài báo được User quan tâm là 𝑦 + , xác suất dự đoán K bài không được User đó quan tâm [ 𝑦1− , 𝑦2 − , . . . , 𝑦𝐾− ] , điểm số được tính bởi công thức :",[],[],[],[],[],[],[],[]
132,kiến trúc mạng nơ-ron của các mô hình và công thức hàm dự đoán sẽ thể hiện cho tính thứ tự của hành vi tiềm ẩn và rõ ràng . 4.1 Mô hình ITE- onehot,"[[136, 147]]",[],['ITE- onehot'],[],[],[],[],[]
133,Tính chất của các phân phối Gamma :  E ( X ) = 𝛾𝑠ℎ𝑝,[],[],[],[],[],[],[],[]
134,"encoder , mô hình ngôn ngữ sẽ biến đổi chuỗi đầu vào x = ( x1 , x2 , . . . xn ) thành ma trận Z = W + P trong đó W = ( w2 , f w2 , . . . , wn ) là ma trận các vector embedding thuộc không gian Rf , P = ( p1 , p2 , . . . , pn ) là các ma trận các ánh xạ vị trí vào không gian Rf tương ứng",[],[],[],[],[],[],[],[]
135,"Để thống nhất với VD , p được chọn làm giá trị đại diện cho độ quan trọng của nơ-ron trong các phần trình bày phía dưới . Mỗi tác vụ , mô hình chỉ cần lưu lại bộ","[[18, 20]]",[],['VD'],[],['VD'],[],"[[18, 20]]",[]
136,HAT 10 Tác v 10,"[[0, 3]]",[],['HAT'],[],['HAT'],[],"[[0, 3]]",[]
137,"[ log p(D|θ )] − KL ( qφ (θ ) | p (θ ) ) . Trong công thức biểu diễn ELBO 2. 14 , thành phần Eqφ (θ ) [ log p( D|θ ) ] chính là kỳ","[[69, 73]]",[],['ELBO'],[],[],[],[],[]
138,"có dạng tường minh ( PT 2.17 ) và thuận tiện cho việc tính toán , đạo hàm . Giải thuật của VCL được diễn giải trong Hình 2 . 3 .","[[21, 23], [91, 94]]",[],"['PT', 'VCL']",[],"['PT', 'VCL']",[],"[[21, 23], [91, 94]]",[]
139,"ITE- onehot , ITE- item_pcat , ITE - user _ item _ pcat , NMTR và MTMF • So sánh các mô hình khi số thuộc tính ẩn k thay đổi","[[0, 11], [14, 28], [31, 55], [58, 62], [66, 70]]",[],"['ITE- onehot', 'ITE- item_pcat', 'ITE - user _ item _ pcat', 'NMTR', 'MTMF']",[],"['ITE-onehot', 'NMTR', 'MTMF']",[],"[[-1, 9], [58, 62], [66, 70]]",[]
140,17 CHƯƠNG 3 . CÔNG VIỆC LIÊN QUAN,[],[],[],[],[],[],[],[]
141,"gọn hơn . Ngoài ra , mô hình hệ gợi ý dựa trên ý tưởng của mô hình BERT trong xử lý ngôn ngữ tự nhiên cũng cho kết quả tốt , mang nhiều ý nghĩa từ việc học","[[67, 71]]",[],['BERT'],[],['BERT'],['xử lý ngôn ngữ tự nhiên'],"[[67, 71]]","[[78, 101]]"
142,"Đầu vào : Tập dữ liệu { 𝑥1 , 𝑥2 , … , 𝑥𝑁 } ; 𝑥𝑛 ∈ ℝ𝑀 ; số cụm K ( K < N ) . Đầu ra : Tập các tâm cụm { 𝜇1 , 𝜇2 , … , 𝜇𝐾 } ; 𝜇𝑘 ∈ ℝ𝑀 ; tập nhãn ứng với từng quan 28",[],[],[],[],[],[],[],[]
143,Quá trình tính toán này được biểu diễn bởi công thức sau : QK T,[],[],[],[],[],[],[],[]
144,"đồ thị tích chập ( GCN ) vào phân loại văn bản , đồng thời đưa ra cải tiến bằng việc kết hợp mô hình này với mô hình chủ đề ( topic modeling ) để giải quyết bài toán phân loại văn bản .","[[19, 22]]",[],['GCN'],[],['GCN'],['đồ thị tích chập'],"[[19, 22]]","[[0, 16]]"
145,"cũng xét đến việc kết hợp nhiều loại DA khác nhau : 1 . Liên quan đến không gian : cropping , resizing , cutout , rotate","[[37, 39]]",[],['DA'],[],['DA'],[],"[[37, 39]]",[]
146,"ích nhất , trong đó các ma trận ban đầu có thể đại diện cho một bức ảnh hoặc một câu . Để hiểu rõ hơn ta lấy ví dụ : Hình 1 : Cơ chế nhân chập trong CNN","[[149, 152]]",[],['CNN'],[],[],[],[],[]
147,mô hình của ALV . b . Kết quả thử nghiệm trên Split CIFAR100 : Hình 4 . 8 đưa ra kết quả thử nghiệm trên bộ dữ liệu Split CIFAR100 của các,"[[12, 15], [52, 60], [122, 130]]",[],"['ALV', 'CIFAR100', 'CIFAR100']",[],"['ALV', 'CIFAR100']",[],"[[12, 15], [52, 60]]",[]
148,Số điện thoại : 037 9696 690 Lớp : KHMT. 02 – K62 Mã số sinh viên : 20173264,"[[35, 39]]",[],['KHMT'],[],['KHMT'],[],"[[35, 39]]",[]
149,"tố nhiễu vào trong bức ảnh . Trong khi đó , như mô tả bên trên , sự khác biệt giữa Variational Autoencoder ( VAE ) và mô hình Autoencoder gốc nằm ở việc xử lý đầu","[[109, 112]]",[],['VAE'],[],['VAE'],[],"[[109, 112]]",[]
150,"Khác với 2 mô hình được giới thiệu trên , mô hình LSTUR được xây dựng dựa trên một số luận điểm cải thiện : -","[[50, 55]]",[],['LSTUR'],[],['LSTUR'],[],"[[50, 55]]",[]
151,• số thuộc tính ẩn k ( number factors ) : số thuộc tính trong tầng ẩn cuối cùng của hai mô hình con GMF và MLP ( xem lại Hình 8 ) . Đối với mô hình,"[[100, 103], [107, 110]]",[],"['GMF', 'MLP']",[],"['GMF', 'MLP']",[],"[[100, 103], [107, 110]]",[]
152,"phối mà VAE sử dụng để xấp xỉ phân phối hậu nghiệm . Đứng trước giả thuyết này , mô hình Denoising Variational Autoencoder ( DVAE ) đã được sủ dụng nhằm nâng cao khả năng xấp xỉ xác suất của mô hình dữ liệu gốc","[[8, 11], [125, 129]]",[],"['VAE', 'DVAE']",[],['VAE'],[],"[[8, 11]]",[]
153,"dữ liệu tương tác , từ đó cải thiện chất lượng gợi ý hay không ? Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 38","[[128, 139]]",[],['KSTN - CNTT'],[],['KSTN-CNTT'],[],"[[-1, 8]]",[]
154,‘ polynomial ’ . 24 Em chọn LinearSVC ( nhân linear ) để tuning và so sánh với mô hình học HAN .,"[[28, 37], [91, 94]]",[],"['LinearSVC', 'HAN']",[],['HAN'],['nhân linear'],"[[91, 94]]","[[40, 51]]"
155,"đều có thể tạo ra được tương tự từ việc sử dụng phép DA random cropping ( giải thích ở hình 13 , với hình nét liền là bức ảnh ban đầu còn hình nét chấm là các phần ảnh bị crop một cách ngẫu nhiên , tạo thành 2 pretext task tương tự với các","[[53, 55]]",[],['DA'],[],['DA'],[],"[[53, 55]]",[]
156,Sinh viên thực hiện Lê Hải Nam MỤC LỤC,[],[],[],[],[],[],[],[]
157,quan qua Hình 2 . 2 . Hình 2 . 2 Ý tưởng trực quan của EWC . Nguồn [ 1 ] .,"[[55, 58]]",[],['EWC'],[],['EWC'],[],"[[55, 58]]",[]
158,"[ ∇ log p( x, y | θ ) ∇ log p ( x , y |θ)T ] ( 3.2 ) Với Pxy là phân phối mô hình học được từ D = { x, y} tuân theo phân phối thực",[],[],[],[],[],[],[],[]
159,"𝑠 ( 𝑢 , 𝑒𝑥 ) = 𝑢𝑇 𝑒𝑥 Quá trình huấn luyện được kế thừa cách lấy mẫu được giới thiệu bởi [ 9 ] và [ 13 ] , với mỗi bài báo được click ( Positive Sample ) , lấy ngẫu nhiên K bài báo không",[],[],[],[],[],[],[],[]
