[
  {
    "text": "2 Related Work The availability of emotion-rich text has helped to promote studies of sentiments from a boutique science into the mainstream of Text Data Mining (TDM). The ?",
    "acronyms": [
      [
        162,
        165
      ]
    ],
    "long-forms": [
      [
        144,
        160
      ]
    ],
    "ID": "1"
  },
  {
    "text": "WebDict 0.2919 Backoff 0.3282 Table 1: Mean Average Precision (MAP), averaged over 34 topics",
    "acronyms": [
      [
        63,
        66
      ]
    ],
    "long-forms": [
      [
        39,
        61
      ]
    ],
    "ID": "2"
  },
  {
    "text": "Baselines are a unigram query likelihood (QL) model (bag of words) and a highly effective sequential dependence (SD) variant of the Markov random field (MRF) model (Metzler and Croft,",
    "acronyms": [
      [
        113,
        115
      ],
      [
        42,
        45
      ],
      [
        153,
        156
      ]
    ],
    "long-forms": [
      [
        90,
        111
      ],
      [
        132,
        151
      ]
    ],
    "ID": "3"
  },
  {
    "text": "tion. Then, we extract expansion terms from these clusters using pseudo relevance feedback (PRF) as implemented in Terrier.",
    "acronyms": [
      [
        92,
        95
      ]
    ],
    "long-forms": [
      [
        65,
        90
      ]
    ],
    "ID": "4"
  },
  {
    "text": "person, mood, voice and case, CATiB uses 6 POS tags: NOM (non-proper nominals including nouns, pronouns, adjectives and adverbs), PROP (proper nouns), VRB (verbs), VRB-PASS (passive-voice",
    "acronyms": [
      [
        130,
        134
      ],
      [
        30,
        35
      ],
      [
        43,
        46
      ],
      [
        53,
        56
      ],
      [
        151,
        154
      ],
      [
        164,
        172
      ]
    ],
    "long-forms": [
      [
        136,
        142
      ],
      [
        156,
        161
      ]
    ],
    "ID": "5"
  },
  {
    "text": "(LDA), Maximum Likelihood Linear Transform (MLLT), Boosted Maximum Mutual Information (BMMI), Minimum Phone Error (MPE). It pro-",
    "acronyms": [
      [
        115,
        118
      ],
      [
        44,
        48
      ],
      [
        87,
        91
      ]
    ],
    "long-forms": [
      [
        94,
        113
      ],
      [
        7,
        42
      ],
      [
        51,
        85
      ]
    ],
    "ID": "6"
  },
  {
    "text": "gold label (Section 3). Our algorithm is called The Structured Weighted Violations Perceptron (SWVP) as its update rule is based on a weighted sum of up-",
    "acronyms": [
      [
        95,
        99
      ]
    ],
    "long-forms": [
      [
        52,
        93
      ]
    ],
    "ID": "7"
  },
  {
    "text": "phrase markers or words. For simplicity of manipulation but without loss of general-  ity, we will limit the productions to the Chomsky Normal Form (CNF). That is, only ",
    "acronyms": [
      [
        149,
        152
      ]
    ],
    "long-forms": [
      [
        128,
        147
      ]
    ],
    "ID": "8"
  },
  {
    "text": " When a CFG is associated with probabilistic information, as in a Probabilistic CFG (PCFG), it can be interesting to process only the n most likely",
    "acronyms": [
      [
        85,
        89
      ],
      [
        8,
        11
      ]
    ],
    "long-forms": [
      [
        66,
        83
      ]
    ],
    "ID": "9"
  },
  {
    "text": "1] proposed a language-neutral framework for representing semantic tense. This framework is called the Language Neutral Syntax (LNS). Based on ",
    "acronyms": [
      [
        128,
        131
      ]
    ],
    "long-forms": [
      [
        103,
        126
      ]
    ],
    "ID": "10"
  },
  {
    "text": "12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3 64.2 53.5 60.0 56.6 51.1 57.9 54.3 13 Base+Verb Pairs (VP) 62.1 72.2 66.8 60.1 69.3 64.4 54.4 60.1 57.1 51.9 58.2 54.9 14 Base+Appositives (AP) 63.1 71.7 67.1 60.5 69.4 64.6 54.1 60.1 56.9 51.9 57.8 54.7 Table 1: Results obtained by applying different types of features in isolation to the Baseline system.",
    "acronyms": [
      [
        187,
        189
      ],
      [
        18,
        20
      ],
      [
        102,
        104
      ]
    ],
    "long-forms": [
      [
        169,
        185
      ],
      [
        8,
        16
      ],
      [
        90,
        99
      ]
    ],
    "ID": "11"
  },
  {
    "text": "Model 3 (Figure 3) illustrates how the source language text and MT component may be replaced by a natural language generation (NLG) system, given a rich enough semantic representation.",
    "acronyms": [
      [
        127,
        130
      ],
      [
        64,
        66
      ]
    ],
    "long-forms": [
      [
        98,
        125
      ]
    ],
    "ID": "12"
  },
  {
    "text": "in the V column indicates that the verb conditions were used in the distance measure. LR = labeled recall; LP = labeled precision.",
    "acronyms": [
      [
        86,
        88
      ],
      [
        107,
        109
      ]
    ],
    "long-forms": [
      [
        91,
        105
      ],
      [
        112,
        129
      ]
    ],
    "ID": "13"
  },
  {
    "text": "The big blue door.?  In this case, the GrM asks  the Response Planner (RP) to provide an elaboration for the current UU; the RP generates this ",
    "acronyms": [
      [
        71,
        73
      ],
      [
        39,
        42
      ],
      [
        117,
        119
      ],
      [
        125,
        127
      ]
    ],
    "long-forms": [
      [
        53,
        69
      ]
    ],
    "ID": "14"
  },
  {
    "text": "TI = terse information  INT = interrupted  TRUN = truncated  TRANS = transposed sentence (discussed in ",
    "acronyms": [
      [
        43,
        47
      ],
      [
        0,
        2
      ],
      [
        24,
        27
      ],
      [
        61,
        66
      ]
    ],
    "long-forms": [
      [
        50,
        59
      ],
      [
        5,
        22
      ],
      [
        30,
        41
      ],
      [
        69,
        79
      ]
    ],
    "ID": "15"
  },
  {
    "text": "classes in both original text and main text corpora.  chose Support Vector Machines (SVM) because it has been shown by other researchers in AGI",
    "acronyms": [
      [
        85,
        88
      ],
      [
        140,
        143
      ]
    ],
    "long-forms": [
      [
        60,
        83
      ]
    ],
    "ID": "16"
  },
  {
    "text": "as in figure 3.  It is parsed as an adverb (AA), whereas it should be a verb group (VG).",
    "acronyms": [
      [
        44,
        46
      ],
      [
        84,
        86
      ]
    ],
    "long-forms": [
      [
        33,
        42
      ],
      [
        72,
        82
      ]
    ],
    "ID": "17"
  },
  {
    "text": "can develop after exposure to a terrifying event.  Q-based Union PTSD (posttraumatic stress disorder) is a psychological disorder caused by a mental trauma (also called psychotrauma) that can develop after exposure to a terrifying event.",
    "acronyms": [
      [
        65,
        69
      ]
    ],
    "long-forms": [
      [
        71,
        100
      ]
    ],
    "ID": "18"
  },
  {
    "text": "It allows for testing interaction scenarios that employ one or more Language Technology Components (LTC). ",
    "acronyms": [
      [
        100,
        103
      ]
    ],
    "long-forms": [
      [
        68,
        98
      ]
    ],
    "ID": "19"
  },
  {
    "text": "Feature Description lexical the words of the product attribute(PA) the POS for each word of the PA",
    "acronyms": [
      [
        63,
        65
      ],
      [
        71,
        74
      ],
      [
        96,
        98
      ]
    ],
    "long-forms": [
      [
        45,
        61
      ]
    ],
    "ID": "20"
  },
  {
    "text": " The tagging has been done using a GUI-based tool  called the Discourse Tagging Tool (DTTool) ac-  cording to \"The Discourse Tagging Guidelines\" we ",
    "acronyms": [
      [
        86,
        92
      ],
      [
        35,
        38
      ]
    ],
    "long-forms": [
      [
        62,
        84
      ]
    ],
    "ID": "21"
  },
  {
    "text": "Och and Ney (2003) show that for larger corpora, using word classes leads to lower Alignment Error Rate (AER). This is not",
    "acronyms": [
      [
        105,
        108
      ]
    ],
    "long-forms": [
      [
        83,
        103
      ]
    ],
    "ID": "22"
  },
  {
    "text": "2 Relation Extraction System In this section, we describe the features used in our basic relation extraction (RE) system. Given",
    "acronyms": [
      [
        110,
        112
      ]
    ],
    "long-forms": [
      [
        89,
        108
      ]
    ],
    "ID": "23"
  },
  {
    "text": "target question, or zero if the target question was not found. The Mean Reciprocal Rank (MRR) is the mean of the reciprocal ranks over all the input ques-",
    "acronyms": [
      [
        89,
        92
      ]
    ],
    "long-forms": [
      [
        67,
        87
      ]
    ],
    "ID": "24"
  },
  {
    "text": "discussion in Ritchie(1984).  Functional unification (FU) grammar is a  grammatical formalism which allows descriptions of ",
    "acronyms": [
      [
        54,
        56
      ]
    ],
    "long-forms": [
      [
        30,
        52
      ]
    ],
    "ID": "25"
  },
  {
    "text": "Bigram Perplex. ( PP) MDI Missed Samples (MS) Bigram Missed Samples (MS)",
    "acronyms": [
      [
        42,
        44
      ],
      [
        18,
        20
      ],
      [
        22,
        25
      ],
      [
        69,
        71
      ]
    ],
    "long-forms": [
      [
        26,
        40
      ],
      [
        7,
        14
      ],
      [
        53,
        67
      ]
    ],
    "ID": "26"
  },
  {
    "text": "networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). ",
    "acronyms": [
      [
        92,
        96
      ],
      [
        32,
        36
      ]
    ],
    "long-forms": [
      [
        51,
        90
      ]
    ],
    "ID": "27"
  },
  {
    "text": "Therefore, identification methods like Tsuchiya et al (2006) which uses Support Vector Machines(SVM) have been proposed to solve this problem.",
    "acronyms": [
      [
        96,
        99
      ]
    ],
    "long-forms": [
      [
        72,
        94
      ]
    ],
    "ID": "28"
  },
  {
    "text": "domain-independent mpirical induction algorithm. We test this idea by examining the  machine learning of simple Sound Pattern of English (SPE)-style phonological rules  (Chomsky and Halle 1968), beginning by representing phonological rules as finite- ",
    "acronyms": [
      [
        138,
        141
      ]
    ],
    "long-forms": [
      [
        112,
        136
      ]
    ],
    "ID": "29"
  },
  {
    "text": "ents) *100%  ? F1-score = 2*P*R / (P+R)  Two correctness criteria are used for constitu-",
    "acronyms": [
      [
        35,
        38
      ]
    ],
    "long-forms": [
      [
        26,
        31
      ]
    ],
    "ID": "30"
  },
  {
    "text": " 4 Corpus description The GRN corpus is a set of 201 sentences selected from PubMed abstracts, which are  mainly about the sporulation phenomenon in Bacillus subtilis. This corpus is an extended version of the LLL and BI (BioNLP-ST?11) corpora. The additional sentences ensure a better coverage of the description of the sporulation.",
    "acronyms": [
      [
        218,
        220
      ],
      [
        210,
        213
      ],
      [
        26,
        29
      ],
      [
        77,
        83
      ]
    ],
    "long-forms": [
      [
        222,
        228
      ]
    ],
    "ID": "31"
  },
  {
    "text": "The DlmSum Summarization Clientprovldes a sum-  mary of a document in multiple dlmeuslons through  a graphical user interface (GUI) to smt dflferent  users' needs In contrast o a static view of a doc- ",
    "acronyms": [
      [
        127,
        130
      ]
    ],
    "long-forms": [
      [
        101,
        125
      ]
    ],
    "ID": "32"
  },
  {
    "text": "is encoded in the attributes MODALITY and POLARITY. Modality at the syntactic level is encoded as an attribute of the tag SLINK (Subordination Link), which can have several values: factive, counterfactive, evidential, negative evidential, modal,",
    "acronyms": [
      [
        122,
        127
      ]
    ],
    "long-forms": [
      [
        129,
        147
      ]
    ],
    "ID": "33"
  },
  {
    "text": "So, the  first matrix (with the data from IBL trained with Encoding 1) shows in its upper row  the classification of the words that have final stress (FIN). It appears that the classifier ",
    "acronyms": [
      [
        151,
        154
      ],
      [
        42,
        45
      ]
    ],
    "long-forms": [
      [
        137,
        142
      ]
    ],
    "ID": "34"
  },
  {
    "text": "stream. In Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI), 20?29. ",
    "acronyms": [
      [
        89,
        92
      ]
    ],
    "long-forms": [
      [
        49,
        87
      ]
    ],
    "ID": "35"
  },
  {
    "text": "We propose a method to solve this problem, which also results in a new topic model, called AKL (Automated Knowledge LDA), whose inference can exploit the automatically learned",
    "acronyms": [
      [
        91,
        94
      ]
    ],
    "long-forms": [
      [
        96,
        119
      ]
    ],
    "ID": "36"
  },
  {
    "text": "lated work. We then introduce Markov logic and our Markov Logic Network (MLN) for joint bio-event extraction.",
    "acronyms": [
      [
        73,
        76
      ]
    ],
    "long-forms": [
      [
        51,
        71
      ]
    ],
    "ID": "37"
  },
  {
    "text": " We explore how to utilize the source-language test corpus for adapting the language model (LM) and the translation model (TM).",
    "acronyms": [
      [
        92,
        94
      ],
      [
        123,
        125
      ]
    ],
    "long-forms": [
      [
        76,
        90
      ],
      [
        104,
        121
      ]
    ],
    "ID": "38"
  },
  {
    "text": "INAN=inanimate NP, ANIM=animate NP, VBZ--inflected  main verb, IS=is, VBG=gerund, PP=prepositional phrase,  TO=to (prep.), ONmon (prep.).",
    "acronyms": [
      [
        108,
        110
      ],
      [
        0,
        4
      ],
      [
        15,
        17
      ],
      [
        19,
        23
      ],
      [
        32,
        34
      ],
      [
        36,
        39
      ],
      [
        63,
        65
      ],
      [
        70,
        73
      ],
      [
        82,
        84
      ]
    ],
    "long-forms": [
      [
        111,
        113
      ],
      [
        5,
        14
      ],
      [
        24,
        31
      ],
      [
        66,
        68
      ],
      [
        74,
        80
      ],
      [
        85,
        105
      ]
    ],
    "ID": "39"
  },
  {
    "text": "and WLLR in this paper.  3.3 Information Gain (IG)  IG measures the number of bits of information ",
    "acronyms": [
      [
        47,
        49
      ],
      [
        4,
        8
      ],
      [
        52,
        54
      ]
    ],
    "long-forms": [
      [
        29,
        45
      ]
    ],
    "ID": "40"
  },
  {
    "text": "Three query tasks were defined in TREC-2003 and TREC-2004 web track, which are home page finding (HP), named page finding (NP) and topic distillation (TD) (Voorhees, 2003; Voorhees, 2004).",
    "acronyms": [
      [
        123,
        125
      ],
      [
        34,
        38
      ],
      [
        48,
        52
      ],
      [
        98,
        100
      ],
      [
        151,
        153
      ]
    ],
    "long-forms": [
      [
        103,
        113
      ],
      [
        79,
        88
      ],
      [
        131,
        149
      ]
    ],
    "ID": "41"
  },
  {
    "text": "329 Figure 1: The different structures (left: constituency trees, right: predicate argument structure) derived from Sentence (1) for the opinion holder candidate Malaysia used as input for convolution kernels (CK). ",
    "acronyms": [
      [
        210,
        212
      ]
    ],
    "long-forms": [
      [
        189,
        208
      ]
    ],
    "ID": "42"
  },
  {
    "text": "There are two other threads of research literature relevant to our work. Named entity (NE) extraction attempts to identify entities of interest",
    "acronyms": [
      [
        87,
        89
      ]
    ],
    "long-forms": [
      [
        73,
        85
      ]
    ],
    "ID": "43"
  },
  {
    "text": "Eat_On_Time? Theme 3.3 Planning for Linguistic Variation Sentences generated by Picture Books vary in length and word complexity according to the user?s age. The variation in length is handled by specificity character goals (SCG) that are ap-pended as supporting details to their respective character goals. SCGs are designed so that their existence will give more detail to the preceding character goal while ensuring consistency, and that their non-existence will still make the story complete.",
    "acronyms": [
      [
        225,
        228
      ],
      [
        308,
        312
      ]
    ],
    "long-forms": [
      [
        196,
        223
      ]
    ],
    "ID": "44"
  },
  {
    "text": "correct class of an item from its context. The Maximum Entropy (MaxEnt) framework is especially suited for integrating evidence from var-",
    "acronyms": [
      [
        64,
        70
      ]
    ],
    "long-forms": [
      [
        47,
        62
      ]
    ],
    "ID": "45"
  },
  {
    "text": " and  As for perceptron criterion, we employ the  average perceptron (AvgP) (Freund and  Sc",
    "acronyms": [
      [
        70,
        74
      ]
    ],
    "long-forms": [
      [
        50,
        68
      ]
    ],
    "ID": "46"
  },
  {
    "text": "               XOAJC + LC*Ave(XOAJC)/Ave(XOAR)  The second strategy consists in building a prediction model for BLAST bit score (BBS) using the  XOA score and the log-cosine LC as predictors ",
    "acronyms": [
      [
        129,
        132
      ],
      [
        41,
        45
      ],
      [
        30,
        35
      ],
      [
        15,
        20
      ],
      [
        23,
        25
      ],
      [
        145,
        148
      ],
      [
        174,
        176
      ]
    ],
    "long-forms": [
      [
        112,
        127
      ]
    ],
    "ID": "47"
  },
  {
    "text": "22M 35.88 27.16 30.20 36.21 27.26 30.48 -0.33 -0.10 -0.28 Table 2: BLEU Score results for the Spanish Treelet Penalty experiments EX Treelet Phrasal Diff (T-P) Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010 Req Log WMT 2009 WMT 2010",
    "acronyms": [
      [
        155,
        158
      ],
      [
        67,
        71
      ],
      [
        130,
        132
      ],
      [
        177,
        180
      ],
      [
        194,
        197
      ],
      [
        203,
        206
      ],
      [
        220,
        223
      ],
      [
        229,
        232
      ]
    ],
    "long-forms": [
      [
        133,
        148
      ]
    ],
    "ID": "48"
  },
  {
    "text": "Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs",
    "acronyms": [
      [
        84,
        86
      ],
      [
        49,
        51
      ],
      [
        130,
        132
      ]
    ],
    "long-forms": [
      [
        65,
        82
      ],
      [
        32,
        48
      ],
      [
        100,
        129
      ]
    ],
    "ID": "49"
  },
  {
    "text": "mented in song sentiment classification, i.e. audiobased (AB) approach, knowledge-based (KB) approach and machine learning (ML) approach, in  which the latter two approaches are also referred to ",
    "acronyms": [
      [
        124,
        126
      ],
      [
        58,
        60
      ],
      [
        89,
        91
      ]
    ],
    "long-forms": [
      [
        106,
        122
      ],
      [
        46,
        56
      ],
      [
        72,
        87
      ]
    ],
    "ID": "50"
  },
  {
    "text": "standard measures such as AUC values. For each positive+unlabeled (PU) corpus used in our evaluation we randomly selected x% of the positive ex-",
    "acronyms": [
      [
        67,
        69
      ],
      [
        26,
        29
      ]
    ],
    "long-forms": [
      [
        47,
        65
      ]
    ],
    "ID": "51"
  },
  {
    "text": "3 Experimental Results and Discussion We conducted closed track experiments on the Hong Kong City University (CityU) corpus in The Second International Chinese Word Segmen-",
    "acronyms": [
      [
        110,
        115
      ]
    ],
    "long-forms": [
      [
        93,
        108
      ]
    ],
    "ID": "52"
  },
  {
    "text": "related to the segments bi and bj .  We obtain Dbest = argmaxD P (D|B) taking into all the combination of these probabilities.",
    "acronyms": [
      [
        66,
        69
      ]
    ],
    "long-forms": [
      [
        47,
        52
      ]
    ],
    "ID": "53"
  },
  {
    "text": "DOC system, and they observe a close correspondence.  We have employed Functional Grammar (FG) (c.f \\[6\\])  as a principal analysis tool to developing representations ",
    "acronyms": [
      [
        91,
        93
      ],
      [
        0,
        3
      ]
    ],
    "long-forms": [
      [
        71,
        89
      ]
    ],
    "ID": "54"
  },
  {
    "text": "to generate these features from the training data.  In the NE (named entities) feature ? PERSON?",
    "acronyms": [
      [
        59,
        61
      ],
      [
        89,
        96
      ]
    ],
    "long-forms": [
      [
        63,
        77
      ]
    ],
    "ID": "55"
  },
  {
    "text": "WSD, which rely on knowledge represented as  attribute-value vectors: C4.5 (decision-trees),  Naive Bayes and Support Vector Machine (SVM)1. ",
    "acronyms": [
      [
        134,
        137
      ],
      [
        0,
        3
      ]
    ],
    "long-forms": [
      [
        110,
        132
      ]
    ],
    "ID": "56"
  },
  {
    "text": "Intensional Allan Ramsay University of Manchester (UK) email: allan.ramsay@manchester.ac.uk",
    "acronyms": [
      [
        51,
        53
      ]
    ],
    "long-forms": [
      [
        25,
        49
      ]
    ],
    "ID": "57"
  },
  {
    "text": " 6 BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire;",
    "acronyms": [
      [
        3,
        5
      ],
      [
        33,
        35
      ],
      [
        54,
        57
      ],
      [
        93,
        95
      ]
    ],
    "long-forms": [
      [
        8,
        31
      ],
      [
        38,
        52
      ],
      [
        60,
        91
      ],
      [
        98,
        106
      ]
    ],
    "ID": "58"
  },
  {
    "text": "3.1 Overview   Graph-based Representation  Attribute Relation Graph (ARG) (Tsai and Fu, 1979)  is used to represent information in our approach.",
    "acronyms": [
      [
        69,
        72
      ]
    ],
    "long-forms": [
      [
        43,
        67
      ]
    ],
    "ID": "59"
  },
  {
    "text": " Added to the usual space of local permutations defined by a low distortion limit (DL), this results in a linguistically informed definition of the search space",
    "acronyms": [
      [
        83,
        85
      ]
    ],
    "long-forms": [
      [
        65,
        81
      ]
    ],
    "ID": "60"
  },
  {
    "text": " 3.2 HL-MRFs for Tweet Stance Classification Finding the maximum a posteriori (MAP) state is a difficult discrete optimization problem and, in gen-",
    "acronyms": [
      [
        79,
        82
      ],
      [
        5,
        11
      ]
    ],
    "long-forms": [
      [
        57,
        77
      ]
    ],
    "ID": "61"
  },
  {
    "text": "1 Introduction Part-Of-Speech(POS) tagging is the essential basis of Natural language processing(NLP). It is the pro-",
    "acronyms": [
      [
        97,
        100
      ],
      [
        30,
        33
      ]
    ],
    "long-forms": [
      [
        69,
        95
      ],
      [
        15,
        29
      ]
    ],
    "ID": "62"
  },
  {
    "text": "We therefore use a different type of filter in order to detect these errors, which we call the Verb Arity Sampling Test (VAST). ",
    "acronyms": [
      [
        121,
        125
      ]
    ],
    "long-forms": [
      [
        95,
        119
      ]
    ],
    "ID": "63"
  },
  {
    "text": "that is as close as possible to the gold standard C. Most work on verb clustering has used the Fmeasure or the Rand Index (RI) (Rand, 1971) for evaluation, which rely on counting pairwise",
    "acronyms": [
      [
        123,
        125
      ]
    ],
    "long-forms": [
      [
        111,
        121
      ]
    ],
    "ID": "64"
  },
  {
    "text": "and test splits, and bias the results.  We trained a Support Vector Machine (SVM) for regression with RBF kernel using scikit-learn (Pe-",
    "acronyms": [
      [
        77,
        80
      ],
      [
        102,
        105
      ]
    ],
    "long-forms": [
      [
        53,
        75
      ]
    ],
    "ID": "65"
  },
  {
    "text": "Output (0 < x < 1)  Figure 3  Neural network architecture (DA = descriptor array of 20 items). ",
    "acronyms": [
      [
        59,
        61
      ]
    ],
    "long-forms": [
      [
        64,
        80
      ]
    ],
    "ID": "66"
  },
  {
    "text": "Extraction (from now on AVE) is performed with two approaches: a) Rule-based approaches apply Regular Expressions (RE) to map the words realizing a concept into a normalized value.",
    "acronyms": [
      [
        115,
        117
      ],
      [
        24,
        27
      ]
    ],
    "long-forms": [
      [
        94,
        113
      ]
    ],
    "ID": "67"
  },
  {
    "text": "to-fine n-Best Parsing and MaxEnt Discriminative Reranking Proceedings of the 43th Annual Meeting on Association for Computational Linguistics(ACL) 173?180.",
    "acronyms": [
      [
        143,
        146
      ]
    ],
    "long-forms": [
      [
        101,
        141
      ]
    ],
    "ID": "68"
  },
  {
    "text": "We annotate  the semantic roles for the 50 most frequent verbs in  the Quranic Arabic Dependency Treebank (QATB)  (Dukes and Buckwalter 2010).",
    "acronyms": [
      [
        107,
        111
      ]
    ],
    "long-forms": [
      [
        71,
        105
      ]
    ],
    "ID": "69"
  },
  {
    "text": "We parse each corpus sentence pair using the OpenCCG parser to yield a logical form (LF) as a semantic dependency graph with the gold-standard alignments projected",
    "acronyms": [
      [
        85,
        87
      ],
      [
        45,
        52
      ]
    ],
    "long-forms": [
      [
        71,
        83
      ]
    ],
    "ID": "70"
  },
  {
    "text": "WDS 0.931 [0.905, 0.958] 0.813 [0.738, 0.887] 0.872 R=Random; LR=LexRank; DR=DivRank; DR(p)=DivRank with Priors; CLR=C-LexRank; WDS=Word Distributional Similarity; C.I.=Confidence Interval",
    "acronyms": [
      [
        128,
        131
      ],
      [
        0,
        3
      ],
      [
        62,
        64
      ],
      [
        74,
        76
      ],
      [
        86,
        91
      ],
      [
        113,
        116
      ],
      [
        164,
        167
      ]
    ],
    "long-forms": [
      [
        132,
        162
      ],
      [
        54,
        60
      ],
      [
        65,
        72
      ],
      [
        77,
        84
      ],
      [
        92,
        111
      ],
      [
        117,
        126
      ],
      [
        169,
        188
      ]
    ],
    "ID": "71"
  },
  {
    "text": "(PART+), (e.g., +\u000b s+ will [future]). Most shallow is the class of conjunctions (CONJ+), (e.g., +  w+",
    "acronyms": [
      [
        81,
        86
      ],
      [
        1,
        6
      ]
    ],
    "long-forms": [
      [
        67,
        79
      ]
    ],
    "ID": "72"
  },
  {
    "text": "1 In t roduct ion   This paper deals with the discovery, representation,  and use of lexical rules (LRs) in the process of large-  scale semi-automatic computational lexicon acqui- ",
    "acronyms": [
      [
        100,
        103
      ]
    ],
    "long-forms": [
      [
        85,
        98
      ]
    ],
    "ID": "73"
  },
  {
    "text": ").  Rand Index (RI) (Rand, 1971) measures the percentage of decisions that are correct, penalizing false pos-",
    "acronyms": [
      [
        16,
        18
      ]
    ],
    "long-forms": [
      [
        4,
        14
      ]
    ],
    "ID": "74"
  },
  {
    "text": "uew stngc in the development toward a grammar and  style chc~ker can emerge: the organization of an  algorithmic on(foiled grammar (ALCOGRAM). The ",
    "acronyms": [
      [
        132,
        140
      ]
    ],
    "long-forms": [
      [
        101,
        130
      ]
    ],
    "ID": "75"
  },
  {
    "text": "Since many responses in ETLA are expected to  follow certain patterns, it is intuitive to construct  limited regular expressions (RegEx) to match gold  standard responses for candidates with high profi-",
    "acronyms": [
      [
        130,
        135
      ],
      [
        24,
        28
      ]
    ],
    "long-forms": [
      [
        109,
        128
      ]
    ],
    "ID": "76"
  },
  {
    "text": "advP Adverb phrase(ADVP)  punct Punctuation(,)  adjP Adjective phrase(ADJP)  OP  advP, np and/or pp ",
    "acronyms": [
      [
        70,
        74
      ],
      [
        19,
        23
      ],
      [
        48,
        52
      ],
      [
        77,
        79
      ],
      [
        81,
        85
      ],
      [
        97,
        99
      ],
      [
        87,
        89
      ]
    ],
    "long-forms": [
      [
        53,
        68
      ],
      [
        5,
        18
      ]
    ],
    "ID": "77"
  },
  {
    "text": "composition algorithms have been proposed in the meantime. Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version",
    "acronyms": [
      [
        90,
        93
      ]
    ],
    "long-forms": [
      [
        65,
        88
      ]
    ],
    "ID": "78"
  },
  {
    "text": "tion probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). ",
    "acronyms": [
      [
        149,
        151
      ]
    ],
    "long-forms": [
      [
        133,
        147
      ]
    ],
    "ID": "79"
  },
  {
    "text": "AST = Adjectival Sta~ VST = Verb Stem  DET = Determine~ N-FLEX = Nominal Inflexion  NST = Noun Stem V-FLEX = Verbal Inflsxion  PRN = Pronoun A-FLEX = Adjectival Inflexion ",
    "acronyms": [
      [
        84,
        87
      ],
      [
        0,
        3
      ],
      [
        22,
        25
      ],
      [
        39,
        42
      ],
      [
        56,
        62
      ],
      [
        100,
        106
      ],
      [
        127,
        130
      ],
      [
        141,
        147
      ]
    ],
    "long-forms": [
      [
        90,
        99
      ],
      [
        6,
        20
      ],
      [
        28,
        37
      ],
      [
        45,
        54
      ],
      [
        65,
        82
      ],
      [
        109,
        125
      ],
      [
        133,
        140
      ],
      [
        150,
        170
      ]
    ],
    "ID": "80"
  },
  {
    "text": " 1 Introduction The use of Support Vector Machines (SVMs) in supervised learning frameworks is spreading",
    "acronyms": [
      [
        52,
        56
      ]
    ],
    "long-forms": [
      [
        27,
        50
      ]
    ],
    "ID": "81"
  },
  {
    "text": "We have tested our algorithms on both the handcoded tag set used in (Chen et al, 1999) and supertags extracted for Penn Treebank(PTB). On the",
    "acronyms": [
      [
        129,
        132
      ]
    ],
    "long-forms": [
      [
        115,
        128
      ]
    ],
    "ID": "82"
  },
  {
    "text": "non-standard token?s formation process.  Machine translation (MT) is another commonly chosen method for text normalization.",
    "acronyms": [
      [
        62,
        64
      ]
    ],
    "long-forms": [
      [
        41,
        60
      ]
    ],
    "ID": "83"
  },
  {
    "text": "y?i / _ + [+ANY] Features: VWL = vowel ANY = any char.",
    "acronyms": [
      [
        27,
        30
      ],
      [
        39,
        42
      ]
    ],
    "long-forms": [
      [
        33,
        38
      ],
      [
        45,
        53
      ]
    ],
    "ID": "84"
  },
  {
    "text": "The research systems were:  ? CANDIDE (IBM Research: French - English(FE)),  produced both FA and human-assisted (HA) outputs.",
    "acronyms": [
      [
        70,
        72
      ],
      [
        39,
        42
      ],
      [
        91,
        93
      ],
      [
        114,
        116
      ],
      [
        30,
        37
      ]
    ],
    "long-forms": [
      [
        53,
        68
      ],
      [
        98,
        112
      ]
    ],
    "ID": "85"
  },
  {
    "text": "An Organization for a Dictionary of Word Senses  3. A \"Sense Data Item\" (SDI) represents one distinct sense common to a set of wordq  and/or phrases.",
    "acronyms": [
      [
        73,
        76
      ]
    ],
    "long-forms": [
      [
        55,
        70
      ]
    ],
    "ID": "86"
  },
  {
    "text": "tion (Briscoe, 1994), more recently using rule-based filters (White and Rajkumar, 2008) in a combinatory categorial grammar (CCG). Our focus is specifically",
    "acronyms": [
      [
        125,
        128
      ]
    ],
    "long-forms": [
      [
        93,
        123
      ]
    ],
    "ID": "87"
  },
  {
    "text": "we annotate ? Chen? as the Agent (ARG0)  of the full predicate ?[",
    "acronyms": [
      [
        34,
        38
      ]
    ],
    "long-forms": [
      [
        20,
        32
      ]
    ],
    "ID": "88"
  },
  {
    "text": "Table 3: The configurations of our systems. The abbreviations in the last column mean  training set(TS) and validating set(VS) explaining in section 5.1. ",
    "acronyms": [
      [
        100,
        102
      ],
      [
        123,
        125
      ]
    ],
    "long-forms": [
      [
        87,
        99
      ],
      [
        108,
        122
      ]
    ],
    "ID": "89"
  },
  {
    "text": "  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1804?1809, October 25-29, 2014, Doha, Qatar.",
    "acronyms": [
      [
        90,
        95
      ]
    ],
    "long-forms": [
      [
        40,
        88
      ]
    ],
    "ID": "90"
  },
  {
    "text": "1 Introduction Many text understanding applications, such as Question Answering (QA) and Information Extraction (IE), need to infer a target textual mean-",
    "acronyms": [
      [
        81,
        83
      ],
      [
        113,
        115
      ]
    ],
    "long-forms": [
      [
        61,
        79
      ],
      [
        89,
        111
      ]
    ],
    "ID": "91"
  },
  {
    "text": "Given query Q = (q1, ? ? ? , qL), for each document D, expected term frequencies (ETF) of all sub-strings Q[i,j] = (qi, ? ? ? ,",
    "acronyms": [
      [
        82,
        85
      ]
    ],
    "long-forms": [
      [
        55,
        80
      ]
    ],
    "ID": "92"
  },
  {
    "text": "whole of MARY = KMary>.  referent of MARY = Mary. ",
    "acronyms": [
      [
        37,
        41
      ],
      [
        9,
        13
      ]
    ],
    "long-forms": [
      [
        44,
        48
      ],
      [
        17,
        21
      ]
    ],
    "ID": "93"
  },
  {
    "text": "Abbreviations: Trig./Arg./Group./Modif.=event trigger detection/argument detection/argument grouping/modification detection, BI=Bioinformatician, NLP=Natural Language Processing researcher, CS=Computer scientist, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snowball=Snowball stemmer McCCJ=McClosky-Charniak-Johnson parser, LGP=Link Grammar Parser, SD=Stanford De-",
    "acronyms": [
      [
        190,
        192
      ],
      [
        353,
        355
      ],
      [
        328,
        331
      ],
      [
        288,
        293
      ],
      [
        213,
        220
      ],
      [
        230,
        237
      ]
    ],
    "long-forms": [
      [
        193,
        211
      ],
      [
        356,
        364
      ],
      [
        332,
        351
      ],
      [
        294,
        319
      ]
    ],
    "ID": "94"
  },
  {
    "text": "data are used to train a classifier, using any cost-sensitive classification (CSC) method (line 15). New pi",
    "acronyms": [
      [
        78,
        81
      ]
    ],
    "long-forms": [
      [
        47,
        76
      ]
    ],
    "ID": "95"
  },
  {
    "text": "Learning Summary Content Units with Topic Modeling Leonhard Hennig Ernesto William De Luca Distributed Artificial Intelligence Laboratory (DAI-Lab) Technische Universita?t Berlin",
    "acronyms": [
      [
        139,
        146
      ]
    ],
    "long-forms": [
      [
        91,
        137
      ]
    ],
    "ID": "96"
  },
  {
    "text": "analysis as discussed in the next section.  Analysis of Variance (ANOVA) tests were performed on the full 5 years for each sector, to com-",
    "acronyms": [
      [
        66,
        71
      ]
    ],
    "long-forms": [
      [
        44,
        64
      ]
    ],
    "ID": "97"
  },
  {
    "text": " Connectivity Strength Features provide two scores, Source Connectivity Strength (SCS) and Target Connectivity Strength (TCS).",
    "acronyms": [
      [
        82,
        85
      ],
      [
        121,
        124
      ]
    ],
    "long-forms": [
      [
        52,
        80
      ],
      [
        91,
        119
      ]
    ],
    "ID": "98"
  },
  {
    "text": "Since finding the optimal set of edges respecting transitivity is NP-hard, they employed Integer Linear Programming (ILP) to find the exact solution.",
    "acronyms": [
      [
        117,
        120
      ]
    ],
    "long-forms": [
      [
        89,
        115
      ]
    ],
    "ID": "99"
  },
  {
    "text": "The e-rater system TM ~ is an operational  automated essay scoring system, developed  at Educational Testing Service (ETS). The ",
    "acronyms": [
      [
        118,
        121
      ],
      [
        19,
        21
      ]
    ],
    "long-forms": [
      [
        89,
        116
      ]
    ],
    "ID": "100"
  },
  {
    "text": "mensionality reduction, such as Latent Semantic Analysis (LSA) in (Pad?o and Lapata, 2007) or Non-negative Matrix Factorization (NMF) (Zheng et al.,",
    "acronyms": [
      [
        129,
        132
      ],
      [
        58,
        61
      ]
    ],
    "long-forms": [
      [
        98,
        127
      ],
      [
        32,
        56
      ]
    ],
    "ID": "101"
  },
  {
    "text": "of the adjacency pair involving speaker B, we use four categories of features: structural, durational, lexical, and dialog act (DA) information. For the",
    "acronyms": [
      [
        128,
        130
      ]
    ],
    "long-forms": [
      [
        116,
        126
      ]
    ],
    "ID": "102"
  },
  {
    "text": " 3 Latent Semantic Analysis Latent Semantic Analysis (LSA) (Deerwester et al 1990) is a widely used continuous vector space",
    "acronyms": [
      [
        54,
        57
      ]
    ],
    "long-forms": [
      [
        28,
        52
      ]
    ],
    "ID": "103"
  },
  {
    "text": "2. Corpus Resource  This study uses the Switchboard Dialog Act (SWBD-DA)  Corpus as the corpus resource, which is available online ",
    "acronyms": [
      [
        64,
        71
      ]
    ],
    "long-forms": [
      [
        40,
        62
      ]
    ],
    "ID": "104"
  },
  {
    "text": "reflecting the distribution of the genres in the MTC (Zeyrek et al 2009). The main objective of the project is to annotate discourse connectives with their two arguments, modifiers and supplementary text spans. Following the Penn Discourse Tree Bank (PDTB), we take discourse connectives as discourse-level predicates taking two (and only  two) arguments, called Arg1 and Arg2, which may span one or more clauses and sentences that are adjacent or nonadjacent to the connective (Prasad et al 2007, Webber, 2004).",
    "acronyms": [
      [
        251,
        255
      ],
      [
        49,
        52
      ]
    ],
    "long-forms": [
      [
        225,
        249
      ]
    ],
    "ID": "105"
  },
  {
    "text": "data. The first is a parser trained on the standard training sections of the PennTreebank (PTB) and the second is a parser trained on the training por-",
    "acronyms": [
      [
        91,
        94
      ]
    ],
    "long-forms": [
      [
        77,
        89
      ]
    ],
    "ID": "106"
  },
  {
    "text": "query large amounts of data is a key requirement for data-driven dialog systems, in which the data is generated by the spoken dialog system (SDS) components (spoken language understanding (SLU), di-",
    "acronyms": [
      [
        141,
        144
      ],
      [
        189,
        192
      ]
    ],
    "long-forms": [
      [
        119,
        139
      ],
      [
        158,
        187
      ]
    ],
    "ID": "107"
  },
  {
    "text": "Ah receptor recognizes the B cell transcription factor, BSAP (b) Grf40 binds to linker for activation of T cells (LAT) (c)",
    "acronyms": [
      [
        114,
        117
      ],
      [
        56,
        60
      ]
    ],
    "long-forms": [
      [
        80,
        106
      ]
    ],
    "ID": "108"
  },
  {
    "text": "pendencies. Hochreiter and Schmidhuber (1997), thus proposed long short term memory (LSTMs), a variant of recurrent neural networks.",
    "acronyms": [
      [
        85,
        90
      ]
    ],
    "long-forms": [
      [
        61,
        83
      ]
    ],
    "ID": "109"
  },
  {
    "text": "the affect projection rules are useful. However, when we use automated coreference (ACoref), recall goes down and precision goes up.",
    "acronyms": [
      [
        84,
        90
      ]
    ],
    "long-forms": [
      [
        61,
        82
      ]
    ],
    "ID": "110"
  },
  {
    "text": "tion) strategies. All systems are evaluated according to their Mean Average Precision 6 (MAP) as computed by the trec eval software on the pre-",
    "acronyms": [
      [
        89,
        92
      ]
    ],
    "long-forms": [
      [
        63,
        85
      ]
    ],
    "ID": "111"
  },
  {
    "text": "HN=highly negative terms, N=negative, P=positive, HP=highly positive, INV=invertors, DIM=diminishers, INV=invertors. ",
    "acronyms": [
      [
        85,
        88
      ],
      [
        0,
        2
      ],
      [
        50,
        52
      ],
      [
        70,
        73
      ],
      [
        102,
        105
      ]
    ],
    "long-forms": [
      [
        89,
        100
      ],
      [
        3,
        18
      ],
      [
        28,
        36
      ],
      [
        40,
        48
      ],
      [
        53,
        68
      ],
      [
        74,
        83
      ],
      [
        106,
        115
      ]
    ],
    "ID": "112"
  },
  {
    "text": "4.1 Test Dataset The dataset used in our experiments comes from the Automated Student Assessment Prize (ASAP)1, which is sponsored by the William and Flora",
    "acronyms": [
      [
        104,
        108
      ]
    ],
    "long-forms": [
      [
        68,
        102
      ]
    ],
    "ID": "113"
  },
  {
    "text": " These are typically expressed in simple syntactic frames called subcategorization frames (SCFs). ",
    "acronyms": [
      [
        91,
        95
      ]
    ],
    "long-forms": [
      [
        65,
        89
      ]
    ],
    "ID": "114"
  },
  {
    "text": " 1 Introduction Assigning each word its most frequent sense (MFS) is commonly used as a baseline in Word Sense Dis-",
    "acronyms": [
      [
        61,
        64
      ]
    ],
    "long-forms": [
      [
        40,
        59
      ]
    ],
    "ID": "115"
  },
  {
    "text": "of CoNLL-2000 and LLL-2000, Lisbon, Portugal.  Tageszeitung (TAZ) Corpus. Contrapress Media GmbH.",
    "acronyms": [
      [
        61,
        64
      ],
      [
        3,
        13
      ],
      [
        18,
        26
      ],
      [
        92,
        96
      ]
    ],
    "long-forms": [
      [
        47,
        59
      ]
    ],
    "ID": "116"
  },
  {
    "text": "wisdom of crowds. Our approach is based on the Latent Mixture of Discriminative Experts (LMDE) model originally introduced for multimodal fu-",
    "acronyms": [
      [
        89,
        93
      ]
    ],
    "long-forms": [
      [
        47,
        87
      ]
    ],
    "ID": "117"
  },
  {
    "text": "In Proceedings of the 22nd International Conference on World Wide Web (WWW), pp. 1009-1020, 2013.",
    "acronyms": [
      [
        71,
        74
      ]
    ],
    "long-forms": [
      [
        55,
        69
      ]
    ],
    "ID": "118"
  },
  {
    "text": "When the morphophonology returns multiple parse candidates, the system employs an N -gram language model (LM) 21",
    "acronyms": [
      [
        106,
        108
      ],
      [
        82,
        89
      ]
    ],
    "long-forms": [
      [
        90,
        104
      ]
    ],
    "ID": "119"
  },
  {
    "text": "fore and after the SVD improved results slightly.  For the accumulated tag counts (ACT) we annotate the data with our baseline model and extract word-",
    "acronyms": [
      [
        83,
        86
      ]
    ],
    "long-forms": [
      [
        59,
        81
      ]
    ],
    "ID": "120"
  },
  {
    "text": "cal markedness constraints directly from the distribution of input-output mappings in an Optimality Theory (OT) setting. The",
    "acronyms": [
      [
        108,
        110
      ]
    ],
    "long-forms": [
      [
        89,
        106
      ]
    ],
    "ID": "121"
  },
  {
    "text": "sense as a group of similar contexts of target word.  The context group discrimination (CGD) algorithm presented in (Schu?tze, 1998) adopted this strategy.",
    "acronyms": [
      [
        88,
        91
      ]
    ],
    "long-forms": [
      [
        58,
        86
      ]
    ],
    "ID": "122"
  },
  {
    "text": "on these intuitions, we define the following features: Document frequency of constituents (DF): We use the document frequency of a constituent as",
    "acronyms": [
      [
        91,
        93
      ]
    ],
    "long-forms": [
      [
        55,
        73
      ]
    ],
    "ID": "123"
  },
  {
    "text": "  To address the issues in transliteration, we  propose a direct orthographic mapping (DOM)  framework through a joint source-channel model ",
    "acronyms": [
      [
        87,
        90
      ]
    ],
    "long-forms": [
      [
        58,
        85
      ]
    ],
    "ID": "124"
  },
  {
    "text": "parsing accuracy will be reported using the standard metrics of labeled attachment score (LAS) and unlabeled attachment score (UAS).13 Statistical significance is checked using Dan Bikel?s",
    "acronyms": [
      [
        127,
        130
      ],
      [
        90,
        93
      ]
    ],
    "long-forms": [
      [
        99,
        125
      ],
      [
        64,
        88
      ]
    ],
    "ID": "125"
  },
  {
    "text": "veloped for machine learning and data mining, to  determine the data classification performance of  support vector machine (SVM) learning on the                                                   ",
    "acronyms": [
      [
        124,
        127
      ]
    ],
    "long-forms": [
      [
        100,
        122
      ]
    ],
    "ID": "126"
  },
  {
    "text": "the work of language specialists. They often need to perform extensive corpus research, including Natural Language Processing (NLP), statistical modelling and data visualisation. Our ",
    "acronyms": [
      [
        127,
        130
      ]
    ],
    "long-forms": [
      [
        98,
        125
      ]
    ],
    "ID": "127"
  },
  {
    "text": "are shown in Table 4. Not surprisingly, using all gazetteer features (AllG) boosts the F1 score from 85.14 % to 88.30%, confirming the power",
    "acronyms": [
      [
        70,
        74
      ]
    ],
    "long-forms": [
      [
        46,
        59
      ]
    ],
    "ID": "128"
  },
  {
    "text": " A simple approach is presented in (Cardie and  Pierce, 1998) called Treebank Apl)roach (TA). This ",
    "acronyms": [
      [
        89,
        91
      ]
    ],
    "long-forms": [
      [
        69,
        87
      ]
    ],
    "ID": "129"
  },
  {
    "text": "respectively. Hacioglu et al (2004) showed that  tagging phrase by phrase (P-by-P) is better than  word by word (W-by-W).",
    "acronyms": [
      [
        75,
        81
      ],
      [
        113,
        119
      ]
    ],
    "long-forms": [
      [
        57,
        73
      ],
      [
        99,
        111
      ]
    ],
    "ID": "130"
  },
  {
    "text": " Since precision is measured as the proportion of true positives (TP) to the sum of true positives and false positives (FP): 1748",
    "acronyms": [
      [
        120,
        122
      ],
      [
        66,
        68
      ]
    ],
    "long-forms": [
      [
        103,
        118
      ],
      [
        50,
        64
      ]
    ],
    "ID": "131"
  },
  {
    "text": "tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, Massachusetts, USA.",
    "acronyms": [
      [
        89,
        93
      ]
    ],
    "long-forms": [
      [
        48,
        87
      ]
    ],
    "ID": "132"
  },
  {
    "text": "sponsored by the U.S. government. ACE 2004  defined 7 major entity types: PER (Person), ORG  (Organization), FAC (Facility), GPE (Geo-Political ",
    "acronyms": [
      [
        74,
        77
      ],
      [
        17,
        20
      ],
      [
        34,
        37
      ],
      [
        88,
        91
      ],
      [
        109,
        112
      ],
      [
        125,
        128
      ]
    ],
    "long-forms": [
      [
        79,
        85
      ],
      [
        94,
        106
      ],
      [
        114,
        122
      ],
      [
        130,
        143
      ]
    ],
    "ID": "133"
  },
  {
    "text": "Semantic Role Labeling (SRL) has been used successfully in several stages of automated Question Answering (QA) systems but its inherent slow procedures make it",
    "acronyms": [
      [
        107,
        109
      ],
      [
        24,
        27
      ]
    ],
    "long-forms": [
      [
        87,
        105
      ],
      [
        0,
        22
      ]
    ],
    "ID": "134"
  },
  {
    "text": " 1 Introduction Named Entity Recognition (NER) is usually solved by a supervised learning approach, where",
    "acronyms": [
      [
        42,
        45
      ]
    ],
    "long-forms": [
      [
        16,
        40
      ]
    ],
    "ID": "135"
  },
  {
    "text": "+Valency = Adding valency filtering to the setting in the preceding row. SUC = Subset of Stockholm-Umea? ",
    "acronyms": [
      [
        73,
        76
      ]
    ],
    "long-forms": [
      [
        79,
        98
      ]
    ],
    "ID": "136"
  },
  {
    "text": "tying the feature parameters. In particular, we perform maximum entropy (MaxEnt) estimation over the conditional distribution using second-order gra-",
    "acronyms": [
      [
        73,
        79
      ]
    ],
    "long-forms": [
      [
        56,
        71
      ]
    ],
    "ID": "137"
  },
  {
    "text": "due to French proper names, which are left untranslated in the English parallel text.  15 CLEF = Cross Language Evaluation Forum, ? www.clef-campaign.org?.",
    "acronyms": [
      [
        90,
        94
      ]
    ],
    "long-forms": [
      [
        97,
        128
      ]
    ],
    "ID": "138"
  },
  {
    "text": "For our simulations, we built two subcorpora by filtering out entity annotations: the PENNBIOIE gene corpus (PBgene), including the three gene entity subtypes generic, protein, and rna,",
    "acronyms": [
      [
        109,
        115
      ]
    ],
    "long-forms": [
      [
        86,
        100
      ]
    ],
    "ID": "139"
  },
  {
    "text": "Our participation in the STS task is inspired by previous work on paraphrase recognition, in which machine translation (MT) evaluation metrics are used to identify whether a pair of sentences are",
    "acronyms": [
      [
        120,
        122
      ],
      [
        25,
        28
      ]
    ],
    "long-forms": [
      [
        99,
        118
      ]
    ],
    "ID": "140"
  },
  {
    "text": "Efficient Algorithms for Parsing the DOP  Model\", Proceedings Empirical Methods in Natural Language  Processing, Philadelphia (PA),  J. Goodman, 1998.",
    "acronyms": [
      [
        127,
        129
      ],
      [
        37,
        40
      ]
    ],
    "long-forms": [
      [
        113,
        125
      ]
    ],
    "ID": "141"
  },
  {
    "text": "case.  We choose Support Vector Machines (SVMs) as our learning algorithm for their widely acclaimed",
    "acronyms": [
      [
        42,
        46
      ]
    ],
    "long-forms": [
      [
        17,
        40
      ]
    ],
    "ID": "142"
  },
  {
    "text": "clue words characteristic to the nine relation types.  In order to measure the semantic relatedness (SR) of targets and clues, we used the Explicit Seman-",
    "acronyms": [
      [
        101,
        103
      ]
    ],
    "long-forms": [
      [
        79,
        99
      ]
    ],
    "ID": "143"
  },
  {
    "text": " Consider, for example, the tags DT (determiner) and NN (noun), and the four possible ordered tagpairs.",
    "acronyms": [
      [
        53,
        55
      ],
      [
        33,
        35
      ]
    ],
    "long-forms": [
      [
        57,
        61
      ],
      [
        37,
        47
      ]
    ],
    "ID": "144"
  },
  {
    "text": "Measures for Semantic Relations Extraction Alexander Panchenko Center for Natural Language Processing (CENTAL) Universite?",
    "acronyms": [
      [
        103,
        109
      ]
    ],
    "long-forms": [
      [
        63,
        90
      ]
    ],
    "ID": "145"
  },
  {
    "text": "tilogue, but rather a number of parallel dialogues.  The Mission Rehearsal Exercise (MRE) Project (Traum and Rickel, 2002), one of the largest multilogue",
    "acronyms": [
      [
        85,
        88
      ]
    ],
    "long-forms": [
      [
        57,
        83
      ]
    ],
    "ID": "146"
  },
  {
    "text": "tween anaphor and antecedent, the feature ddist captures the distance in sentences, the feature mdist the number of markables (NPs) between anaphor and antecedent.",
    "acronyms": [
      [
        127,
        130
      ]
    ],
    "long-forms": [
      [
        106,
        125
      ]
    ],
    "ID": "147"
  },
  {
    "text": "6 , Guardian Weekly and its manually simplified versions for language learners (Allen, 2009), and the FIRST corpus of various texts simplified for people with autism spectrum disorder (ASD) 7",
    "acronyms": [
      [
        185,
        188
      ],
      [
        102,
        107
      ]
    ],
    "long-forms": [
      [
        159,
        183
      ]
    ],
    "ID": "148"
  },
  {
    "text": "that word in a sentence. Another covering  grammar is the so called null grammar (NG), in  which a word can follow any other word.",
    "acronyms": [
      [
        82,
        84
      ]
    ],
    "long-forms": [
      [
        68,
        80
      ]
    ],
    "ID": "149"
  },
  {
    "text": "Table 1: Categorization of suitability-labels.  3.1.2 Beneficial (BENEF) While SUIT only states that the consumption of",
    "acronyms": [
      [
        66,
        71
      ]
    ],
    "long-forms": [
      [
        54,
        64
      ]
    ],
    "ID": "150"
  },
  {
    "text": " The  noun phrases we ex-  tract start with an optional determiner (DT)  or  possessive pronoun (PRP$) ,  followed by  a se-  quence of cardinal numbers  (CDs),  adjectives ",
    "acronyms": [
      [
        97,
        101
      ],
      [
        68,
        70
      ],
      [
        155,
        158
      ]
    ],
    "long-forms": [
      [
        77,
        95
      ],
      [
        56,
        66
      ],
      [
        136,
        144
      ]
    ],
    "ID": "151"
  },
  {
    "text": "In Proceedings of the 24th International Conference on Computational Linguistics (COLING),Mumbai, India.",
    "acronyms": [
      [
        82,
        88
      ]
    ],
    "long-forms": [
      [
        55,
        80
      ]
    ],
    "ID": "152"
  },
  {
    "text": "To train models using this information we use 870 generalized expectation (GE) criteria. GE criteria",
    "acronyms": [
      [
        75,
        77
      ],
      [
        89,
        91
      ]
    ],
    "long-forms": [
      [
        50,
        73
      ]
    ],
    "ID": "153"
  },
  {
    "text": " Quasi-valency complementations: \u0000 DIFF (difference): The number has swollen by 200. ",
    "acronyms": [
      [
        35,
        39
      ]
    ],
    "long-forms": [
      [
        41,
        51
      ]
    ],
    "ID": "154"
  },
  {
    "text": "and the other for the target phrase. We call the unit of the two RAEs the Leaf Node Network (LNN). ",
    "acronyms": [
      [
        93,
        96
      ],
      [
        65,
        69
      ]
    ],
    "long-forms": [
      [
        74,
        91
      ]
    ],
    "ID": "155"
  },
  {
    "text": " ? Implicit Attitude (IA) - n ? t dimensions are ex-",
    "acronyms": [
      [
        22,
        24
      ]
    ],
    "long-forms": [
      [
        3,
        20
      ]
    ],
    "ID": "156"
  },
  {
    "text": "mt. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING) - Volume 1, pages 1145?1152.",
    "acronyms": [
      [
        86,
        92
      ]
    ],
    "long-forms": [
      [
        59,
        84
      ]
    ],
    "ID": "157"
  },
  {
    "text": "The lattice representing a union of several confusion networks can then be directly rescored with an n-gram language model (LM). A transforma-",
    "acronyms": [
      [
        124,
        126
      ]
    ],
    "long-forms": [
      [
        108,
        122
      ]
    ],
    "ID": "158"
  },
  {
    "text": "cial type of MWEs, for which we are mainly interested in its type, rather than individual lexias, during the annotation: named entities (NE).3 Treatment of NEs together with other MWEs is important, be-",
    "acronyms": [
      [
        137,
        139
      ]
    ],
    "long-forms": [
      [
        121,
        135
      ]
    ],
    "ID": "159"
  },
  {
    "text": "Document d1 Document d2 Customization Requirement : City, County or State names within sports articles may refer to a sports team  or to the location itself. Customization Solution (CS) :Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location Organization Location",
    "acronyms": [
      [
        182,
        184
      ],
      [
        9,
        11
      ],
      [
        21,
        23
      ]
    ],
    "long-forms": [
      [
        158,
        180
      ]
    ],
    "ID": "160"
  },
  {
    "text": "There have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment.",
    "acronyms": [
      [
        147,
        149
      ]
    ],
    "long-forms": [
      [
        125,
        145
      ]
    ],
    "ID": "161"
  },
  {
    "text": "alignment problems. In Annual Meeting of the Association for Computational Linguistics (ACL), Columbus, Ohio, June.",
    "acronyms": [
      [
        88,
        91
      ]
    ],
    "long-forms": [
      [
        45,
        86
      ]
    ],
    "ID": "162"
  },
  {
    "text": "the style of Shriberg (1994) and the Switchboard corpus. FP=Filled Pause, RM=Reparandum, IM=Interregnum, RP=Repair.",
    "acronyms": [
      [
        57,
        59
      ],
      [
        74,
        76
      ],
      [
        89,
        91
      ]
    ],
    "long-forms": [
      [
        60,
        72
      ],
      [
        77,
        87
      ],
      [
        92,
        103
      ]
    ],
    "ID": "163"
  },
  {
    "text": "The final reward is calculated as follows: TaskCompletionReward(TCR) = 1000 TurnCost(TC) = 10 TotalTurnCost(TTC) = #(Turns) ?",
    "acronyms": [
      [
        85,
        87
      ],
      [
        64,
        68
      ],
      [
        108,
        111
      ]
    ],
    "long-forms": [
      [
        76,
        83
      ],
      [
        43,
        63
      ],
      [
        94,
        107
      ]
    ],
    "ID": "164"
  },
  {
    "text": "operation!  On the other hand, we did start working on machine translation (MT) in 1987. As",
    "acronyms": [
      [
        76,
        78
      ]
    ],
    "long-forms": [
      [
        55,
        74
      ]
    ],
    "ID": "165"
  },
  {
    "text": "following table shows the results of U-DOP on the WSJ40 using 10 different 90-10 splits, compared to a  supervised binarized PCFG (S-PCFG) and a supervised binarized DOP model (S-DOP) on the",
    "acronyms": [
      [
        131,
        137
      ],
      [
        37,
        42
      ],
      [
        50,
        55
      ],
      [
        177,
        182
      ]
    ],
    "long-forms": [
      [
        104,
        129
      ],
      [
        145,
        169
      ]
    ],
    "ID": "166"
  },
  {
    "text": "of the above three neural classifiers.  Recursive Autoencoder (RAE) has proven to be an effective model to compose words vectors in",
    "acronyms": [
      [
        63,
        66
      ]
    ],
    "long-forms": [
      [
        40,
        61
      ]
    ],
    "ID": "167"
  },
  {
    "text": "guishes tile outputs of these two phases by the  data types l:l.hetRep (rhetorical representation)  and DocRep (document representation). ",
    "acronyms": [
      [
        104,
        110
      ]
    ],
    "long-forms": [
      [
        112,
        135
      ]
    ],
    "ID": "168"
  },
  {
    "text": "m a t i c a l l y  related s t r u c t u r e  is termed a prpposll t ion. -  Four r e l a t i o n s  of p a r t i c i p a t i o n  are dis t ingu ished :  agent  (AGT),  ins t rumenta l  (INS), objective (ORJ), and experiencer  (EXP) .",
    "acronyms": [
      [
        163,
        166
      ],
      [
        188,
        192
      ],
      [
        205,
        208
      ],
      [
        229,
        232
      ]
    ],
    "long-forms": [
      [
        155,
        160
      ],
      [
        170,
        185
      ],
      [
        194,
        203
      ],
      [
        215,
        226
      ]
    ],
    "ID": "169"
  },
  {
    "text": "an~ (AN)  art~fact (AR)  attribute (AT)  body (BO) ",
    "acronyms": [
      [
        36,
        38
      ],
      [
        5,
        7
      ],
      [
        20,
        22
      ],
      [
        47,
        49
      ]
    ],
    "long-forms": [
      [
        25,
        34
      ],
      [
        10,
        18
      ],
      [
        41,
        45
      ],
      [
        0,
        3
      ]
    ],
    "ID": "170"
  },
  {
    "text": "respect o silence and noise words are removed from  the Nbest lists 7, next the word stream is tagged with  Brill's part of speech (POS) tagger (Brill, 1994),  Version 1.14, adapted to the SWITCHBOARD Cor- ",
    "acronyms": [
      [
        132,
        135
      ]
    ],
    "long-forms": [
      [
        116,
        130
      ]
    ],
    "ID": "171"
  },
  {
    "text": "(such as Noun, Verb, Adjective etc). De-lexicalized text representations through POS tagging were first considered for native language identification (NLI), where they were used as a proxy for syntax in order to capture certain types of grammatical errors (Wong and Dras, 2009).",
    "acronyms": [
      [
        151,
        154
      ],
      [
        81,
        84
      ]
    ],
    "long-forms": [
      [
        119,
        149
      ]
    ],
    "ID": "172"
  },
  {
    "text": " The weight selection is performed by using the  minimum error rate training (MERT) for log-linear  model parameter estimation (Och, 2003).",
    "acronyms": [
      [
        78,
        82
      ]
    ],
    "long-forms": [
      [
        49,
        76
      ]
    ],
    "ID": "173"
  },
  {
    "text": "  NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 1?4, Montre?al, Canada, June 7?8, 2012.",
    "acronyms": [
      [
        88,
        93
      ],
      [
        2,
        11
      ]
    ],
    "long-forms": [
      [
        29,
        86
      ]
    ],
    "ID": "174"
  },
  {
    "text": " In the RST framework, a text is first divided into several elementary discourse units (EDUs). Each",
    "acronyms": [
      [
        88,
        92
      ],
      [
        8,
        11
      ]
    ],
    "long-forms": [
      [
        60,
        86
      ]
    ],
    "ID": "175"
  },
  {
    "text": "2008) and a corpus annotated for the entities Finding, Substance and Body was used for training a conditional random fields (CRF) system (Wang, 2009) as well as for training an en-",
    "acronyms": [
      [
        125,
        128
      ]
    ],
    "long-forms": [
      [
        98,
        123
      ]
    ],
    "ID": "176"
  },
  {
    "text": "plex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR).",
    "acronyms": [
      [
        118,
        120
      ],
      [
        92,
        94
      ],
      [
        150,
        152
      ]
    ],
    "long-forms": [
      [
        97,
        116
      ],
      [
        72,
        90
      ],
      [
        127,
        148
      ]
    ],
    "ID": "177"
  },
  {
    "text": "ation for Computational Linguistics (NAACL) from 2011?2013, and he has served on the editorial boards of the journals Transactions of the ACL (TACL) and Computational Linguistics.",
    "acronyms": [
      [
        143,
        147
      ]
    ],
    "long-forms": [
      [
        118,
        141
      ]
    ],
    "ID": "178"
  },
  {
    "text": "Next we consider the recurrent neural network (RNN) based architecture called the Dual Encoder (DE) model (Lowe et al, 2015). The",
    "acronyms": [
      [
        96,
        98
      ],
      [
        47,
        50
      ]
    ],
    "long-forms": [
      [
        82,
        94
      ],
      [
        21,
        45
      ]
    ],
    "ID": "179"
  },
  {
    "text": "- meaningful expression (ME): Any physical act  carrying a non-contextual meaning;  - communicative act (CAct): An instance of ME  issued by a specific \"issuer\" and received by a ",
    "acronyms": [
      [
        105,
        109
      ],
      [
        25,
        27
      ],
      [
        127,
        129
      ]
    ],
    "long-forms": [
      [
        86,
        103
      ],
      [
        2,
        23
      ]
    ],
    "ID": "180"
  },
  {
    "text": " For an annotation project, text pieces from a source database (DB) are often copied in a local storage and annotations are attached to them.",
    "acronyms": [
      [
        64,
        66
      ]
    ],
    "long-forms": [
      [
        54,
        62
      ]
    ],
    "ID": "181"
  },
  {
    "text": "consist of two verbs. The first  verb is termed as Full Verb (FV) that is present  at surface level either as conjunctive participial ",
    "acronyms": [
      [
        62,
        64
      ]
    ],
    "long-forms": [
      [
        51,
        60
      ]
    ],
    "ID": "182"
  },
  {
    "text": "work of KH and FG was supported by the Academy of Finland, and of SVL by the Research Foundation Flanders (FWO). YVdP and SVL ac-",
    "acronyms": [
      [
        107,
        110
      ],
      [
        8,
        10
      ],
      [
        15,
        17
      ],
      [
        66,
        69
      ],
      [
        113,
        117
      ],
      [
        122,
        125
      ]
    ],
    "long-forms": [
      [
        86,
        96
      ]
    ],
    "ID": "183"
  },
  {
    "text": "2007. Identifying authorship by byte-level n-grams: The source code author profile (SCAP) method. Journal of Digital Evidence, 6(1).",
    "acronyms": [
      [
        84,
        88
      ]
    ],
    "long-forms": [
      [
        56,
        82
      ]
    ],
    "ID": "184"
  },
  {
    "text": " A related task was explored at the Document Understanding Conference (DUC) in 2007.5 Here the goal was to find new information with respect to a",
    "acronyms": [
      [
        71,
        74
      ]
    ],
    "long-forms": [
      [
        36,
        69
      ]
    ],
    "ID": "185"
  },
  {
    "text": "? and possibly split up ? into True Positive (TP) and False Positive (FP) entities.",
    "acronyms": [
      [
        46,
        48
      ],
      [
        70,
        72
      ]
    ],
    "long-forms": [
      [
        31,
        44
      ],
      [
        54,
        68
      ]
    ],
    "ID": "186"
  },
  {
    "text": "One possibility is the use of semantic annotation, using sentence-level propositional Logical Forms (LF). It seems more cogni-",
    "acronyms": [
      [
        101,
        103
      ]
    ],
    "long-forms": [
      [
        86,
        99
      ]
    ],
    "ID": "187"
  },
  {
    "text": "basic word-level edit operations (insertion, deletion and substitution) to transform one text into the other: Levenshtein with substitution penalty (LEV2): This feature is a variant of LEV1 in which substi-",
    "acronyms": [
      [
        149,
        153
      ],
      [
        185,
        189
      ]
    ],
    "long-forms": [
      [
        110,
        147
      ]
    ],
    "ID": "188"
  },
  {
    "text": "matrix of the IBM Model 1. However, there is a signficant out of vocabulary (OOV) issue in the model since training data is limited.",
    "acronyms": [
      [
        77,
        80
      ],
      [
        14,
        17
      ]
    ],
    "long-forms": [
      [
        58,
        75
      ]
    ],
    "ID": "189"
  },
  {
    "text": "of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP), pages 342?350, Singapore.",
    "acronyms": [
      [
        99,
        109
      ],
      [
        7,
        10
      ],
      [
        92,
        97
      ]
    ],
    "long-forms": [
      [
        23,
        84
      ]
    ],
    "ID": "190"
  },
  {
    "text": "gories Auxiliary-final VP For auxiliary verbs parsed as verb phrases (VP), this feature checks if the final element in the VP",
    "acronyms": [
      [
        70,
        72
      ],
      [
        23,
        25
      ],
      [
        123,
        125
      ]
    ],
    "long-forms": [
      [
        56,
        68
      ]
    ],
    "ID": "191"
  },
  {
    "text": "Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.",
    "acronyms": [
      [
        124,
        128
      ],
      [
        26,
        31
      ],
      [
        73,
        77
      ]
    ],
    "long-forms": [
      [
        93,
        122
      ],
      [
        0,
        24
      ],
      [
        33,
        71
      ]
    ],
    "ID": "192"
  },
  {
    "text": "2 ? Lemma, PoS, and Dependency relation (DepRel) for the node itself, the parent, and the left and right sibling",
    "acronyms": [
      [
        41,
        47
      ],
      [
        11,
        14
      ],
      [
        4,
        9
      ]
    ],
    "long-forms": [
      [
        20,
        39
      ]
    ],
    "ID": "193"
  },
  {
    "text": "erol (DAG)?). Note that the position of the long form (LF) and short form (SF) is interchangeable. To dis-",
    "acronyms": [
      [
        75,
        77
      ],
      [
        6,
        9
      ],
      [
        55,
        57
      ]
    ],
    "long-forms": [
      [
        63,
        73
      ],
      [
        44,
        53
      ]
    ],
    "ID": "194"
  },
  {
    "text": "M27 vp move past, vp pass, vp pass by, vp walk past (all AS) Actions correspond to expansions of lexemes (FV = filling status): 0=unfilled, 1=filled. ( SV = shared variables): the variables np actor (FV), relatum (FV), sentence (FV) and information need (0=low, 1=high) are shared by several subagents; the same applies to their",
    "acronyms": [
      [
        152,
        154
      ],
      [
        57,
        59
      ],
      [
        106,
        108
      ],
      [
        200,
        202
      ],
      [
        214,
        216
      ],
      [
        229,
        231
      ]
    ],
    "long-forms": [
      [
        157,
        173
      ],
      [
        111,
        125
      ]
    ],
    "ID": "195"
  },
  {
    "text": "x, y) where t(x) is the true label of the observation sequence x. We can get a quadratic program (QP) using a standard transformation to eliminate ?",
    "acronyms": [
      [
        98,
        100
      ]
    ],
    "long-forms": [
      [
        79,
        96
      ]
    ],
    "ID": "196"
  },
  {
    "text": "46 date the current hypothesis after each observation; b) Confidence Weighted (CW) learning?a probabilistic large margin online learning algorithm (Dredze et",
    "acronyms": [
      [
        79,
        81
      ]
    ],
    "long-forms": [
      [
        58,
        77
      ]
    ],
    "ID": "197"
  },
  {
    "text": "(see details below).  pronoun, PUNC = punctuation, PRT = particle, and X = residual (a category for language-specific cat-",
    "acronyms": [
      [
        31,
        35
      ],
      [
        51,
        54
      ]
    ],
    "long-forms": [
      [
        38,
        49
      ],
      [
        57,
        65
      ]
    ],
    "ID": "198"
  },
  {
    "text": "In this paper we restrict our attention to the  translation from the English-oriented level (EL)  to the domain model level (DML) since this is  where CEs are disambiguated bychoosing unam- ",
    "acronyms": [
      [
        125,
        128
      ],
      [
        93,
        95
      ],
      [
        151,
        154
      ]
    ],
    "long-forms": [
      [
        105,
        123
      ],
      [
        69,
        91
      ]
    ],
    "ID": "199"
  },
  {
    "text": "In Addition to the visualhaptic interface, iDrive includes a speech dialogue system (SDS) as well. The SDS allows the driver",
    "acronyms": [
      [
        85,
        88
      ]
    ],
    "long-forms": [
      [
        77,
        83
      ]
    ],
    "ID": "200"
  },
  {
    "text": "phrase (BADVP), base noun phrase (BNP),  73  base temporal phrase (BTN), base location  phrase (BNS), base verb phrase (BVP) and ",
    "acronyms": [
      [
        67,
        70
      ],
      [
        8,
        13
      ],
      [
        34,
        37
      ],
      [
        96,
        99
      ],
      [
        120,
        123
      ]
    ],
    "long-forms": [
      [
        45,
        65
      ],
      [
        16,
        32
      ],
      [
        102,
        118
      ],
      [
        73,
        94
      ]
    ],
    "ID": "201"
  },
  {
    "text": "7 Conclusions We introduce a Laplacian structured sparsity model for computational branding analytics (CBA). In the",
    "acronyms": [
      [
        103,
        106
      ]
    ],
    "long-forms": [
      [
        69,
        101
      ]
    ],
    "ID": "202"
  },
  {
    "text": "of modals and auxilliaries is deterministic.  Syntactic Chunk (CHUNK): This feature explicitly models the syntactic phrases in which our",
    "acronyms": [
      [
        63,
        68
      ]
    ],
    "long-forms": [
      [
        56,
        61
      ]
    ],
    "ID": "203"
  },
  {
    "text": "In order to capture the compositional effects with higher accuracy, we propose a new model called the Recursive Neural Tensor Network (RNTN). Recur-",
    "acronyms": [
      [
        135,
        139
      ]
    ],
    "long-forms": [
      [
        102,
        133
      ]
    ],
    "ID": "204"
  },
  {
    "text": "called CPDB. We used these clues to build a Small Dataset (SD) and a Large Dataset (LD) for reranking.",
    "acronyms": [
      [
        84,
        86
      ],
      [
        7,
        11
      ],
      [
        59,
        61
      ]
    ],
    "long-forms": [
      [
        69,
        82
      ],
      [
        44,
        57
      ]
    ],
    "ID": "205"
  },
  {
    "text": "Nu' (a) ~ Nu (a) AP (a)  OH  Nu' (a) = Nu (a) A(P (a)~ P' (a))  (3) la propri~t~ P contredit le noyau ; on a ~ la lois ",
    "acronyms": [
      [
        25,
        27
      ],
      [
        17,
        19
      ]
    ],
    "long-forms": [],
    "ID": "206"
  },
  {
    "text": "data are summarized in Table 6. We also report the Wordnet first sense baseline (WFS). ",
    "acronyms": [
      [
        81,
        84
      ]
    ],
    "long-forms": [
      [
        51,
        79
      ]
    ],
    "ID": "207"
  },
  {
    "text": "We note there also exist various multilingual or cross-lingual semantic processing works. Most of such works focus on semantic role labeling(SRL), the task of recovery of shallow meaning. Examples",
    "acronyms": [
      [
        141,
        144
      ]
    ],
    "long-forms": [
      [
        118,
        140
      ]
    ],
    "ID": "208"
  },
  {
    "text": "5th Conference of the Association for Machine Translation in the Americas (AMTA). Boston, Massachusetts.",
    "acronyms": [
      [
        75,
        79
      ]
    ],
    "long-forms": [
      [
        22,
        73
      ]
    ],
    "ID": "209"
  },
  {
    "text": " In this paper we deal with automatic acquisition of verbal selectional preferences (SPs) for Latin, i. e. the semantic preferences of verbs on their ar-",
    "acronyms": [
      [
        85,
        88
      ]
    ],
    "long-forms": [
      [
        60,
        83
      ]
    ],
    "ID": "210"
  },
  {
    "text": "= primary source; C06?C09 = CoNLL 2006?2009; I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip experiment described in Section 5.",
    "acronyms": [
      [
        191,
        194
      ],
      [
        28,
        33
      ],
      [
        62,
        64
      ],
      [
        84,
        86
      ],
      [
        106,
        108
      ],
      [
        122,
        125
      ],
      [
        150,
        153
      ],
      [
        183,
        185
      ]
    ],
    "long-forms": [
      [
        197,
        223
      ],
      [
        67,
        82
      ],
      [
        89,
        97
      ]
    ],
    "ID": "211"
  },
  {
    "text": "Computational Linguistics Volume 21, Number 4  Table 6  Growth of Hypothesis Space: S = sentence; TL = Total number of links; RL= Relevant Links;  AC = Number of Active Chains; G = Growth rate ",
    "acronyms": [
      [
        98,
        100
      ],
      [
        126,
        128
      ],
      [
        147,
        149
      ]
    ],
    "long-forms": [
      [
        103,
        108
      ],
      [
        88,
        96
      ],
      [
        130,
        144
      ],
      [
        162,
        174
      ],
      [
        181,
        192
      ]
    ],
    "ID": "212"
  },
  {
    "text": "In our experiments, we used the Penn Treebank (PTB) (Marcus et al 1993) for English and the Chinese Treebank version 5.1 (CTB5) (Xue et al 2005) for Chinese.",
    "acronyms": [
      [
        122,
        126
      ],
      [
        47,
        50
      ]
    ],
    "long-forms": [
      [
        92,
        120
      ],
      [
        32,
        45
      ]
    ],
    "ID": "213"
  },
  {
    "text": "3 MaxEnt Model and Features  3.1 MaxEnt Model for NOR  The principle of maximum entropy (MaxEnt)  model is that given a collection of facts, choose a ",
    "acronyms": [
      [
        89,
        95
      ],
      [
        2,
        8
      ],
      [
        33,
        39
      ],
      [
        50,
        53
      ]
    ],
    "long-forms": [
      [
        72,
        87
      ]
    ],
    "ID": "214"
  },
  {
    "text": "the following.  The Basic Additive model (BA) (introduced in (Mitchell and Lapata, 2008)) computes the disti-",
    "acronyms": [
      [
        42,
        44
      ]
    ],
    "long-forms": [
      [
        20,
        34
      ]
    ],
    "ID": "215"
  },
  {
    "text": "Two-liners generated by three different algorithms were evaluated by each subject: Script model + Concept clustering (SM+CC) Both script opposition and incongruity are",
    "acronyms": [
      [
        118,
        123
      ]
    ],
    "long-forms": [
      [
        83,
        116
      ]
    ],
    "ID": "216"
  },
  {
    "text": "All-before 8 0.0313 88.03 Table 1. Instance classification accuracy (CA) using  different feature sets.",
    "acronyms": [
      [
        69,
        71
      ]
    ],
    "long-forms": [
      [
        44,
        67
      ]
    ],
    "ID": "217"
  },
  {
    "text": "tim+ fo ! lowiug  se lnant i t :  d i~ iens iens~ dependeucy re la t ion   (\\]IR), coordieatien ned apposition constructions (CA) and  i :op ic . .",
    "acronyms": [
      [
        126,
        128
      ],
      [
        78,
        80
      ]
    ],
    "long-forms": [
      [
        83,
        110
      ]
    ],
    "ID": "218"
  },
  {
    "text": "for our experiment: Telecommunication Services (TS, the sector with the smallest number of companies), Information Technology (IT), and Consumer Staples (CS), due to our familiarity with the",
    "acronyms": [
      [
        127,
        129
      ],
      [
        48,
        50
      ],
      [
        154,
        156
      ]
    ],
    "long-forms": [
      [
        103,
        125
      ],
      [
        20,
        46
      ],
      [
        136,
        152
      ]
    ],
    "ID": "219"
  },
  {
    "text": "and there are many local minima on the error surface.  Therefore, we use an alternative loss function, minimum squared error (MSE) in equation (5), where Score(.)",
    "acronyms": [
      [
        126,
        129
      ]
    ],
    "long-forms": [
      [
        103,
        124
      ]
    ],
    "ID": "220"
  },
  {
    "text": "ment data time constraints, we were not able to test which of our modules leads to false negative (FN) instances. ",
    "acronyms": [
      [
        99,
        101
      ]
    ],
    "long-forms": [
      [
        83,
        97
      ]
    ],
    "ID": "221"
  },
  {
    "text": "? Rule One. For the first noun phrase (NP) encountered by the system, if 1) this NP has a name entity",
    "acronyms": [
      [
        39,
        41
      ]
    ],
    "long-forms": [
      [
        26,
        37
      ]
    ],
    "ID": "222"
  },
  {
    "text": "(ARTG, PR.OC)I ^  (SUBS, SUSU, VT)2  , ARTG = article g~n&al  SUBS = substantif  compl~ment  VT  = verbe conjugu6 ",
    "acronyms": [
      [
        62,
        66
      ],
      [
        1,
        5
      ],
      [
        7,
        12
      ],
      [
        19,
        23
      ],
      [
        25,
        29
      ],
      [
        31,
        33
      ],
      [
        39,
        43
      ],
      [
        93,
        95
      ]
    ],
    "long-forms": [
      [
        69,
        79
      ],
      [
        46,
        60
      ],
      [
        99,
        104
      ]
    ],
    "ID": "223"
  },
  {
    "text": "tions. The CSR techniques are based on a continuous-  observation Hidden Markov Model (HMM) approach. ",
    "acronyms": [
      [
        87,
        90
      ],
      [
        11,
        14
      ]
    ],
    "long-forms": [
      [
        66,
        85
      ]
    ],
    "ID": "224"
  },
  {
    "text": "word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 27(7):1063?1074.",
    "acronyms": [
      [
        91,
        95
      ],
      [
        27,
        31
      ]
    ],
    "long-forms": [
      [
        48,
        89
      ]
    ],
    "ID": "225"
  },
  {
    "text": "lingual interpersonal communication. The improvements in automatic speech recognition (ASR), statistical machine translation (MT), and, text-to-speech",
    "acronyms": [
      [
        87,
        90
      ],
      [
        126,
        128
      ]
    ],
    "long-forms": [
      [
        57,
        85
      ],
      [
        105,
        124
      ]
    ],
    "ID": "226"
  },
  {
    "text": "Table 1: Comparison of knowledge acquisition strategies. Interactive query expansion (IQE)?s poor task completion indicates keywords can?t bridge the knowledge",
    "acronyms": [
      [
        86,
        89
      ]
    ],
    "long-forms": [
      [
        57,
        84
      ]
    ],
    "ID": "227"
  },
  {
    "text": "We experimented with the following four machine learning algorithms: Support Vector Machine (SVM), Multilayer Perceptron(MLP),  Decision Trees(DT) and AdaBoost(AB).",
    "acronyms": [
      [
        121,
        124
      ],
      [
        93,
        96
      ],
      [
        143,
        145
      ],
      [
        160,
        162
      ]
    ],
    "long-forms": [
      [
        99,
        119
      ],
      [
        69,
        91
      ],
      [
        128,
        141
      ],
      [
        151,
        159
      ]
    ],
    "ID": "228"
  },
  {
    "text": "parsed as the specified category.  The 'target condition(TCND)' represents  conditions on variables in tile 'target pattern.'",
    "acronyms": [
      [
        57,
        61
      ]
    ],
    "long-forms": [
      [
        40,
        56
      ]
    ],
    "ID": "229"
  },
  {
    "text": "Probabilistic ID/LP grammars  The idea of separating simple context-free rules into two, orthogunal rule sets, immediate  dominance(ID) rules, and linear precedence(LP) rules, gives a notation for writing grammars called  ID/LP.",
    "acronyms": [
      [
        165,
        167
      ],
      [
        14,
        19
      ],
      [
        132,
        134
      ],
      [
        222,
        227
      ]
    ],
    "long-forms": [
      [
        147,
        163
      ],
      [
        111,
        131
      ]
    ],
    "ID": "230"
  },
  {
    "text": " 2 Related Work Locality sensitive hashing (LSH) (Indyk and Motwani, 1998) is an example of an approximate",
    "acronyms": [
      [
        44,
        47
      ]
    ],
    "long-forms": [
      [
        16,
        42
      ]
    ],
    "ID": "231"
  },
  {
    "text": " 2 Methodologies The Hidden Vector State (HVS) model (He and Young, 2005) is a discrete Hidden Markov Model",
    "acronyms": [
      [
        42,
        45
      ]
    ],
    "long-forms": [
      [
        21,
        40
      ]
    ],
    "ID": "232"
  },
  {
    "text": "The output of our experiments was evaluated using two metrics, (1) BLEU (Papineni et al, 2002), and (2) Lexical Accuracy (LexAcc). Lexical ac-",
    "acronyms": [
      [
        122,
        128
      ],
      [
        67,
        71
      ]
    ],
    "long-forms": [
      [
        104,
        120
      ]
    ],
    "ID": "233"
  },
  {
    "text": "corrccmess as follows:  ? Translation Correctness (TA). This is tile percentage of",
    "acronyms": [
      [
        51,
        53
      ]
    ],
    "long-forms": [
      [
        26,
        37
      ]
    ],
    "ID": "234"
  },
  {
    "text": "We consider two common ways of calculating the translation probability: using the maximum likelihood estimator (MLE) and smoothing the MLE using lexical weighting.",
    "acronyms": [
      [
        112,
        115
      ],
      [
        135,
        138
      ]
    ],
    "long-forms": [
      [
        82,
        110
      ]
    ],
    "ID": "235"
  },
  {
    "text": "studied for text categorization task (Forman, 2003).  Information gain (IG) is one of state of the art criteria for feature selection, which measures the de-",
    "acronyms": [
      [
        72,
        74
      ]
    ],
    "long-forms": [
      [
        54,
        70
      ]
    ],
    "ID": "236"
  },
  {
    "text": "They are Independent Word Probability (IWP), Anti-Word Pair  (AWP), and Word Formation Analogy (WFA). ",
    "acronyms": [
      [
        96,
        99
      ],
      [
        39,
        42
      ],
      [
        62,
        65
      ]
    ],
    "long-forms": [
      [
        72,
        94
      ],
      [
        9,
        37
      ],
      [
        45,
        59
      ]
    ],
    "ID": "237"
  },
  {
    "text": "freely available framenets for a number of languages (Boas, 2009), among these the Swedish FrameNet (SweFN) (Borin et al, 2010).5",
    "acronyms": [
      [
        101,
        106
      ]
    ],
    "long-forms": [
      [
        83,
        99
      ]
    ],
    "ID": "238"
  },
  {
    "text": "1. the mention type: person (PER), organization (ORG), location (LOC), geopolitical entity (GPE), facility (FAC), vehicle (VEH), and",
    "acronyms": [
      [
        65,
        68
      ]
    ],
    "long-forms": [
      [
        55,
        63
      ]
    ],
    "ID": "239"
  },
  {
    "text": "6 Related Work Two most prevalent discourse parsing treebanks are RST Discourse Treebank (RST-DT) (Carlson et al.,",
    "acronyms": [
      [
        90,
        96
      ]
    ],
    "long-forms": [
      [
        66,
        88
      ]
    ],
    "ID": "240"
  },
  {
    "text": "ranging from generating weather forecasts to summarizing medical information (Reiter and Dale 2000). Of all the subtasks of NLG, Referring Expression Generation (REG) is among those that have received most scholarly attention.",
    "acronyms": [
      [
        162,
        165
      ],
      [
        124,
        127
      ]
    ],
    "long-forms": [
      [
        129,
        160
      ]
    ],
    "ID": "241"
  },
  {
    "text": "Table 1. Judgment count for the sample instances (HA=high attachment; LA=low attachment; and A=Ambiguous)   ",
    "acronyms": [
      [
        70,
        72
      ]
    ],
    "long-forms": [
      [
        73,
        87
      ]
    ],
    "ID": "242"
  },
  {
    "text": "8 Conclusion We proposed a framework to generate characteristicrich questions for question answering (QA) evaluation.",
    "acronyms": [
      [
        102,
        104
      ]
    ],
    "long-forms": [
      [
        82,
        100
      ]
    ],
    "ID": "243"
  },
  {
    "text": "    Two additional general features were used.  The preposition feature (PREP) captures the  most indicative preposition among connected ",
    "acronyms": [
      [
        73,
        77
      ]
    ],
    "long-forms": [
      [
        52,
        63
      ]
    ],
    "ID": "244"
  },
  {
    "text": "In this paper we present methods for reducing the compu-  tation time of joint segmentation a d recognition of phones  using the Stochastic Segment Model (SSM). Our approach ",
    "acronyms": [
      [
        155,
        158
      ]
    ],
    "long-forms": [
      [
        129,
        153
      ]
    ],
    "ID": "245"
  },
  {
    "text": "Baselines We use the following baselines. The first is the Homogenous Poisson Process (HPP) trained on the training set of the rumour.",
    "acronyms": [
      [
        87,
        90
      ]
    ],
    "long-forms": [
      [
        59,
        85
      ]
    ],
    "ID": "246"
  },
  {
    "text": "features extracted from the LFG parse.  Lexical Functional Grammar (LFG) (Bresnan, 2000) is a constraint-based theory of grammar.",
    "acronyms": [
      [
        68,
        71
      ],
      [
        28,
        31
      ]
    ],
    "long-forms": [
      [
        40,
        66
      ]
    ],
    "ID": "247"
  },
  {
    "text": "to learn a lexicon. The theory of the TAD states is Universal Theory (UT); a UT metalanguage enables an abstract characteriza-",
    "acronyms": [
      [
        70,
        72
      ],
      [
        38,
        41
      ],
      [
        77,
        79
      ]
    ],
    "long-forms": [
      [
        52,
        68
      ]
    ],
    "ID": "248"
  },
  {
    "text": "appficability o new tasks, such as indications/warn-  ings, text tagging, and document detection support  \\[1\\] The Text REtrieval Conferences (TREC's) are described  in the Document Detecfien section.",
    "acronyms": [
      [
        144,
        150
      ]
    ],
    "long-forms": [
      [
        116,
        142
      ]
    ],
    "ID": "249"
  },
  {
    "text": "two aspects: how well the generated text reflects the source data, whether it be text in another language for machine translation (MT), a natural language generation (NLG) input representation, a doc-",
    "acronyms": [
      [
        131,
        133
      ],
      [
        167,
        170
      ]
    ],
    "long-forms": [
      [
        110,
        129
      ],
      [
        138,
        165
      ]
    ],
    "ID": "250"
  },
  {
    "text": "pris VPE (participe assE)  la AR  TD  par PREP (prEposition)  main SUBS (substamif) ",
    "acronyms": [
      [
        42,
        46
      ]
    ],
    "long-forms": [
      [
        48,
        59
      ]
    ],
    "ID": "251"
  },
  {
    "text": "The UMLS includes the Metathesaurus (MT),  which contains over one million biomedical concepts and the Semantic Network (SN), which  represents a high-level abstraction from the UMLS ",
    "acronyms": [
      [
        121,
        123
      ],
      [
        37,
        39
      ],
      [
        4,
        8
      ],
      [
        178,
        182
      ]
    ],
    "long-forms": [
      [
        103,
        119
      ],
      [
        22,
        35
      ]
    ],
    "ID": "252"
  },
  {
    "text": "H and T . This set of features is later used by two support vector machine (SVM) classifiers for detecting CLTE separately in both directions (T ?",
    "acronyms": [
      [
        76,
        79
      ],
      [
        107,
        111
      ]
    ],
    "long-forms": [
      [
        52,
        74
      ],
      [
        81,
        106
      ]
    ],
    "ID": "253"
  },
  {
    "text": "Conceptual structures (ConcSs);  ? Parsed syntactic structures (PSyntSs). ",
    "acronyms": [
      [
        64,
        71
      ],
      [
        23,
        29
      ]
    ],
    "long-forms": [
      [
        35,
        62
      ],
      [
        0,
        21
      ]
    ],
    "ID": "254"
  },
  {
    "text": "To this  end, more and more information extraction (IE)  systems using natural language processing (NLP)  have been developed for use in the biomedical ",
    "acronyms": [
      [
        100,
        103
      ],
      [
        52,
        54
      ]
    ],
    "long-forms": [
      [
        71,
        98
      ],
      [
        28,
        50
      ]
    ],
    "ID": "255"
  },
  {
    "text": "elements and punctuation. We used the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR) as defined in Klein (2005: 21-",
    "acronyms": [
      [
        87,
        89
      ],
      [
        113,
        115
      ]
    ],
    "long-forms": [
      [
        66,
        85
      ],
      [
        95,
        111
      ]
    ],
    "ID": "256"
  },
  {
    "text": "of the Spanish language. Our experimental project is a collaboration  of the Royal Spanish Academy (R.A.E.) and the Computer Center of  the University of Madrid (CCVM).",
    "acronyms": [
      [
        100,
        106
      ],
      [
        162,
        166
      ]
    ],
    "long-forms": [
      [
        77,
        98
      ],
      [
        116,
        160
      ]
    ],
    "ID": "257"
  },
  {
    "text": "2008.  Deciding strictly local (SL) languages. In Jon Breit-",
    "acronyms": [
      [
        32,
        34
      ]
    ],
    "long-forms": [
      [
        16,
        30
      ]
    ],
    "ID": "258"
  },
  {
    "text": "an all time high.  \u0000 HER (heritage): He named the new villa after his wife.",
    "acronyms": [
      [
        21,
        24
      ]
    ],
    "long-forms": [
      [
        26,
        34
      ]
    ],
    "ID": "259"
  },
  {
    "text": "tion algorithm to chase after undecidable cases. For example, consider prepositional phrases (PPs) with in as the head. These PPs occur frequently, and about half of them",
    "acronyms": [
      [
        94,
        97
      ],
      [
        126,
        129
      ]
    ],
    "long-forms": [
      [
        85,
        92
      ]
    ],
    "ID": "260"
  },
  {
    "text": "consists of following elements: Topic  Number(NUM),Topic Title(TITLE),Topic  question(DESC),Topic Narrative(NARR) and  Topic Concepts(CONC).",
    "acronyms": [
      [
        108,
        112
      ],
      [
        63,
        68
      ],
      [
        46,
        49
      ],
      [
        86,
        90
      ],
      [
        134,
        138
      ]
    ],
    "long-forms": [
      [
        98,
        106
      ],
      [
        57,
        62
      ],
      [
        39,
        45
      ],
      [
        125,
        133
      ]
    ],
    "ID": "261"
  },
  {
    "text": "MR = MN/length (5) ? The Continuous Match Value(CMV): Continuous match should be better than the",
    "acronyms": [
      [
        48,
        51
      ],
      [
        0,
        2
      ],
      [
        5,
        7
      ]
    ],
    "long-forms": [
      [
        25,
        46
      ]
    ],
    "ID": "262"
  },
  {
    "text": "Improvements in a simulated speech-recognition example. Nine versions of a phonemically  identical oronym, ordered by weighted average (W.A.) probability (x 10-20). The W.A. ",
    "acronyms": [
      [
        136,
        140
      ],
      [
        169,
        173
      ]
    ],
    "long-forms": [
      [
        118,
        134
      ]
    ],
    "ID": "263"
  },
  {
    "text": "The central components of our non-parametric Bayesian models are the Chinese Restaurant Processes (CRPs) and the closely related Dirichlet Processes (DPs) (Ferguson, 1973).",
    "acronyms": [
      [
        99,
        103
      ],
      [
        150,
        153
      ]
    ],
    "long-forms": [
      [
        69,
        97
      ],
      [
        129,
        148
      ]
    ],
    "ID": "264"
  },
  {
    "text": "L&L is suggestive of a particular approach to supervised learning ? maximum entropy (MaxEnt) ? in",
    "acronyms": [
      [
        85,
        91
      ],
      [
        0,
        3
      ]
    ],
    "long-forms": [
      [
        68,
        83
      ]
    ],
    "ID": "265"
  },
  {
    "text": "read speech and two-party dialogue, multi-party dialogues typically exhibit a considerably higher word error rate (WER) (Morgan et al, 2003). ",
    "acronyms": [
      [
        115,
        118
      ]
    ],
    "long-forms": [
      [
        98,
        113
      ]
    ],
    "ID": "266"
  },
  {
    "text": "junct verbs (ConjVs), the Non-MonoClausal  Verbs (NMCV) and Auxiliary Construction  (AC) occur as conjunct verbs (ConjVs).  The ",
    "acronyms": [
      [
        114,
        120
      ],
      [
        13,
        19
      ],
      [
        50,
        54
      ],
      [
        85,
        87
      ]
    ],
    "long-forms": [
      [
        98,
        112
      ],
      [
        0,
        11
      ],
      [
        26,
        48
      ],
      [
        60,
        82
      ]
    ],
    "ID": "267"
  },
  {
    "text": " bo und Figure 1: Accuracy for Na??veBayes classifier (NBC) and Majority Rule (MR) 4 Experimental results",
    "acronyms": [
      [
        79,
        81
      ],
      [
        55,
        58
      ]
    ],
    "long-forms": [
      [
        64,
        77
      ],
      [
        31,
        53
      ]
    ],
    "ID": "268"
  },
  {
    "text": "3.3 Adapting the POS tagset (STTS) To account for important differences between modern and Early Modern German (EMG), and to facilitate more accurate searches, we adapted the STTS",
    "acronyms": [
      [
        112,
        115
      ],
      [
        17,
        20
      ],
      [
        29,
        33
      ],
      [
        175,
        179
      ]
    ],
    "long-forms": [
      [
        91,
        110
      ]
    ],
    "ID": "269"
  },
  {
    "text": "The rules  in an AG have a considerably different formal character as compared  to the 'rewrite rule' in a general phrase structure grmmmar (PSG). ",
    "acronyms": [
      [
        141,
        144
      ],
      [
        17,
        19
      ]
    ],
    "long-forms": [
      [
        115,
        139
      ]
    ],
    "ID": "270"
  },
  {
    "text": "Previous shared tasks for grammar error correction, such as the HOO shared task of 2012 (HOO-2012) and the CoNLL-2013 shared task(CoNLL-2013),",
    "acronyms": [
      [
        89,
        97
      ],
      [
        130,
        140
      ],
      [
        107,
        117
      ]
    ],
    "long-forms": [
      [
        64,
        87
      ]
    ],
    "ID": "271"
  },
  {
    "text": "languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis). ",
    "acronyms": [
      [
        93,
        97
      ],
      [
        39,
        42
      ]
    ],
    "long-forms": [
      [
        99,
        137
      ],
      [
        44,
        74
      ]
    ],
    "ID": "272"
  },
  {
    "text": "For example, an infinitival clause  (INFCL) may contain a noun phrase with an  embedded relative clause (RELCL). Elimination ",
    "acronyms": [
      [
        105,
        110
      ],
      [
        37,
        42
      ]
    ],
    "long-forms": [
      [
        88,
        103
      ],
      [
        16,
        34
      ]
    ],
    "ID": "273"
  },
  {
    "text": "natural problems, and outperform several widelyused conventional models, e.g., support vector machines (SVMs), conditional random fields (CRFs) and hidden Markov models (HMMs).",
    "acronyms": [
      [
        138,
        142
      ],
      [
        104,
        108
      ],
      [
        170,
        174
      ]
    ],
    "long-forms": [
      [
        111,
        136
      ],
      [
        79,
        102
      ],
      [
        148,
        168
      ]
    ],
    "ID": "274"
  },
  {
    "text": "by the organizers). The results are compared to the best system and the MFS (Most Frequent Sense) baseline.",
    "acronyms": [
      [
        72,
        75
      ]
    ],
    "long-forms": [
      [
        77,
        96
      ]
    ],
    "ID": "275"
  },
  {
    "text": "Abstract This paper reports about the development of a Named Entity Recognition (NER) system for Bengali using the statistical Conditional",
    "acronyms": [
      [
        81,
        84
      ]
    ],
    "long-forms": [
      [
        55,
        79
      ]
    ],
    "ID": "276"
  },
  {
    "text": "David Scott Warren and Joyce Friedman Using Semantics in Non-Context-Free Parsing  As a more intuitive example of refinement, consider  an English-like language with categories term (TE) and  intransitive verb phrase (IV) that both include singular ",
    "acronyms": [
      [
        183,
        185
      ],
      [
        218,
        220
      ]
    ],
    "long-forms": [
      [
        177,
        181
      ],
      [
        192,
        209
      ]
    ],
    "ID": "277"
  },
  {
    "text": " The absence of reliable single-sentence estimates points to a gap in natural language processing (NLP) research.",
    "acronyms": [
      [
        99,
        102
      ]
    ],
    "long-forms": [
      [
        70,
        97
      ]
    ],
    "ID": "278"
  },
  {
    "text": "Evaluation of Online Dialogue Policy Learning Techniques, Proceedings of the 8th Conference on Language Resources and Evaluation (LREC) 2012, to appear.",
    "acronyms": [
      [
        130,
        134
      ]
    ],
    "long-forms": [
      [
        95,
        113
      ]
    ],
    "ID": "279"
  },
  {
    "text": " The ACE 2005 data set alo contains a set of ariticles from the broadcast news (BN) source which is written entirely in lower case.",
    "acronyms": [
      [
        80,
        82
      ],
      [
        5,
        8
      ]
    ],
    "long-forms": [
      [
        64,
        78
      ]
    ],
    "ID": "280"
  },
  {
    "text": "mosque, . . .   Detai l izat ion (DET) - a subst i tut ion of a  detai led description Y1 of a thing, s i tuat ion ",
    "acronyms": [
      [
        34,
        37
      ]
    ],
    "long-forms": [
      [
        16,
        32
      ]
    ],
    "ID": "281"
  },
  {
    "text": "(RA (NULL))))))  (NPOS (NULL)))  (NVAR (N='BYPASS':(SINGULAR) \" ('BYPASS')))  (RN (NULL))))) ",
    "acronyms": [
      [
        34,
        38
      ],
      [
        18,
        22
      ],
      [
        1,
        3
      ],
      [
        5,
        9
      ],
      [
        24,
        28
      ],
      [
        79,
        81
      ],
      [
        83,
        87
      ]
    ],
    "long-forms": [
      [
        40,
        60
      ]
    ],
    "ID": "282"
  },
  {
    "text": "http://wordnet.princeton.edu/glosstag.shtml 1462 tive baseline, based on Greedy String Tiling (GST) (Wise, 1996).",
    "acronyms": [
      [
        95,
        98
      ]
    ],
    "long-forms": [
      [
        73,
        93
      ]
    ],
    "ID": "283"
  },
  {
    "text": ". . . ( 7) To shorten notation, we use state abbreviations (e.g., CA = California :state). ",
    "acronyms": [
      [
        66,
        68
      ]
    ],
    "long-forms": [
      [
        71,
        81
      ]
    ],
    "ID": "284"
  },
  {
    "text": " 3 Tools and Component Combination   We use the support vector machine (SVM) multi-class classifier (Crammer and Singer (2002),  ",
    "acronyms": [
      [
        72,
        75
      ]
    ],
    "long-forms": [
      [
        48,
        70
      ]
    ],
    "ID": "285"
  },
  {
    "text": "servations.  Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been de-",
    "acronyms": [
      [
        47,
        51
      ]
    ],
    "long-forms": [
      [
        13,
        45
      ]
    ],
    "ID": "286"
  },
  {
    "text": "making valuable data resources publicly available . The author is a member of the Institute for Robotics and Intelligent Systems (IRIS) and wishes to acknowledge the support of the Networks of Centres of Excellenc e Program of the Government of Canada, the Natural Sciences and Engineering Research Council (NSERC) ,",
    "acronyms": [
      [
        130,
        134
      ],
      [
        308,
        313
      ]
    ],
    "long-forms": [
      [
        82,
        128
      ],
      [
        257,
        306
      ]
    ],
    "ID": "287"
  },
  {
    "text": "NP re-annotation very helpful for the performance. We think it is because of the annotation style of the Upenn Chinese Treebank (CTB). According to Xue et al",
    "acronyms": [
      [
        129,
        132
      ],
      [
        0,
        2
      ]
    ],
    "long-forms": [
      [
        111,
        127
      ]
    ],
    "ID": "288"
  },
  {
    "text": "mender systems using a multidimensional approach.  ACM Transactions on Information Systems (TOIS), 23(1):103?145.",
    "acronyms": [
      [
        92,
        96
      ],
      [
        51,
        54
      ]
    ],
    "long-forms": [
      [
        55,
        90
      ]
    ],
    "ID": "289"
  },
  {
    "text": "tracted from a corpus of parsed sentences.  A super abstract role value (SuperARV) is an abstraction of the joint assignment of dependencies for",
    "acronyms": [
      [
        73,
        81
      ]
    ],
    "long-forms": [
      [
        46,
        71
      ]
    ],
    "ID": "290"
  },
  {
    "text": "duction Factor, and the \"expected utility\" of a PA-  SSF is estimated as the Global Reduction Factor:  Reduct ion  Factor  The Reduction Factor (RF)  of  a given SSFssf is RF(ss f )  = n(ssf)  - 1, where ",
    "acronyms": [
      [
        145,
        147
      ],
      [
        48,
        56
      ],
      [
        172,
        174
      ],
      [
        162,
        168
      ]
    ],
    "long-forms": [
      [
        127,
        143
      ]
    ],
    "ID": "291"
  },
  {
    "text": "most frequent sense of the training corpus (TRAIN-MFS). However, all of them are far below to the Topic Signatures acquired using the training corpus (TRAIN). ",
    "acronyms": [
      [
        151,
        156
      ],
      [
        44,
        53
      ]
    ],
    "long-forms": [
      [
        134,
        142
      ],
      [
        27,
        35
      ]
    ],
    "ID": "292"
  },
  {
    "text": "O. Introduction.  Since 1971 the research group \"Maschinelle Syntaxanalyse\" (MasA)  has been working as a part of the project \"Linguistische Datenverar- ",
    "acronyms": [
      [
        77,
        81
      ]
    ],
    "long-forms": [
      [
        49,
        74
      ]
    ],
    "ID": "293"
  },
  {
    "text": "as given in (6)b, the v-hon type with the -(u)si suffix adds the information that its subject (the first element in the ARG-ST (argument structure)) is [HON +], in addition to the information that the",
    "acronyms": [
      [
        120,
        126
      ],
      [
        153,
        156
      ]
    ],
    "long-forms": [
      [
        128,
        146
      ]
    ],
    "ID": "294"
  },
  {
    "text": "second indicator of effective query is the recall at R document retrieved (Recall at R).  The last indicator measures the human effort (HE) in finding the answer. HE is ",
    "acronyms": [
      [
        136,
        138
      ],
      [
        163,
        165
      ]
    ],
    "long-forms": [
      [
        122,
        134
      ]
    ],
    "ID": "295"
  },
  {
    "text": "Figure 1 Examples of contextual phenomena,  ellipsis depends on the context and means 'credit card'; it  is both a focus and an object (OBJ). ",
    "acronyms": [
      [
        136,
        139
      ]
    ],
    "long-forms": [
      [
        128,
        134
      ]
    ],
    "ID": "296"
  },
  {
    "text": "6.7 Presence of  four digits 8 Evaluation  If the token is a four digit number, it is likelier to  be a NETI (Named Entity Time). For example, ",
    "acronyms": [
      [
        104,
        108
      ]
    ],
    "long-forms": [
      [
        110,
        127
      ]
    ],
    "ID": "297"
  },
  {
    "text": " 1 Introduction Data-driven machine translation (MT) relies on models that can be efficiently estimated from par-",
    "acronyms": [
      [
        49,
        51
      ]
    ],
    "long-forms": [
      [
        28,
        47
      ]
    ],
    "ID": "298"
  },
  {
    "text": "TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al, 2009a). In Machine Translation (MT), TM can improve alignment at training time and help enrich phrase tables with named entities that may not appear in parallel training data. More broadly, TM is a character mapping problem.",
    "acronyms": [
      [
        389,
        391
      ],
      [
        68,
        70
      ],
      [
        105,
        108
      ],
      [
        155,
        157
      ],
      [
        159,
        161
      ],
      [
        394,
        396
      ]
    ],
    "long-forms": [
      [
        368,
        387
      ],
      [
        45,
        66
      ],
      [
        76,
        103
      ]
    ],
    "ID": "299"
  },
  {
    "text": "One team participated in the GRO task, and their results were compared with those of a preliminary system prepared by the task organizers. An analysis of the evaluation results leads us to study issues such as the need to consider the ontology structure and the need for semantic analysis, which are not seriously dealt with by current approaches to event extraction. 6 Organization of the workshop The BioNLP Shared Task 2013 (BioNLP-ST) workshop was organized as part of the ACL BioNLP 2013 workshop. After submission of their system results, participants were invited to submit a paper on their systems to the workshop.",
    "acronyms": [
      [
        428,
        437
      ],
      [
        29,
        32
      ],
      [
        477,
        480
      ],
      [
        481,
        487
      ]
    ],
    "long-forms": [
      [
        403,
        421
      ]
    ],
    "ID": "300"
  },
  {
    "text": "A study of global inference algorithms in multi-document summarization. In Proceedings of the 29th European Conference on Information Retrieval (ECIR), pages 557?564. ",
    "acronyms": [
      [
        145,
        149
      ]
    ],
    "long-forms": [
      [
        99,
        143
      ]
    ],
    "ID": "301"
  },
  {
    "text": "sentence, but many equally good translation options.  Often, machine translation (MT) systems are only evaluated quantitatively, e.g. by the use of automatic",
    "acronyms": [
      [
        82,
        84
      ]
    ],
    "long-forms": [
      [
        61,
        80
      ]
    ],
    "ID": "302"
  },
  {
    "text": "syntactic tree fragments (STFs) and partial tree fragments (PTFs) 2.2.1 Syntactic Tree Kernels (STK) An STF is a connected subset of the nodes and",
    "acronyms": [
      [
        96,
        99
      ],
      [
        26,
        30
      ],
      [
        60,
        64
      ],
      [
        104,
        107
      ]
    ],
    "long-forms": [
      [
        72,
        94
      ],
      [
        0,
        24
      ],
      [
        36,
        58
      ]
    ],
    "ID": "303"
  },
  {
    "text": "PAST (- +)  Finite verbs are specified as (PAST +) if they are in  the past tense, and as (PAST -) otherwise. ",
    "acronyms": [
      [
        91,
        97
      ],
      [
        0,
        4
      ],
      [
        43,
        47
      ]
    ],
    "long-forms": [
      [
        71,
        81
      ]
    ],
    "ID": "304"
  },
  {
    "text": "cal and clinical applications. Early work relied on the Gene Ontology (GO)3, which is a hierarchy of terms used to describe genomic information.",
    "acronyms": [
      [
        71,
        73
      ]
    ],
    "long-forms": [
      [
        56,
        69
      ]
    ],
    "ID": "305"
  },
  {
    "text": "cke et al, 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing al-",
    "acronyms": [
      [
        62,
        65
      ]
    ],
    "long-forms": [
      [
        42,
        60
      ]
    ],
    "ID": "306"
  },
  {
    "text": "these instances. We treat the language ID task as a coreference resolution (CoRef) problem: a mention is an IGT or a language name appearing in a",
    "acronyms": [
      [
        76,
        81
      ],
      [
        39,
        41
      ],
      [
        108,
        111
      ]
    ],
    "long-forms": [
      [
        52,
        74
      ]
    ],
    "ID": "307"
  },
  {
    "text": "Figure 1: Vocabulary growth rates for English,  Spanish, German and Korean for the Spontaneous  Scheduling Task (SST). ",
    "acronyms": [
      [
        113,
        116
      ]
    ],
    "long-forms": [
      [
        83,
        111
      ]
    ],
    "ID": "308"
  },
  {
    "text": "Sellers algorithms  2.1 Algorithm Principle  The Wagner & Fischer (W&F) dynamic  programming algorithm in Figure 3 gives tile ",
    "acronyms": [
      [
        67,
        70
      ]
    ],
    "long-forms": [
      [
        49,
        65
      ]
    ],
    "ID": "309"
  },
  {
    "text": "sponding to each of the cluster. Fairly intuitively we computed the Longest Common Subsequence(LCS) between the sentences in each cluster which we then",
    "acronyms": [
      [
        95,
        98
      ]
    ],
    "long-forms": [
      [
        68,
        93
      ]
    ],
    "ID": "310"
  },
  {
    "text": "(selected expansion  terms are in italic)  OR(locator, finder, location,  directory) ",
    "acronyms": [
      [
        43,
        45
      ]
    ],
    "long-forms": [
      [
        47,
        53
      ]
    ],
    "ID": "311"
  },
  {
    "text": "Coarse-to-fine n-best parsing and MaxEnt discriminative reranking Eugene Charniak and Mark Johnson Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University",
    "acronyms": [
      [
        155,
        160
      ],
      [
        34,
        40
      ]
    ],
    "long-forms": [
      [
        99,
        153
      ]
    ],
    "ID": "312"
  },
  {
    "text": "In the table, P is precision; R is  recall; P&R is the harmonic mean of precision and  recall  (P&R = (2*P*R) / (P+R), corresponding to a  F-measure with a ?",
    "acronyms": [
      [
        96,
        99
      ],
      [
        44,
        47
      ]
    ],
    "long-forms": [
      [
        103,
        108
      ],
      [
        19,
        28
      ],
      [
        36,
        42
      ]
    ],
    "ID": "313"
  },
  {
    "text": "Similarity  task  at  SEM  2013.  The  Semantic  Textual Similarity Core task  (STS)  computes the  degree  of  semantic  equivalence  between  two ",
    "acronyms": [
      [
        80,
        83
      ],
      [
        22,
        25
      ]
    ],
    "long-forms": [
      [
        57,
        77
      ]
    ],
    "ID": "314"
  },
  {
    "text": "tion, corpora from seven different genres are used: the MSNBC broadcasting conversation (BC), the CNN broadcasting news (BN), the Sinorama news magazine (MZ), the WSJ newswire (NW), and the",
    "acronyms": [
      [
        121,
        123
      ],
      [
        56,
        61
      ],
      [
        89,
        91
      ],
      [
        98,
        101
      ],
      [
        154,
        156
      ],
      [
        163,
        166
      ],
      [
        177,
        179
      ]
    ],
    "long-forms": [
      [
        102,
        119
      ],
      [
        62,
        87
      ],
      [
        144,
        152
      ],
      [
        167,
        175
      ]
    ],
    "ID": "315"
  },
  {
    "text": "Using the XTAG Tree Adjoining Grammar [Gro01], we start by listing these variations. Indeed a Tree Adjoining Grammar (TAG) lists the set of all possible syntactic configurations for basic clauses and groups them into so-called (tree) families.",
    "acronyms": [
      [
        118,
        121
      ],
      [
        10,
        14
      ],
      [
        39,
        44
      ]
    ],
    "long-forms": [
      [
        94,
        116
      ]
    ],
    "ID": "316"
  },
  {
    "text": "In order to quantify the level of noise in the collected SMS data, we built a character-level language model(LM)13 using the questions in the FAQ data-set (vocabulary",
    "acronyms": [
      [
        109,
        111
      ],
      [
        57,
        60
      ],
      [
        142,
        145
      ]
    ],
    "long-forms": [
      [
        94,
        107
      ]
    ],
    "ID": "317"
  },
  {
    "text": "Chinese research and applications. We refer to it  as CLO (Chinese Lexical Ontology). In addition ",
    "acronyms": [
      [
        54,
        57
      ]
    ],
    "long-forms": [
      [
        59,
        83
      ]
    ],
    "ID": "318"
  },
  {
    "text": "Stochastic Gradient Descent (SGD) (Tsuruoka et al., 2009) `2 regularization, hinge loss Bernoulli Naive Bayes (B-NB) (McCallum et al., 1998) ?",
    "acronyms": [
      [
        111,
        115
      ],
      [
        29,
        32
      ]
    ],
    "long-forms": [
      [
        88,
        109
      ],
      [
        0,
        27
      ]
    ],
    "ID": "319"
  },
  {
    "text": "issue. Send membership applications and  address changes to Betty Walker (ACL),  Bellcore, 445 South Street, MRE 2A379, ",
    "acronyms": [
      [
        74,
        77
      ],
      [
        109,
        112
      ]
    ],
    "long-forms": [
      [
        41,
        72
      ]
    ],
    "ID": "320"
  },
  {
    "text": "training (Blum and Mitchell, 1998) has been successfully applied to English named entity recognition (NER) (Collins & Singer [henceforth C&S] (1999)).",
    "acronyms": [
      [
        102,
        105
      ],
      [
        137,
        140
      ]
    ],
    "long-forms": [
      [
        94,
        100
      ]
    ],
    "ID": "321"
  },
  {
    "text": "Center who will send the artillery fire when given the appropriate information.  In line 1, G19?s utterance is interpreted as a Warning Order - Method of Fire (WO-MOF), describing the kind of artillery fire requested, whose value is ?",
    "acronyms": [
      [
        160,
        166
      ]
    ],
    "long-forms": [
      [
        128,
        158
      ]
    ],
    "ID": "322"
  },
  {
    "text": "REPRESENTATION OF INTENSIONAI,  CON'FEXTS  The main extension to the work reported in \\[Di Eugenio &  Lesnro II'/l consists in the introduction of CONTEXT SPACES (CS),  which enable us to treat the intensional contexts along the lines pro.. ",
    "acronyms": [
      [
        163,
        165
      ]
    ],
    "long-forms": [
      [
        147,
        161
      ]
    ],
    "ID": "323"
  },
  {
    "text": "machine learning algorithms from the Python scikit-learn4 package: Na??ve Bayes (NB), Maximum Entropy (MaxEnt), and Support Vector Machines (SVM).",
    "acronyms": [
      [
        103,
        109
      ],
      [
        81,
        83
      ],
      [
        141,
        144
      ]
    ],
    "long-forms": [
      [
        86,
        101
      ],
      [
        67,
        79
      ],
      [
        116,
        139
      ]
    ],
    "ID": "324"
  },
  {
    "text": "In TAC 2008 Summarization track, all submitted runs were scored with the ROUGE (Lin, 2004) and Basic Elements (BE) metrics (Hovy et al.,",
    "acronyms": [
      [
        111,
        113
      ],
      [
        3,
        6
      ]
    ],
    "long-forms": [
      [
        95,
        109
      ]
    ],
    "ID": "325"
  },
  {
    "text": "on specific characteristics of the signs, have appeared in the international community: HamNoSys  (Prillwitz et al 1989), SEA (Sistema de Escritura  Alfab?tica) (Herrero, A., 2004) and SignWriting ",
    "acronyms": [
      [
        122,
        125
      ],
      [
        88,
        96
      ]
    ],
    "long-forms": [
      [
        127,
        134
      ]
    ],
    "ID": "326"
  },
  {
    "text": "take advantage of the capabilities of a vectofized  concurrent processor (an Alliant FX/80) which consists  of a cluster of up to 8 computing elements (CE's) that  can execute code in vector concurrent mode.",
    "acronyms": [
      [
        152,
        156
      ],
      [
        85,
        90
      ]
    ],
    "long-forms": [
      [
        132,
        150
      ]
    ],
    "ID": "327"
  },
  {
    "text": "Concerning the second objective, we will devise our new measure, known as the Odds of Unithood (OU), which are derived using Bayes Theorem and founded on a few elementary probabil-",
    "acronyms": [
      [
        96,
        98
      ]
    ],
    "long-forms": [
      [
        83,
        94
      ]
    ],
    "ID": "328"
  },
  {
    "text": " 1 Introduction  Word sense disambiguation (WSD) is perhaps the  great open problem at the lexical level of natural ",
    "acronyms": [
      [
        44,
        47
      ]
    ],
    "long-forms": [
      [
        17,
        42
      ]
    ],
    "ID": "329"
  },
  {
    "text": "Introduction  In a large natural language processing system,  such as a machine translation system (MTS), am-  biguity resolution is a critical problem.",
    "acronyms": [
      [
        100,
        103
      ]
    ],
    "long-forms": [
      [
        72,
        98
      ]
    ],
    "ID": "330"
  },
  {
    "text": "rules are learned from the alignment of manuallytranscribed text (T ) with automatically-generated transcripts (TASR) of training data, ranked according to a scoring function (S) and applied to the",
    "acronyms": [
      [
        112,
        116
      ]
    ],
    "long-forms": [
      [
        99,
        110
      ],
      [
        60,
        64
      ]
    ],
    "ID": "331"
  },
  {
    "text": "resented by an order domain (DOM), which is a list  of domain objects, whose relative order must satisfy  a set of linear precedence (LP) constraints. The or- ",
    "acronyms": [
      [
        134,
        136
      ],
      [
        29,
        32
      ]
    ],
    "long-forms": [
      [
        115,
        132
      ],
      [
        21,
        27
      ]
    ],
    "ID": "332"
  },
  {
    "text": "Donnell, 1982).  Def in i t ion 1 A deterministic tree automaton (DTA)  is a 5-tuple M = (Q, ~, ~, qo, F), where Q is a finite ",
    "acronyms": [
      [
        66,
        69
      ]
    ],
    "long-forms": [
      [
        36,
        64
      ]
    ],
    "ID": "333"
  },
  {
    "text": "Cui et al experimented with two approaches to rank the candidate answers, called Bigram Model and Profile Hidden Markov Model (PHMM). Both",
    "acronyms": [
      [
        127,
        131
      ]
    ],
    "long-forms": [
      [
        98,
        125
      ]
    ],
    "ID": "334"
  },
  {
    "text": "\\canonical\" dependency direction under wellde\fned conditions, distinguishing between ordre lin\u0013eaire (linear precedence(LP)) and ordre structural (immediate dominance(ID)).",
    "acronyms": [
      [
        120,
        122
      ]
    ],
    "long-forms": [
      [
        102,
        118
      ]
    ],
    "ID": "335"
  },
  {
    "text": "both tasks together would be desirable. In this section, we propose the use of factorial CRF (F-CRF) (Sutton et al, 2007), which has previously been",
    "acronyms": [
      [
        94,
        99
      ]
    ],
    "long-forms": [
      [
        79,
        92
      ]
    ],
    "ID": "336"
  },
  {
    "text": " 1 Introduction Word sense disambiguation (WSD) is a key enabling technology.",
    "acronyms": [
      [
        43,
        46
      ]
    ],
    "long-forms": [
      [
        16,
        41
      ]
    ],
    "ID": "337"
  },
  {
    "text": "Turning to emerging structure, PTT assumes that participants perform (often fragmentary) contributions, discourse units (DUs), which are dynamic propositions (DRSs in the",
    "acronyms": [
      [
        121,
        124
      ],
      [
        31,
        34
      ],
      [
        159,
        163
      ]
    ],
    "long-forms": [
      [
        104,
        119
      ]
    ],
    "ID": "338"
  },
  {
    "text": "languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of colloca-",
    "acronyms": [
      [
        82,
        85
      ]
    ],
    "long-forms": [
      [
        68,
        80
      ]
    ],
    "ID": "339"
  },
  {
    "text": "In this paper we have systematically studied these complex relations involving SVC and VopC for BP, which constitute a challenge to Natural Language Processing (NLP) systems, and have been often ignored in related work.",
    "acronyms": [
      [
        161,
        164
      ],
      [
        79,
        82
      ],
      [
        96,
        98
      ],
      [
        87,
        91
      ]
    ],
    "long-forms": [
      [
        132,
        159
      ]
    ],
    "ID": "340"
  },
  {
    "text": "specification whose precise informational content and form is in turn defined by an appropriate Document Type Definition (DTD). ",
    "acronyms": [
      [
        122,
        125
      ]
    ],
    "long-forms": [
      [
        96,
        120
      ]
    ],
    "ID": "341"
  },
  {
    "text": "general idea widely used in this kind of systems: if  an input sentence is syntactically ill-formed, i.e. it  cannot be assigned a syntactic structure (SyntS),  the system considers minimal changes that enable it ",
    "acronyms": [
      [
        152,
        157
      ]
    ],
    "long-forms": [
      [
        131,
        150
      ]
    ],
    "ID": "342"
  },
  {
    "text": "1203 Figure 1: Bootstrapped Learning. ( HT = hashtag; HP = hashtag pattern) dov et al.,",
    "acronyms": [
      [
        54,
        56
      ],
      [
        40,
        42
      ]
    ],
    "long-forms": [
      [
        59,
        74
      ],
      [
        45,
        52
      ]
    ],
    "ID": "343"
  },
  {
    "text": "2003) on the term-sentence matrix of human model summaries used in the Document Understanding Conference (DUC) 2007 Pyramid evaluation1.",
    "acronyms": [
      [
        106,
        109
      ]
    ],
    "long-forms": [
      [
        71,
        104
      ]
    ],
    "ID": "344"
  },
  {
    "text": "1 Introduction Visualizing Web search results remains an open problem in Information Retrieval (IR). For exam-",
    "acronyms": [
      [
        96,
        98
      ]
    ],
    "long-forms": [
      [
        73,
        94
      ]
    ],
    "ID": "345"
  },
  {
    "text": "Here, we describe a naive baseline approach to arrange nearest neighbors next to each other by using Independent Random Projections (IRP). In this",
    "acronyms": [
      [
        133,
        136
      ]
    ],
    "long-forms": [
      [
        101,
        131
      ]
    ],
    "ID": "346"
  },
  {
    "text": "Figure 2: Algorithm for scope detection by MRS crawling a Formally: If an EP shares its label with the negation cue, or is a quantifier whose restriction (RSTR) is = q",
    "acronyms": [
      [
        155,
        159
      ],
      [
        43,
        46
      ],
      [
        74,
        76
      ]
    ],
    "long-forms": [
      [
        142,
        153
      ]
    ],
    "ID": "347"
  },
  {
    "text": "x?D(X) P (X = x|W1 = w1, . . . , WN = wN ). ( 1)",
    "acronyms": [
      [
        33,
        35
      ]
    ],
    "long-forms": [
      [
        38,
        40
      ]
    ],
    "ID": "348"
  },
  {
    "text": "fr, veronis@fraixll  .univ-aix. fr  Abstract, MULTEXT (Multilingual Text \"Fools and  Corpora) is the largest project funded in the Commission ",
    "acronyms": [
      [
        46,
        53
      ],
      [
        22,
        30
      ]
    ],
    "long-forms": [
      [
        55,
        72
      ]
    ],
    "ID": "349"
  },
  {
    "text": "In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96). ",
    "acronyms": [
      [
        82,
        91
      ]
    ],
    "long-forms": [
      [
        55,
        80
      ]
    ],
    "ID": "350"
  },
  {
    "text": "(located at lower levels) in a taxonomy.  In the Information Retrieval (IR) community, browsing taxonomies.",
    "acronyms": [
      [
        72,
        74
      ]
    ],
    "long-forms": [
      [
        49,
        70
      ]
    ],
    "ID": "351"
  },
  {
    "text": " 11 4 Lexical Analysis and Named Entity Recognition (NE ) The lexicon we used contains only syntactic information such as parts of speech and subcategorization frames .",
    "acronyms": [
      [
        53,
        55
      ]
    ],
    "long-forms": [
      [
        28,
        39
      ]
    ],
    "ID": "352"
  },
  {
    "text": " 2) Prediction precision(PP) =  number of words with correct BPs(CortBP)  total word number (TWN) ",
    "acronyms": [
      [
        65,
        71
      ]
    ],
    "long-forms": [
      [
        53,
        63
      ]
    ],
    "ID": "353"
  },
  {
    "text": "We used a  3-pass decoding strategy, in which the first pass uses the  speaker independent (SI) vowelized system, the second  pass uses the speaker adaptive (SA) non-vowelized ",
    "acronyms": [
      [
        92,
        94
      ],
      [
        158,
        160
      ]
    ],
    "long-forms": [
      [
        71,
        90
      ],
      [
        140,
        156
      ]
    ],
    "ID": "354"
  },
  {
    "text": "To perform this relocation quickly, Yata et al. ( 2009) introduced two additional one-dimensional arrays, called NLINK (node link) and BLOCK. For each node, NLINK stores the label needed to reach its",
    "acronyms": [
      [
        113,
        118
      ],
      [
        157,
        162
      ]
    ],
    "long-forms": [
      [
        120,
        129
      ]
    ],
    "ID": "355"
  },
  {
    "text": "Our translation method performs sense-based translation and pronunciation-based translation on the basis of statistical machine translation (SMT) methods.",
    "acronyms": [
      [
        141,
        144
      ]
    ],
    "long-forms": [
      [
        108,
        139
      ]
    ],
    "ID": "356"
  },
  {
    "text": "from dictionaries and hand-tailored heuristics. It applies statistical named entity recognition (NER) methods to the more challenging task of deidenti-",
    "acronyms": [
      [
        97,
        100
      ]
    ],
    "long-forms": [
      [
        71,
        95
      ]
    ],
    "ID": "357"
  },
  {
    "text": "(SO) weighting in the spirit of (Zhang et al, 2007; Li et al, 2007), and finally (f) the same system but with the target order (TO) weighting. ",
    "acronyms": [
      [
        128,
        130
      ],
      [
        1,
        3
      ]
    ],
    "long-forms": [
      [
        114,
        126
      ],
      [
        73,
        80
      ]
    ],
    "ID": "358"
  },
  {
    "text": "::= RL RL ::= prep rel loc word(S, object word) RL=rel loc word |",
    "acronyms": [
      [
        48,
        50
      ],
      [
        4,
        6
      ],
      [
        7,
        9
      ]
    ],
    "long-forms": [
      [
        51,
        58
      ]
    ],
    "ID": "359"
  },
  {
    "text": "Figure 4: Weight of links and category selection  3 .1  Representat ion  o f  OPED  The Oxford Pictorial English Dictionary(OPED) h,~s  very simple form of text and picture (Fig.3).",
    "acronyms": [
      [
        124,
        128
      ],
      [
        78,
        82
      ]
    ],
    "long-forms": [
      [
        88,
        122
      ]
    ],
    "ID": "360"
  },
  {
    "text": "Furthermore, if we can rank preferred verbs by domains, inferred verbs can be more useful to applications that focus on specific domains. Hence we defined Domain Verb Association (DVA) to measure how frequently inferred verbs are used with domain instances that can be used as subjects or",
    "acronyms": [
      [
        180,
        183
      ]
    ],
    "long-forms": [
      [
        155,
        178
      ]
    ],
    "ID": "361"
  },
  {
    "text": "follow-up study, Le Calvez (2007) compared KL to other indicators, namely the Jensen?Shannon divergence (JS) and the Bhattacharyya coefficient (BC).3 2.2 Lexical indicators",
    "acronyms": [
      [
        144,
        146
      ],
      [
        43,
        45
      ],
      [
        105,
        107
      ]
    ],
    "long-forms": [
      [
        117,
        142
      ]
    ],
    "ID": "362"
  },
  {
    "text": "that symbol?). The categories are:  1) Words as Words (WW): Within the context of  the sentence, the candidate phrase is used to ",
    "acronyms": [
      [
        55,
        57
      ]
    ],
    "long-forms": [
      [
        39,
        53
      ]
    ],
    "ID": "363"
  },
  {
    "text": "200 Acknowledgments This research is developed by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Qatar Foundation in collaboration with MIT.",
    "acronyms": [
      [
        84,
        87
      ],
      [
        134,
        138
      ]
    ],
    "long-forms": [
      [
        54,
        82
      ],
      [
        98,
        132
      ]
    ],
    "ID": "364"
  },
  {
    "text": "Net?s ontological concepts and the correct sense of each word assigned by a Word Sense Disambiguation(WSD) module to extract three sets of pat-",
    "acronyms": [
      [
        102,
        105
      ]
    ],
    "long-forms": [
      [
        76,
        100
      ]
    ],
    "ID": "365"
  },
  {
    "text": "guage model. Instead, with information gathered  from interviews of subject matter experts (SME's),  we developed a handwritten grammar using Gemini ",
    "acronyms": [
      [
        92,
        97
      ]
    ],
    "long-forms": [
      [
        68,
        90
      ]
    ],
    "ID": "366"
  },
  {
    "text": "The next most confident 600 tuples (i.e., those numbered 1201?1800) were used to build a development set (DEV1) and the next most confident 600 (those numbered 1801?2400) were used",
    "acronyms": [
      [
        106,
        110
      ]
    ],
    "long-forms": [
      [
        89,
        104
      ]
    ],
    "ID": "367"
  },
  {
    "text": " INTRODUCTION  Compound Nouns: Compound nouns (CNs) are a  commonly occurring construction in language ",
    "acronyms": [
      [
        47,
        50
      ]
    ],
    "long-forms": [
      [
        31,
        45
      ]
    ],
    "ID": "368"
  },
  {
    "text": "more details of this at the end of the next section.  4 Compiling the DiCo (dictionary-like) database into a lexical system",
    "acronyms": [
      [
        70,
        74
      ]
    ],
    "long-forms": [
      [
        76,
        86
      ]
    ],
    "ID": "369"
  },
  {
    "text": "in this paper. We choose the classical optimization algorithm limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989).",
    "acronyms": [
      [
        83,
        89
      ]
    ],
    "long-forms": [
      [
        62,
        81
      ]
    ],
    "ID": "370"
  },
  {
    "text": "provide an exhaustive list of connectives in the                                                    4 S=Subject; IO=Indirect Object; DO=Direct Object;  V=Verb; ERG=Ergative; DAT=Dative ",
    "acronyms": [
      [
        113,
        115
      ],
      [
        133,
        135
      ],
      [
        160,
        163
      ],
      [
        174,
        177
      ]
    ],
    "long-forms": [
      [
        116,
        131
      ],
      [
        136,
        149
      ],
      [
        104,
        111
      ],
      [
        154,
        158
      ],
      [
        164,
        172
      ],
      [
        178,
        184
      ]
    ],
    "ID": "371"
  },
  {
    "text": "Non Verbal Behaviors.   Sources abbreviated as: SPKR = Speaker; F = Friendship; V = Visibility; Rte = Route,  ",
    "acronyms": [
      [
        48,
        52
      ],
      [
        96,
        99
      ]
    ],
    "long-forms": [
      [
        55,
        62
      ],
      [
        68,
        78
      ],
      [
        84,
        94
      ],
      [
        102,
        107
      ]
    ],
    "ID": "372"
  },
  {
    "text": "Supertagging involves assigning words lexical entries based on a lexicalized grammatical theory, such as Combinatory Categorial Grammar (CCG) (Steedman, 2000) Tree-adjoining Grammar (Joshi,",
    "acronyms": [
      [
        137,
        140
      ]
    ],
    "long-forms": [
      [
        105,
        135
      ]
    ],
    "ID": "373"
  },
  {
    "text": "In Proc, of the lOth Pacific Asia  Conference on Language, Information and Com-  putation (PACLING), pages 163-172. ",
    "acronyms": [
      [
        91,
        98
      ],
      [
        3,
        7
      ]
    ],
    "long-forms": [
      [
        21,
        89
      ]
    ],
    "ID": "374"
  },
  {
    "text": "Beth Bryson). In addition, Robert Moore and Eric Jack-  son (SRI) provided critical help in designing and debug-  ging the code to support the minimal/maximal nswer ",
    "acronyms": [
      [
        61,
        64
      ]
    ],
    "long-forms": [],
    "ID": "375"
  },
  {
    "text": "plies that 58% of the time, our approach has improved the question relevance compared to that of the original candidate list (OList). ",
    "acronyms": [
      [
        126,
        131
      ]
    ],
    "long-forms": [
      [
        101,
        124
      ]
    ],
    "ID": "376"
  },
  {
    "text": "In the hierarchy of relations, Arguments (ARG) include Subject (SUBJ), Object (OBJ), Indirect Object (INDOBJ), Indirect Complement (IN-",
    "acronyms": [
      [
        64,
        68
      ],
      [
        79,
        82
      ],
      [
        42,
        45
      ],
      [
        102,
        108
      ]
    ],
    "long-forms": [
      [
        55,
        62
      ],
      [
        71,
        77
      ],
      [
        31,
        40
      ],
      [
        85,
        100
      ]
    ],
    "ID": "377"
  },
  {
    "text": "Higher log-likelihood corresponds to improved model fit. However, typi-cally it is desirable to penalize a higher number of hidden states, since increasing the model complexi-ty results in tradeoffs that may not be fully warrant-ed by the improvement in model fit. In this work, we utilize the Akaike Information Criterion (AIC), a standard penalized log-likelihood metric (Akaike, 1976).     ",
    "acronyms": [
      [
        324,
        327
      ]
    ],
    "long-forms": [
      [
        294,
        322
      ]
    ],
    "ID": "378"
  },
  {
    "text": "and van Zaanen, 2006).  A Logical Graph (LG) is a directed, bipartite graph with two types of vertices, concepts and re-",
    "acronyms": [
      [
        41,
        43
      ]
    ],
    "long-forms": [
      [
        26,
        39
      ]
    ],
    "ID": "379"
  },
  {
    "text": " ? Abbreviation features (ABB): For every term in the noslang dictionary, we checked whether",
    "acronyms": [
      [
        26,
        29
      ]
    ],
    "long-forms": [
      [
        3,
        15
      ]
    ],
    "ID": "380"
  },
  {
    "text": "tion.   In our decoders, language model(LM) is used  for translating edus in Formula(5),(6),(7),(8), but ",
    "acronyms": [
      [
        40,
        42
      ]
    ],
    "long-forms": [
      [
        25,
        38
      ]
    ],
    "ID": "381"
  },
  {
    "text": " The two subgraphs are a parse structure subgraph (PSS) and a linear order subgraph (LOS). ",
    "acronyms": [
      [
        85,
        88
      ],
      [
        51,
        54
      ]
    ],
    "long-forms": [
      [
        62,
        83
      ],
      [
        25,
        49
      ]
    ],
    "ID": "382"
  },
  {
    "text": "The proposals directly affect the organization o f   groups w i t h i n  EOP including the Office of Telecommmicatians Policy  (UP) ; the 0fiice of Science and Technology Policy (OSTP) ; the Intergovernmental  Science, Engineering, and Technology Advisory Panel (ISETAP) ; the President s ",
    "acronyms": [
      [
        179,
        183
      ],
      [
        73,
        76
      ],
      [
        128,
        130
      ],
      [
        263,
        269
      ]
    ],
    "long-forms": [
      [
        138,
        177
      ],
      [
        191,
        260
      ]
    ],
    "ID": "383"
  },
  {
    "text": "Machine learning in automated text categorization. In ACM computing surveys (CSUR). ",
    "acronyms": [
      [
        77,
        81
      ],
      [
        54,
        57
      ]
    ],
    "long-forms": [
      [
        58,
        75
      ]
    ],
    "ID": "384"
  },
  {
    "text": "4.1 Base Extraction As first step of the mention selection stage, we extracted all the noun phrases (NP), pronouns (PRP), and possessive pronouns (PRP$) for English and",
    "acronyms": [
      [
        101,
        103
      ]
    ],
    "long-forms": [
      [
        87,
        99
      ]
    ],
    "ID": "385"
  },
  {
    "text": "strict our attention to documents on this topic.  Thomson Reuter?s Web of Science (WOS), a database of scientific journal and conference arti-",
    "acronyms": [
      [
        83,
        86
      ]
    ],
    "long-forms": [
      [
        67,
        81
      ]
    ],
    "ID": "386"
  },
  {
    "text": "It has emerged in the context of information extraction (IE) and text mining (TM). The automatic recog-",
    "acronyms": [
      [
        78,
        80
      ],
      [
        57,
        59
      ]
    ],
    "long-forms": [
      [
        65,
        76
      ],
      [
        33,
        55
      ]
    ],
    "ID": "387"
  },
  {
    "text": "Format: type_of_word TAG type_of_word TAG ...  NN = Noun, NN-PL = Plural Noun  DET = Determiner, PREP = Preposition  POS = Possessive, J J = Adjective ",
    "acronyms": [
      [
        79,
        82
      ],
      [
        97,
        101
      ],
      [
        21,
        24
      ],
      [
        38,
        41
      ],
      [
        47,
        49
      ],
      [
        58,
        63
      ],
      [
        117,
        120
      ],
      [
        135,
        138
      ]
    ],
    "long-forms": [
      [
        85,
        95
      ],
      [
        104,
        115
      ],
      [
        52,
        56
      ],
      [
        66,
        77
      ],
      [
        123,
        133
      ],
      [
        141,
        150
      ]
    ],
    "ID": "388"
  },
  {
    "text": "age caption generator. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June.",
    "acronyms": [
      [
        90,
        94
      ],
      [
        30,
        34
      ]
    ],
    "long-forms": [
      [
        49,
        88
      ]
    ],
    "ID": "389"
  },
  {
    "text": "F = F-Measure with Recall and Precision Weighted Equally  J = Japanese S = Spanish  ME = Microelectronics  Mul t i l ingua l  ",
    "acronyms": [
      [
        84,
        86
      ]
    ],
    "long-forms": [
      [
        89,
        105
      ],
      [
        4,
        13
      ],
      [
        62,
        70
      ],
      [
        75,
        82
      ]
    ],
    "ID": "390"
  },
  {
    "text": " Generation Challenges 2010 brought together three sets of STECs: the three GREC Challenges, GREC Named Entity Generation (GREC-NEG), Named Entity Reference Detection (GREC-NER), and Named Entity Reference Regeneration",
    "acronyms": [
      [
        123,
        131
      ],
      [
        59,
        64
      ],
      [
        76,
        80
      ],
      [
        168,
        176
      ]
    ],
    "long-forms": [
      [
        93,
        121
      ],
      [
        183,
        218
      ]
    ],
    "ID": "391"
  },
  {
    "text": "(the concept <food,nutrient>).  The aim of using Selectional Preferences (SP) in SRL is to generalize from the argument heads in",
    "acronyms": [
      [
        74,
        76
      ],
      [
        81,
        84
      ]
    ],
    "long-forms": [
      [
        49,
        72
      ]
    ],
    "ID": "392"
  },
  {
    "text": "the use of lexical cohesion devices in each version of MT and HT in terms of the following two ratios, LC = lexical cohesion devices / content words, RC = repetition / content words.",
    "acronyms": [
      [
        103,
        105
      ],
      [
        55,
        57
      ],
      [
        62,
        64
      ],
      [
        150,
        152
      ]
    ],
    "long-forms": [
      [
        108,
        124
      ],
      [
        155,
        175
      ]
    ],
    "ID": "393"
  },
  {
    "text": "  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177?182, October 25-29, 2014, Doha, Qatar.",
    "acronyms": [
      [
        90,
        95
      ]
    ],
    "long-forms": [
      [
        40,
        88
      ]
    ],
    "ID": "394"
  },
  {
    "text": "pus (EBC) (Christodouloupoulos and Steedman, 2014), and our own collection of online texts published by the Wathctower Society (WTC). ",
    "acronyms": [
      [
        128,
        131
      ],
      [
        5,
        8
      ]
    ],
    "long-forms": [
      [
        108,
        126
      ]
    ],
    "ID": "395"
  },
  {
    "text": "utes of training time on an average laptop computer.  This model, the deep averaging network (DAN), works in three simple steps:",
    "acronyms": [
      [
        94,
        97
      ]
    ],
    "long-forms": [
      [
        70,
        92
      ]
    ],
    "ID": "396"
  },
  {
    "text": "pose augmenting queries in the style of relevance  feedback (Salton and Buckley 1990), Kalashnikov  (2007) treat Web Person Search (WePS) as a disambiguation problem whose objective is to distin-",
    "acronyms": [
      [
        132,
        136
      ]
    ],
    "long-forms": [
      [
        113,
        130
      ]
    ],
    "ID": "397"
  },
  {
    "text": "derstand what is being tested. In projects where independent verification and validation (IVV) is required this might be a problem, as most stake-",
    "acronyms": [
      [
        90,
        93
      ]
    ],
    "long-forms": [
      [
        49,
        88
      ]
    ],
    "ID": "398"
  },
  {
    "text": "ken varieties of Arabic, are still lacking. We present ELISSA, a machine translation (MT) system for DA to MSA.",
    "acronyms": [
      [
        86,
        88
      ]
    ],
    "long-forms": [
      [
        65,
        84
      ]
    ],
    "ID": "399"
  },
  {
    "text": " CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority",
    "acronyms": [
      [
        81,
        84
      ],
      [
        1,
        6
      ]
    ],
    "long-forms": [
      [
        51,
        79
      ]
    ],
    "ID": "400"
  },
  {
    "text": "corpora. For the preposition and determiner errors, we adopt a statistical machine translation (SMT)based approach, aiming at correcting errors in con-",
    "acronyms": [
      [
        96,
        99
      ]
    ],
    "long-forms": [
      [
        63,
        94
      ]
    ],
    "ID": "401"
  },
  {
    "text": "umn.  Log frequency (LF) and global entropy (GE) are correlated.",
    "acronyms": [
      [
        21,
        23
      ],
      [
        45,
        47
      ]
    ],
    "long-forms": [
      [
        6,
        19
      ],
      [
        29,
        43
      ]
    ],
    "ID": "402"
  },
  {
    "text": "2002. DAML agent semantic communications service (ASCS) http://oak.teknowledge.com:8080/daml/damlquery.jsp",
    "acronyms": [
      [
        50,
        54
      ],
      [
        6,
        10
      ]
    ],
    "long-forms": [
      [
        11,
        48
      ]
    ],
    "ID": "403"
  },
  {
    "text": "The most common approach to document classification is to fit a linear model (e.g., Logistic Regression) over bag of words (BoW) features. To",
    "acronyms": [
      [
        124,
        127
      ]
    ],
    "long-forms": [
      [
        110,
        122
      ]
    ],
    "ID": "404"
  },
  {
    "text": "tences, or the NP is an exophora, this is NULL.  Speaker Binary Whether the speakers of the predicate and the NP are the same (SAME) or not (OTHER).",
    "acronyms": [
      [
        127,
        131
      ],
      [
        42,
        46
      ],
      [
        110,
        112
      ]
    ],
    "long-forms": [
      [
        121,
        125
      ]
    ],
    "ID": "405"
  },
  {
    "text": "ton (within the Perseus Digital Library) based on  texts of the Classical era (Bamman, 2006), and  the Index Thomisticus Treebank (IT-TB) at the  Catholic University of the Sacred Heart in Milan, ",
    "acronyms": [
      [
        131,
        136
      ]
    ],
    "long-forms": [
      [
        103,
        129
      ]
    ],
    "ID": "406"
  },
  {
    "text": " University of Chicago Given a constraint set with k constraints in the framework of Optimality Theory (OT), what is its capacity as a classification scheme for linguistic data?",
    "acronyms": [
      [
        104,
        106
      ]
    ],
    "long-forms": [
      [
        85,
        102
      ]
    ],
    "ID": "407"
  },
  {
    "text": " Figure 4: Algorithm for calculating the F-measure confusion matrix of True Positives (T.P.), False Positives (F.P.), True Negatives (T.N.), and False Negatives (F.N.). The ranking technique described in this paper creates a list of",
    "acronyms": [
      [
        134,
        138
      ],
      [
        162,
        166
      ],
      [
        87,
        92
      ],
      [
        111,
        115
      ]
    ],
    "long-forms": [
      [
        118,
        132
      ],
      [
        145,
        160
      ],
      [
        71,
        85
      ],
      [
        94,
        109
      ]
    ],
    "ID": "408"
  },
  {
    "text": "scopes are not necessarily contiguous.  Conditional Random Field (CRF) sequence tag? ",
    "acronyms": [
      [
        66,
        69
      ]
    ],
    "long-forms": [
      [
        40,
        64
      ]
    ],
    "ID": "409"
  },
  {
    "text": "evaluating work in speech recognition (SR), but until  recently no community-wide methodology existed for either  natural language (NL) researchers or speech understanding  (SU) researchers for evaluating the systems they developed.",
    "acronyms": [
      [
        132,
        134
      ],
      [
        39,
        41
      ],
      [
        174,
        176
      ]
    ],
    "long-forms": [
      [
        114,
        130
      ],
      [
        19,
        37
      ],
      [
        151,
        171
      ]
    ],
    "ID": "410"
  },
  {
    "text": "man evaluators 5.2 Mean Absolute Difference Analysis Here we calculated the mean absolute difference(MAD) between a human rater?s evaluation and SELSA (LSA)",
    "acronyms": [
      [
        101,
        104
      ],
      [
        145,
        150
      ],
      [
        152,
        155
      ]
    ],
    "long-forms": [
      [
        76,
        100
      ]
    ],
    "ID": "411"
  },
  {
    "text": "question answering over RDF data. In World Wide Web (WWW), pages 639?648. ",
    "acronyms": [
      [
        53,
        56
      ],
      [
        24,
        27
      ]
    ],
    "long-forms": [
      [
        37,
        51
      ]
    ],
    "ID": "412"
  },
  {
    "text": "2 mark up tool in Shakti Standard Format (SSF) (Bharati et al, 2005). For",
    "acronyms": [
      [
        42,
        45
      ]
    ],
    "long-forms": [
      [
        18,
        40
      ]
    ],
    "ID": "413"
  },
  {
    "text": "word in context. In this article we present a WSD algorithm based on random walks over large Lexical Knowledge Bases (LKB). We show that our algorithm performs better than other graph-",
    "acronyms": [
      [
        118,
        121
      ],
      [
        46,
        49
      ]
    ],
    "long-forms": [
      [
        93,
        116
      ]
    ],
    "ID": "414"
  },
  {
    "text": " 1.1 Elman and RCC networks  Simple recurrent networks (SRN's) of the Elman type  are similar to three-layer perceptrons but with recurrent ",
    "acronyms": [
      [
        56,
        61
      ]
    ],
    "long-forms": [
      [
        29,
        54
      ]
    ],
    "ID": "415"
  },
  {
    "text": "methods.  For example, consider the ways in which evaluation of machine translation (MT) systems is carried out.",
    "acronyms": [
      [
        85,
        87
      ]
    ],
    "long-forms": [
      [
        64,
        83
      ]
    ],
    "ID": "416"
  },
  {
    "text": " 4.1. Document Input (DI)  The DI process is the interface between ADEPT ",
    "acronyms": [
      [
        22,
        24
      ],
      [
        67,
        72
      ],
      [
        31,
        33
      ]
    ],
    "long-forms": [
      [
        6,
        20
      ]
    ],
    "ID": "417"
  },
  {
    "text": "method does not select any dimension.  Median Selection (MSel). As a further method",
    "acronyms": [
      [
        57,
        61
      ]
    ],
    "long-forms": [
      [
        39,
        55
      ]
    ],
    "ID": "418"
  },
  {
    "text": "1 Introduction Many algorithms in speech and language processing can be viewed as instances of dynamic programming (DP) (Bellman, 1957). The basic idea of",
    "acronyms": [
      [
        116,
        118
      ]
    ],
    "long-forms": [
      [
        95,
        114
      ]
    ],
    "ID": "419"
  },
  {
    "text": "reasoning? In International Conference on Learning Representations (ICLR). ",
    "acronyms": [
      [
        68,
        72
      ]
    ],
    "long-forms": [
      [
        14,
        66
      ]
    ],
    "ID": "420"
  },
  {
    "text": " 2.2 DIRT data The DIRT (Discovering Inference Rules from Text) method is based on extending Harris Distributional",
    "acronyms": [
      [
        19,
        23
      ],
      [
        5,
        9
      ]
    ],
    "long-forms": [
      [
        25,
        62
      ]
    ],
    "ID": "421"
  },
  {
    "text": "2 Classification Algorithms  2.1 Conditional Random Fields  Conditional random field (CRF) was an extension  of both Maximum Entropy Model (MEMs) and ",
    "acronyms": [
      [
        86,
        89
      ],
      [
        140,
        144
      ]
    ],
    "long-forms": [
      [
        60,
        84
      ],
      [
        117,
        138
      ]
    ],
    "ID": "422"
  },
  {
    "text": "7 (Ogren and Bethard, 2009) can be used to design and execute pipelines made up of a sequence of AEs (and potentially some more complex flows), and UIMA-AS 8",
    "acronyms": [
      [
        97,
        100
      ],
      [
        148,
        155
      ]
    ],
    "long-forms": [
      [
        102,
        122
      ]
    ],
    "ID": "423"
  },
  {
    "text": "LM. This approach has been successfully applied in automatic speech recognition (ASR) (Tam and Schultz, 2006) using the Latent Dirichlet Alloca-",
    "acronyms": [
      [
        81,
        84
      ],
      [
        0,
        2
      ]
    ],
    "long-forms": [
      [
        51,
        79
      ]
    ],
    "ID": "424"
  },
  {
    "text": "in all kind of NLP applications. As a particular case, Semantic Role Labeling (SRL) is currently a welldefined task with a substantial body of work and",
    "acronyms": [
      [
        79,
        82
      ],
      [
        15,
        18
      ]
    ],
    "long-forms": [
      [
        55,
        77
      ]
    ],
    "ID": "425"
  },
  {
    "text": "and answer sentences that emphasizes three types  of metadata:   (i) Main Verbs (MVerb), identified by the link  parser (Sleator and Temperley 1993);  ",
    "acronyms": [
      [
        81,
        86
      ]
    ],
    "long-forms": [
      [
        69,
        79
      ]
    ],
    "ID": "426"
  },
  {
    "text": "controlled vocabulary. As an application of the  Resource Description Framework (RDF), SKOS  allows concepts to be composed and published on the ",
    "acronyms": [
      [
        81,
        84
      ],
      [
        87,
        91
      ]
    ],
    "long-forms": [
      [
        49,
        79
      ]
    ],
    "ID": "427"
  },
  {
    "text": "scheme category in the corpus: Results (RES) is by far the most frequent zone (accounting for 40% of the corpus), while Background (BKG), Objective (OBJ), Method (METH) and Conclusion (CON) cover",
    "acronyms": [
      [
        132,
        135
      ],
      [
        40,
        43
      ],
      [
        149,
        152
      ],
      [
        163,
        167
      ],
      [
        185,
        188
      ]
    ],
    "long-forms": [
      [
        120,
        130
      ],
      [
        31,
        37
      ],
      [
        138,
        147
      ],
      [
        155,
        161
      ],
      [
        173,
        183
      ]
    ],
    "ID": "428"
  },
  {
    "text": "1994). Following (Collins, 2002), we used sections 0-18 of the Wall Street Journal (WSJ) corpus for training, sections 19-21 for development, and",
    "acronyms": [
      [
        84,
        87
      ]
    ],
    "long-forms": [
      [
        63,
        82
      ]
    ],
    "ID": "429"
  },
  {
    "text": "For our experiments, we collect debate posts from four popular domains, Abortion (ABO), Gay Rights (GAY), Obama (OBA), and Marijuana (MAR), from an online debate forum1.",
    "acronyms": [
      [
        100,
        103
      ],
      [
        113,
        116
      ],
      [
        82,
        85
      ],
      [
        134,
        137
      ]
    ],
    "long-forms": [
      [
        88,
        91
      ],
      [
        106,
        111
      ],
      [
        72,
        80
      ],
      [
        123,
        132
      ]
    ],
    "ID": "430"
  },
  {
    "text": "rlt*s*rle TR AN SFORHATIONS *S***   SCAN CALLED AT 1 I  ANTEST CALLED FOR 4l'I NG l1 (AACC) ,SD= 5. RES= 0.",
    "acronyms": [
      [
        70,
        73
      ],
      [
        86,
        90
      ],
      [
        93,
        95
      ],
      [
        100,
        103
      ]
    ],
    "long-forms": [
      [
        56,
        69
      ]
    ],
    "ID": "431"
  },
  {
    "text": "P,,nnsylvania (UPenn) Treebank. We plan to include  the parsed ICE-GB (Great Britain component of ICE)  and the BNC (British National Corpus) in the project ",
    "acronyms": [
      [
        67,
        69
      ],
      [
        98,
        101
      ],
      [
        112,
        115
      ],
      [
        15,
        20
      ]
    ],
    "long-forms": [
      [
        71,
        84
      ],
      [
        117,
        140
      ],
      [
        0,
        13
      ]
    ],
    "ID": "432"
  },
  {
    "text": "lief chunk), B-NCB (Beginning of non committed belief chunk), I-NCB (Inside of a non committed belief chunk), B-NA (Beginning of a not applicable chunk), I-NA (Inside a not applicable",
    "acronyms": [
      [
        110,
        114
      ],
      [
        13,
        18
      ],
      [
        62,
        67
      ],
      [
        154,
        158
      ]
    ],
    "long-forms": [
      [
        116,
        130
      ],
      [
        20,
        59
      ],
      [
        69,
        107
      ],
      [
        160,
        183
      ]
    ],
    "ID": "433"
  },
  {
    "text": "5 Experiments and Results 5.1 Data We use the British National Corpus (BNC),3 which contains 100M words, because it draws its",
    "acronyms": [
      [
        71,
        74
      ]
    ],
    "long-forms": [
      [
        46,
        69
      ]
    ],
    "ID": "434"
  },
  {
    "text": "  An Underspecified Segmented Discourse Representation Theory (USDRT)  Frank Schilder ",
    "acronyms": [
      [
        63,
        68
      ]
    ],
    "long-forms": [
      [
        5,
        61
      ]
    ],
    "ID": "435"
  },
  {
    "text": "gel Balaban,  ? hsan Yal??nkaya Middle East Technical University, Ankara, Turkey and   ? mit Deniz Turan Anadolu University, Eski?ehir, Turkey  Corresponding author: dezeyrek@metu.edu.tr  Abstract  In this paper, we report on the annotation procedures we developed for annotating the Turkish Discourse Bank (TDB), an effort that extends the Penn Discourse Tree Bank (PDTB) annotation style by using it for annotating Turkish discourse. After a brief introduction to the TDB, we describe the annotation cycle and the annotation scheme we developed, defining which parts of the scheme are an extension of the PDTB and which parts are different.",
    "acronyms": [
      [
        308,
        311
      ],
      [
        367,
        371
      ],
      [
        470,
        473
      ],
      [
        607,
        611
      ]
    ],
    "long-forms": [
      [
        284,
        306
      ],
      [
        341,
        365
      ]
    ],
    "ID": "436"
  },
  {
    "text": "siderable interest in robust knowledge extraction, both as an end in itself and as an intermediate step in a variety of Natural Language Processing (NLP) applications.",
    "acronyms": [
      [
        149,
        152
      ]
    ],
    "long-forms": [
      [
        120,
        147
      ]
    ],
    "ID": "437"
  },
  {
    "text": "features. Stolcke et al (1998) then expanded the prosodic tree model with a hidden event language model (LM) to identify sentence boundaries, filled pauses and IPs in",
    "acronyms": [
      [
        105,
        107
      ]
    ],
    "long-forms": [
      [
        89,
        103
      ]
    ],
    "ID": "438"
  },
  {
    "text": "The stated  goal for this language model adaptation spoke was \"to  evaluate an incremental supervised language model (LM)  adaptation algorithm on a problem of sublanguage ",
    "acronyms": [
      [
        118,
        120
      ]
    ],
    "long-forms": [
      [
        102,
        116
      ]
    ],
    "ID": "439"
  },
  {
    "text": "Synthesis) is a learning system \\[12,13,14\\] which  consists of a learning element (Meta-XMAS), a  knowledge base (KB), and two inference ngines  of a morphological nalyzer (MOA) and a mor- ",
    "acronyms": [
      [
        115,
        117
      ]
    ],
    "long-forms": [
      [
        99,
        113
      ]
    ],
    "ID": "440"
  },
  {
    "text": "errors; English had three, and German one.   While it is worth noting that (II) is not without  counterexamples, it is significant that true ",
    "acronyms": [
      [
        76,
        78
      ]
    ],
    "long-forms": [
      [
        54,
        69
      ]
    ],
    "ID": "441"
  },
  {
    "text": "We need to \"cross\" an instrument the_phone to reach another person, thus we also refer to the intermediate locus IME(LOC) in the domain of communication.",
    "acronyms": [
      [
        117,
        120
      ],
      [
        113,
        116
      ]
    ],
    "long-forms": [
      [
        107,
        112
      ]
    ],
    "ID": "442"
  },
  {
    "text": "5 Tools for the development of the prototype experiment For the practical development of our prototype experiments we are considering to use the upper cate-gories of the NIFSTD ontology and wikis as a collaborative tool. The availability and suitability for our research of the former has been considered in Maroto (2013).  NIFSTD (NIF Standard) ontology stands out as the most comprehensive ontology of the neurosci-ences available on the web. Its wide coverage and its degree of normalisation and reusability make this ontology particularly suitable for our research purposes.",
    "acronyms": [
      [
        324,
        330
      ],
      [
        170,
        176
      ]
    ],
    "long-forms": [
      [
        332,
        344
      ]
    ],
    "ID": "443"
  },
  {
    "text": "results are reported together with the error propagation from argument position classification for Same Sentence (SS), Previous Sentence (PS) models and joined results (ALL) as precision (P), recall",
    "acronyms": [
      [
        114,
        116
      ],
      [
        138,
        140
      ],
      [
        169,
        172
      ],
      [
        188,
        190
      ]
    ],
    "long-forms": [
      [
        99,
        112
      ],
      [
        119,
        136
      ],
      [
        177,
        186
      ]
    ],
    "ID": "444"
  },
  {
    "text": "354  Table 1: Precision and recMI tables for experiments starting with words-only queries (Words) through phrase (Del l )   and word (Del2) deletion to proper noun (Caps) and noun phrase (NP) grouping. The queries were evaluated on ",
    "acronyms": [
      [
        188,
        190
      ],
      [
        134,
        138
      ],
      [
        114,
        119
      ]
    ],
    "long-forms": [
      [
        175,
        186
      ]
    ],
    "ID": "445"
  },
  {
    "text": " 1 Introduction Minimum error rate training (MERT)?also known as direct loss minimization in machine learning?is a",
    "acronyms": [
      [
        45,
        49
      ]
    ],
    "long-forms": [
      [
        16,
        43
      ]
    ],
    "ID": "446"
  },
  {
    "text": "All conditions were assigned a section and are  thereby excluded. TE = temporal expression; TT = trigger term; V = scoped by verb.",
    "acronyms": [
      [
        66,
        68
      ],
      [
        92,
        94
      ]
    ],
    "long-forms": [
      [
        71,
        90
      ],
      [
        97,
        109
      ],
      [
        125,
        129
      ]
    ],
    "ID": "447"
  },
  {
    "text": "   1 Introduction  Named Entities (NEs) play a critical role in many  Natural Language Processing and Information ",
    "acronyms": [
      [
        35,
        38
      ]
    ],
    "long-forms": [
      [
        19,
        33
      ]
    ],
    "ID": "448"
  },
  {
    "text": "recognition) which describes a relationship between an input signal sequence and a word, the other is a language model (LM) which measures the likelihood of a sequence of words as a sen-",
    "acronyms": [
      [
        120,
        122
      ]
    ],
    "long-forms": [
      [
        104,
        118
      ]
    ],
    "ID": "449"
  },
  {
    "text": "jansche.1@osu.edu 1 Introduction Our approach to multilingual named entity (NE) recognition in the context of the CoNLL Shared",
    "acronyms": [
      [
        76,
        78
      ]
    ],
    "long-forms": [
      [
        62,
        74
      ],
      [
        114,
        119
      ]
    ],
    "ID": "450"
  },
  {
    "text": "  The word sea is ambiguous and has three senses as  given in the Princeton Wordnet (PWN):  S1: (n) sea (a division of an ocean or a large body ",
    "acronyms": [
      [
        85,
        88
      ]
    ],
    "long-forms": [
      [
        66,
        83
      ]
    ],
    "ID": "451"
  },
  {
    "text": "Other language technology applications, such as Question Answering (QA) systems or information retrieval (IR) systems, also suffer from the poor contextual disambiguation of word senses.",
    "acronyms": [
      [
        106,
        108
      ],
      [
        68,
        70
      ]
    ],
    "long-forms": [
      [
        83,
        104
      ],
      [
        48,
        66
      ]
    ],
    "ID": "452"
  },
  {
    "text": "Contextual relationship attributes:  1. Prefix Counted Rule (PRC): The selected  sense is the most commonly appended sense by the ",
    "acronyms": [
      [
        61,
        64
      ]
    ],
    "long-forms": [
      [
        40,
        54
      ]
    ],
    "ID": "453"
  },
  {
    "text": "metric TF*S~, since we base the importance of a  ? SF = Segment frequency (How many segments does  the term occur in) ",
    "acronyms": [
      [
        51,
        53
      ],
      [
        7,
        11
      ]
    ],
    "long-forms": [
      [
        56,
        73
      ]
    ],
    "ID": "454"
  },
  {
    "text": " {UK, US, AU}.   Figure 1: Proposed architecture      Figure 2: Baseline GMM based dialect classification 5.2 Latent Semantic Analysis for Dialect ID One approach used to address topic classification problems has been latent semantic analysis (LSA), which was first explored for document indexing in (Deerwester et al, 1990). This addresses the issues of synonymy - many ways to refer to the same idea and polysemy ?",
    "acronyms": [
      [
        244,
        247
      ],
      [
        2,
        4
      ],
      [
        6,
        8
      ],
      [
        10,
        12
      ],
      [
        73,
        76
      ],
      [
        147,
        149
      ]
    ],
    "long-forms": [
      [
        218,
        242
      ]
    ],
    "ID": "455"
  },
  {
    "text": "Tu?r, Oflazer, and Tu?r 2002; Oflazer 2003; Oflazer et al 2003; Eryig?it and Oflazer 2006) has represented the morphological structure of Turkish words by splitting them into inflectional groups (IGs). The root and derivational elements of a word are represented",
    "acronyms": [
      [
        196,
        199
      ]
    ],
    "long-forms": [
      [
        175,
        194
      ]
    ],
    "ID": "456"
  },
  {
    "text": "to match portions of a PAS.  3.2 The Shallow Semantic Tree Kernel (SSTK) The SSTK is based on two ideas: first, we change",
    "acronyms": [
      [
        67,
        71
      ],
      [
        23,
        26
      ],
      [
        77,
        81
      ]
    ],
    "long-forms": [
      [
        37,
        65
      ]
    ],
    "ID": "457"
  },
  {
    "text": "However, this hypothesis is reasonable if the  monolingual wordnets are reliable and correctly  linked to the interlingual index (ILI). Quality ",
    "acronyms": [
      [
        130,
        133
      ]
    ],
    "long-forms": [
      [
        110,
        128
      ]
    ],
    "ID": "458"
  },
  {
    "text": "Main verb of main clause (MV)F8. Boolean feature for MV (BMV)F9. Previous sentence feature (PREV)Additional feature used only for Arg1F10. Arg2 Labels",
    "acronyms": [
      [
        92,
        96
      ],
      [
        26,
        28
      ],
      [
        57,
        60
      ],
      [
        130,
        137
      ],
      [
        139,
        143
      ]
    ],
    "long-forms": [
      [
        65,
        73
      ],
      [
        0,
        9
      ],
      [
        33,
        55
      ]
    ],
    "ID": "459"
  },
  {
    "text": "projected expectations from English. To this end, we adopt the Generalized Expectation (GE) Criteria framework introduced by Mann and McCallum",
    "acronyms": [
      [
        88,
        90
      ]
    ],
    "long-forms": [
      [
        63,
        86
      ]
    ],
    "ID": "460"
  },
  {
    "text": "SOl,licit(co+ t{ach of these phrases conlpriscs a contigtlous  sequence o1 tags that satisfies a strut+h: gral,illilar, l\"or  example, a II(,itlll pluase eltil be simi)ly a plonoull  t~ig or (,in  all:l itlaly setitlellce (:,I lie(It1 lind ad.iective lags, pms ib ly  ",
    "acronyms": [
      [
        137,
        139
      ]
    ],
    "long-forms": [
      [
        141,
        159
      ]
    ],
    "ID": "461"
  },
  {
    "text": "(Person Names) by prometheus e. V.16, and Getty ULAN (United List of Artist Names)17 There are two modes of use for name authorities",
    "acronyms": [
      [
        48,
        52
      ]
    ],
    "long-forms": [
      [
        54,
        84
      ]
    ],
    "ID": "462"
  },
  {
    "text": "procedures\" (ReP). The RePs work on a memory structure which is adequate for the  representation of knowledge about objects, the \"referential net\" (RefN) 4*. A RefN ",
    "acronyms": [
      [
        148,
        152
      ],
      [
        13,
        16
      ],
      [
        23,
        27
      ],
      [
        160,
        164
      ]
    ],
    "long-forms": [
      [
        130,
        145
      ]
    ],
    "ID": "463"
  },
  {
    "text": "rates of sentences (46.1%) (53.9%) (40.4%) (59.6%) (70.7%) (29.3%) (54.3%) (45.7%) (64.3%) (35.7%) supervised CRF (baseline) 46.78 60.99 48.57 60.01 56.92 67.91 79.60 97.35 75.69 91.03 JESS-CM (CRF/HMM) 49.02 62.60 50.79 61.24 62.47 71.30 85.87 97.47 80.84 92.85 (gain from supervised CRF) (+2.24) (+1.61) (+2.22) (+1.23) (+5.55) (+3.40) (+6.27) (+0.12) (+5.15) (+1.82)",
    "acronyms": [
      [
        194,
        201
      ],
      [
        185,
        192
      ]
    ],
    "long-forms": [],
    "ID": "464"
  },
  {
    "text": "[byline : SAN SALVADOR, 19 APR 89 (ACAN-EFE) --] [bracket : [TEXT]] [fullname : SALVADORAN PRESIDENT-ELECT ALFREDO CRISTIIII] CONDEMNED THE TERRORIST KILLING OF [fullname : ATTORIEY GENERAL ROBERTO GARCI A ALVARADO] AND (comp : ACCUSED THE FARABUIDO MARTI NATIONAL LIBERATION FROIT [bracket : (FMLN)) OF) THE CRIME . ",
    "acronyms": [
      [
        294,
        298
      ]
    ],
    "long-forms": [
      [
        240,
        275
      ]
    ],
    "ID": "465"
  },
  {
    "text": "BLEU uses 2 reference translations. WER=word error rate, PER=position independent WER. ",
    "acronyms": [
      [
        82,
        85
      ]
    ],
    "long-forms": [
      [
        61,
        81
      ]
    ],
    "ID": "466"
  },
  {
    "text": " In (Speriosu et al., 2011), a label propagation (LProp) approach is proposed, while Go et al. ( 2009)",
    "acronyms": [
      [
        50,
        55
      ]
    ],
    "long-forms": [
      [
        31,
        48
      ]
    ],
    "ID": "467"
  },
  {
    "text": "eugene@mathcs.emory.edu Abstract Community question answering (CQA) websites contain millions of question and answer",
    "acronyms": [
      [
        63,
        66
      ]
    ],
    "long-forms": [
      [
        33,
        61
      ]
    ],
    "ID": "468"
  },
  {
    "text": " means a type of source ? newswire (NW), broadcast news (BN), broadcast conversation (BC), mag-",
    "acronyms": [
      [
        36,
        38
      ],
      [
        57,
        59
      ],
      [
        86,
        88
      ]
    ],
    "long-forms": [
      [
        26,
        34
      ],
      [
        41,
        55
      ],
      [
        62,
        84
      ]
    ],
    "ID": "469"
  },
  {
    "text": "either lexically encoded, or depends on the intrinsic properties of G, or coincides with a salient VPT (viewpoint). In striking contrast with",
    "acronyms": [
      [
        99,
        102
      ]
    ],
    "long-forms": [
      [
        104,
        113
      ]
    ],
    "ID": "470"
  },
  {
    "text": "Abstract  This paper attempts to use an off-the-shelf  anaphora resolution (AR) system for Bengali. ",
    "acronyms": [
      [
        76,
        78
      ]
    ],
    "long-forms": [
      [
        55,
        74
      ]
    ],
    "ID": "471"
  },
  {
    "text": "The SemEval?2007 task for extracting frame semantic structures relies on the human annotated data available in the FrameNet (FN) database. The",
    "acronyms": [
      [
        125,
        127
      ],
      [
        4,
        11
      ]
    ],
    "long-forms": [
      [
        115,
        123
      ]
    ],
    "ID": "472"
  },
  {
    "text": "hauer, haver, haber) and the corpus does not contain any other linguistic information, such as lemma and part of speech (PoS). ",
    "acronyms": [
      [
        121,
        124
      ]
    ],
    "long-forms": [
      [
        105,
        119
      ]
    ],
    "ID": "473"
  },
  {
    "text": "  The ungrammatical distracter, e.g., are in Figure 1,  has a different part of speech (POS) than the correct answer germs.",
    "acronyms": [
      [
        88,
        91
      ]
    ],
    "long-forms": [
      [
        72,
        86
      ]
    ],
    "ID": "474"
  },
  {
    "text": "Words/Phrases as Themselves (WD)  Symbols/Nonliteral Marks (SY)  Phonetic/Sound (PH)  Spelling (SP) ",
    "acronyms": [
      [
        81,
        83
      ],
      [
        29,
        31
      ],
      [
        60,
        62
      ],
      [
        96,
        98
      ]
    ],
    "long-forms": [
      [
        65,
        73
      ],
      [
        34,
        41
      ],
      [
        86,
        94
      ]
    ],
    "ID": "475"
  },
  {
    "text": "DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department,  GI = Operative Gastrointestinal, RAD = Radiology and  SP = Surgical Pathology. (%)",
    "acronyms": [
      [
        109,
        112
      ],
      [
        0,
        2
      ],
      [
        25,
        29
      ],
      [
        48,
        50
      ],
      [
        76,
        78
      ],
      [
        130,
        132
      ]
    ],
    "long-forms": [
      [
        115,
        128
      ],
      [
        5,
        22
      ],
      [
        32,
        46
      ],
      [
        53,
        73
      ],
      [
        91,
        107
      ],
      [
        135,
        153
      ]
    ],
    "ID": "476"
  },
  {
    "text": "mdiab@ccls.columbia.edu Abstract We analyze overt displays of power (ODPs) in written dialogs.",
    "acronyms": [
      [
        69,
        73
      ]
    ],
    "long-forms": [
      [
        44,
        58
      ]
    ],
    "ID": "477"
  },
  {
    "text": "(PERS), organization (ORG), geo-political entity (GPE), weapon (WEA), vehicle (VEH), location (LOC), and facility (FAC). Since the person type",
    "acronyms": [
      [
        115,
        118
      ],
      [
        1,
        5
      ],
      [
        22,
        25
      ],
      [
        50,
        53
      ],
      [
        64,
        67
      ],
      [
        79,
        82
      ],
      [
        95,
        98
      ]
    ],
    "long-forms": [
      [
        105,
        113
      ],
      [
        8,
        20
      ],
      [
        28,
        48
      ],
      [
        56,
        62
      ],
      [
        70,
        77
      ],
      [
        85,
        93
      ]
    ],
    "ID": "478"
  },
  {
    "text": "Abstract In this paper, we propose a new syntaxbased machine translation (MT) approach based on reducing the MT task to a tree-",
    "acronyms": [
      [
        74,
        76
      ],
      [
        109,
        111
      ]
    ],
    "long-forms": [
      [
        53,
        72
      ]
    ],
    "ID": "479"
  },
  {
    "text": "  The second one is a variant that we named  Double Levenshtein?s Edit Distance (DLED)  (see Table 9 for detail).",
    "acronyms": [
      [
        81,
        85
      ]
    ],
    "long-forms": [
      [
        45,
        79
      ]
    ],
    "ID": "480"
  },
  {
    "text": "by HG's (HL). In particular, we show that HL's are included in TAL's andthat TAG's are equivalent toa modification ofHG:s  called Modified Head Grammars (MHG's). The inclusion of MHL in HI.,,",
    "acronyms": [
      [
        154,
        159
      ],
      [
        3,
        5
      ],
      [
        9,
        11
      ],
      [
        42,
        44
      ],
      [
        63,
        66
      ],
      [
        77,
        80
      ],
      [
        179,
        182
      ],
      [
        117,
        119
      ]
    ],
    "long-forms": [
      [
        130,
        152
      ]
    ],
    "ID": "481"
  },
  {
    "text": "TEMPLATE GENERATO R Template Generation Algorithm The memory-based parser generates one or several concept sequence instances (CSI's) for each sentence . ",
    "acronyms": [
      [
        127,
        132
      ]
    ],
    "long-forms": [
      [
        99,
        125
      ]
    ],
    "ID": "482"
  },
  {
    "text": " (Ramshaw and Marcus, 1995) approached chucking by using Transformation Based Learning(TBL). ",
    "acronyms": [
      [
        87,
        90
      ]
    ],
    "long-forms": [
      [
        57,
        86
      ]
    ],
    "ID": "483"
  },
  {
    "text": "demonstrate such dependencies.  The Maximum Entropy (MaxEnt) model (Berger et al, 1996) estimates the probability of a time-bin",
    "acronyms": [
      [
        53,
        59
      ]
    ],
    "long-forms": [
      [
        36,
        51
      ]
    ],
    "ID": "484"
  },
  {
    "text": "dialogues categorized into multiple domains, we create a particular type of hidden Markov model (HMM) called Class Speaker HMM (CSHMM) to model operator/caller utterance sequences.",
    "acronyms": [
      [
        128,
        133
      ]
    ],
    "long-forms": [
      [
        109,
        126
      ]
    ],
    "ID": "485"
  },
  {
    "text": "shared task, namely FLORIAN (Florian et al.,  2003) and CHIEU-NG (Chieu and Ng, 2003). ",
    "acronyms": [
      [
        56,
        64
      ],
      [
        20,
        27
      ]
    ],
    "long-forms": [
      [
        66,
        78
      ],
      [
        29,
        36
      ]
    ],
    "ID": "486"
  },
  {
    "text": " 1 Introduction Open-domain Question Answering (QA) systems are concerned with the problem of trying",
    "acronyms": [
      [
        48,
        50
      ]
    ],
    "long-forms": [
      [
        28,
        46
      ]
    ],
    "ID": "487"
  },
  {
    "text": "229  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1810?1815, October 25-29, 2014, Doha, Qatar.",
    "acronyms": [
      [
        93,
        98
      ]
    ],
    "long-forms": [
      [
        43,
        91
      ]
    ],
    "ID": "488"
  },
  {
    "text": "1 Introduction Large-scale open-domain question answering from structured Knowledge Base (KB) provides a good balance of precision and recall in everyday QA",
    "acronyms": [
      [
        90,
        92
      ],
      [
        154,
        156
      ]
    ],
    "long-forms": [
      [
        74,
        88
      ]
    ],
    "ID": "489"
  },
  {
    "text": "th fo frture representations abstracts (AbT), titles (ArT),  authors (Aut), Journals (Jou), and Mesh Headings",
    "acronyms": [
      [
        40,
        43
      ],
      [
        54,
        57
      ],
      [
        70,
        73
      ],
      [
        86,
        89
      ]
    ],
    "long-forms": [
      [
        29,
        38
      ],
      [
        46,
        52
      ],
      [
        61,
        68
      ],
      [
        76,
        84
      ]
    ],
    "ID": "490"
  },
  {
    "text": "787 After labeling the reference BINet, we train a learning to rank (L2R) model2 using the following features for scoring nodes in the target BINet",
    "acronyms": [
      [
        69,
        72
      ]
    ],
    "long-forms": [
      [
        51,
        67
      ]
    ],
    "ID": "491"
  },
  {
    "text": "for the annotation process.  Topic models (TMs) are a suite of unsuper992",
    "acronyms": [
      [
        43,
        46
      ]
    ],
    "long-forms": [
      [
        29,
        41
      ]
    ],
    "ID": "492"
  },
  {
    "text": "Evidence for a text?s topic and genre comes, in part, from its lexical and syntactic features?features used in both Automatic Topic Classification and Automatic Genre Classification (AGC). Because an ideal AGC system should",
    "acronyms": [
      [
        183,
        186
      ],
      [
        206,
        209
      ]
    ],
    "long-forms": [
      [
        151,
        181
      ]
    ],
    "ID": "493"
  },
  {
    "text": "0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co",
    "acronyms": [
      [
        40,
        42
      ]
    ],
    "long-forms": [
      [
        25,
        38
      ]
    ],
    "ID": "494"
  },
  {
    "text": "The query (Figure 4a) will match adjectives (ADJA) adjacent to a following noun (NN) which must not have another dependent that is either a modifying noun or name (NE).",
    "acronyms": [
      [
        81,
        83
      ],
      [
        45,
        49
      ],
      [
        164,
        166
      ]
    ],
    "long-forms": [
      [
        75,
        79
      ],
      [
        33,
        43
      ],
      [
        158,
        162
      ]
    ],
    "ID": "495"
  },
  {
    "text": "the middle (Baxendale, 1958).  Sentence Position Yield (SPY) is obtained separately for both types of documents.",
    "acronyms": [
      [
        56,
        59
      ]
    ],
    "long-forms": [
      [
        31,
        54
      ]
    ],
    "ID": "496"
  },
  {
    "text": "   By contrast, our approach operates at the level  of inflectional property sets (IPS), or more  properly, at the level of inflectional paradigms.",
    "acronyms": [
      [
        83,
        86
      ]
    ],
    "long-forms": [
      [
        55,
        81
      ]
    ],
    "ID": "497"
  }
]