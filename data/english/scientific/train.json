[{"text": "92\\]. This selective approach led to significant  results in some restricted applications (ATIS...). ", "acronyms": [[91, 98]], "long-forms": [[77, 89]], "ID": "1"}, {"text": "We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter.", "acronyms": [[119, 122]], "long-forms": [[93, 117]], "ID": "2"}, {"text": " An arguably better approach to representation learning is Canonical Correlation Analysis (CCA) that induces representations that are maximally cor-", "acronyms": [[91, 94]], "long-forms": [[59, 89]], "ID": "3"}, {"text": "23-28, 1992   Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 110?117, Denver, Colorado, June 1, 2015.", "acronyms": [[71, 74], [29, 38]], "long-forms": [[44, 69]], "ID": "4"}, {"text": "itly mark objects of prepositions (POBJ), possessors in idafa construction (IDAFA), conjuncts (CONJ) and conjunctions (CC), and the accusative specifier, tamyiz (TMZ).", "acronyms": [[119, 121], [35, 39], [76, 81], [95, 99], [162, 165]], "long-forms": [[105, 117], [10, 33], [56, 74], [84, 93], [154, 160]], "ID": "5"}, {"text": "The metrics Precision (P), Recall (R),  F-score (F) (F=2PR/(P+R)), Recall of OOV  (ROOV) and Recall of IV (RIV) are used to  evaluate the results.", "acronyms": [[107, 110], [83, 87]], "long-forms": [[93, 105], [12, 21], [27, 33], [40, 47], [67, 80]], "ID": "6"}, {"text": "158  I. CONSTRUCT THE PROPOSED ANCHORS for Un  (a) Create set of referring expressions (RE's). ", "acronyms": [[88, 92]], "long-forms": [[65, 86]], "ID": "7"}, {"text": "Hong Kong Laws, Sinorama, Xinhua News, and English translation of Chinese Treebank), available from the Linguistic Data Consortium (LDC). To", "acronyms": [[132, 135]], "long-forms": [[104, 130]], "ID": "8"}, {"text": "uation test sets. Equal Error Rates (EER), where FA = FR, are given in Table 5. Results on EVAL", "acronyms": [[54, 56], [37, 40], [49, 51], [91, 95]], "long-forms": [[18, 35]], "ID": "9"}, {"text": "Draw Team (DT_DT); Team ? Competition (TM_CP); Team ? City / Province / Country ", "acronyms": [[39, 44], [11, 16]], "long-forms": [[33, 37], [0, 9]], "ID": "10"}, {"text": "syntactic skeleton defined in Eq. 1, namely, Subject(S), Verb(V), Direct Object(DO), Indirect Object(IO), Preposition(P) and Noun(Object) of the Preposition(N).", "acronyms": [[80, 82], [101, 103]], "long-forms": [[66, 78], [85, 99], [45, 52], [57, 61], [106, 117]], "ID": "11"}, {"text": "Constant 5.23 1.18 19.67 <0.000* 187.25 ADAG, n=242; HAG, n = 242; S.E = standard error; OR = Odds ratio or Exp(?); CI = confidence Interval. ", "acronyms": [[116, 118], [89, 91], [53, 56]], "long-forms": [[121, 140], [94, 104]], "ID": "12"}, {"text": "The availability of large scale data sets of manually annotated predicate?argument structures has recently favored the use of machine learning approaches to the design of automated semantic role labeling (SRL) systems. The main research in this area relates to the design choices", "acronyms": [[205, 208]], "long-forms": [[181, 203]], "ID": "13"}, {"text": "Table 1 FactBank annotation scheme. CT = certain; PR = probable; PS = possible; U = underspecified; + = positive; ?", "acronyms": [[36, 38], [50, 52], [65, 67]], "long-forms": [[41, 48], [55, 63], [70, 78], [84, 98], [104, 112]], "ID": "14"}, {"text": "served as the founding Editor-in-Chief of ACM  Transactions on Knowledge Discovery from Data (TKDD). He has received ACM SIGKDD In-", "acronyms": [[94, 98], [42, 45], [117, 120], [121, 127]], "long-forms": [[47, 92]], "ID": "15"}, {"text": "list of Frames alphabetically surrounding Compliance runs as follows: Compatibility, Competition, Complaining, Completeness, Compliance, Concessive.... Next, we attempt to catalogue the Lexical Units (LUs) associated with the frame. ", "acronyms": [[201, 204]], "long-forms": [[186, 199]], "ID": "16"}, {"text": "{kmpark, rim}@nlp.korea.ac.kr 1 Introduction The semantic role labeling (SRL) refers to finding the semantic relation (e.g. Agent, Patient, etc.)", "acronyms": [[73, 76]], "long-forms": [[49, 71]], "ID": "17"}, {"text": "the current work, we used a subset of that corpus consisting of examples whose question types were PRC (procedure), RSN (reason) or ATR (atrans). ", "acronyms": [[99, 102], [116, 119], [132, 135]], "long-forms": [[104, 113], [121, 127], [137, 143]], "ID": "18"}, {"text": " Its data-driven approach learns a sub-word lexicon from a training corpus of words by using a Minimum Description Length (MDL) algorithm (Creutz et Lagus, 2005). It has been used with", "acronyms": [[123, 126]], "long-forms": [[95, 121]], "ID": "19"}, {"text": "4 = 1, 440 items) (ADD=additive, MUL=multiplicative, PLSR=Partial Least Squares Regression, OBS=observed vectors) . ", "acronyms": [[92, 95], [19, 22], [33, 36], [53, 57]], "long-forms": [[96, 104], [23, 31], [37, 51], [58, 90]], "ID": "20"}, {"text": " The sentence-level extraction is done with the subsequence kernel (SSK) approach from (Bunescu and Mooney, 2005), which was shown to give good re-", "acronyms": [[68, 71]], "long-forms": [[51, 66]], "ID": "21"}, {"text": " 1 Introduction Natural Language Processing (NLP) and Machine Learning (ML) are making a significant impact in", "acronyms": [[45, 48], [72, 74]], "long-forms": [[16, 43], [54, 70]], "ID": "22"}, {"text": " Proceedings of the lOth International Conference on  Computational Linguistics (COLING-84). Stanford ", "acronyms": [[81, 90]], "long-forms": [[54, 79]], "ID": "23"}, {"text": "for the three sponsoring agencies. The TIPSTER  Research and Evaluation Committee (REC) was  charged with oversight responsibility of the 15 ", "acronyms": [[83, 86]], "long-forms": [[48, 81]], "ID": "24"}, {"text": "above.  Iconic Inflectional Classes (IICs) are ICs that are manually fully annotated, i.e., they have all the tem-", "acronyms": [[37, 41]], "long-forms": [[8, 35]], "ID": "25"}, {"text": "probabilities. Analysis showed that the correct tag most fre-  quently missing from the lattice was the DT (determiner)  tag.", "acronyms": [[104, 106]], "long-forms": [[108, 118]], "ID": "26"}, {"text": "ing. In Proceedings of the A CL Fifth Conference  on Applied Natural Language Processing (ANLP),  pages 139-146, Washington, DC.", "acronyms": [[90, 94], [29, 31], [125, 127]], "long-forms": [[53, 88]], "ID": "27"}, {"text": "tion of the reference xamples takes place.  Translation Memories (TMs) are such purely  memory based MT-systems.", "acronyms": [[66, 69], [101, 103]], "long-forms": [[44, 64]], "ID": "28"}, {"text": "discourses presented to the human subjects.  6.1 Semantically Slanted Discourse (SSD) Methodology: The Motivation for the  First Part ", "acronyms": [[81, 84]], "long-forms": [[49, 79]], "ID": "29"}, {"text": " We integrate two sets of linguistic features into a maximum entropy (MaxEnt) model and develop aMaxEnt-based binary classifier to predict the cat-", "acronyms": [[70, 76], [97, 103]], "long-forms": [[53, 68]], "ID": "30"}, {"text": "quences here).  1PARTMOD=participial modifier, PREP=prepositional modifier, POBJ=object of preposition.", "acronyms": [[47, 51], [16, 24], [76, 80]], "long-forms": [[52, 65], [25, 45], [81, 102]], "ID": "31"}, {"text": "middle value of 6.5, was tested.  The Lexile-like measure (LX) used the same two features as the Lexile measure: mean log frequency", "acronyms": [[59, 61]], "long-forms": [[38, 44]], "ID": "32"}, {"text": "With respect to the EUROTRA MT system this has  important implications for the translation between the syntactic  dependency level - the EUROTRA Relational Structure (ERS)  and the semant ic  level  - the in ter face  St ructure  (IS).", "acronyms": [[167, 170], [20, 27], [28, 30], [231, 233]], "long-forms": [[137, 165], [205, 228]], "ID": "33"}, {"text": "  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54, Boulder, Colorado, June 2009.", "acronyms": [[87, 92]], "long-forms": [[46, 85]], "ID": "34"}, {"text": "\u0011\u000f \u0019 \u0019 \u0019 \u0019 \u0019 \u0018 Figure 3: Scored dependency forest 2.5 Semantic Dependency Graph (SDG) The SDG is a semantic-label word DG designed", "acronyms": [[81, 84], [90, 93], [119, 121]], "long-forms": [[54, 79]], "ID": "35"}, {"text": " 2.4 Stanford Parser The Stanford Parser (SP) is an unlexicalized parser that rivals state-of-the-art lexical-", "acronyms": [[42, 44]], "long-forms": [[25, 40]], "ID": "36"}, {"text": "attributes of the observation vectors and a  specific label).we can represent the input-output  pairs via joint feature map (JFM)  1", "acronyms": [[125, 128]], "long-forms": [[106, 123]], "ID": "37"}, {"text": "tence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP = copula, EXCL = exclamation mark. ", "acronyms": [[106, 110], [29, 32], [49, 52], [75, 78], [92, 95]], "long-forms": [[113, 124], [23, 27], [35, 40], [55, 64], [81, 90], [98, 104]], "ID": "38"}, {"text": "a question written in natural language is called ? Question Answering?(QA), and has gotten a lot of attention recently.", "acronyms": [[71, 73]], "long-forms": [[51, 69]], "ID": "39"}, {"text": " In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 1698?1703.", "acronyms": [[90, 94]], "long-forms": [[55, 73]], "ID": "40"}, {"text": "protein interaction as an example. In Proceedings  of the Pacific Symposium on Biocomputing (PSB),  Hawaii, USA.", "acronyms": [[93, 96], [108, 111]], "long-forms": [[58, 91]], "ID": "41"}, {"text": " 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of apply-", "acronyms": [[13, 16]], "long-forms": [[18, 40]], "ID": "42"}, {"text": " tage of the OpenCCG realizer?s ability to generate from disjunctive logical forms (DLFs), i.e. packed semantic dependency graphs (White, 2004; White,", "acronyms": [[84, 88], [13, 20]], "long-forms": [[57, 82]], "ID": "43"}, {"text": "2http://www.statmt.org/wmt12/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (MT). ", "acronyms": [[121, 123]], "long-forms": [[100, 119]], "ID": "44"}, {"text": "2010). Alternatively, iteratively optimized embeddings such as Skip Gram (SG) model (Mikolov et al.,", "acronyms": [[74, 76]], "long-forms": [[63, 72]], "ID": "45"}, {"text": "and EN-DE) with token frequency, sense distribution and most frequent translations ordered by the corresponding senses (T = temporal, CO = concession, CT = contrast). ", "acronyms": [[134, 136], [151, 153], [4, 9]], "long-forms": [[139, 149], [156, 164], [124, 132]], "ID": "46"}, {"text": "nications Advancement Foundation, Japan, in part by the Center for Intelligent Information Retrieval, and in part by the Defense Advanced Research Projects Agency (DARPA), USA under contract number HR0011-06-C-0023.", "acronyms": [[164, 169], [172, 175]], "long-forms": [[121, 162]], "ID": "47"}, {"text": "Rule-Based Machine Translation(MT)(Hutchins and Somers, 1992) requires large-scale knowledge to analyze both source language(SL) sentences and target language(TL) sentences.", "acronyms": [[125, 127], [31, 33], [159, 161]], "long-forms": [[109, 123], [11, 30], [143, 158]], "ID": "48"}, {"text": "3 CLaC Methodology Preprocessing consists of tokenizing, lemmatizing, sentence splitting, and part of speech (POS) tagging. ", "acronyms": [[110, 113], [2, 6]], "long-forms": [[94, 108]], "ID": "49"}, {"text": "performs two vital functions in the case of our Japanese  processing:  1 CN = common noun; SN = sa-inflection noun (  nominalized .verb); VB = verb; VSUF = verb suffix; CM = ", "acronyms": [[73, 75], [138, 140], [149, 153], [169, 171]], "long-forms": [[78, 89], [143, 147], [156, 167]], "ID": "50"}, {"text": " Before we discuss the significant sentence in answer mails, we classified answer mails into three types: (1) direct answer (DA) mail, (2) questioner?s reply (QR) mail, and (3) the others.", "acronyms": [[125, 127], [159, 161]], "long-forms": [[110, 123], [139, 157]], "ID": "51"}, {"text": "This paper examines the processing predictions of the ERH on a systematic class of relative clause types, the Accessibility Hierarchy (AH) shown in figure 1.", "acronyms": [[135, 137], [54, 57]], "long-forms": [[110, 133]], "ID": "52"}, {"text": "person names, organization names, location names, etc. The template element (TE) task extracts information centered around an entity, like the acronym,", "acronyms": [[77, 79]], "long-forms": [[59, 75]], "ID": "53"}, {"text": "We use multiple epochs of minibatch stochastic gradient descent and update all parameters to minimize the negative log likelihood (NLL) of our training set.", "acronyms": [[131, 134]], "long-forms": [[106, 129]], "ID": "54"}, {"text": "the MDL methods.  ConVote (CONVOTE) Our second dataset is taken from segments of speech from United States", "acronyms": [[27, 34], [4, 7]], "long-forms": [[18, 25]], "ID": "55"}, {"text": "We however noticed the relative degradation of quality in coordinating conjunctions (CC), determiners (DT) and personal pronouns (PRP). ", "acronyms": [[130, 133], [85, 87], [103, 105]], "long-forms": [[111, 128], [58, 83], [90, 101]], "ID": "56"}, {"text": " ? Unlabeled Elementary Dependencies (UED) identical to LED, except ignoring all labeling", "acronyms": [[38, 41], [56, 59]], "long-forms": [[3, 36]], "ID": "57"}, {"text": "Various traditional  information retrieval(IR) techniques combined  with natural language processing(NLP) tech-  niques have been re-targeted to enable efficient ", "acronyms": [[101, 104], [43, 45]], "long-forms": [[73, 99], [21, 42]], "ID": "58"}, {"text": " 3.1 Ske le ta l  Phrase  S t ructure  Component   The role of phrase structure (PS) rules in our parser is similar to  their role in Lexical Functional Grammar \\[Kaplan 83\\], however they ", "acronyms": [[81, 83]], "long-forms": [[63, 79]], "ID": "59"}, {"text": " This paper proposes an approach for implicit feature detection based on SVM and Topic Model(TM). ", "acronyms": [[93, 95], [73, 76]], "long-forms": [[81, 91]], "ID": "60"}, {"text": " On the other hand, the decline in performance for the composite feature vector baseline (CFV) may be attributed to the data sparseness phenomenon", "acronyms": [[90, 93]], "long-forms": [[55, 79]], "ID": "61"}, {"text": "vised taggers. One commonly-used unsupervised tagger is the Hidden Markov model (HMM), which models the joint distribution of a word se-", "acronyms": [[81, 84]], "long-forms": [[60, 79]], "ID": "62"}, {"text": " 2. Tezt routing, tezt fdter/n9, and SDI (selective dissemination of information): These terms  refer to a loose collection of text classification tasks such as managing personal electronic mail, ", "acronyms": [[37, 40]], "long-forms": [[42, 65]], "ID": "63"}, {"text": "Estimating Proficiency  Item Response Theory (IRT)  Item Response Theory (IRT) is the basis of modern  language tests such as TOEIC, and enables Com-", "acronyms": [[74, 77], [46, 49], [126, 131]], "long-forms": [[52, 72], [24, 44]], "ID": "64"}, {"text": "( NN?? ) speech 1:1 ( NP ( NR ) ( NN ) ) X1 | X2 1:2 2:1 ( NP ( NR?? ) (", "acronyms": [[34, 36], [2, 6], [59, 61], [64, 68]], "long-forms": [[22, 29]], "ID": "65"}, {"text": "Table 3 presents the total number of training examples extracted from SemCor (SC) and from the background documents (BG). As expected, by", "acronyms": [[117, 119], [78, 80]], "long-forms": [[95, 105], [70, 76]], "ID": "66"}, {"text": "alignment G (both A and G can be split into two subsets AS ,AP and GS , GP , respectively representing Sure and Probable alignments) Precision (PT ), Recall (RT ), F-measure (FT ) and Alignment Error", "acronyms": [[144, 146], [56, 58], [60, 62], [67, 69], [72, 74], [158, 160], [175, 177]], "long-forms": [[112, 131], [150, 156], [164, 173]], "ID": "67"}, {"text": " 4 Active Learning Active Learning (AL) is a machine learning paradigm that let the learner decide which data it", "acronyms": [[36, 38]], "long-forms": [[19, 34]], "ID": "68"}, {"text": "below.  4.3.1 English Noun Compounds (ENC) Our first dataset is made up of 90 binary English", "acronyms": [[38, 41]], "long-forms": [[14, 36]], "ID": "69"}, {"text": "account for these generalizations by decom-  posing the grammar rules to Immediate Dom-  inance(ID) rules and Linear Preeedence(LP)  rules.", "acronyms": [[128, 130], [96, 98]], "long-forms": [[110, 126], [73, 95]], "ID": "70"}, {"text": "  Abstract  Our previous work focuses on combining translation memory (TM) and statistical machine translation  (SMT) when the TM database and the SMT training set are the same.", "acronyms": [[71, 73], [113, 116], [127, 129], [147, 150]], "long-forms": [[51, 69], [79, 110]], "ID": "71"}, {"text": "Hodellng Temporal Knowledge  Hodellng time, dasoite its olovlous importance, has  proved an elusive goal for artificial Intelligence (AI). ", "acronyms": [[134, 136]], "long-forms": [[109, 132]], "ID": "72"}, {"text": "? P5E3N3S3, F W A Computer Processable English (CPE) (Pulman 1996; Sukkarieh and Pulman 1999) is a controlled language that can be ?", "acronyms": [[48, 51], [2, 10]], "long-forms": [[18, 46]], "ID": "73"}, {"text": "We also have investigated our two-step solution on two existing treebanks, the Penn Chinese Treebank (CTB) (Xue et al.,", "acronyms": [[102, 105]], "long-forms": [[84, 100]], "ID": "74"}, {"text": "ual resources on pairwise comparison task (Diff. = Difficulty lexicon, CF = Crowdflower) Features", "acronyms": [[71, 73]], "long-forms": [[76, 87]], "ID": "75"}, {"text": "distribution which underlies natural language text   -- which is if not a pure Zipfian distribution at least  an LNRE (large number of rare events, cf. Baayen ", "acronyms": [[113, 117]], "long-forms": [[119, 146]], "ID": "76"}, {"text": " 4 Algorithms 4.1 Topic?sentence graph matching (GM) We treat a sentence and a topic as graphs.", "acronyms": [[49, 51]], "long-forms": [[33, 47]], "ID": "77"}, {"text": "Table 1: Number of routes, directions, and tokens for the different settings. GM = Google Maps, CI = Campus Indoor, CO = Campus Outdoor.", "acronyms": [[78, 80], [96, 98], [116, 118]], "long-forms": [[83, 94], [101, 114], [121, 135]], "ID": "78"}, {"text": "For each text pair on four cross levels, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (S-Ph), Phrase to Word (Ph-W) and Word to Sense (W-Se), participants are required to re-", "acronyms": [[119, 123]], "long-forms": [[103, 117]], "ID": "79"}, {"text": "mantic content Ks, sends a message m to R and R  interprets m as meaning CR. CS = cn is a neces-  sary condition for this turn of communication to ", "acronyms": [[77, 79], [15, 17], [73, 75]], "long-forms": [[82, 87]], "ID": "80"}, {"text": "istic conversational systems. In Proceedings of Intelligent User Interfaces 2001 (IUI-01), pages 1?8, Santa Fe, NM, January.", "acronyms": [[82, 88], [112, 114], [108, 110]], "long-forms": [[48, 80]], "ID": "81"}, {"text": "  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 53?57, Gothenburg, Sweden, April 26-30 2014.", "acronyms": [[70, 72], [28, 32]], "long-forms": [[50, 68]], "ID": "82"}, {"text": "study on NER is mainly focused either on the proper name identification of person(PER), location(LOC), organization(ORG), time(TIM) and numeral(NUM) expressions almost in news do-", "acronyms": [[116, 119], [9, 12], [82, 85], [97, 100], [127, 130], [144, 147]], "long-forms": [[103, 114], [75, 81], [88, 96], [122, 126], [136, 143]], "ID": "83"}, {"text": "utterance. The interface default allowing this is called Noun meaning extended (NMExt)13 NMExt: Noun(u), sem(u) is ?", "acronyms": [[80, 85], [89, 94]], "long-forms": [[57, 78]], "ID": "84"}, {"text": "Table 1). Firstly, each term candidate is mapped to  a specific canonical representative (CR) by  semantically isomorphic transformations.", "acronyms": [[90, 92]], "long-forms": [[64, 88]], "ID": "85"}, {"text": "tags and word.  Rich Morphological Features (Rich-MF): In addition to the elementary features we use the am-", "acronyms": [[45, 52]], "long-forms": [[16, 43]], "ID": "86"}, {"text": "Figure 4: Canonical example pub from Saltzman and Munhall (1989) representing overlapping goals for tongue blade constriction degree (TBCD), lip aperture (LA), and glottis (GLO).", "acronyms": [[134, 138], [155, 157], [173, 176]], "long-forms": [[100, 132], [141, 153], [164, 171]], "ID": "87"}, {"text": "Han, C-H., Han, N-R., Ko, E-S.and Palmer, M.: Development and Evaluation of a Korean  Treebank and Its Application to NLP.in Proceedings of the 3rd International Conference on  Language Resources and Evaluation (LREC).(2002)  5.", "acronyms": [[212, 216]], "long-forms": [[177, 195]], "ID": "88"}, {"text": "perceptual space between each pair. We can do this with a multidimensional scaling (MDS) algorithm. Let us call", "acronyms": [[84, 87]], "long-forms": [[58, 82]], "ID": "89"}, {"text": "corresponding to the point P.  Once a n  object has been added to the geometric model by specifying values  for its GSTART, GSIZE, and ROTN (rotation), the geometric coordinates for any  location on the object may be obtained by calling the funtion EXECLOCA with the ", "acronyms": [[135, 139], [249, 257], [116, 122], [124, 129]], "long-forms": [[141, 149]], "ID": "90"}, {"text": "known as conditional random fields (CRFs) (Lafferty et al, 2001), when all variables are observed, and as hidden conditional random fields (HCRFs) (Quattoni et al, 2007), when only a subset of the variables are", "acronyms": [[140, 145], [36, 40]], "long-forms": [[106, 138], [9, 34]], "ID": "91"}, {"text": "Linggle: a Web-scale Linguistic Search Engine for Words in Context    Joanne Boisson+, Ting-Hui Kao*, Jian-Cheng Wu*, Tzu-His Yen*, Jason S. Chang* +Institute of Information Systems and Applications *Department of Computer Science National Tsing Hua University HsinChu, Taiwan, R.O.C. 30013 {joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang} @gmail.com     Abstract  In this paper, we introduce a Web-scale lin-guistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and ad-ditional regular expression (RE) operators. In our approach, we incorporate inverted file in-dexing, PoS information from BNC, and se-mantic indexing based on Latent Dirichlet Al-location with Google Web 1T. The method in-volves parsing the query to transforming it in-to several keyword retrieval commands.", "acronyms": [[584, 587], [636, 638], [278, 283], [708, 711], [729, 732]], "long-forms": [[567, 582], [616, 634]], "ID": "92"}, {"text": "3.2 Graph-based Label Propagation Graph-based label propagation, a critical subclass of semi-supervised learning (SSL), has been widely used and shown to outperform other SSL meth-", "acronyms": [[114, 117], [171, 174]], "long-forms": [[88, 112]], "ID": "93"}, {"text": "overlapping relations. In Annual Meeting of the Association for Computational Linguistics (ACL). ", "acronyms": [[91, 94]], "long-forms": [[48, 88]], "ID": "94"}, {"text": "guages: British sign language (BSL), Danish (DSL), French Belgian (FBSL), Flemish (FSL), Greek (GSL), and Dutch (NGT). The data for the", "acronyms": [[96, 99], [31, 34], [45, 48], [67, 71], [83, 86], [113, 116]], "long-forms": [[89, 94], [8, 29], [37, 43], [51, 65], [74, 81], [106, 111]], "ID": "95"}, {"text": " 04 (a) Same Topic (ST) 0.00 0.01 0.02 0.03 0.04 0.05 0.06", "acronyms": [[20, 22]], "long-forms": [[8, 18]], "ID": "96"}, {"text": "Examples of failure of analysis  (i) JISSAI (in fact), CHOSHA-TACHI-WA (authors) {\\[KORE-WO (it) TSUKATTE  (using), JUURYOKU-SOUGO-SAYOU-GA (gravitationally interacting) SHIHAI-SURU (govern-  ing)\\] TENTAI-NO (astronomical) UNDOU-NI-TSUITE (about the motion), KOUSEIDO-DE (high- ", "acronyms": [[137, 139], [37, 43], [55, 70], [170, 181], [199, 208], [224, 239]], "long-forms": [[141, 156], [183, 195], [210, 222], [241, 257]], "ID": "97"}, {"text": "In Proceedings of the 19th International Conference on Computational Linguistics (COLING?02), pages 218? ", "acronyms": [[82, 91]], "long-forms": [[55, 80]], "ID": "98"}, {"text": "uses the mapping of concept dog to the class of alternative  expressions for named individual (such as using the name,  2 VSFL (\"Very Simple Frame Language\") and SCORE CSproket  Core\") were developed at BBN Systems and Technologies by ", "acronyms": [[122, 126], [162, 167], [168, 176], [203, 206]], "long-forms": [[129, 155]], "ID": "99"}, {"text": "2 Complexity of GPSG Components  A generalized phrase structure grammar contains five language-  particular components - -  immediate dominance (ID) rules, meta-  rules, linear precedence (LP) statements, feature co-occurrence ", "acronyms": [[145, 147], [16, 20], [189, 191]], "long-forms": [[124, 143], [170, 187]], "ID": "100"}, {"text": "den Markov Models (HMMs), Conditional Random Fields (CRFs), Maximum Entropy Markov  Models (MEMMs), etc. ", "acronyms": [[92, 97], [19, 23], [53, 57]], "long-forms": [[60, 90], [4, 17], [26, 51]], "ID": "101"}, {"text": "a topic model are compared. Latent Dirichlet Allocation (LDA) (Blei et al 2003) is a widely used type of topic model in which documents can be", "acronyms": [[57, 60]], "long-forms": [[28, 55]], "ID": "102"}, {"text": "are either too wordy or too ungrammatical. Table 1 shows the compression rates (CompR) for the two 8", "acronyms": [[80, 85]], "long-forms": [[61, 78]], "ID": "103"}, {"text": "This paper proposes a contextdependent phrase reordering approach that uses the maximum entropy (MaxEnt) model to help the HPB decoder select appropriate re-", "acronyms": [[97, 103], [123, 126]], "long-forms": [[80, 95]], "ID": "104"}, {"text": " 3.2 ,6  Addi t ion  of T rans la t ion  Rules  FinMly, translation rules (TRis) are added to the set  of GLTPC, s. TRis are descriptions in which concepts ", "acronyms": [[75, 79], [106, 111], [116, 120]], "long-forms": [[56, 73]], "ID": "105"}, {"text": "lation of term-lists, related studies are found in the  area of target word selection (for content words) in  conventional full-text machine translation (MT). ", "acronyms": [[154, 156]], "long-forms": [[133, 152]], "ID": "106"}, {"text": "BG 80,757 1.34 EN 94,725 2.58 Table 2: Corpus statistics: SR=Serbian, SL=Slovene, EN=English, BG=Bulgarian", "acronyms": [[58, 60], [70, 72], [15, 17], [0, 2], [82, 84], [94, 96]], "long-forms": [[61, 68], [73, 80], [85, 92], [97, 106]], "ID": "107"}, {"text": "al., 2005; Vapnik, 1998) based on the Gaussian  Radial Basis kernel function (RBF). We tuned ", "acronyms": [[78, 81]], "long-forms": [[48, 76]], "ID": "108"}, {"text": "Swedish 83.01 (82.44) 88.53 (87.36) 81.20 (81.10) 86.50 (85.86) 82.95 (82.66*) 88.29 (87.45*) 82.89 (82.44) 88.61 (87.55) Turkish 62.70 (71.27) 73.67 (78.57) 59.83 (68.31) 70.15 (75.17) 63.27* (71.63*) 73.93* (78.72*) 62.58 (70.96) 73.09 (77.95) Table 1: Parsing accuracy of the undirected planar parser with naive (UPlanarN) and label-based (UPlanarL) postprocessing in comparison to the directed planar (Planar) and the MaltParser arc-eager projective (MaltP)", "acronyms": [[316, 324], [343, 351], [406, 412], [455, 460]], "long-forms": [[279, 314], [398, 404], [422, 432]], "ID": "109"}, {"text": "  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181?1191, October 25-29, 2014, Doha, Qatar.", "acronyms": [[90, 95]], "long-forms": [[40, 88]], "ID": "110"}, {"text": "fidence information about each system?s hypothesis. This feature class includes the confusion network (CN) word confidence, CN slot entropy, and the number of alter-", "acronyms": [[103, 105], [124, 126]], "long-forms": [[84, 101]], "ID": "111"}, {"text": " 1 Introduction Question answering (QA) from a knowledge base (KB) has a long history within natural language", "acronyms": [[36, 38], [63, 65]], "long-forms": [[16, 34], [47, 61]], "ID": "112"}, {"text": "level for natural  language unders tand ing  system. In case of a  Module  Package Layer( MPL ), there are two kinds of program  packages.", "acronyms": [[90, 93]], "long-forms": [[67, 88]], "ID": "113"}, {"text": "pus Linguistics 2001 Conference, pages 274?280. Lancaster University (UK). ", "acronyms": [[70, 72]], "long-forms": [[58, 68]], "ID": "114"}, {"text": "859  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 217?226, Seoul, South Korea, 5-6 July 2012.", "acronyms": [[101, 108]], "long-forms": [[51, 99]], "ID": "115"}, {"text": "tion device, a Nippon Electric Corporation DP-200, was  added to an existing natural anguage processing system,  the Natural Language Computer (NLC) (Ballard 1979,  Biermann and Ballard 1980).", "acronyms": [[144, 147], [43, 45]], "long-forms": [[117, 142]], "ID": "116"}, {"text": "Abstract Regardless of language, the standard character set for text messages (SMS) and many other social media platforms is the Roman alphabet.", "acronyms": [[79, 82]], "long-forms": [[56, 77]], "ID": "117"}, {"text": "Associative Texture Is Lost In Translation. In Proceedings  of the Workshop on Discourse in Machine Translation (DiscoMT),  pages 27?32. ACL 2013 Conference, ", "acronyms": [[113, 120], [137, 140]], "long-forms": [[79, 111]], "ID": "118"}, {"text": "describing each video. We then clustered these verbs using Hierarchical Agglomerative Clustering (HAC) using the res metric from WordNet::Similarity by", "acronyms": [[98, 101]], "long-forms": [[59, 96]], "ID": "119"}, {"text": "(adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and", "acronyms": [[102, 105], [118, 122], [14, 17], [29, 33], [46, 49], [65, 68], [139, 142], [156, 160]], "long-forms": [[107, 115], [124, 136], [19, 26], [35, 43], [51, 62], [70, 99], [144, 153], [162, 173]], "ID": "120"}, {"text": "search has encore'aged the FI{UM1 ) api)roach. The  SUMMONS (SUMMarizing Online News artMes)  system (McKeown and Radev, 1999) takes tem- ", "acronyms": [[52, 59]], "long-forms": [[61, 84]], "ID": "121"}, {"text": "same time, Intelligent Computer-Assisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also tend to focus more on gram-", "acronyms": [[102, 105], [60, 65]], "long-forms": [[71, 100], [11, 58]], "ID": "122"}, {"text": "Pivot, RHS),  generate all subconstituents  generate _rhs ( RHS ),  generate material on path to root ", "acronyms": [[60, 63], [7, 10]], "long-forms": [[54, 57]], "ID": "123"}, {"text": "fn-n1?. The annotation contains a feature structure with three features: FE (Frame element), GF (Grammatical Function), and PT", "acronyms": [[73, 75], [93, 95], [124, 126]], "long-forms": [[77, 90], [97, 117]], "ID": "124"}, {"text": "1. Introduction  In a Spoken Language System (SLS) we must use all avail-  able knowledge sources (KSs) to decide on the spoken sen- ", "acronyms": [[46, 49], [99, 102]], "long-forms": [[22, 44], [80, 97]], "ID": "125"}, {"text": "{afader,soderlan,etzioni}@cs.washington.edu Abstract Open Information Extraction (IE) is the task of extracting assertions from massive corpora", "acronyms": [[82, 84]], "long-forms": [[58, 80]], "ID": "126"}, {"text": "Table 3: Results on DevTest and Test Sets compared with the Average Performance in CoNLL?07. LAS = Labelled Attachment Score, UAS = Unlabelled Attachment Score, LAcc = Label Accuracy, AV = Average score.", "acronyms": [[126, 129], [184, 186], [93, 96], [161, 165], [83, 88]], "long-forms": [[132, 159], [189, 196], [99, 124], [168, 182]], "ID": "127"}, {"text": " The SU detection task is conducted on two corpora: Broadcast News (BN) and Conversational Telephone Speech (CTS).", "acronyms": [[68, 70]], "long-forms": [[52, 66]], "ID": "128"}, {"text": "local  cons t i tuents .  These i n c l u d e  Hsimplem noun phrases (NPs) and  prepositional phrases (PPs), (\"simplen meaning 'up to the head noun but  not including any modifying clauses or phrases\"),  and verb groups (VGs) ", "acronyms": [[103, 106], [70, 73], [221, 224]], "long-forms": [[80, 101], [56, 68], [208, 219]], "ID": "129"}, {"text": "In Proceedings of the Fifth Mediterranean Morphology Meeting (MMM5), pages 269?290, Fr?jus.", "acronyms": [[62, 66]], "long-forms": [[42, 60]], "ID": "130"}, {"text": " 5.2 Corpus Benchmark Tool The Corpus Benchmark Tool(CBT) is one of the components in GATE which enables automatic evaluation of an", "acronyms": [[53, 56]], "long-forms": [[31, 51]], "ID": "131"}, {"text": "tution is the same as the cost of insertion or deletion. A normalized edit distance (NED) is calculated by dividing the total edit cost by the length of", "acronyms": [[85, 88]], "long-forms": [[59, 83]], "ID": "132"}, {"text": " 1 Introduction Base Phrase Chunking (BPC), also known as shallow syntactic parsing, is the process by which ad-", "acronyms": [[38, 41]], "long-forms": [[16, 36]], "ID": "133"}, {"text": "In Proceedings of the Third International Conference on Web Search and Web Data Mining (WSDM), pages 101?110, New York, NY, USA, 2010.", "acronyms": [[88, 92], [120, 122], [124, 127]], "long-forms": [[56, 86]], "ID": "134"}, {"text": "When these approaches are applied to normal Twitter users accuracy results significantly decrease.  Sentiment Analysis (SA) has been widely studied in the last decade in multiple domains. Most work", "acronyms": [[120, 122]], "long-forms": [[100, 118]], "ID": "135"}, {"text": "ing decision can be made directly based on attention weights from the two query components or further rescored by the maximum spanning tree (MST) search algorithm.", "acronyms": [[141, 144]], "long-forms": [[118, 139]], "ID": "136"}, {"text": "As further evidence of the effectiveness of our framework, we have recently adapted our phrase-structure parser in Section 6 to parsing with a lexicalized grammar formalism, Combinatory Categorial Grammar (CCG), and achieved higher F-scores than the state-of-the-art C&C CCG parser (Clark and Curran 2007).", "acronyms": [[206, 209], [267, 270], [271, 274]], "long-forms": [[174, 204]], "ID": "137"}, {"text": "put language context to bias translation choices is in some sense a neural network analogy to the PSD (phrase sense disambiguation) approach for context-dependent translation probabilities of", "acronyms": [[98, 101]], "long-forms": [[103, 130]], "ID": "138"}, {"text": "The most successful stochastic language models  have been based on finite-state descriptions such  as n-grams or hidden Markov models (HMMs)  (Jelinek et al, 1992).", "acronyms": [[135, 139]], "long-forms": [[113, 133]], "ID": "139"}, {"text": " 4.1 KNN classification The basic idea of the K nearest neighbor (KNN) classification algorithm is to use already categorized", "acronyms": [[66, 69], [5, 8]], "long-forms": [[46, 64]], "ID": "140"}, {"text": "Ney discounting and interpolation. The evaluation of stream counts is done on EP+afe+nyt (EAN) corpus, consisting of 1.1 billion words.", "acronyms": [[90, 93]], "long-forms": [[78, 88]], "ID": "141"}, {"text": "pora. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 993?1000.", "acronyms": [[88, 94]], "long-forms": [[61, 86]], "ID": "142"}, {"text": "Donald, 2006). As a joint approach, we consider Reluctant Trimmer (RT), a simplification system that employs synchronous dependency grammars", "acronyms": [[67, 69]], "long-forms": [[48, 65]], "ID": "143"}, {"text": "maries C are overall better for answering questions than summaries B. Comparison between B and C (B-C) precision recall", "acronyms": [[98, 101]], "long-forms": [[89, 96]], "ID": "144"}, {"text": "However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009).", "acronyms": [[124, 127]], "long-forms": [[95, 122]], "ID": "145"}, {"text": "3 Framework We model the information extraction task as a markov decision process (MDP), where the model learns to utilize external sources to improve upon", "acronyms": [[83, 86]], "long-forms": [[58, 81]], "ID": "146"}, {"text": "5 Conclusion and Further Works   In this paper we have proposed a reestimation algorithm and a best-first parsing algorithm  for probabilistic dependency grammars(PDG). The reestimation algorithm is a variation of ", "acronyms": [[163, 166]], "long-forms": [[129, 161]], "ID": "147"}, {"text": "semantic links between NEs extracted from the answer sentence and the question focus word, which encodes the expected lexical answer type (LAT). We", "acronyms": [[139, 142], [23, 26]], "long-forms": [[118, 137]], "ID": "148"}, {"text": "swer sequence tagging.  bels: B-ANSWER (beginning of answer), I-ANSWER (inside of answer), O (outside of answer).", "acronyms": [[30, 38], [62, 70]], "long-forms": [[40, 59], [94, 111]], "ID": "149"}, {"text": " Given the good results of the pilot we decided to deploy the task in Amazon Mechanical Turk (AMT) in order to crowd source the annotation task.", "acronyms": [[94, 97]], "long-forms": [[70, 92]], "ID": "150"}, {"text": "Pengfei Lu Department of Computer Science Graduate Center City University of New York (CUNY) 365 Fifth Ave, New York, NY 10016 pengfei.lu@qc.cuny.edu  Matt Huenerfauth Department of Computer Science Queens College and Graduate Center City University of New York (CUNY) 65-30 Kissena Blvd, Flushing, NY 11367 matt@cs.qc.cuny.edu  Abstract  American Sign Language (ASL) synthesis software can improve the accessibility of in-formation and services for deaf individuals with low English literacy. The synthesis com-ponent of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse refer-ents.", "acronyms": [[363, 366], [87, 91], [118, 120], [263, 267], [299, 301], [530, 533], [611, 614]], "long-forms": [[339, 361], [58, 85], [108, 116], [234, 261]], "ID": "151"}, {"text": " 1 Introduction Language identification (LangID) is the problem of determining what natural language a document is written in.", "acronyms": [[41, 47]], "long-forms": [[16, 39]], "ID": "152"}, {"text": "date translations. While the traditional Maximum A Posteriori (MAP) decision rule can be optimized as a piecewise lin-", "acronyms": [[63, 66]], "long-forms": [[41, 61]], "ID": "153"}, {"text": "ambiguous word are included as features.  Medical Subject Headings (MeSH): The final feature is also specific to the biomedical do-", "acronyms": [[68, 72]], "long-forms": [[42, 66]], "ID": "154"}, {"text": "given the precorrected sentence. Each Noun Phrase (NP) in  the test sentence will be pre-corrected as correc-", "acronyms": [[51, 53]], "long-forms": [[38, 49]], "ID": "155"}, {"text": "bines the output of all the 13 base models introduced previously. We implemented the meta-classifier using Support Vector Machines (SVMs)15 with a quadratic polynomial kernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 shows the results", "acronyms": [[132, 136]], "long-forms": [[107, 130]], "ID": "156"}, {"text": "curacy of an automatic classifier means to compare its output with the correct semantic tags on a Gold Standard (GS) dataset. Within our formal", "acronyms": [[113, 115]], "long-forms": [[98, 111]], "ID": "157"}, {"text": "CoTrain vs. TSVM(EN) 1.41E-05 0.00311 2.17E-08 CoTrain vs. TSVM(ENCN) 1.37E-08 0.0113 0.0396 CoTrain vs. SelfTrain(CN) 7.07E-18 2.79E-11 6.53E-07 CoTrain vs. SelfTrain(EN) 1.01E-07 0.0192 1.35E-07", "acronyms": [[115, 117], [12, 16], [17, 19], [59, 63], [64, 68], [168, 170]], "long-forms": [[93, 100]], "ID": "158"}, {"text": "ducted. The administrative body governing these decisions is the Institutional Review Board (IRB). ", "acronyms": [[93, 96]], "long-forms": [[65, 91]], "ID": "159"}, {"text": " 1 Introduction A Chinese natural language processing (NLP) platform always includes lexical analysis (word", "acronyms": [[55, 58]], "long-forms": [[26, 53]], "ID": "160"}, {"text": "FA8750-09-C-0181. The first author also thanks the Vietnam Education Foundation (VEF) for its sponsorship.", "acronyms": [[81, 84]], "long-forms": [[51, 79]], "ID": "161"}, {"text": "els on a previously unseen test set ? the test split of part 3 of the PATB (PATB3-TEST). Table 6 shows", "acronyms": [[76, 86]], "long-forms": [[56, 74]], "ID": "162"}, {"text": "1 Introduction Since the introduction of BLEU (Papineni et al, 2002), automatic machine translation (MT) evaluation has received a lot of research interest.", "acronyms": [[101, 103], [41, 45]], "long-forms": [[80, 99]], "ID": "163"}, {"text": "native (see \\[2\\]).  Task  Manager  (TM)   In our previous experience, speech recognition systems ", "acronyms": [[37, 39]], "long-forms": [[21, 34]], "ID": "164"}, {"text": "(http://ieee.rkbexplorer.com/) repository7. The corpus of cancer research (COCR) contains 3334 domain specific abstracts of scientific publica-", "acronyms": [[75, 79]], "long-forms": [[48, 73]], "ID": "165"}, {"text": "ticular, this includes a model of the grounding process (Clark, 1996) that involves recognition and construction of common ground units (CGUs) (see (Traum, 2003)). ", "acronyms": [[137, 141]], "long-forms": [[116, 135]], "ID": "166"}, {"text": "denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ", "acronyms": [[92, 94], [113, 115], [79, 81]], "long-forms": [[95, 111], [116, 131], [82, 90]], "ID": "167"}, {"text": "manual mapping. In the rest of the paper, we shall first  describe the Switchboard Dialogue Act (SWBD-DA)  Corpus and its annotation scheme (i.e. SWBD-DAMSL).", "acronyms": [[97, 104], [146, 156]], "long-forms": [[71, 95]], "ID": "168"}, {"text": "- coordination (COORD) : traite les c,'~s imples de  coordination,  - statistique (STAT) : utilis6 sur des s6quences qu'il  est impossible de d6sambigui'ser A l'aide ", "acronyms": [[83, 87], [16, 21]], "long-forms": [[70, 81], [0, 14]], "ID": "169"}, {"text": " In Table 3 the number of questions that get a higher and lower reciprocal rank (RR) after applying the individual lexico-semantic resources are", "acronyms": [[81, 83]], "long-forms": [[64, 79]], "ID": "170"}, {"text": "sion with LSA and filtering according to the ET.  Further on, we apply sentiment analysis (SA)  using the approach described in Section 5.3 and ", "acronyms": [[91, 93], [10, 13], [45, 47]], "long-forms": [[71, 89]], "ID": "171"}, {"text": "search tool, but it is insufficient for domain-  specific text, such as that encountered in  the MUCs (Message Understanding Confer-  ences).", "acronyms": [[97, 101]], "long-forms": [[103, 132]], "ID": "172"}, {"text": "Apple Events  1 Introduction  The SlmSum (Slmulatmn of Summarizing)  system does what its name pronuses It simu- ", "acronyms": [[34, 40]], "long-forms": [[42, 66]], "ID": "173"}, {"text": "annotating unlabeled data, for adapting  existing CRF-based named entity recognition (NER) systems to new texts or  domains.", "acronyms": [[86, 89], [50, 53]], "long-forms": [[60, 84]], "ID": "174"}, {"text": "Our work is most similar to the content selection method of the multimedia conversation system RIA (Responsive Information Architect) (Zhou and Aggarwal, 2004).", "acronyms": [[95, 98]], "long-forms": [[100, 132]], "ID": "175"}, {"text": "its nondecomposability, as well as a cross between B??? and word error rate (WER) that is decomposable down to the subsentential level (in a sense to be", "acronyms": [[77, 80]], "long-forms": [[60, 75]], "ID": "176"}, {"text": "PROD = Predicate $SUBJ -- SUBJECT value of feature ~OL  DET = Determiner NO = Noun Phrase (B~R I)  ADJ = Adjective GD = Gender  NU '~= Number PS = Person ", "acronyms": [[115, 117], [0, 4], [17, 22], [52, 54], [56, 59], [73, 75], [99, 102], [128, 130], [142, 144]], "long-forms": [[120, 126], [7, 16], [26, 33], [62, 72], [78, 82], [105, 114], [135, 141], [147, 153]], "ID": "177"}, {"text": "Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). The number of entities", "acronyms": [[116, 121]], "long-forms": [[100, 114]], "ID": "178"}, {"text": "MSEAS (MS,MSEA, VP,i, OUT)  (1) Start with VP = VAR ({X1, \" \" ,X.}), MSEA = f~,  i=1, and OUT = O. When the computation is completed, MS is bound to the set of active ", "acronyms": [[90, 93], [0, 5], [7, 9], [10, 14], [16, 18], [22, 25], [43, 45], [48, 51], [69, 73], [134, 136]], "long-forms": [[96, 119]], "ID": "179"}, {"text": "We implement MVM using generative model primitives drawn from Latent Dirichlet Allocation (LDA) and the Dirichlet Process (DP). |M | disparate", "acronyms": [[123, 125], [13, 16], [91, 94]], "long-forms": [[104, 121], [62, 89]], "ID": "180"}, {"text": "at meeting the needs of CSR research, and at serving in  a complementary ole to the corpora being collected in  the interactive Air Travel Information System (ATIS) do-  main.", "acronyms": [[159, 163], [24, 27]], "long-forms": [[128, 157]], "ID": "181"}, {"text": " One example of this would be in providing support for the curation of the Gene Expression Database (GXD).4 This support could come in the form of a named entity recog-", "acronyms": [[101, 104]], "long-forms": [[75, 99]], "ID": "182"}, {"text": "1 Introduction Annotated corpora are essential for most research in natural language processing (NLP). For exam-", "acronyms": [[97, 100]], "long-forms": [[68, 95]], "ID": "183"}, {"text": "contains the result for Eisner?s algorithm using no transformation (N-Proj), projectivized training data (Proj), and pseudo-projective parsing (P-Proj). The", "acronyms": [[144, 150], [68, 74], [106, 110]], "long-forms": [[124, 142]], "ID": "184"}, {"text": "The partitioning of the dataset is listed in the Table 1, where we also give the partitioning of Wall Street Journal (WSJ) (Marcus et al, 1993) used to train the English grammar.", "acronyms": [[118, 121]], "long-forms": [[97, 116]], "ID": "185"}, {"text": "conceptualizing the relation of coincidence or proximity with the whole  Landmark (LM) when it is conceived as a  point.", "acronyms": [[83, 85]], "long-forms": [[73, 81]], "ID": "186"}, {"text": "far and formulate new ones inspired by latent semantic analysis (LSA), which was developed within the information retrieval (IR) community to treat synonymous and polysemous terms (Deerwester et", "acronyms": [[125, 127], [65, 68]], "long-forms": [[102, 123], [39, 63]], "ID": "187"}, {"text": " ? Reverse Gap (RG), if (i2 + 1) < i3 for OL and if (i6 + 1) < i1 for OR. (", "acronyms": [[16, 18]], "long-forms": [[3, 14]], "ID": "188"}, {"text": "Figure 3: System I performance for each relation (CC=CAUSE-EFFECT, IA=INSTRUMENTAGENCY, PP=PRODUCT-PRODUCER, OE=ORIGIN-ENTITY, TT=THEME-TOOL,", "acronyms": [[50, 52], [67, 69], [88, 90], [109, 111], [127, 129]], "long-forms": [[53, 65], [70, 86], [91, 107], [112, 125], [130, 140]], "ID": "189"}, {"text": "ZC05 (Zettlemoyer and Collins 2005) 79.3 ?  ZC07 (Zettlemoyer and Collins 2007) 86.1 ? ", "acronyms": [[44, 48], [0, 4]], "long-forms": [[50, 78]], "ID": "190"}, {"text": "Figure 1(a), the node @VP indicates that a binarization has been performed on the subtree VP (VBD PRT PP). All remaining rules that", "acronyms": [[90, 92], [23, 25]], "long-forms": [[94, 101]], "ID": "191"}, {"text": "ilarity between given texts. The first approach is based on vector space models (VSMs) (Meadow, 1992).", "acronyms": [[81, 85]], "long-forms": [[60, 79]], "ID": "192"}, {"text": "1 25   2. Loca l  Word  Grouper  (LWG)  The funct ion  of th i s  b lock  is to fo rm ", "acronyms": [[34, 37]], "long-forms": [[10, 31]], "ID": "193"}, {"text": " 2.3 Evaluation We use the Dutch part of EuroWordNet (DWN) (Vossen, 1998) for evaluation of our hypernym ex-", "acronyms": [[54, 57]], "long-forms": [[27, 52]], "ID": "194"}, {"text": " 1 Introduction  Natural Language Generation (NLG) systems must of  course be evaluated, like all NLP systems.", "acronyms": [[46, 49], [98, 101]], "long-forms": [[17, 44]], "ID": "195"}, {"text": " REF = obj123 SIZE = sizesensorreading85 SHAPE = shapesensorreading62", "acronyms": [[14, 18], [1, 4], [41, 46]], "long-forms": [[21, 40], [49, 69]], "ID": "196"}, {"text": "2008) since it also includes structural information of arguments. It is based on the Argumentation Markup Language (AML) that models argument components in a XML-based tree structure.", "acronyms": [[116, 119], [158, 161]], "long-forms": [[85, 114]], "ID": "197"}, {"text": "Noun Variation (NV) ? Adjective Variation (AdjV) ?", "acronyms": [[43, 47]], "long-forms": [[22, 41]], "ID": "198"}, {"text": " 1 Introduction Question answering (QA) has emerged as a practical research problem for pushing the boundaries", "acronyms": [[36, 38]], "long-forms": [[16, 34]], "ID": "199"}, {"text": "refers to the fact that either a particular lexical item or a particular grammatical construction must be present for the omission of a frame element (FE) to occur.", "acronyms": [[151, 153]], "long-forms": [[136, 149]], "ID": "200"}, {"text": "In Proc. of the Eighth  Text Retrieval Conference (TREC-8), pages 151162.", "acronyms": [[51, 57]], "long-forms": [[16, 49]], "ID": "201"}, {"text": "are verbs.  Translation Filter (TF) handles both Predicate Mismatch and Verb?Non-Verb translation shift er-", "acronyms": [[32, 34]], "long-forms": [[12, 30]], "ID": "202"}, {"text": "started.  British National Corpus (BNC)4, American  National Corpus (ANC)5 had been referenced ", "acronyms": [[35, 38], [69, 72]], "long-forms": [[10, 33], [42, 67]], "ID": "203"}, {"text": "characteristic, we regard the sentiment  classification as a sequence labeling problem and  use conditional random field (CRFs) model to  capture the relation between two adjacent ", "acronyms": [[122, 126]], "long-forms": [[96, 120]], "ID": "204"}, {"text": " 3.2 Convolutional Neural Network model The Convolutional Neural Network (CNN) model using raw audio as input is shown in Figure 1.", "acronyms": [[74, 77]], "long-forms": [[44, 72]], "ID": "205"}, {"text": "2Although independently-developed implementations of  essentially the same algorithm can be found in the source code  of The Attribute Logic Engine (ALE) version 3.2 (Carpenter  & Penn, 1999) and the SICStus Prolog term utilities library ", "acronyms": [[149, 152]], "long-forms": [[125, 147]], "ID": "206"}, {"text": " On the other hand, the final, \"concrete\", level  of semantic representation (SemRep) is more  like a fully-fledged logical form and it is no ", "acronyms": [[78, 84]], "long-forms": [[53, 76]], "ID": "207"}, {"text": "Semantic Feature Performance  The semantic features include Named Entity  (NE), Noun Hypernym (NHype) and Head Verb  Synset (HVSyn).", "acronyms": [[95, 100], [75, 77], [125, 130]], "long-forms": [[80, 93], [60, 72], [106, 123]], "ID": "208"}, {"text": "al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organi-", "acronyms": [[42, 45], [11, 15], [82, 85]], "long-forms": [[36, 40], [74, 80]], "ID": "209"}, {"text": "Question is defined as a Question term (QTerm).  The Answer Term (ATerm) is the Answer given by the KM corpus.", "acronyms": [[66, 71], [40, 45], [100, 102]], "long-forms": [[53, 64], [25, 38]], "ID": "210"}, {"text": "2003). Another kind of verb-based multiword expression is light verb constructions (LVCs), such as the examples in (1).", "acronyms": [[84, 88]], "long-forms": [[58, 82]], "ID": "211"}, {"text": "results in Die event. Recent improvements of convolutional neural networks (CNNs) have been proven to be efficient for capturing syntactic and", "acronyms": [[76, 80]], "long-forms": [[45, 74]], "ID": "212"}, {"text": "    The difference between the two models was the design of the input layer. The first model (henceforth, the diagnostics model DIAG) took diagnostics as input nodes, whereas the second model (henceforth, the semantic parameters model SEMANP) took semantic parameters as input nodes, as presented in detail below.    The Diagnostics Model (DIAG): Binary ac-ceptability values of the phrases or sentences formed by the syntactic diagnostics constituted the input nodes for the network (see above for the SI diagnostics). Each syntactic diagnostic provided a binary value (either 0 or 1) to one of the input nodes.", "acronyms": [[340, 344], [128, 132], [235, 241]], "long-forms": [[321, 338], [110, 120], [209, 234]], "ID": "213"}, {"text": "sociations to a number of affect categories including the six Ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 General Inquirer (GI) (Stone et al, 1966) has 11,788 words labeled with 182 cat-", "acronyms": [[148, 150]], "long-forms": [[130, 146]], "ID": "214"}, {"text": "An alternative to QE is to perform the expansion in the document. Document Expansion (DE) was first proposed in the speech retrieval commu-", "acronyms": [[86, 88], [18, 20]], "long-forms": [[66, 84]], "ID": "215"}, {"text": "For the discourse  structure analysis, we suggest a statistical model  with discourse segment boundaries (DSBs)  similar to the idea of gaps suggested for a ", "acronyms": [[106, 110]], "long-forms": [[76, 104]], "ID": "216"}, {"text": "  SVM Classification  SVM (Support Vector Machines) has attracted  much attention since it was introduced in (Boser et ", "acronyms": [[22, 25], [2, 5]], "long-forms": [[27, 50]], "ID": "217"}, {"text": " 1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two tex-", "acronyms": [[37, 39]], "long-forms": [[16, 35]], "ID": "218"}, {"text": "representation and transformation are necessary.  (2)  Latent Semantic Indexing (LSI) without  transformation (LSI-Com): we first merge the ", "acronyms": [[81, 84], [111, 118]], "long-forms": [[55, 79]], "ID": "219"}, {"text": "The processing of BADGER is not significantly different than that of the CIRCUS system used i n previous MUC evaluations [4, 5, 6] . Concept node (CN) definitions are still used to create case frame instantiation s and multiple CN definitions can apply to the same text fragment .", "acronyms": [[147, 149], [105, 108], [228, 230]], "long-forms": [[133, 145]], "ID": "220"}, {"text": "  Secondly, as for the word order of prepositional  phrases (PP), Arabic and English are similar in  that PPs generally appear at the end of the sen-", "acronyms": [[61, 63], [106, 109]], "long-forms": [[52, 59]], "ID": "221"}, {"text": "Next subsections show the results obtained by TIPSem system in each one of the TempEval-2 tasks for English (EN) and Spanish (ES). More-", "acronyms": [[109, 111], [46, 52], [79, 89], [126, 128]], "long-forms": [[100, 107], [117, 124]], "ID": "222"}, {"text": "ambiguation as a binary classification problem. Li  then uses Support Vector Machine (SVM) with  mutual information between each Chinese charac-", "acronyms": [[86, 89]], "long-forms": [[62, 84]], "ID": "223"}, {"text": "inforced by the proposed method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used.", "acronyms": [[68, 70]], "long-forms": [[53, 66]], "ID": "224"}, {"text": "185  Table h Size of the corpora of HTML pages (in Mb) collected on the four patterns (1.a-d)  through AltaVista (AV) and Northern Light (NL). ", "acronyms": [[114, 116], [138, 140], [36, 40], [51, 53]], "long-forms": [[103, 112], [122, 136]], "ID": "225"}, {"text": "the second sentence provides some further description of that entity. An Entity Relation (EntRel) was annotated for such sentence pairs as below.", "acronyms": [[90, 96]], "long-forms": [[73, 88]], "ID": "226"}, {"text": "rank verb pairs with respect to the strength of their association with a particular discourse relation. We adapted versions of standard lexical association measures like PMI (pointwise mutual information) and their variants, as well as some measures specific to the association of a causal relation between items (Do", "acronyms": [[170, 173]], "long-forms": [[175, 203]], "ID": "227"}, {"text": "spect to the ISSC. We will refer to this expanded version of the SSC as the processed SSC (PSSC). ", "acronyms": [[91, 95], [13, 17], [65, 68]], "long-forms": [[76, 89]], "ID": "228"}, {"text": "tried two types of expansion, one mainly using synonyms (SYN), and one mainly using hypernyms or related links (LNK). ", "acronyms": [[112, 115], [57, 60]], "long-forms": [[105, 110], [47, 55]], "ID": "229"}, {"text": " 2003. Voice extensible markup language (VoiceXML) version 2.0.", "acronyms": [[41, 49]], "long-forms": [[7, 39]], "ID": "230"}, {"text": " 5 Conclusions Multiword expressions (MWEs) are a major obstacle that hinder precise natural language processing", "acronyms": [[38, 42]], "long-forms": [[15, 36]], "ID": "231"}, {"text": " 1 Introduction Natural Language Inference (NLI), i.e. the task of determining whether an NL hypothesis can be in-", "acronyms": [[44, 47], [90, 92]], "long-forms": [[16, 42]], "ID": "232"}, {"text": "SaRAD .891 .919 .905 ALICE .961 .920 .940 Chang & Sch?utze (CS) .942 .900 .921 Nadeau & Turney (NT) .954 .871 .910", "acronyms": [[60, 62], [0, 5], [96, 98]], "long-forms": [[42, 58], [79, 94]], "ID": "233"}, {"text": "coding-related concepts that appear in the EHR.  We use General Equivalence Mappings (GEMs) between ICD-9 and ICD-10 codes (CMS, 2014)", "acronyms": [[86, 90], [43, 46], [100, 105], [110, 116], [124, 127]], "long-forms": [[56, 84]], "ID": "234"}, {"text": "LR = Lagrangian relaxation; DP = exhaustive dynamic programming; ILP = integer linear programming; LP = linear programming (LP does not recover an exact solution).", "acronyms": [[65, 68], [0, 2], [28, 30], [99, 101], [124, 126]], "long-forms": [[71, 97], [5, 26], [44, 63], [104, 122]], "ID": "235"}, {"text": "  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 928?937, October 25-29, 2014, Doha, Qatar.", "acronyms": [[90, 95]], "long-forms": [[40, 88]], "ID": "236"}, {"text": "the bird comes.  Table 2: Stories generated by a system that uses plots and genetic search (PlotGA), a system that uses only plots (Plot), McIntyre and Lapata (2009)?s rank-based system (Rank) and a system that randomly", "acronyms": [[92, 98]], "long-forms": [[66, 90]], "ID": "237"}, {"text": "fcn@dsic.upv.es Abstract State-of-the-art Machine Translation (MT) systems are still far from being perfect.", "acronyms": [[63, 65]], "long-forms": [[42, 61]], "ID": "238"}, {"text": "3 Results  The experiments were completed using the revised  RTE3 development set (RTE3Devmt) before the  RTE3Test results were released.", "acronyms": [[83, 92], [106, 114]], "long-forms": [[61, 81]], "ID": "239"}, {"text": " In this paper, we propose a new, structured vector space model for word meaning (SVS) that addresses these problems.", "acronyms": [[82, 85]], "long-forms": [[34, 57]], "ID": "240"}, {"text": " 6 Conclusion In this work, we manually constructed a Phased Predicate Template Taxonomy (PPTT), which is a network of semantically coherent classes of templates and derived semantic relations including entailment", "acronyms": [[90, 94]], "long-forms": [[54, 88]], "ID": "241"}, {"text": "1997). Theory retinement is mainly used (and has its  origin) in Knowledge Based Systems (KBS) (Craw  and Sleeman, 1990).", "acronyms": [[90, 93]], "long-forms": [[65, 88]], "ID": "242"}, {"text": "User Group (WON).4 Research and development  is carried out in close collaboration with user  groups and intellectual property (IP) professionals to ensure solutions and software are delivered ", "acronyms": [[128, 130], [12, 15]], "long-forms": [[105, 126]], "ID": "243"}, {"text": "  Definition: Asymmetric nearest common  ancestor (ANCA)  The asymmetric nearest common ancestors from ", "acronyms": [[51, 55]], "long-forms": [[14, 49]], "ID": "244"}, {"text": " Disco-En-Gold consists of 349 expressions divided into training (TrainD), validation (ValD), and test data (TestD) manually assigned scores from 0", "acronyms": [[87, 91], [109, 114], [66, 72]], "long-forms": [[75, 85], [98, 107], [43, 64]], "ID": "245"}, {"text": "labels.5 We simulate a user?s constraints by ranking words in the training split by their information gain (IG).6 After ranking the top 200 words for each class", "acronyms": [[108, 110]], "long-forms": [[90, 106]], "ID": "246"}, {"text": "heres to the dimensions presented in Table 2, and negation scopes are modeled using a first order linear-chain conditional random field (CRF)2, with a label set of size two indicating whether a", "acronyms": [[137, 140]], "long-forms": [[111, 135]], "ID": "247"}, {"text": "Snow follows that of (Escudero et al, 2000c).  2.4 LazyBoost ing  (LB)  The main idea of boosting algorithms is to ", "acronyms": [[67, 69]], "long-forms": [[51, 60]], "ID": "248"}, {"text": "egie Group, Inc. (CGI) of Pittsburgh, PA is to promote  and further develop automatic Text Summarization  using a Maximal Marginal Relevance (MMR) metric to  generate summaries of documents hat are directly rele- ", "acronyms": [[142, 145], [18, 21], [38, 40]], "long-forms": [[114, 140]], "ID": "249"}, {"text": "of units. '  i Note that unlike RST, Veins Theory (VT) is not  concerned with the type of relations which hold ", "acronyms": [[51, 53], [32, 35]], "long-forms": [[37, 49]], "ID": "250"}, {"text": "There are two other functional tags which, unlike those listed above, can also be associated with numbered arguments in the frames files. The first one, EXT (extent), indicates that a constituent is a numerical argument on its verb, as in climbed 15%", "acronyms": [[153, 156]], "long-forms": [[158, 164]], "ID": "251"}, {"text": "176 Figure 2: The embedding graph for the sentence Our previous results show that recombinant gp41 (aa565-647), the extracellular domain of HIV-1 transmembrane glycoprotein, stimulates interleukin-10 (IL-10) production in human monocytes.", "acronyms": [[201, 206], [140, 145]], "long-forms": [[185, 199]], "ID": "252"}, {"text": "Arabic Penn TreeBank POS tagset. Base Phrase (BP) Chunking is the process of creating non-recursive base phrases such", "acronyms": [[46, 48], [21, 24]], "long-forms": [[33, 44]], "ID": "253"}, {"text": "It includes the four original partners  of the LATER project and the following new partners: University of Amsterdam (UvA) in the Netherlands, Free University of Bolzano-Bozen (FUB) ", "acronyms": [[118, 121], [47, 52], [177, 180]], "long-forms": [[93, 116], [143, 175]], "ID": "254"}, {"text": "The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF)", "acronyms": [[69, 75], [112, 115]], "long-forms": [[52, 67], [85, 110]], "ID": "255"}, {"text": "ity? have been designed so far for French L1 and only one for French as a foreign language (FFL) (see Section 2).", "acronyms": [[92, 95], [42, 44]], "long-forms": [[62, 90]], "ID": "256"}, {"text": "/fi:/. 2.1 Foreign Words From an information retrieval (IR) perspective, foreign words in Arabic can be classified into two gen-", "acronyms": [[56, 58]], "long-forms": [[33, 54]], "ID": "257"}, {"text": "by the name of its author or the author?s place of  work.  In computational linguistic (CL) terms thi s  exercise relies on proper noun extra ction.", "acronyms": [[88, 90]], "long-forms": [[62, 86]], "ID": "258"}, {"text": "found in each row, as well as the level of granularity of analysis in each row.3 2KEY: ABS=abstract, COM=completive, CL=classifier, DEM=demonstrative, E=ergative, EV=evidential, S=singular,", "acronyms": [[87, 90], [101, 104], [117, 119]], "long-forms": [[91, 99], [105, 115], [120, 130]], "ID": "259"}, {"text": "them, and finally generating and displaying them.  The Input Analyzer (IA) of the system is the  most stable end experimented component and it is ", "acronyms": [[71, 73]], "long-forms": [[55, 69]], "ID": "260"}, {"text": "Abstract  This paper describes a reestimation method for stochastic language models uch as the  N-gram model and the Hidden Maxkov Model(HMM) from ambiguous observations. It is ", "acronyms": [[137, 140]], "long-forms": [[117, 135]], "ID": "261"}, {"text": " 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al.,", "acronyms": [[37, 39]], "long-forms": [[16, 35]], "ID": "262"}, {"text": "We first describe how to generate multiple FDTs for each sentence pair in training corpus C based on the forced decoding (FD) technique, which performs via the following four steps:", "acronyms": [[122, 124], [43, 47]], "long-forms": [[105, 120], [83, 89]], "ID": "263"}, {"text": " Figure 5 also gives a speed comparison of our method to a linear programming (LP) solver that solves the LP relaxation defined by constraints D0?", "acronyms": [[79, 81]], "long-forms": [[59, 77]], "ID": "264"}, {"text": "Lowe HJ, Barnett GO. ( 1994) Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches.", "acronyms": [[83, 87], [5, 7]], "long-forms": [[57, 81]], "ID": "265"}, {"text": "The RASP toolkit (Briscoe et al, 2006) is used for sentence boundary detection, tokenisation, PoS tagging and finding grammatical relations (GR) between words in the text.", "acronyms": [[141, 143], [4, 8], [94, 97]], "long-forms": [[118, 139]], "ID": "266"}, {"text": "Common error measures are the Word Error Rate (WER) and the Position Independent Word Error Rate (PER) as well as evaluation metric on the n-gram level like the BLEU and", "acronyms": [[98, 101], [47, 50], [161, 165]], "long-forms": [[60, 96], [30, 45]], "ID": "267"}, {"text": "Discovery of ambiguous and unambiguous discourse connectives via annotation projection. In Proceedings of Workshop on Annotation and Exploitation of Parallel Corpora (AEPC), pages 83?82, Tartu, Estonia.", "acronyms": [[167, 171]], "long-forms": [[118, 165]], "ID": "268"}, {"text": " 2. Character Error Rate (CER): Edit distance in terms of characters between the target sentence", "acronyms": [[26, 29]], "long-forms": [[4, 24]], "ID": "269"}, {"text": "show how to construct MWE-aware training resources for them.  The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the French Treebank (FTB) (Abeill?,", "acronyms": [[128, 131], [22, 31], [179, 182]], "long-forms": [[111, 126], [162, 177]], "ID": "270"}, {"text": "corpus (DUC2002) and the second is automatically extracted from related web news stories (WNS) automatically extracted.", "acronyms": [[90, 93], [8, 15]], "long-forms": [[72, 88]], "ID": "271"}, {"text": "Markov Model (HHMM) word), all the corresponding TYP candidates triggered by categorized word features(CWF) should be removed.", "acronyms": [[103, 106], [14, 18], [49, 52]], "long-forms": [[77, 101], [0, 12]], "ID": "272"}, {"text": "concept, BLESS contains several relata,  connected to it through one relation, such as cohyponymy (COORD), hypernymy (HYPER),  meronymy (MERO) or no-relation (RANDOM-N).2 ", "acronyms": [[118, 123]], "long-forms": [[107, 116]], "ID": "273"}, {"text": "participant Japanese systems were developed in a four-  month period of time and output results comparable to the Message Understanding Conference-6 (MUC-6) \\[1\\]  English language systems with F-Measures between 70 - ", "acronyms": [[150, 155]], "long-forms": [[114, 148]], "ID": "274"}, {"text": "known as NP The corpus of English Wikipedia pages, known as EnWiki NP ( * NP) Hidden Markov Model (HMM) is used to solve ...", "acronyms": [[99, 102], [74, 76], [67, 69], [9, 11]], "long-forms": [[78, 97]], "ID": "275"}, {"text": "forward and backward application (FA and BA), implies a  standard notion of constituency, rules like type raising (TR)  and functional composition (FC) give rise to a more  generous notion of constituency (this is what makes 'non- ", "acronyms": [[148, 150]], "long-forms": [[124, 146]], "ID": "276"}, {"text": "Revue des Sciences de l?Education (RSE) ? Traduction, Terminologie et R?daction (TTR) ?", "acronyms": [[81, 84], [35, 38]], "long-forms": [[42, 79], [0, 33]], "ID": "277"}, {"text": "URL?, excluded utterances with the symbols that indicate the re-posting (RT) or quoting (QT) of others?", "acronyms": [[89, 91], [0, 3], [73, 75]], "long-forms": [[80, 87], [61, 71]], "ID": "278"}, {"text": "On the one hand, we built machine learning classifiers based on Support Vector Machines (SVMs) and Conditional Random Fields (CRFs).", "acronyms": [[89, 93], [126, 130]], "long-forms": [[64, 87], [99, 124]], "ID": "279"}, {"text": "3.2 Structural model We go beyond traditional feature vectors by employing structural models (STRUCT), which encode each comment into a shallow syntactic tree.", "acronyms": [[94, 100]], "long-forms": [[75, 85]], "ID": "280"}, {"text": "4.1 Syntactic template selection PERSONAGE?s input generation dictionary is made of 27 Deep Syntactic Structures (DSyntS): 9 for the recommendation claim, 12 for the comparison", "acronyms": [[114, 120]], "long-forms": [[87, 112]], "ID": "281"}, {"text": "Although later systems such as Wu and Ng (2013); Rozovskaya and Roth (2013) use Integer Linear Programming (ILP) to decode a global optimized result, the input scores", "acronyms": [[108, 111]], "long-forms": [[80, 106]], "ID": "282"}, {"text": "ture. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). ", "acronyms": [[91, 95], [13, 16]], "long-forms": [[75, 89]], "ID": "283"}, {"text": "2 values (ARG2-4, ARGM-DIS (discourse), ARGM-LOC (locative), ARGM-MNR (manner), and ARGM-TMP (temporal)), but given the large number of degrees of free-", "acronyms": [[84, 92], [40, 48], [61, 69]], "long-forms": [[94, 102], [50, 58], [71, 77]], "ID": "284"}, {"text": " As far as discriminative models are concerned,  the Maximum Entropy (MaxEnt) model has been  applied (Bohus and Rudnicky, 2006).", "acronyms": [[70, 76]], "long-forms": [[53, 68]], "ID": "285"}, {"text": "2003; Shen et al, 2006; Wubben et al, 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al, 2004), consisting of 5,801 sentence", "acronyms": [[129, 133]], "long-forms": [[98, 127]], "ID": "286"}, {"text": "chanical Turk service. Two classifiers, Na??ve Bayes (NB) and a support vector machine (SVM), were applied on the tokenized and stemmed state-", "acronyms": [[88, 91], [54, 56]], "long-forms": [[64, 86], [40, 52]], "ID": "287"}, {"text": "puterization, Technical Report 6-CICC-MT55 (1995)  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 174?182, Boulder, Colorado, June 2009.", "acronyms": [[136, 141], [33, 42]], "long-forms": [[95, 134]], "ID": "288"}, {"text": "2010). Domain event extraction has been advanced in particular by the BioNLP Shared Task (ST) events (Kim et al, 2011a; Kim et al, 2011b), which have", "acronyms": [[90, 92], [70, 76]], "long-forms": [[77, 88]], "ID": "289"}, {"text": "The results can be seen at:  http:/ /c lwww.essex.ac.uk/w3c/.  The project was  funded by JISC (the Joint Information Systems Com-  mittee of the UK Higher Education Funding Coun- ", "acronyms": [[90, 94], [146, 148]], "long-forms": [[100, 138]], "ID": "290"}, {"text": "sisting of analyses automatically created by systems participating in the recent BioNLP Shared Task (ST) 2011. In providing for the", "acronyms": [[101, 103], [81, 87]], "long-forms": [[88, 99]], "ID": "291"}, {"text": " First of all, we now first formally introduce DUDES: Definition 1 (DUDES) A DUDES is a 7-tuple (m, l, t, U,A, S,C) consisting of", "acronyms": [[68, 73], [77, 82]], "long-forms": [[54, 66]], "ID": "292"}, {"text": "ferently by (1) and (2)8.  5 The Neutral Edge Direction (NED) Measure", "acronyms": [[57, 60]], "long-forms": [[33, 55]], "ID": "293"}, {"text": "Abstract   This paper describes two algorithms which construct two differ-  ent types of generators for lexical functional grammars (LFGs). The ", "acronyms": [[133, 137]], "long-forms": [[104, 131]], "ID": "294"}, {"text": "4 GTS with the Idea of Coordination Problem Game GTS (Game Theoretic Semantics) has been developed as an alternative semantics where major se-", "acronyms": [[49, 52], [2, 5]], "long-forms": [[54, 78]], "ID": "295"}, {"text": " As stated earlier, our unlabeled data consists of email (EMAIL) and online forum (FORUM) data.", "acronyms": [[58, 63], [83, 88]], "long-forms": [[51, 56], [76, 81]], "ID": "296"}, {"text": "Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task.", "acronyms": [[74, 77]], "long-forms": [[58, 72]], "ID": "297"}, {"text": "services (person or company information bureau), ht this  situation, the event aml the attitude of file inforumtion  possessor (IP) is transported to a speaker (SP);journalist. ", "acronyms": [[161, 163], [128, 130]], "long-forms": [[152, 159], [104, 126]], "ID": "298"}, {"text": "more, 1976; Fillmore, 1982). The classes of events are the semantic frames of the lexical units (LUs) that evoke them, and the roles associated with the", "acronyms": [[97, 100]], "long-forms": [[82, 95]], "ID": "299"}, {"text": "LDEP(NEXT) + Table 1: History-based features (TOP = token on top of stack; NEXT = next token in input buffer; HEAD(w) = head of w; LDEP(w) = leftmost depen-", "acronyms": [[75, 79], [0, 4], [46, 49], [5, 9], [110, 117], [131, 138]], "long-forms": [[82, 92], [52, 64], [120, 129], [141, 155]], "ID": "300"}, {"text": " 3.2 Query by Committee  Query by Committee (QBC) was introduced by  Seung, Opper, and Sompolinsky (1992).", "acronyms": [[45, 48]], "long-forms": [[25, 43]], "ID": "301"}, {"text": "egories (left side) and with respect to the assessor?s own expertise (right side). ( Key: B=beneficial, LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE) the assessor had no idea from which condition an", "acronyms": [[150, 152], [104, 106]], "long-forms": [[153, 172], [92, 102], [107, 124], [128, 137], [141, 148], [176, 183], [187, 190]], "ID": "302"}, {"text": "Table 1: The acoustic features which are extracted from the audio clips using Praat (Boersma and Weenink, 2010).  (MFCCs)1 and Teager Energy Operator (TEO)2 (Kaiser, 1990) based features have also been con-", "acronyms": [[151, 154], [115, 120]], "long-forms": [[127, 149]], "ID": "303"}, {"text": "to facilitate understanding. We used latent Dirichlet alocation (LDA) (Blei, Ng, and Jordan, 2003) as our exploratory tool.", "acronyms": [[65, 68]], "long-forms": [[37, 63]], "ID": "304"}, {"text": "In this shared task we test the feasibility of eliciting parallel data for Machine Translation (MT) using Mechanical Turk (MTurk). MT poses an interesting", "acronyms": [[123, 128], [96, 98], [131, 133]], "long-forms": [[106, 121], [75, 94]], "ID": "305"}, {"text": "ways available. We therefore generate the training data using a na??ve phrase aligner (NPA) instead of resorting to a real one.", "acronyms": [[87, 90]], "long-forms": [[64, 85]], "ID": "306"}, {"text": "various respects. While CKY requires a grammar in Chomsky Normal Form (CNF), LSCP takes an ordinary PG grammar, since no equivalent of the", "acronyms": [[71, 74], [24, 27], [77, 81], [100, 102]], "long-forms": [[50, 69]], "ID": "307"}, {"text": "counted through specifier phrases. A Malay example, where biji is the count classifier (CL) for fruit, is given in (1).", "acronyms": [[88, 90]], "long-forms": [[76, 86]], "ID": "308"}, {"text": " 1 Introduction  Open domain question answering (QA), as defined  by the TREC competitions (Voorhees, 2003), ", "acronyms": [[49, 51], [73, 77]], "long-forms": [[29, 47]], "ID": "309"}, {"text": "Hidden topic markov models. Artificial Intelligence and Statistics (AISTATS). ", "acronyms": [[68, 75]], "long-forms": [[28, 66]], "ID": "310"}, {"text": "On the other hand, multi-lingual  ontology is very important for natural language  processing, such as machine translation (MT), web  mining (Oyama et al 2004) and cross language ", "acronyms": [[124, 126]], "long-forms": [[103, 122]], "ID": "311"}, {"text": "User-generated content (UGC), and specially the microblog genre, has become an interesting resource for Natural Language Processing (NLP) tools and applications.", "acronyms": [[133, 136], [24, 27]], "long-forms": [[104, 131], [0, 22]], "ID": "312"}, {"text": "a major Department of Agriculture system, due to inadequate agency planning.  Complete sets of the Federal mfomtion processing staodards (FIPS) are now avail-  able from the National Bureau of Standards at $46.00 each.", "acronyms": [[138, 142]], "long-forms": [[99, 136]], "ID": "313"}, {"text": "They show improvements of up to 5.3% on two real tasks: pitch accent prediction and optical character recognition (OCR). ", "acronyms": [[115, 118]], "long-forms": [[84, 113]], "ID": "314"}, {"text": "the best among all other symmetrization heuristics. The other was a Tree Edit Distance (TED) model, popularly used in a series of NLP appli-", "acronyms": [[88, 91], [130, 133]], "long-forms": [[68, 86]], "ID": "315"}, {"text": "In International Conference on Autonomous Agents and Multiagent Systems (AAMAS). ", "acronyms": [[73, 78]], "long-forms": [[31, 71]], "ID": "316"}, {"text": "features to argument classification models, but also  represent full parsing information as constraints in  integer linear programs (ILP) to resolve label inconsistencies.", "acronyms": [[133, 136]], "long-forms": [[108, 131]], "ID": "317"}, {"text": "as defined in Section 2. For tagging, we experimented with Support Vector Machines (SVM) and Conditional Random Fields (CRF).", "acronyms": [[84, 87], [120, 123]], "long-forms": [[59, 82], [93, 118]], "ID": "318"}, {"text": " 2. All the named entities(NE) in the question are extracted as NE set.", "acronyms": [[27, 29], [64, 66]], "long-forms": [[12, 25]], "ID": "319"}, {"text": " 1 Introduction  Statistical Machine Translation (SMT) is attracting more attentions than rule-based and example-", "acronyms": [[50, 53]], "long-forms": [[17, 48]], "ID": "320"}, {"text": " In Proceedings of the 24th International Conference on Computational Linguistics (COLING). ", "acronyms": [[83, 89]], "long-forms": [[56, 81]], "ID": "321"}, {"text": "eling the sequential nature of the output. The constraint satisfaction inference (CSInf) approach (Bosch and Canisius, 2006) improves the", "acronyms": [[82, 87]], "long-forms": [[47, 80]], "ID": "322"}, {"text": "The full TBCNN-pair model outperforms all existing sentence encoding-based approaches, including a 1024d gated recurrent unit (GRU)-based RNN with ?", "acronyms": [[127, 130], [9, 14], [138, 141]], "long-forms": [[105, 125]], "ID": "323"}, {"text": " 3.2 Taboo Constraint Taboo constraint (TABOO) requires that the substitute word is a taboo word or frequently used", "acronyms": [[40, 45]], "long-forms": [[22, 38]], "ID": "324"}, {"text": "Daniele Vannella, 2013). The two system types are WSI (Word Sense Induction) and WSD (Word Sense Disambiguation).", "acronyms": [[50, 53], [81, 84]], "long-forms": [[55, 75], [86, 111]], "ID": "325"}, {"text": " 3.1 The Beta Process and the Bernoulli process The beta process(BP) (Thibaux and Jordan, 2007; Paisley and Carin, 2009) and the related Indian buf-", "acronyms": [[65, 67]], "long-forms": [[52, 64]], "ID": "326"}, {"text": " 3 Keystroke Data Collection Amazon?s Mechanical Turk (MTurk) is a web service that enables crowdsourcing of tasks that are dif-", "acronyms": [[55, 60]], "long-forms": [[38, 53]], "ID": "327"}, {"text": "three modules: (1) a question processing (QP) module; (2) a passage retrieval (PR) module; and (3) an answer processing (AP) module. Questions", "acronyms": [[121, 123]], "long-forms": [[102, 119]], "ID": "328"}, {"text": "University of Southern California  Los Angeles, California 90089  Abstract Tiffs paper describes Kind Types (KT), a system which uses  commonsense knowledge to reason about natural anguage text.", "acronyms": [[109, 111]], "long-forms": [[97, 107]], "ID": "329"}, {"text": " Definition  Default Unification (first version) AU!B = A ~ U B, where A ~ is the maximal (i.e. most  specific) element in the subsumption ordering such that A' r- A and A ~ U B is defined.", "acronyms": [[49, 53]], "long-forms": [[56, 63]], "ID": "330"}, {"text": "vp?np?pp .83 .80 .83 .80 .83 .80 vp?pp?pp .75 .74 .75 .74 .75 .74 Lexical association (LAsim) Sequences PrEC PrPGR RecEC RecPGR F-SRV F-SPGR", "acronyms": [[87, 92]], "long-forms": [[66, 85]], "ID": "331"}, {"text": "Two document collections are used in this study.  CE (Chinese Encyclopedia): This is from the electronic version of the Chinese Encyclopedia.", "acronyms": [[50, 52]], "long-forms": [[54, 74]], "ID": "332"}, {"text": "Among the NEs we select six of them as the recognized objects, that is, personal name (PN), date or time (DT), location name (LN), team name (TN), competition title (CT) and per-", "acronyms": [[106, 108], [126, 128]], "long-forms": [[92, 104], [111, 124]], "ID": "333"}, {"text": "number, fifteen predefined verbs (Paul, 2010)  are chosen as Light Verbs (LVs) for framing  the compound verbs (CompVs).  A manually ", "acronyms": [[112, 118], [74, 77]], "long-forms": [[96, 110], [61, 72]], "ID": "334"}, {"text": "Syntactic Tree  Ambiguous Semantic  Expression (EFL)  Unambiguous Semantic ", "acronyms": [[48, 51]], "long-forms": [[36, 46]], "ID": "335"}, {"text": "Based on these two conditions we have come up with a linear combination of two cost components, similar to Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998).", "acronyms": [[135, 138]], "long-forms": [[107, 133]], "ID": "336"}, {"text": " 207  Informational Contribution (IC) function for each element  in a pair.", "acronyms": [[34, 36]], "long-forms": [[6, 32]], "ID": "337"}, {"text": "Concerned about this inflation of the grammar constant, (DeNero et al, 2009) consider a superset of CNF called Lexical Normal Form (LNF). A rule is", "acronyms": [[132, 135], [100, 103]], "long-forms": [[111, 130]], "ID": "338"}, {"text": "negative has an incorrect, but plausible, stress pattern, u. We adopt a Support Vector Machine (SVM) solution to these ranking constraints as described by", "acronyms": [[96, 99]], "long-forms": [[72, 94]], "ID": "339"}, {"text": "1540  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 7?14, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[75, 80], [84, 88]], "long-forms": [[41, 73]], "ID": "340"}, {"text": " Other valuable categories are, for example,  pronominal adverbs (PAV) and infinitives of auxil-  iary verbs (VAINF), where the difference between ", "acronyms": [[66, 69], [110, 115]], "long-forms": [[46, 64]], "ID": "341"}, {"text": "well. The formal framework for analysis will be the Discourse Representation Theory (DRT). ", "acronyms": [[85, 88]], "long-forms": [[52, 83]], "ID": "342"}, {"text": "MusicalArtist:album or Book:genre. Therefore, in addition to recognising entities with Stanford NERC, we also implement our own named entity recogniser (NER), which only recognises entity boundaries, but does not classify them.", "acronyms": [[153, 156], [96, 100]], "long-forms": [[128, 151]], "ID": "343"}, {"text": "ID full papers domain (TCS) 10 BB web pages domain (BB) 2 BI abstracts domain (BS) 10 Table 1: Characteristics of BioNLP-ST 2011 main tasks.", "acronyms": [[79, 81], [0, 2], [23, 26], [31, 33], [52, 54], [114, 123]], "long-forms": [[58, 70]], "ID": "344"}, {"text": "Semantic Interpretation of Prepositions for NLP Applications Sven Hartrumpf Hermann Helbig Rainer Osswald Intelligent Information and Communication Systems (IICS) University of Hagen (FernUniversita?t in Hagen)", "acronyms": [[157, 161], [44, 47]], "long-forms": [[106, 155]], "ID": "345"}, {"text": "Abbreviations POS = Part of Speech NE = Named Entity CE = Correlated Entity", "acronyms": [[35, 37]], "long-forms": [[40, 52]], "ID": "346"}, {"text": "  Abstract  The Colorado Literacy Tutor (CLT) is a  technology-based literacy program, designed ", "acronyms": [[41, 44]], "long-forms": [[16, 39]], "ID": "347"}, {"text": "restriction to do top-down filtering.  1986 Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood.", "acronyms": [[117, 120]], "long-forms": [[102, 115]], "ID": "348"}, {"text": "tions.  3.1 Simple Classification (SC) Given an expression x consisting of n words x1,", "acronyms": [[35, 37]], "long-forms": [[12, 33]], "ID": "349"}, {"text": "tributed to idiosyncrasies in the translation. For instance, Emma (EM) seems very difficult to align, which can be attributed to the use of an old transla-", "acronyms": [[67, 69]], "long-forms": [[61, 65]], "ID": "350"}, {"text": "example, we have defined that all the subclasses of #COMMUNICATION-EVENTS (e.g.  #REPORT#,  #CONFIRM#, etc.) map their sentential complements (SENT-COMP)  to THEME, as shown below.", "acronyms": [[143, 152], [158, 163]], "long-forms": [[119, 141]], "ID": "351"}, {"text": "presidential nomination to seek the backing of the {{w|Libertarian Party (United States)|Libertarian Party}} (LP). ", "acronyms": [[110, 112]], "long-forms": [[89, 107]], "ID": "352"}, {"text": "Wiebe, 2000).  4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more", "acronyms": [[47, 53]], "long-forms": [[21, 45]], "ID": "353"}, {"text": "To tackle this  problem, we propose to employ the  Support Vector Machines(SVM) in  determining the grammatical functions.", "acronyms": [[75, 78]], "long-forms": [[51, 73]], "ID": "354"}, {"text": "obtain a bilingual database. The database is  called the ATR Dialogue Database(ADD). ", "acronyms": [[79, 82]], "long-forms": [[57, 77]], "ID": "355"}, {"text": "(2) works7. This algorithm is evaluated in (Jin and Tanaka-Ishii, 2006) using Peking University (PKU) 7Three segmentation criteria are given in (Jin and Tanaka-", "acronyms": [[97, 100]], "long-forms": [[78, 95]], "ID": "356"}, {"text": "processing beyond keyword indexing, typically supported by Natural Language Processing (NLP)  and Information Extraction (IE) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, Li ", "acronyms": [[122, 124], [88, 91]], "long-forms": [[98, 120], [59, 86]], "ID": "357"}, {"text": "In Table 1, we show the precision, recall, and F1 measures of our domain-aware method (DOM) and the baseline method (BL) in all three sets of experiments.", "acronyms": [[117, 119], [87, 90]], "long-forms": [[100, 108], [66, 85]], "ID": "358"}, {"text": "R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms  13th International Conference on  Computational Linguistics (COLING'90). ", "acronyms": [[125, 134]], "long-forms": [[98, 123]], "ID": "359"}, {"text": "TC 59% 59% 79% 85% BTC 86% 86% 86% 92% Table 1: Task Completion (TC) and Binary Task Completion (BTC) prediction results, using auto-", "acronyms": [[65, 67], [0, 2], [19, 22], [97, 100]], "long-forms": [[48, 63], [73, 95]], "ID": "360"}, {"text": "step, we apply the CFG IDENTIFICATION to the string  under (2) in order to \"transform\" the sequence of simple  syntactic units into so-called Segmentation Units (SU)  \\[we use the following conventions: \"( )\" for facultativi- ", "acronyms": [[162, 164], [19, 22]], "long-forms": [[142, 160]], "ID": "361"}, {"text": "in the preceding verb phrase (PV), the head noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available (Chodorow et al, 2007).", "acronyms": [[131, 133], [30, 32], [79, 81]], "long-forms": [[108, 122], [17, 28], [56, 77]], "ID": "362"}, {"text": " In unvocalised text, the standard written form of Modern Standard Arabic (MSA), it may happen that the stem and the root of a word form are one and the", "acronyms": [[75, 78]], "long-forms": [[51, 73]], "ID": "363"}, {"text": " The test set (Table 13) consisted of the beginnings of three short stories by  Ernest  Hemingway,  15 three articles f rom the New York Times (NYT), 16 the first three chapters of  a novel  by  Uwe JohnsonS the first two chapters of a short story by Heiner Mfiller, TM ", "acronyms": [[144, 147], [267, 269]], "long-forms": [[128, 142]], "ID": "364"}, {"text": "1 3 Note that there is only one column for recall, which is unaffected by the choice o f Matched/Missing (M/M) versus All Templates (AT) . ", "acronyms": [[133, 135], [106, 109]], "long-forms": [[118, 131], [89, 104]], "ID": "365"}, {"text": "associated with each. The next column indicates the percentage of the majority class (MAJ.) and count", "acronyms": [[86, 90]], "long-forms": [[70, 78]], "ID": "366"}, {"text": "4.2 Results Table 3 shows the results of our compression models by compression rate (CompR), dependencybased F1 (F1-Dep), and SRL-based F1 (F1-SRL).", "acronyms": [[85, 90], [126, 129], [140, 147]], "long-forms": [[67, 83]], "ID": "367"}, {"text": "Orlando, Florida 32810  AFIPS CONSTITUENT SOCIETIES  Instrument Society of America (ISA)  Purpose ", "acronyms": [[84, 87], [24, 29]], "long-forms": [[53, 82]], "ID": "368"}, {"text": "This paper describes the UNITOR system participating in both tasks of the *SEM 2013 shared task on Semantic Textual Similarity (STS), described in (Agirre et al 2013):", "acronyms": [[128, 131], [25, 31], [75, 78]], "long-forms": [[99, 126]], "ID": "369"}, {"text": " This representation treats clitics as separate tokens and abstracts the orthographic rewrites they undergo when cliticized. See the handling of the l/PREP+Al/DET in word #6 in Table 5.  This representation is used by the LDC in the Penn Arabic Treebank (PATB) (Maamouri  et al., 2004) and tools such as MADAMIRA (Pasha et al.,", "acronyms": [[255, 259], [222, 225], [149, 155], [156, 162], [304, 312]], "long-forms": [[233, 253]], "ID": "370"}, {"text": "Na?ve Bayes (De Sitter and Daelemans, 2003).  Maximum Entropy (ME) conditional models like  ME Markov models (McCallum et al, 2000) and ", "acronyms": [[63, 65], [92, 94]], "long-forms": [[46, 61]], "ID": "371"}, {"text": "1 Scope and Prior Work We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems.", "acronyms": [[120, 123]], "long-forms": [[88, 118]], "ID": "372"}, {"text": "the curve (AUC) from the trained detector on the heldout test set. Results are reported in terms of the mean and standard deviation (SD) of the AUC across all splits. ", "acronyms": [[133, 135], [11, 14], [144, 147]], "long-forms": [[113, 131]], "ID": "373"}, {"text": "TIGER treebank. In Proceedings of the 1st Workshop on Treebanks and Linguistic Theories (TLT), pages 24?42. ", "acronyms": [[89, 92], [0, 5]], "long-forms": [[54, 87]], "ID": "374"}, {"text": "Since it is costly to compute the partition function over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model (Mnih and", "acronyms": [[118, 121]], "long-forms": [[87, 116]], "ID": "375"}, {"text": "Linear Program relaxation based on single-commodity flow. LP(M): Linear Program relaxation based on multi-commodity flow.", "acronyms": [[58, 60]], "long-forms": [[65, 79]], "ID": "376"}, {"text": "  Figure 2.  Words Correct (WC) scores from Teachers  and Machine scoring at the reader level (n = 87).", "acronyms": [[28, 30]], "long-forms": [[13, 26]], "ID": "377"}, {"text": "+ raising verb - non-raising verb KEYAGR (key agreement) ?", "acronyms": [[34, 40]], "long-forms": [[42, 55]], "ID": "378"}, {"text": "Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Transducer. ", "acronyms": [[125, 127], [35, 38], [57, 60], [86, 90]], "long-forms": [[128, 143], [61, 84], [91, 123]], "ID": "379"}, {"text": "(section 24). Legend of models: ST=Split Tags; EC=enhanced connectivity. ", "acronyms": [[47, 49]], "long-forms": [[50, 71]], "ID": "380"}, {"text": "Because of a scarcity of such corpora, most work has used the International Corpus of Learner English (ICLEv2) (Granger et al 2009) for training and evaluation", "acronyms": [[103, 109]], "long-forms": [[62, 101]], "ID": "381"}, {"text": "teams employed vector-based linear classifiers of different types: Hacioglu et al (2004) and Park et al (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al (2004) used Voted Percep-", "acronyms": [[141, 144]], "long-forms": [[116, 139]], "ID": "382"}, {"text": " 1 Introduction As Machine Translation (MT) systems become widely adopted both for gisting purposes and to", "acronyms": [[40, 42]], "long-forms": [[19, 38]], "ID": "383"}, {"text": " ITSPOKE-WOZ is a semi-automatic version of ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), which is a speech-enhanced ver-", "acronyms": [[44, 51], [1, 12]], "long-forms": [[53, 80]], "ID": "384"}, {"text": "twelve NE tags. The NE tagged corpus has been  used to develop Named Entity Recognition (NER)  system in Bengali using pattern directed shallow ", "acronyms": [[89, 92], [7, 9], [20, 22]], "long-forms": [[63, 87]], "ID": "385"}, {"text": "vided within scientific articles. In addition, image Regions of Interest (ROIs) are commonly referred to within the image caption.", "acronyms": [[74, 78]], "long-forms": [[53, 72]], "ID": "386"}, {"text": "C5 Compound Analyser for Robust Morphological Analysis]. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.", "acronyms": [[85, 90]], "long-forms": [[57, 83]], "ID": "387"}, {"text": "The majority of dependency-based features are constructed using the properties of edges and vertices along the shortest path (SP) of an entity pair. ", "acronyms": [[126, 128]], "long-forms": [[111, 124]], "ID": "388"}, {"text": "to the recent evaluations of domain-independent Q/A systems organized in the context of the Text REtrieval Conference (TREC)1. The TREC", "acronyms": [[119, 123], [48, 51], [131, 135]], "long-forms": [[92, 117]], "ID": "389"}, {"text": "not observable from the sentence.  The dynamic nature of named entities (NEs) makes it difficult to enumerate all of their evolv-", "acronyms": [[73, 76]], "long-forms": [[57, 71]], "ID": "390"}, {"text": "Table 4: Accuracies on a balanced test set (random baseline: 0.5). NB = NaiveBayes. JRip?2 = Jripper with ?", "acronyms": [[67, 69], [84, 88]], "long-forms": [[72, 82], [93, 100]], "ID": "391"}, {"text": "{csjxu, csluqin}@comp.polyu.edu.hk Abstract The Semantic Textual Similarity (STS) task aims to exam the degree of semantic", "acronyms": [[77, 80]], "long-forms": [[48, 75]], "ID": "392"}, {"text": "3.1 Feature Structure To implement the twin model, we adopt the log linear or maximum entropy (MaxEnt) model (Berger et al, 1996) for its flexibility of combining diverse sources of", "acronyms": [[95, 101]], "long-forms": [[78, 93]], "ID": "393"}, {"text": "conditional models are computed directly from data.  In this study, we use a Maximum Entropy (MaxEnt) classifier to combine the decision characteristic fea-", "acronyms": [[94, 100]], "long-forms": [[77, 92]], "ID": "394"}, {"text": " won out on F-measure while giza++ syllabized attained better alignment error rate (AER). Refer to Table 3 for", "acronyms": [[84, 87]], "long-forms": [[62, 82]], "ID": "395"}, {"text": "A Linear Support Vector Machine text classifier (Joachims, 1999) was trained on Web pages from the Open Directory Project (ODP)6. These pages ef-", "acronyms": [[123, 126]], "long-forms": [[99, 121]], "ID": "396"}, {"text": "Our objective is to study the effect of using bigram features against co?occurrences in first (PB) and second (SC) order context vectors while using relatively small amounts of training data per word.", "acronyms": [[111, 113], [95, 97]], "long-forms": [[103, 109]], "ID": "397"}, {"text": "ticPhrase, Predicate, Argument, ? the MILE Data Categories (MDC) which constitute the attributes and values to adorn", "acronyms": [[60, 63]], "long-forms": [[38, 58]], "ID": "398"}, {"text": "From raw text extract the temporal entities (events and timexes), identify the pairs of temporal entities that have a temporal link (TLINK) and classify the temporal relation between them.", "acronyms": [[133, 138]], "long-forms": [[118, 131]], "ID": "399"}, {"text": "six different domains: Newswire (NW), Broadcast News (BN), Broadcast Conversations (BC), Webblog (WL), Usenet (UN), and Conversational Telephone Speech (CTS).", "acronyms": [[111, 113], [33, 35], [54, 56], [84, 86], [98, 100], [153, 156]], "long-forms": [[103, 109], [23, 31], [38, 52], [59, 82], [89, 96], [120, 151]], "ID": "400"}, {"text": " Within this framework, in this paper we describe our attempt to bridge Semantic Role Labeling (SRL) and CE by modeling proposition-level semantics for", "acronyms": [[96, 99], [105, 107]], "long-forms": [[72, 94]], "ID": "401"}, {"text": "perimented with three classifiers available in R?  logistic regression (LogR), decision tree (DTree) and support vector machines (SVM).", "acronyms": [[72, 76], [94, 99], [130, 133]], "long-forms": [[51, 70], [79, 92], [105, 127]], "ID": "402"}, {"text": "5.1 Candidate Generation We generate a list of candidate antecedents by first extracting all VPs and ADJPs (and all contiguous combinations of their constituents) from the current", "acronyms": [[101, 106], [93, 96]], "long-forms": [[108, 126]], "ID": "403"}, {"text": " ? Minimum Bayes Risk (MBR). We rescore the", "acronyms": [[23, 26]], "long-forms": [[3, 21]], "ID": "404"}, {"text": "  2.1. The behavior of manner adverbs (MA) and sentence  adverbs (SA) in the sentence is widely different: ", "acronyms": [[39, 41], [66, 68]], "long-forms": [[23, 37], [47, 64]], "ID": "405"}, {"text": "Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1?47.", "acronyms": [[74, 78], [51, 54]], "long-forms": [[55, 72]], "ID": "406"}, {"text": "We tested two differing algorithms on text from the Wall Street  Journal (WSJ). Using BBN's part of speech tagger (POST), tagged  text was parsed using the full unification grammar of Delphi to fred ", "acronyms": [[115, 119], [74, 77]], "long-forms": [[92, 113], [52, 72]], "ID": "407"}, {"text": " The headline  generation system we present uses  Singular Value Decomposition (SVD) to  guide the generation of a headline ", "acronyms": [[80, 83]], "long-forms": [[50, 78]], "ID": "408"}, {"text": "a different word, such as job, in a given context) by defining a one-class learning algorithm based on support vector machines (SVM). They train a one-", "acronyms": [[128, 131]], "long-forms": [[103, 126]], "ID": "409"}, {"text": "Huang and Chiang, 2005), which is used to represent all derivations (i.e. parse trees) for a given  sentence under a context free grammar (CFG). A ", "acronyms": [[139, 142]], "long-forms": [[117, 137]], "ID": "410"}, {"text": " At least for case schemata we have as first al-   ternative the choice between the realization types :CLAUSE  and :NG (noun group). For semantic structures from titles we ", "acronyms": [[116, 118]], "long-forms": [[120, 130]], "ID": "411"}, {"text": "                                                           8  Feature type: CB=Clause boundary based feature type,  PT=Parse tree based feature type  9A ?", "acronyms": [[116, 118], [76, 78]], "long-forms": [[119, 129], [79, 94]], "ID": "412"}, {"text": "tion of precise probability scores for partial hypotheses contain-  ing islands, in the context of  a Stochastic-Context-Free-Grammar  (SCFG) for Language Modeling (LM). The second issue is the ", "acronyms": [[165, 167], [136, 140]], "long-forms": [[146, 163], [102, 133]], "ID": "413"}, {"text": "be expressed via correspondences. We will define a  variant of SSTC called synchronous SSTC (S-SSTC).  ", "acronyms": [[93, 99], [63, 67]], "long-forms": [[75, 91]], "ID": "414"}, {"text": "1 Automatic Discovery of Phone(me)s Statistical models learnt from data are extensively used in modern automatic speech recognition (ASR) systems.", "acronyms": [[133, 136]], "long-forms": [[103, 131]], "ID": "415"}, {"text": "subcategorisation information in the form of a Y in col-  umn 8, then we can assign the label for wh-pronou,  head (HWH). An exception list is used to map to the ", "acronyms": [[116, 119]], "long-forms": [[110, 114]], "ID": "416"}, {"text": "structures. 4 Such an algorithm has to define for every LFG a relation  F?(~,s)  (s is generable from (I)) between directed acyclic graphs (DAGs)  and terminal strings.", "acronyms": [[140, 144], [56, 59], [72, 74]], "long-forms": [[115, 138]], "ID": "417"}, {"text": "Based on RenCECps (an annotated emotion corpus) and MaxEnt (Maximum entropy) model, several contextual features and their com-", "acronyms": [[52, 58], [9, 17]], "long-forms": [[60, 75]], "ID": "418"}, {"text": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 75? ", "acronyms": [[92, 95]], "long-forms": [[49, 89]], "ID": "419"}, {"text": " 1 Introduction The approach to factoid question answering (QA) that we adopt was first described in (Whittaker et", "acronyms": [[60, 62]], "long-forms": [[40, 58]], "ID": "420"}, {"text": "sentence length. This gives us five metrics of complexity: sentence length (SL), tree depth (TD), branching factor (BF), normalized tree depth", "acronyms": [[76, 78], [93, 95], [116, 118]], "long-forms": [[59, 74], [81, 91], [98, 114]], "ID": "421"}, {"text": ".  Longest TM Candidate Indicator (LTC):  Which indicates whether the given  is the ", "acronyms": [[35, 38]], "long-forms": [[3, 33]], "ID": "422"}, {"text": "the number of sentences in each review.  Latent Dirichlet Allocation (LDA) We also apply Latent Dirichlet Allocation (Blei et al.,", "acronyms": [[70, 73]], "long-forms": [[41, 68]], "ID": "423"}, {"text": "While named entity recognition (NER) and relation or event extraction are regarded as standard tasks of information extraction (IE), coreference resolution (Ng, 2010; Bejan and Harabagiu, 2010) is more", "acronyms": [[128, 130]], "long-forms": [[104, 126]], "ID": "424"}, {"text": "Multiword Expressions (MWEs) are one of the stumbling blocks for more precise Natural Language Processing (NLP) systems.", "acronyms": [[107, 110], [23, 27]], "long-forms": [[78, 105], [0, 21]], "ID": "425"}, {"text": "ing different linguistic aspects and an effective way to combine such information is made available by Support Vector (SV) regression, described in (Smola and Scho?lkopf, 2004).", "acronyms": [[119, 121]], "long-forms": [[103, 117]], "ID": "426"}, {"text": "NLP-Based  Index ing  in  In fo rmat ion   Ret r ieva l   In information retrieval (IR), a typical task is to  fetch relevant documents from a large archive in ", "acronyms": [[84, 86], [0, 3]], "long-forms": [[61, 82]], "ID": "427"}, {"text": "occurrence restrictions (FCRs), feature specification  defaults (FSDs), linear precedence (LP) statements,  and universal feature instantiation principles (UIPs). ", "acronyms": [[156, 160], [25, 29], [65, 69], [91, 93]], "long-forms": [[112, 154], [32, 63], [72, 89]], "ID": "428"}, {"text": "automatic speech recognition (ASR), machine translation (MT), text-to-speech synthesis (TTS), as well as technology for language resource and fundamental tool de-", "acronyms": [[88, 91], [30, 33], [57, 59]], "long-forms": [[62, 86], [0, 28], [36, 55]], "ID": "429"}, {"text": "where cLen is the length of a pattern; cMatch is  the number of matched tags; cPtn is the number of  protein name tag (PTN) skipped by the alignment  in the sentence;  cVb is the number of skipped ", "acronyms": [[119, 122], [168, 171], [78, 82], [6, 10], [39, 45]], "long-forms": [[101, 113], [18, 24], [64, 71]], "ID": "430"}, {"text": "Results for the Mention-Pair Model 1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.5 2 Base+YAGO Types (YT) 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.8 3 Base+YAGO Means (YM) 56.7 70.0 62.7 55.3 66.5 60.4 50.6 57.0 53.6 49.3 54.9 51.9", "acronyms": [[121, 123], [204, 206]], "long-forms": [[104, 119], [192, 202]], "ID": "431"}, {"text": "The English supervised NE tagger correctly identifies Asian as a named entity of type MISC (miscellaneous). The word-alignments sug-", "acronyms": [[86, 90], [23, 25]], "long-forms": [[92, 105]], "ID": "432"}, {"text": " 1 Introduction Information extraction (IE), defined as the task of extracting structured information (e.g., events, bi-", "acronyms": [[40, 42]], "long-forms": [[16, 38]], "ID": "433"}, {"text": "A S O W B E  - as + Object of be  A S S E R T I O N ~ S ~ ~ ~ ~ ~ ~  + Tense + V e r b  Object  ASTG = Adjective String  C l  SHOULD -- Subjunctive foYm of ASSERTllgZQ ", "acronyms": [[96, 100]], "long-forms": [[103, 119]], "ID": "434"}, {"text": "case. In this paper, we investigate using Amazon?s Mechanical Turk (MTurk) to make MT test sets cheaply.", "acronyms": [[68, 73]], "long-forms": [[51, 66]], "ID": "435"}, {"text": " 5 for SK with the use of POS information (SK-POS). ", "acronyms": [[43, 49]], "long-forms": [[7, 29]], "ID": "436"}, {"text": "qn  e.g., ? MP (Member of Parliament)? ", "acronyms": [[12, 14]], "long-forms": [[16, 36]], "ID": "437"}, {"text": "AS for word  order,  ther{{ ar<~ basieal\\]}Z two  phrase  'types i n  German:  noun-dependent   phrases,  l i ke  no(:~n phrase  ( NP ) and  prepos i t iona l  phrase  ( !:'", "acronyms": [[131, 133]], "long-forms": [[114, 127]], "ID": "438"}, {"text": "For the time being,  the object of the testing is the generative component (GC) of this  description enumerating semantic representations (SR's) of sentences. ", "acronyms": [[139, 143], [76, 78]], "long-forms": [[113, 137], [54, 74]], "ID": "439"}, {"text": "We use two ways to measure contribution in terms of graphemes: contseq(w, b) is the length of the longest prefix/suffix of word w which blend b begins/ends with, and contlcs(w, b) is the longest common subsequence (LCS) of w and b. This yields four features:", "acronyms": [[215, 218]], "long-forms": [[187, 213]], "ID": "440"}, {"text": "Stanford Dependencies (SDC), as described by de Marneffe et al (2006), were generated by converting Penn Treebank (PTB)-style (Marcus et al, 1993) output using the Stanford CoreNLP Tools2 into the", "acronyms": [[115, 118], [23, 26], [177, 180]], "long-forms": [[100, 113], [0, 21]], "ID": "441"}, {"text": " TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding", "acronyms": [[58, 60], [36, 39], [1, 5]], "long-forms": [[45, 56], [15, 34]], "ID": "442"}, {"text": "entire web corpus. We use the API provided by the Measures of Semantic Relatedness (MSR)4 engine for this purpose.", "acronyms": [[84, 87], [30, 33]], "long-forms": [[50, 82]], "ID": "443"}, {"text": "Abstract We experiment with using different sources of distant supervision to guide unsupervised and semi-supervised adaptation of part-of-speech (POS) and named entity taggers (NER) to Twitter. ", "acronyms": [[178, 181], [147, 150]], "long-forms": [[156, 176], [131, 145]], "ID": "444"}, {"text": "CoTrain vs. SVM(EN) 2.15E-18 1.1E-17 3.46E-13 CoTrain vs. SVM(ENCN) 2.08E-13 8.13E-12 1.05E-12 CoTrain vs. TSVM(CN) 1.2E-19 1.07E-06 0.0624 CoTrain vs. TSVM(EN) 1.41E-05 0.00311 2.17E-08", "acronyms": [[112, 114], [12, 15], [16, 18], [58, 61], [62, 66], [152, 156], [157, 159], [107, 111]], "long-forms": [[95, 102]], "ID": "445"}, {"text": "guistics to guide the solution of locality of arguments. In particular, Maximal Projection (MP) which dominates", "acronyms": [[92, 94]], "long-forms": [[72, 90]], "ID": "446"}, {"text": "contributing to the irregularity of hangul orthography is the differences in spelling between South Korea (S.K.) and North Korea (N.K.). The", "acronyms": [[107, 111], [130, 134]], "long-forms": [[94, 105], [117, 128]], "ID": "447"}, {"text": " 1 Introduction Stanford Dependencies (SD) provide a functional characterization of the grammatical relations in", "acronyms": [[39, 41]], "long-forms": [[16, 37]], "ID": "448"}, {"text": "NG = ngrams, E = emoticon replacement, IR = informal register replacement, TL = tweet length, RT = retweet count, SVO = subject-verbobject structures. %", "acronyms": [[94, 96], [0, 2], [39, 41], [75, 77], [114, 117]], "long-forms": [[99, 106], [5, 11], [17, 25], [44, 61], [80, 92], [120, 138]], "ID": "449"}, {"text": " Assi, S. (1997). Farsi linguistic database (FLDB). ", "acronyms": [[45, 49]], "long-forms": [[18, 43]], "ID": "450"}, {"text": "9http://disi.unitn.it/moschitti/Tree-Kernel.htm 10for STS-2012 we also report the results for a concatenation of all five test sets (ALL) provement with the Mean = 0.7416 and Pearson", "acronyms": [[133, 136]], "long-forms": [[113, 116]], "ID": "451"}, {"text": "1995. A bi-directional Russian-toEnglish machine translation system (ETAP-3). In", "acronyms": [[69, 75]], "long-forms": [[33, 67]], "ID": "452"}, {"text": " Twice it is labeled as a noun phrase (NP) and once as a prepositional phrase (PP). ", "acronyms": [[79, 81]], "long-forms": [[57, 77]], "ID": "453"}, {"text": "reranking step works on top of the generative model.  \u0001 ZC05 (Zettlemoyer and Collins 2005) learns a discriminative log-linear model over CCG derivations.", "acronyms": [[56, 60], [138, 141]], "long-forms": [[62, 90]], "ID": "454"}, {"text": "In the  past a few year, we have put forward methods for Kazakh morphological analysis, which includes  stem extraction, part of speech(POS) tagging, spelling check, etc. Recently, we are working on syntax ", "acronyms": [[136, 139]], "long-forms": [[121, 134]], "ID": "455"}, {"text": "Engineering neighborhood of bank  bank  Subject Code EG = Engineering  river wall flood thick ", "acronyms": [[53, 55]], "long-forms": [[58, 69]], "ID": "456"}, {"text": "HowNet and Its Computation of Meaning. In Actes de COLING-2010, Beijing, 4 p. Francopoulo, G., Bel, N., George, M., Calzolari, N., Monachini, M., Pet, M. and Soria, C. (2009). Multilingual resources for NLP in the lexical markup framework (LMF). In journal de Language Resources and Evaluation, March 2009, Volume 43, pp.", "acronyms": [[240, 243]], "long-forms": [[214, 238]], "ID": "457"}, {"text": "KODIAK (Keystone to Overall Design for Inte-  gration and Application of Knowledge) is an implemen-  tation of CRT (Cognitive Representation Theory), an  approach to knowledge representation that bears simi- ", "acronyms": [[111, 114], [0, 6]], "long-forms": [[116, 147], [8, 82]], "ID": "458"}, {"text": " 3 BUDS dialogue manager The Bayesian Update of Dialogue State (BUDS) dialogue manager is a POMDP-based dialogue", "acronyms": [[64, 68], [3, 7], [92, 97]], "long-forms": [[29, 62]], "ID": "459"}, {"text": "Theorem LG is NP-complete.  Bin Packing (BP) which is NP-complete is  polynomial-t ime Karp reducible to LG.", "acronyms": [[41, 43], [8, 10], [14, 16], [105, 107], [54, 56]], "long-forms": [[28, 39]], "ID": "460"}, {"text": " Each verb-noun pair was presented to turkers via Amazon Mechanical Turk (AMT) and they were asked to describe (by text) the changes of", "acronyms": [[74, 77]], "long-forms": [[50, 72]], "ID": "461"}, {"text": "The s t ruc ture  11ke a combinat ion of  a verb (V) and  an adverb ia l  par t tc le  (ADVPART) tn th ts  sequence  wtth or w i thout  a pronoun (PRON) tn between tn  Engltsh t swr t t ten  as fo l lows .", "acronyms": [[147, 151], [88, 95]], "long-forms": [[138, 145], [44, 48], [61, 85]], "ID": "462"}, {"text": "Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005).", "acronyms": [[126, 128]], "long-forms": [[102, 124]], "ID": "463"}, {"text": "tence is assigned a Sense_Weight_Score (SWS)  for each emotion tag which is calculated by dividing the total Sense_Tag_Weight (STW)of all  occurrences of an emotion tag in the sentence by ", "acronyms": [[127, 130], [40, 43]], "long-forms": [[109, 125], [20, 38]], "ID": "464"}, {"text": "In order to test the classification rules for the extraction of part?whole relations, we selected two different text collections: the LA Times news articles from TREC 9 and the Wall Street Journal (WSJ) articles from Treebank2.10 From each collection we randomly selected 10,000 sentences that formed two distinct test corpora.", "acronyms": [[198, 201], [162, 166], [134, 136]], "long-forms": [[177, 196]], "ID": "465"}, {"text": "Sematnic data models are systems for  constructing precise descriptions of protions of  the real world - semantic data description (SDD)-  using terms that come from the real world rather ", "acronyms": [[132, 135]], "long-forms": [[105, 130]], "ID": "466"}, {"text": "2 BP Neural Network  At the moment, there are about more than 30 kinds of  artificial neural network (ANN) in the domain of  research and application.", "acronyms": [[102, 105], [2, 4]], "long-forms": [[75, 100]], "ID": "467"}, {"text": "for unsupervised learning. In this paper, we are interested in partitioning words into several clusters without any label priori using unsupervised LP (Un-LP) algorithm. Firstly we randomly select K (K ?", "acronyms": [[152, 157]], "long-forms": [[135, 150]], "ID": "468"}, {"text": "morpheme omitted.   We adopt Support Vector Machines(SVM) as  the device by which a given adnoun clause is ", "acronyms": [[53, 56]], "long-forms": [[29, 51]], "ID": "469"}, {"text": "evaluating the attribute subsets. Their evaluation is based on consistency (CBF) and correlation (CFS). ", "acronyms": [[76, 79], [98, 101]], "long-forms": [[63, 74], [85, 96]], "ID": "470"}, {"text": "words words  Fig. 1 Structure of Bunruigoihy6 (BGH)  This paper focuses on classifying only nouns in terms of ", "acronyms": [[47, 50]], "long-forms": [[33, 45]], "ID": "471"}, {"text": "Statistical machine translation based on LDA.  In Universal Communication Symposium (IUCS), 2010 4th International, pages 286?290.", "acronyms": [[85, 89], [41, 44]], "long-forms": [[47, 83]], "ID": "472"}, {"text": "navigation instruction to guide the IF to a convenient location at which she can then use a simple referring expression (RE). That is, there is an inter-", "acronyms": [[121, 123], [36, 38]], "long-forms": [[99, 119]], "ID": "473"}, {"text": "   then  Cast_in_Chain(Antecedent, Anaphor)    then  Cast_in_Chain(Antecedent, Anaphor) RULE-1-Filter-1-Pronoun (R1F1Pron) RULE-1-Filter-1-Nominal (R1F1Nom)", "acronyms": [[113, 121], [148, 155]], "long-forms": [[88, 111], [123, 146]], "ID": "474"}, {"text": "few labels. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on, pages 192?199.", "acronyms": [[64, 70]], "long-forms": [[15, 62]], "ID": "475"}, {"text": "et al, 2000b). A more complex application targets the Aircraft Maintenance Manual (AMM) of the Airbus A320 (Rinaldi et al, 2002b).", "acronyms": [[83, 86]], "long-forms": [[54, 81]], "ID": "476"}, {"text": "In this section, we compare the running time5 of our sampling algorithm (FAST) and our algorithm with the refined bucket (RB) against the unfactored Gibbs sampler (NAI?VE) and examine the effect of sorting.", "acronyms": [[122, 124], [73, 77], [164, 170]], "long-forms": [[106, 120]], "ID": "477"}, {"text": "The lack of a  fully comprehensive bilingual dictionary including  the entries for all named entities (NEs) renders the  task of transliteration necessary for certain natural ", "acronyms": [[103, 106]], "long-forms": [[87, 101]], "ID": "478"}, {"text": " 3 In this work, we use HL (Hu and Liu, 2004), MPQA (Wilson et al.,", "acronyms": [[24, 26], [47, 51]], "long-forms": [[28, 38]], "ID": "479"}, {"text": "increasingly more important.  In ordinary phrase structure grammars (PSG's),  the only mechanism for capturing the kinds of merg- ", "acronyms": [[69, 74]], "long-forms": [[42, 67]], "ID": "480"}, {"text": "specifier phrase with one of a set of (usually numeric) specifiers; the specifier phrase typically occurs in apposition or as a genitive modifier (GEN) to the head noun.", "acronyms": [[147, 150]], "long-forms": [[128, 136]], "ID": "481"}, {"text": "Given an OOV word a and its IV version b we have extracted character transformation rules from a to b using the longest common substring (LCS) algorithm (See Table 5).", "acronyms": [[138, 141], [9, 12], [28, 30]], "long-forms": [[112, 136]], "ID": "482"}, {"text": "Table 2: Key details of semantic orientation (SO) lexicons. ASL = affix seeds lexicon, GI = General Inquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN = SentiWordNet, TLL = Turney?Littman lexicon.", "acronyms": [[110, 114], [157, 160], [46, 48], [60, 63], [87, 89], [190, 193], [210, 213]], "long-forms": [[117, 155], [163, 188], [24, 44], [66, 85], [92, 108], [196, 208], [216, 238]], "ID": "483"}, {"text": "PoS tagF5. Lemma (L)F6. Inflection (INFL)F7. Main verb of main clause (MV)F8.", "acronyms": [[36, 40], [0, 3], [71, 73]], "long-forms": [[24, 34], [11, 16], [45, 54]], "ID": "484"}, {"text": "Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning tech-", "acronyms": [[110, 112]], "long-forms": [[89, 108]], "ID": "485"}, {"text": "The scores returned by the similarity measures are used as features in a Maximum Entropy (ME) classifier (Jaynes, 1957; Good, 1963), which learns to sepa-", "acronyms": [[90, 92]], "long-forms": [[73, 88]], "ID": "486"}, {"text": "From our point of view, it  is very interesting to compare the results of Czech  stochastic POS (SPOS) tagger and a modified RB-  POS tagger for Czech.", "acronyms": [[97, 101]], "long-forms": [[81, 95]], "ID": "487"}, {"text": "In support to the on going project of Multilingual  Machine Translation Sytem for Asian Language organized by  Center for International Cooperaliou inComputerization (CICC)-  Japan and other Asian cotmtries (China.", "acronyms": [[167, 171]], "long-forms": [[111, 165]], "ID": "488"}, {"text": "  Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 143?147, October 25, 2014, Doha, Qatar.", "acronyms": [[80, 84], [21, 26]], "long-forms": [[44, 78]], "ID": "489"}, {"text": "the procedures that manipulate an  intermediate representat ion of the query,  cal led the Meta Query Language (MQL). ", "acronyms": [[112, 115]], "long-forms": [[91, 110]], "ID": "490"}, {"text": "5.1 Experimental Settings To evaluate our algorithm?s performance, we designed a Mechanical Turk (MTurk) experiment in which human annotators assess the quality of the", "acronyms": [[98, 103]], "long-forms": [[81, 96]], "ID": "491"}, {"text": "formation from non-expert bilingual speakers. The  Translation Correction Tool (TCTool) is a userfriendly online tool that allows users to add, delete ", "acronyms": [[80, 86]], "long-forms": [[51, 78]], "ID": "492"}, {"text": " 3. Construct a character list (CH)3, in which the characters are top 20 frequency in training", "acronyms": [[32, 34]], "long-forms": [[16, 25]], "ID": "493"}, {"text": "ging. We ran this set of experiments on the part of Chinese Treebank 4 (CTB-4)6. Ninety percent of the", "acronyms": [[72, 77]], "long-forms": [[52, 70]], "ID": "494"}, {"text": "verbs. But, the second Light Verb (LV) may be  a part of another Complex Predicate (CP). This ", "acronyms": [[84, 86], [35, 37]], "long-forms": [[65, 82], [23, 33]], "ID": "495"}, {"text": "Table 1: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for decipherment of letter", "acronyms": [[118, 123], [29, 32], [56, 59], [75, 77], [153, 157]], "long-forms": [[111, 116], [9, 26], [35, 53], [66, 72]], "ID": "496"}, {"text": "We tested the Arabic sentiment system on two existing Arabic datasets (Mourad and Darwish (2013) (MD) and Refaee and Rieser (2014a) (RR)) and two newly sentiment-annotated Arabic datasets (BBN", "acronyms": [[98, 100]], "long-forms": [[117, 123]], "ID": "497"}, {"text": " ? Generat ionCoverage(GC).Thepercentageofcoml) lete  and correct iuterlingna expressions R}r which the gener- ", "acronyms": [[23, 25]], "long-forms": [[3, 21]], "ID": "498"}, {"text": "3.3 Participants and Task Eighty participants were recruited from Amazon?s Mechanical Turk2 (MTurk) for this between2http://www.mturk.com", "acronyms": [[93, 98]], "long-forms": [[75, 91]], "ID": "499"}, {"text": "pound verb (CompV) is framed with these two  verbs. But, the second Light Verb (LV) may be  a part of another Complex Predicate (CP).", "acronyms": [[80, 82]], "long-forms": [[68, 78]], "ID": "500"}, {"text": "In this paper we propose a system which uses  hybrid methods that combine both rule-based  and machine learning (ML)-based approaches  to solve GENIA Event Extraction of BioNLP ", "acronyms": [[113, 115], [144, 149], [170, 176]], "long-forms": [[95, 111]], "ID": "501"}, {"text": "61 Table 2: Three kinds of preprocessing of a sentence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP", "acronyms": [[78, 81], [98, 101], [124, 127], [141, 144]], "long-forms": [[84, 89], [72, 76], [130, 139], [104, 113]], "ID": "502"}, {"text": " 3.1 Sentence Splitting (SS) Sentence Splitting (SS) is the rewriting of a sentence by breaking it into two or more sentences,", "acronyms": [[49, 51], [25, 27]], "long-forms": [[29, 47], [5, 23]], "ID": "503"}, {"text": "End the tutoring problem? Cause another round of dialogue/essay revision ITSpoke (Intelligent Tutoring SPOKEn dialogue system) 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  156/206", "acronyms": [[73, 80]], "long-forms": [[82, 109]], "ID": "504"}, {"text": "the preferred word attachments.  EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents", "acronyms": [[49, 52]], "long-forms": [[33, 47]], "ID": "505"}, {"text": "Studies such as Cinque (2006) and Rizzi  (1999) propose detailed functional phrases such  as TopP (Topic Phrase) in order to fully describe  the syntactic structures of a language.", "acronyms": [[93, 97]], "long-forms": [[99, 111]], "ID": "506"}, {"text": "2.4 Longest Common Substring  Given two strings, T of length n and H of length m,  the Longest Common Sub-string (LCS) problem  (Dan, 1999) will find the longest string that is a ", "acronyms": [[114, 117]], "long-forms": [[87, 105]], "ID": "507"}, {"text": "MTCL = Machine Translation and Computational  Linguistics (1965-1968)  AJCL = AmericanJournal of Computational  Linguistics (1974-present) ", "acronyms": [[71, 75], [0, 4]], "long-forms": [[78, 110], [7, 57]], "ID": "508"}, {"text": "corporates agreement features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model benefits more from inflectional features.", "acronyms": [[95, 97], [45, 47], [113, 115]], "long-forms": [[87, 93]], "ID": "509"}, {"text": "these areas. Speci\fcally, our goals when entering MUC-7 were to: \u000f Increase the accuracy in the Template Element (TE) task and the Template Relation (TR) task su\u000eciently for operational use, i.e., F-Measures of 85% and 80% respectively,", "acronyms": [[114, 116], [150, 152], [50, 55]], "long-forms": [[96, 112], [131, 148]], "ID": "510"}, {"text": "2115  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 63?67, Gothenburg, Sweden, April 26-30 2014.", "acronyms": [[74, 76], [32, 36]], "long-forms": [[54, 72]], "ID": "511"}, {"text": "  1. RecallCorrectTransliteration  (RTrans)  The recall is going to be computed using the ", "acronyms": [[36, 42]], "long-forms": [[5, 33]], "ID": "512"}, {"text": " 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively", "acronyms": [[49, 52]], "long-forms": [[20, 47]], "ID": "513"}, {"text": "tf iD j  is the inverse document frequency(IDF)  of the j-th word calculated as below: ", "acronyms": [[43, 46]], "long-forms": [[16, 41]], "ID": "514"}, {"text": "tion results in a better AER. Running ALP with deletion (TD) templates followed by multi-word (TMW ) templates results in a lower AER than running ALP", "acronyms": [[57, 59], [25, 28], [38, 41], [130, 133], [147, 150], [95, 98]], "long-forms": [[47, 55], [61, 93]], "ID": "515"}, {"text": "in the clustering.  Cumulative Micro Precision (CMP). As pointed", "acronyms": [[48, 51]], "long-forms": [[20, 46]], "ID": "516"}, {"text": "For each sense s i of the target words, we place a Hierarchical Dirichlet process (HDP) prior on the mixture proportion to latent concepts shown as follows:", "acronyms": [[83, 86]], "long-forms": [[51, 81]], "ID": "517"}, {"text": " 1 Introduction Recurrent Neural Network (RNN)-based conditional language models (LM) have been shown to", "acronyms": [[42, 45], [82, 84]], "long-forms": [[16, 40], [65, 80]], "ID": "518"}, {"text": "lines are published results by Hall and Nivre 2008 (HN08), Maier et al 2012 (M12), van Cranenburgh 2012 (C12), Kallmeyer and Maier 2013 (KM13), van Cranenburgh and Bod 2013 (CB13), and Versley 2014a, 2014b (V14a, V14b).", "acronyms": [[137, 141], [52, 56], [77, 80], [105, 108], [174, 178], [207, 217]], "long-forms": [[111, 135], [31, 50], [60, 75], [87, 103], [148, 172], [185, 205]], "ID": "519"}, {"text": "248   Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284?1295, October 25-29, 2014, Doha, Qatar.", "acronyms": [[94, 99]], "long-forms": [[44, 92]], "ID": "520"}, {"text": " 3.2 Duluth Systems Evaluation The FScore (F-10), F-Measure (F-SC), and Jaccard Coefficient result in a comparable and consistent", "acronyms": [[61, 65]], "long-forms": [[35, 41]], "ID": "521"}, {"text": "in sentence level are not combined. That is why  most of the verb phrases(VP) are inside match  (53.28%).", "acronyms": [[74, 76]], "long-forms": [[61, 72]], "ID": "522"}, {"text": "derivations obtained from all stop states in the chart.  3.6 Minimum Risk Parse (MRP) MPD and MPP aim at obtaining the structure of a", "acronyms": [[81, 84]], "long-forms": [[61, 79]], "ID": "523"}, {"text": "to any text, so we shall comment on them.  Frequent Candidates (FC) ? this is a boosting score", "acronyms": [[64, 66]], "long-forms": [[43, 62]], "ID": "524"}, {"text": "traction) consists of two steps: keyphrase candidate extraction and the genetic programming of keyphrase scoring measures (KSMs).1 3.1 Step 1: Keyphrase candidate extraction", "acronyms": [[123, 127]], "long-forms": [[95, 121]], "ID": "525"}, {"text": "As shown in Table 3, using the transformation of dependency trees, the Dep2Str model implemented in Moses (D2S) is comparable with the standard implementation", "acronyms": [[107, 110]], "long-forms": [[71, 78]], "ID": "526"}, {"text": "10?16). Results for per-predication (PR) and per-whole-graph (GRPH) tagging percentage accuracies are listed. (", "acronyms": [[62, 66], [37, 39]], "long-forms": [[55, 60], [24, 35]], "ID": "527"}, {"text": "Domains  In this section we compare some characteristics of the  English Travel Domain (ETD) and the English Spon-  taneous Scheduling Task (ESST).", "acronyms": [[88, 91], [141, 145]], "long-forms": [[65, 86], [101, 139]], "ID": "528"}, {"text": " A more flexible approach is to do it in two steps: complementation, forming a VP (verb phrase) from the verb and the object, and predication", "acronyms": [[79, 81]], "long-forms": [[83, 94]], "ID": "529"}, {"text": "measure specifically their performance, a selection of them is also given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the", "acronyms": [[127, 132], [87, 90], [104, 107], [152, 157]], "long-forms": [[110, 125], [75, 85], [93, 102], [135, 150]], "ID": "530"}, {"text": "machine learning tasks. We used character n-grams, word n-grams, Parts of Speech (POS) tag n-grams, and perplexity of character trigrams as features.", "acronyms": [[82, 85]], "long-forms": [[65, 80]], "ID": "531"}, {"text": "NLP track report. In Proceedings of the 5th  Text REtrieval Conference (TREC-5). ", "acronyms": [[72, 78], [0, 3]], "long-forms": [[40, 70]], "ID": "532"}, {"text": "Table 1: Purity (Pu), collocation (Co), and F1 scores of our model and the syntactic baseline in percent. Performance on arguments (Argn), adjuncts (ArgM), and overall results (Arg*) are shown separately. ", "acronyms": [[149, 153], [35, 37], [177, 180], [132, 136], [17, 19]], "long-forms": [[121, 130], [22, 33], [9, 15]], "ID": "533"}, {"text": "Table 4: Human expert evaluated accuracy (Acc.)  and full cluster accuracy (FAcc.) of models on", "acronyms": [[76, 81], [42, 45]], "long-forms": [[53, 74], [32, 40]], "ID": "534"}, {"text": " Two sorts of recursion can be distinguished: 1)  middle field (MF) recursion, where the embedded  base clause is framed by the left and right verb parts ", "acronyms": [[64, 66]], "long-forms": [[50, 62]], "ID": "535"}, {"text": "therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). ", "acronyms": [[129, 132]], "long-forms": [[92, 127]], "ID": "536"}, {"text": "JOPER (enclitic personal) DEMON (demonstrative) 1 SING (singular) INTG (interogative) 2 PLUR (plural) CREFX (common reflexive) 3", "acronyms": [[66, 70], [88, 92], [0, 5], [26, 31], [50, 54], [102, 107]], "long-forms": [[72, 84], [94, 100], [33, 46], [7, 24], [56, 64], [109, 125]], "ID": "537"}, {"text": "Table 1). CC = coordinating conjunction; CD = cardinal number; JJ = adjective; MD = modal; NN = singular noun; NNP = proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB =", "acronyms": [[41, 43], [79, 81], [10, 12], [63, 65], [91, 93], [111, 114], [130, 134], [189, 191], [198, 200], [176, 178], [157, 160]], "long-forms": [[46, 54], [84, 89], [15, 39], [68, 77], [105, 109], [117, 128], [137, 155], [163, 174], [181, 187]], "ID": "538"}, {"text": "Hyderabad, India Abstract Named Entity Recognition(NER) is the task of identifying and classifying tokens in a", "acronyms": [[51, 54]], "long-forms": [[26, 49]], "ID": "539"}, {"text": "http://www.cs.ualberta.ca/?yx2/research.html 59 class-based hidden Markov model (HMM) model (Zhang et al, 2003).", "acronyms": [[81, 84]], "long-forms": [[60, 79]], "ID": "540"}, {"text": " 2 Related Work Sentiment analysis (SA) and related topics have been extensively studied in recent years.", "acronyms": [[36, 38]], "long-forms": [[16, 34]], "ID": "541"}, {"text": "Results on final test sets. LAS = labeled attachment score. UAS = unlabeled attachment score. ", "acronyms": [[60, 63], [28, 31]], "long-forms": [[66, 92], [34, 58]], "ID": "542"}, {"text": " 1 Introduction Named entity recognition (NER) is the most studied information extraction (IE) task.", "acronyms": [[42, 45], [91, 93]], "long-forms": [[16, 40], [67, 89]], "ID": "543"}, {"text": "recognized cue phrase. Five systems followed a pure token classification approach (TC) for cue detection while others used sequential labeling tech-", "acronyms": [[83, 85]], "long-forms": [[52, 81]], "ID": "544"}, {"text": "described in Section 3. We then trained linear SVMs (Support Vector Machine) using the LIBLINEAR software (Fan et al, 2008), using L1 loss", "acronyms": [[47, 51], [87, 96]], "long-forms": [[53, 75]], "ID": "545"}, {"text": " The constructed ontology is evaluated using  cluster purity (CP), instances knowledge (IK),  and relation concept (RC).", "acronyms": [[62, 64], [88, 90], [116, 118]], "long-forms": [[46, 60], [67, 86], [98, 114]], "ID": "546"}, {"text": " 3.2.2 Optimization We use stochastic gradient descent (SGD) to maximize the simplified objectives.", "acronyms": [[56, 59]], "long-forms": [[27, 54]], "ID": "547"}, {"text": " 2 Graphical Model Framework A Graphical Model (GM) represents a factorization of a family of joint probability distributions over a", "acronyms": [[48, 50]], "long-forms": [[31, 46]], "ID": "548"}, {"text": "Using the observation that LL is correlated with MRR on the same data set, we expect that optimizing LL on a development set (LLdev) will also improve MRR on an evaluation set (MRReval).", "acronyms": [[126, 131], [27, 29], [49, 52], [151, 154], [177, 184]], "long-forms": [[101, 120]], "ID": "549"}, {"text": "majority instances of all the clusters.   Mutual information (MI) is more theoretically  well-founded than purity.", "acronyms": [[62, 64]], "long-forms": [[42, 60]], "ID": "550"}, {"text": "(MBF).  4 Multilingual PRF (MultiPRF) The schematic of the MultiPRF approach is shown", "acronyms": [[28, 36], [1, 4], [59, 67]], "long-forms": [[10, 26]], "ID": "551"}, {"text": "I first  describe the language used to characterize the semantics of  lexical items, SEL (for Simple Episodic Logic), then the  syntax and interpretation f logical forms.", "acronyms": [[85, 88]], "long-forms": [[94, 115]], "ID": "552"}, {"text": "during the last two years. In this first of four installerments, the  Association of Data Processing Service Organizations, Inc. (ADAPSO) is  considered with respect to its membership, charter, organization and ", "acronyms": [[130, 136]], "long-forms": [[70, 122]], "ID": "553"}, {"text": " 3 The Discourse Model Informally, a DiscourseModel (DM)may be described as the set of entities \"specified\" in a discourse, linked together by the relations they participate in.", "acronyms": [[53, 55]], "long-forms": [[37, 51]], "ID": "554"}, {"text": "ghar.  Parse: The modern town of [NP (np Mumbai)  (punct ,) (advP about 50 km south of Navi ", "acronyms": [[34, 36], [61, 65]], "long-forms": [[38, 40]], "ID": "555"}, {"text": "mance. The query-based selection model utilizes Support Vector Regression (SVR) models to predict the mean average precision (MAP) of each query", "acronyms": [[75, 78], [126, 129]], "long-forms": [[48, 73], [102, 124]], "ID": "556"}, {"text": "Headline Generation. In the Proceedings of the  Document Understanding Conference (DUC). ", "acronyms": [[83, 86]], "long-forms": [[48, 81]], "ID": "557"}, {"text": " ? Subordinate clause reordering (SubCR) which involve moving postnominal relative", "acronyms": [[34, 39]], "long-forms": [[3, 32]], "ID": "558"}, {"text": " To solve this problem, we introduce  Document oriented Preference Sets(DoPS). The ", "acronyms": [[72, 76]], "long-forms": [[38, 70]], "ID": "559"}, {"text": "173 Figure 3: Frequency of word classes in the three corpora (BN = Broadcast News, Est = Press, Euro = Europarl).", "acronyms": [[62, 64], [96, 100]], "long-forms": [[67, 81], [103, 111]], "ID": "560"}, {"text": "wqK ,   and the word sequence of the web page,                    A=WA=wA1, wA2, ?, wAL,  ", "acronyms": [[68, 70]], "long-forms": [[71, 74]], "ID": "561"}, {"text": "(Associativity)  \\[A\\[BC\\]\\] = \\[\\[AB\\]C\\]  (A(BC)) = ((AB)C)  (L, -singleton bidirectionality) ", "acronyms": [[56, 60]], "long-forms": [[45, 49]], "ID": "562"}, {"text": "More  technically, for each syntactic feature {sf1, sf2, ...,  sfn} of the set SF (Syntactic Features) represented  in the lexical typology, we define the goal of our ", "acronyms": [[79, 81]], "long-forms": [[83, 101]], "ID": "563"}, {"text": "Frustration Frustrated (F), Neutral (N), Correctness Correct (C), Incorrect (I) Partially Correct (PC) Percent Correct 50-100% (High), 0-50% (Low)", "acronyms": [[99, 101]], "long-forms": [[80, 97], [12, 22], [28, 35], [41, 60], [66, 75]], "ID": "564"}, {"text": "In LREC 2006, Genoa Yes?im Aksan and Mustafa Aksan 2012. Construction of the Turkish National Corpus (TNC). In LREC 2012,", "acronyms": [[102, 105], [3, 7]], "long-forms": [[77, 100]], "ID": "565"}, {"text": "mance on the NER task.  Maximum entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is", "acronyms": [[56, 62], [13, 16], [69, 75]], "long-forms": [[24, 54]], "ID": "566"}, {"text": "many-to-one word-level alignments produced by the IBM series models (Brown et al, 1993) or the Hidden Markov Model (HMM) (Vogel et al, 1996).", "acronyms": [[116, 119], [50, 53]], "long-forms": [[95, 114]], "ID": "567"}, {"text": "model in their study is the relative height by range model, where (in our notation): (13) relative height by range (RH-R): ? ", "acronyms": [[116, 120]], "long-forms": [[90, 114]], "ID": "568"}, {"text": "3. Formal descriptions about specific games which are classified as formal texts (FoT) 4.", "acronyms": [[82, 85]], "long-forms": [[68, 80]], "ID": "569"}, {"text": "06 33.3 125 33.1 73 Average: 35.4 216 36.1 125 Table 3: Directed dependency accuracies (DDA) and iteration counts for the 10 (of 23) train/test splits affected by", "acronyms": [[88, 91]], "long-forms": [[56, 86]], "ID": "570"}, {"text": " 2.3 Graphical User Interface The graphical user interface (GUI) is an important feature that has been recently added to Clairlib", "acronyms": [[60, 63]], "long-forms": [[34, 58]], "ID": "571"}, {"text": "We focus on predicting an absolute indication of quality rather than only ranking the sentences by quality; this is why we use the Mean Absolute Error (MAE) as the main evaluation measure rather than Spearman?s correlation or DeltaAvg (Callison-Burch et al.,", "acronyms": [[152, 155], [226, 234]], "long-forms": [[131, 150]], "ID": "572"}, {"text": "ogy (GO), Cell Type Ontology (CTO), BRENDA Tissue Ontology (BTO), Foundational Model of Anatomy (FMA), Cell Cycle Ontology (CCO), and Sequence Ontology (SO)?and a small number of", "acronyms": [[124, 127], [5, 7], [30, 33], [60, 63], [97, 100], [153, 155]], "long-forms": [[103, 122], [10, 28], [36, 58], [66, 95], [134, 151]], "ID": "573"}, {"text": "edge associated with it.  Definition 3 (Informative Feature Extraction (IF)) We define the Informative-Features(IF ) feature", "acronyms": [[72, 74], [112, 114]], "long-forms": [[40, 59], [91, 110]], "ID": "574"}, {"text": "This  paper focuses on this problem in the context of  Information Extraction (IE). 2 Many extraction ", "acronyms": [[79, 81]], "long-forms": [[55, 77]], "ID": "575"}, {"text": " ) are based on the pairwise mutual information (PMI) between two phrases.", "acronyms": [[49, 52]], "long-forms": [[20, 47]], "ID": "576"}, {"text": "day(x, fri) ? during(x, night) Here the logical form (LF) is a lambda-calculus expression defining a set of entities that are flights", "acronyms": [[54, 56]], "long-forms": [[40, 52]], "ID": "577"}, {"text": "generative models which are respectively estimated  on their corresponding named entity lists using  maximum likelihood estimation (MLE), together  with smoothing methods4.", "acronyms": [[132, 135]], "long-forms": [[101, 130]], "ID": "578"}, {"text": "518 ? SS = Stanford parser style:5 the first conjunct is the head and the remaining conjuncts (as", "acronyms": [[6, 8]], "long-forms": [[11, 32]], "ID": "579"}, {"text": "Model (LEM) We propose an unsupervised latent variable model, called the Latent Event Model (LEM), to extract events from tweets.", "acronyms": [[93, 96], [7, 10]], "long-forms": [[73, 91]], "ID": "580"}, {"text": "tical finite-state transducer (SFST) as a generative model and a support vector machine (SVM) and conditional random fields (CRF) as discriminative models.", "acronyms": [[125, 128], [31, 35], [89, 92]], "long-forms": [[98, 123], [0, 29], [65, 87]], "ID": "581"}, {"text": "derived from these MDPs: (1) Diff?s: the number of states whose policy differs from the Baseline 2 policy, (2) Percent Policy change (P.C.): the weighted amount of change between the two policies (100%", "acronyms": [[134, 138], [19, 23]], "long-forms": [[119, 132]], "ID": "582"}, {"text": "ment, we compared the following three methods for word  similarity measure:  * the Bunruigoihyo thesaurus (BGH): the similarity  between case fillers is measured by a function be- ", "acronyms": [[107, 110]], "long-forms": [[83, 105]], "ID": "583"}, {"text": "the features N, V, or 0 which indicate a nominal parent (NP or PP), a verbal parent (VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is propagated to the head child.", "acronyms": [[155, 158], [57, 59], [63, 65], [85, 87]], "long-forms": [[136, 153], [70, 83], [41, 55]], "ID": "584"}, {"text": "In all other tests, including all closed  tests, City University of Hong Kong (CityU)  open test and Microsoft Research (MSR) open  test, we trained our system using the relevant ", "acronyms": [[121, 124], [79, 84]], "long-forms": [[101, 119], [49, 64]], "ID": "585"}, {"text": "mantic feature called the latent topic feature, which is extracted by exploiting the Latent Dirichlet Allocation (LDA) algorithm. Unlike syntactic fea-", "acronyms": [[114, 117]], "long-forms": [[85, 112]], "ID": "586"}, {"text": "Translation Equivalents and Semantic  Relations   Note that two translation equivalents (TE)  in a pair of languages stand in a lexical semantic ", "acronyms": [[89, 91]], "long-forms": [[64, 87]], "ID": "587"}, {"text": "morphological types and variables.  The Encyclopedia Specialist (ES) is able to access the Encyclo-  pedia for extracting semantic information and world knowledge.", "acronyms": [[65, 67]], "long-forms": [[40, 63]], "ID": "588"}, {"text": "to systems that rely on brittle features is that many texts are not well-formed. One  such class of texts are those that are the output of optical character recognition (OCR);  typically these texts contain many extraneous or incorrect characters.", "acronyms": [[170, 173]], "long-forms": [[139, 168]], "ID": "589"}, {"text": "Recall all the methods rely on the wrapped classifier. We select two classic but very different classifiers: the Maximum Entropy model (MaxEnt) and the Decision Tree C4.5 (Tree). We implement these", "acronyms": [[136, 142]], "long-forms": [[113, 128]], "ID": "590"}, {"text": "corpora (section 2.2).  The workflow for named entity (NE) and  terminology extraction and mapping from ", "acronyms": [[55, 57]], "long-forms": [[41, 53]], "ID": "591"}, {"text": "  Abstract  Over the last few years, machine translation (MT) has transformed from an academic research  platform into a productivity or gisting tool adopted by several end users.", "acronyms": [[58, 60]], "long-forms": [[37, 56]], "ID": "592"}, {"text": "sources Tony Mullen and Nigel Collier National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku", "acronyms": [[73, 76]], "long-forms": [[38, 71]], "ID": "593"}, {"text": "to this tree as the Hidden Factors Tree (HFT). We use Minimum Discriminative Information (MDI) algorithm (Zitouni, 2007) to build the tree.", "acronyms": [[90, 93], [41, 44]], "long-forms": [[54, 88], [20, 39]], "ID": "594"}, {"text": "The thesis will examine two main areas: modelling cohesive devices within sentences and modelling discourse relations (DRs) across sentences.", "acronyms": [[119, 122]], "long-forms": [[98, 117]], "ID": "595"}, {"text": "923 DOs, Active: \"AGENT STRING AUX active-verb-element DETERMINER * POSTMOD\"DOs, Passive: \"DETERMINER * AUX active-verb-element element\"TVs, Active: \"AGENT STRING AUX * DETERMINER active-noun- element POSTMOD\"TVs, Passive:\"DET active-noun-element AUX * POSTMOD\" Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step. ", "acronyms": [[318, 321], [345, 348], [31, 34], [104, 107], [163, 166], [4, 7], [76, 79], [247, 250], [136, 139], [209, 212]], "long-forms": [[302, 316], [327, 343]], "ID": "596"}, {"text": "tive two-step model. We compare models based on the Akaike Information Criterion (AIC). ", "acronyms": [[82, 85]], "long-forms": [[52, 80]], "ID": "597"}, {"text": "ordinating conjunction; JJ = adjective; JJR = comparative adjective; NN = singular or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = comparative adverb; RP = particle; UH = interjection; VB =", "acronyms": [[116, 119], [24, 26], [40, 43], [69, 71], [97, 100], [141, 143], [154, 157], [180, 182], [195, 197], [214, 216]], "long-forms": [[122, 132], [29, 38], [46, 67], [74, 95], [103, 114], [146, 152], [160, 178], [185, 193], [200, 212]], "ID": "598"}, {"text": "1 Introduction  Turkish Discourse Bank (TDB) is the first discourse-annotated corpus of Turkish, which follows the  principles of Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) and includes annotations for dis-", "acronyms": [[156, 160], [40, 43]], "long-forms": [[130, 154], [16, 38]], "ID": "599"}, {"text": "that first contains TOOV and given by the search  engine. Average_Rank (A_Rank) is the average position of TOOV in the returned snippets.", "acronyms": [[72, 78], [20, 24], [107, 111]], "long-forms": [[58, 70]], "ID": "600"}, {"text": "3.5 A Novel Lattice Statistical Language Model Representation Our final statistical language model is a novel latent-variable statistical language model, called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3. The", "acronyms": [[184, 190]], "long-forms": [[163, 182]], "ID": "601"}, {"text": "data as described in Niehues and Vogel (2008).  The phrase table (PT) is built using the Moses toolkit (Koehn et al.,", "acronyms": [[66, 68]], "long-forms": [[52, 64]], "ID": "602"}, {"text": "(1987) presents in his Case grid.  Case Analysis (CA) extracts the acts and Case  constellations around them from the structure ", "acronyms": [[50, 52]], "long-forms": [[35, 48]], "ID": "603"}, {"text": "The IXM2 is the first massively parallel associative  processor that clearly demonstrates the computing  power of a large Associative Memory (AM). The AM ", "acronyms": [[142, 144], [4, 8], [151, 153]], "long-forms": [[122, 140]], "ID": "604"}, {"text": "Improving part-of-speech tagging for context-free parsing. In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1260?1268, Chiang Mai, Thailand.", "acronyms": [[144, 150]], "long-forms": [[81, 142]], "ID": "605"}, {"text": "The current state of the art within the comment summarisation field is to cluster comments using Latent Dirichlet Allocation (LDA) topic modelling (Khabiri et al, 2011; Ma et al, 2012;", "acronyms": [[126, 129]], "long-forms": [[97, 124]], "ID": "606"}, {"text": "Certain schemes have been aimed at abstracts, e.g., (McKnight  &  Srinivasan, 2003; Ruch et al, 2007; Hirohata et al, 2008; Bj?rne et al, 2009). The work of Hirohata et al (2009) has been integrated with the MEDIE service5 (Miyao et al, 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al, 1999; Teufel  &  Moens, 2002; Teufel et al, 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers.", "acronyms": [[387, 389], [494, 496]], "long-forms": [[365, 385]], "ID": "607"}, {"text": "get, with results deteriorating as we move to information retrieval (IR), multi-document summarization (SUM), and information extraction (IE). ", "acronyms": [[138, 140]], "long-forms": [[114, 136]], "ID": "608"}, {"text": "construct such a treebank from scratch. Fortunately, RST Discourse Treebank (RST-DT)  (Carlson et al, 2001) is an available resource to ", "acronyms": [[77, 83]], "long-forms": [[53, 75]], "ID": "609"}, {"text": "Holes, 2004). Most tools and resources developed for natural language processing (NLP) of Arabic are designed for MSA.", "acronyms": [[82, 85], [114, 117]], "long-forms": [[53, 80]], "ID": "610"}, {"text": "nese personal naming system. Therefore, we hold  Chinese personal name disambiguation (CPND) to  explore those problems.", "acronyms": [[87, 91]], "long-forms": [[49, 85]], "ID": "611"}, {"text": " Like most of the successful AQUAINT QA systems,  LCC?s system uses an answer type (AT) ontology for  the classification of AT categories.", "acronyms": [[84, 86], [29, 36], [37, 39], [50, 53], [124, 126]], "long-forms": [[71, 82]], "ID": "612"}, {"text": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In IEEE International Conference on Computer Vision (ICCV), December.", "acronyms": [[169, 173], [119, 123]], "long-forms": [[124, 167]], "ID": "613"}, {"text": "U, x /y /z ,  T, V ~ VYw\\[Y /X\\ ]   (14)  5Here the 'full' version of (VR) is being used, incorpo-  rating a change of bound variable.", "acronyms": [[71, 73]], "long-forms": [[59, 66]], "ID": "614"}, {"text": "  Although we use the same techniques to derive global features (assessor variety (AV) feature with 2~6 grams) from both training and test-", "acronyms": [[83, 85]], "long-forms": [[65, 81]], "ID": "615"}, {"text": "Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). FN* = functional", "acronyms": [[151, 154], [157, 159]], "long-forms": [[138, 149]], "ID": "616"}, {"text": "Deep-syntactic structures (DSyntSs);  ? Surface syntactic structures (SSyntSs);  61 ", "acronyms": [[70, 77]], "long-forms": [[40, 68]], "ID": "617"}, {"text": "Linguistic expressions can vanish and appear in translation. For example, the preposition (PP) in the source rule does not show up in any of   Machine Translation Based on Constraint-Based Synchronous Grammar 615 ", "acronyms": [[91, 93]], "long-forms": [[78, 89]], "ID": "618"}, {"text": "The general architecture of D2S is represented in Figure 1. It consists of two modules, the Language Gen-  eration Module (LGM), and the Speech Generation Module (SGM). The LGM takes data as input .and ", "acronyms": [[163, 166], [28, 31], [123, 126], [173, 176]], "long-forms": [[137, 161], [92, 121]], "ID": "619"}, {"text": "2011) created a corpus of posts from several online forums about breast cancer, which later was used to extract potential adverse reactions from the most commonly used drugs to treat this disease: tamoxifen, anastrozole, letrozole and axemestane. The authors collected a lexicon of lay medical terms from websites and databases about drugs and adverse events. The lexicon was extended with the Consumer Health Vocabulary (CHV)5, a vocabulary closer to the lay terms, which patients usually use to describe their medical experiences. Then, pairs of terms co-occurring within a window of 20 tokens were considered.", "acronyms": [[422, 425]], "long-forms": [[394, 420]], "ID": "620"}, {"text": "approach and extract features from the names.  They use Maximum Entropy (MaxEnt) model and a number of features based on n-grams,", "acronyms": [[73, 79]], "long-forms": [[56, 71]], "ID": "621"}, {"text": "is the DIAGRAM grammar \\ [9 \\ ] .   It is un for tunate ly  the very power .of APSGs (and ATNs)  that  makes it  d i f f i cu l t  to capture l inguist ic  general izat ions ", "acronyms": [[79, 84], [90, 94]], "long-forms": [], "ID": "622"}, {"text": " 1 Introduction Interactive question answering (QA) has been identified as one of the important directions in QA", "acronyms": [[48, 50], [110, 112]], "long-forms": [[28, 46]], "ID": "623"}, {"text": "5.3 Evaluation Metrics For YA, we used the standard implementations for P@1 and mean reciprocal rank (MRR) (Manning et al, 2008).", "acronyms": [[102, 105], [27, 29]], "long-forms": [[80, 100]], "ID": "624"}, {"text": "erences to the instructors in the posts etc.  3.3 Linear Chain Markov Model (LCMM) The logistic regression model is good at exploit-", "acronyms": [[77, 81]], "long-forms": [[50, 75]], "ID": "625"}, {"text": "The bacteria track consists of two tasks, BB and BI.  2.4.1 Bacteria biotope task (BB) The aim of the BB task (Bossy et al, 2011) is to ex-", "acronyms": [[83, 85], [102, 104], [42, 44], [49, 51]], "long-forms": [[60, 76]], "ID": "626"}, {"text": "Abstract We present the first provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which", "acronyms": [[84, 86]], "long-forms": [[63, 82]], "ID": "627"}, {"text": "1408 then apply our model to the well-established sequence labeling task: noun phrase (NP) chunking. ", "acronyms": [[87, 89]], "long-forms": [[74, 85]], "ID": "628"}, {"text": "data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split", "acronyms": [[118, 123], [35, 37], [71, 74]], "long-forms": [[101, 116], [18, 33]], "ID": "629"}, {"text": " 4.3 The Limited-Memory BFGS Algorithm The limited memory BFGS (L-BFGS) algorithm is a general purpose numerical optimization algorithm (Nocedal and Wright 1999).", "acronyms": [[64, 70], [24, 28]], "long-forms": [[43, 62]], "ID": "630"}, {"text": "In the leftmost column SSE-TRA and SSE-HDC refer to the trained and hand-coded SSE versions; the column NS indicates the number of sessions; the columns PSucc (perceived success), PAtt (perceived attention recognition), PUnd (perceived understanding), PNat (perceived naturalness), and POv (perceived overall performance) give average", "acronyms": [[153, 158], [180, 184], [23, 30], [35, 42], [79, 82], [104, 106], [220, 224], [252, 256]], "long-forms": [[160, 177], [186, 205], [226, 249], [258, 279], [291, 320], [286, 289]], "ID": "631"}, {"text": "Evaluation (LREC?08), Marrakech, Morocco, may.  European Language Resources Association (ELRA). ", "acronyms": [[89, 93], [12, 16]], "long-forms": [[48, 87]], "ID": "632"}, {"text": "search excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-o?konomischer Exzellenz (LOEWE) as part of the research center Digital Humanities.", "acronyms": [[101, 106]], "long-forms": [[26, 99]], "ID": "633"}, {"text": "The Text Encoding Initiative (TEI) is a cooperative undertaking of the Association  for Computers and the Humanities (ACH), the Association for Computational Lin-  guistics (ACL), and the Association for Literary and Linguistic Computing (ALLC). ", "acronyms": [[239, 243], [30, 33], [118, 121], [174, 177]], "long-forms": [[188, 237], [4, 28], [71, 116], [128, 172]], "ID": "634"}, {"text": "If the polarity of expressed statement is not  neutral and reinforcement is negative, then the  polarity of the statement (PP) is reversed and  score is intensified: ", "acronyms": [[123, 125]], "long-forms": [[96, 104]], "ID": "635"}, {"text": " 1 Introduction Amazon?s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in", "acronyms": [[41, 44]], "long-forms": [[16, 39]], "ID": "636"}, {"text": "tween arguments. We thus propose to model the reranking phase (RR) as a HMM sequence labeling task.", "acronyms": [[63, 65], [72, 75]], "long-forms": [[46, 55]], "ID": "637"}, {"text": "six basic emotion tags to the Bengali blog sentences. Conditional Random Field (CRF)  based word level emotion classifier classifies ", "acronyms": [[80, 83]], "long-forms": [[54, 78]], "ID": "638"}, {"text": "of two kinds of data: One is the hand built seg-  mentation dictionary (HBSD)  and the other is the  simple noun dictionary for segmentation (SND). ", "acronyms": [[142, 145], [72, 76]], "long-forms": [[101, 123], [33, 70]], "ID": "639"}, {"text": "Using either method of uncertainty sampling,  the computational cost of picking an example from  T candidates is: O(TD) where D is the number of  model parameters.", "acronyms": [[116, 118]], "long-forms": [[97, 109]], "ID": "640"}, {"text": " Recently, there have been increasing interests for dialogue act (DA) recognition in spoken and written conversations, which include meetings,", "acronyms": [[66, 68]], "long-forms": [[52, 64]], "ID": "641"}, {"text": "labeled DGs.  (C3) Projectivity constraint(PJC): No arc crosses another arc5", "acronyms": [[43, 46], [8, 11]], "long-forms": [[19, 41]], "ID": "642"}, {"text": "integer linear programming (ILP) conditional random field (CRF) support vector machine (SVM) latent semantic analysis (LSA)", "acronyms": [[88, 91], [28, 31], [59, 62], [119, 122]], "long-forms": [[64, 86], [0, 26], [33, 57], [93, 117]], "ID": "643"}, {"text": "2https://www.mturk.com/mturk/. 3http://tartarus.org/martin/PorterStemmer/ 4Reviews with NDr = NTr are regarded as incorrectly classified by TopicSpam.", "acronyms": [[94, 97], [88, 91]], "long-forms": [[98, 125]], "ID": "644"}, {"text": " The objective of this study is to illustrate a  word support model (WSM) that is able to improve our WP-identifier by achieving better ", "acronyms": [[69, 72], [102, 104]], "long-forms": [[49, 67]], "ID": "645"}, {"text": "(I) it performs a translation between  intermediate languages constructed  over source language (SL) and target  language (TL) respectively (called ", "acronyms": [[97, 99], [123, 125]], "long-forms": [[80, 95], [105, 121]], "ID": "646"}, {"text": "press first-order logic. This requirement motivates our use of Inductive Logic Programming (ILP), a learning algorithm capable of inferring logic pro-", "acronyms": [[92, 95]], "long-forms": [[63, 90]], "ID": "647"}, {"text": "   (2)  LSA-based (Latent Semantic Analysis based)  trigger word similarity: LSA (Deerwester et ", "acronyms": [[8, 17], [77, 80]], "long-forms": [[19, 43]], "ID": "648"}, {"text": " 250 Support Vector Machines (SVMs) construct a hyperplane in a multi-dimensional space which yields a good separation between positive and negative training examples, represented as data points.", "acronyms": [[30, 34]], "long-forms": [[5, 28]], "ID": "649"}, {"text": "recursive noun phrases (NPs), main verb groups (MVs), and a common annotation for adjectival and adverbial phrases (APs). Example (3) be-", "acronyms": [[116, 119], [24, 27], [48, 51]], "long-forms": [[97, 114], [10, 22], [30, 46]], "ID": "650"}, {"text": "He washed it. With Kamp's  discourse representation theory (DRT) (Kamp 1981; Kamp and Reyle 1993) a discourse  representation structure (DRS) in which the reference to the pronoun he is constrained ", "acronyms": [[60, 63], [137, 140]], "long-forms": [[27, 58], [100, 135]], "ID": "651"}, {"text": "  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 33?37, Gothenburg, Sweden, April 26-30 2014.", "acronyms": [[70, 72], [28, 32]], "long-forms": [[50, 68]], "ID": "652"}, {"text": "Long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the", "acronyms": [[119, 122]], "long-forms": [[93, 117]], "ID": "653"}, {"text": " We evaluate performance using 3 measures:  exact match (EM), head match (HM), and partial  match (PM), similar to Choi et al (2006).", "acronyms": [[57, 59], [74, 76], [99, 101]], "long-forms": [[44, 55], [62, 72], [83, 97]], "ID": "654"}, {"text": "Prevalence and count of conditions by temporal  category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department, ", "acronyms": [[75, 77]], "long-forms": [[80, 97]], "ID": "655"}, {"text": " 1 Introduction Predicate argument structure (PAS) analysis is a shallow semantic parsing task that identifies ba-", "acronyms": [[46, 49]], "long-forms": [[16, 44]], "ID": "656"}, {"text": "110  ehange(CHA) communication(COMM)  cognition(COG) competition(COMP)  contact(CeNT) motion(MOT) ", "acronyms": [[48, 51], [65, 69]], "long-forms": [[38, 46], [53, 63]], "ID": "657"}, {"text": "(SBAR-TMP (IN after) (S (NP (DT the) (NN sale)) (VP (AUX is) (VP (VBN completed))) ))))))))", "acronyms": [[66, 69], [1, 9], [25, 27], [29, 31], [38, 40], [49, 51], [53, 56], [62, 64]], "long-forms": [], "ID": "658"}, {"text": "mark concept are sent to the kernel-based location belief tracker, while all other concepts are sent to a Dynamic Probabilistic Ontology Trees (DPOT) semantic belief tracker, whose structure is shown in", "acronyms": [[144, 148]], "long-forms": [[106, 142]], "ID": "659"}, {"text": " Since this is a binary classification task, we have 5 different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Beginning an", "acronyms": [[71, 74], [107, 110], [140, 143]], "long-forms": [[76, 98], [112, 131], [145, 157]], "ID": "660"}, {"text": "In Proceedings of the 15th International Conference on  Computational Linguistics (COLING'94),  Kyoto, Japan, August.", "acronyms": [[83, 92]], "long-forms": [[56, 81]], "ID": "661"}, {"text": " 0.7  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random NLPBA", "acronyms": [[60, 62], [83, 86], [107, 110], [118, 123]], "long-forms": [[39, 58], [63, 69]], "ID": "662"}, {"text": "vv@iiit.ac.in Abstract Recognition of Named Entities (NEs) is a difficult process in Indian languages like Hindi,", "acronyms": [[54, 57]], "long-forms": [[38, 52]], "ID": "663"}, {"text": "ing in the non-realizable case. In Advances in Neural Information Processing Systems (NIPS), 2010.", "acronyms": [[86, 90]], "long-forms": [[47, 84]], "ID": "664"}, {"text": "unitary operator. Therefore, the primary problem  of building a quantum classifier (QC) is to find  the correct or optimal unitary operator.", "acronyms": [[84, 86]], "long-forms": [[64, 82]], "ID": "665"}, {"text": "categories, as shown in Figure 1. Messages may  involve a request (REQ), provide information  (INF), or fall into the category of interpersonal ", "acronyms": [[67, 70], [95, 98]], "long-forms": [[58, 65], [81, 92]], "ID": "666"}, {"text": "Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and", "acronyms": [[81, 84]], "long-forms": [[53, 79]], "ID": "667"}, {"text": "? P5E3N4S3, F W I International Language of Service and Maintenance (ILSAM) (Pym 1990) is an influential language similar to Caterpillar Fundamental English, from which it was derived", "acronyms": [[69, 74], [2, 10]], "long-forms": [[18, 67]], "ID": "668"}, {"text": "4.2 Cognate based features Dictionaries mostly fail to return translation entries for named entities (NEs) or specialized terminology.", "acronyms": [[102, 105]], "long-forms": [[86, 100]], "ID": "669"}, {"text": " In the medical domain, Castan?o et al (2002) used UMLS (Unified Medical Language System)7 as their knowledge source.", "acronyms": [[51, 55]], "long-forms": [[57, 90]], "ID": "670"}, {"text": "as the World Wide Web. Various traditional  information retrieval(IR) techniques combined  with natural language processing(NLP) tech- ", "acronyms": [[66, 68], [124, 127]], "long-forms": [[44, 64], [96, 123]], "ID": "671"}, {"text": "BUS (business) Belgium Labour Federation 9 4440 11.0 NOB (nobility) Albert II 6 4179 15.1 COM (comics) Suske and Wiske 3 4000 10.5 MUS (music) Sandra Kim, Urbanus 3 1296 14.6", "acronyms": [[90, 93], [0, 3], [53, 56], [131, 134]], "long-forms": [[95, 101], [5, 13], [58, 66], [136, 141]], "ID": "672"}, {"text": "ayman,R.Gaizauskas@dcs.shef.ac.uk Abstract Disambiguating named entities (NE) in running text to their correct interpretations in a specific knowledge base (KB) is an important problem in NLP.", "acronyms": [[74, 76], [157, 159], [188, 191]], "long-forms": [[58, 72], [141, 155]], "ID": "673"}, {"text": "single weight matrixW . In contrast, the CVG uses a syntactically untied RNN (SU-RNN) which has a set of such weights.", "acronyms": [[78, 84]], "long-forms": [[52, 76]], "ID": "674"}, {"text": "integrate Chinese word segmentation and NE  identification into a unified framework using a  class-based language model (LM).\u0016 &ODVV\u0010EDVHG\u0003/0 IRU\u00031(\u0003,GHQWLILFDWLRQ The n-gram LM is a stochastic model which ", "acronyms": [[121, 123], [40, 42], [175, 177]], "long-forms": [[105, 119]], "ID": "675"}, {"text": "New in three aspects. First, the basic units of their model are elementary discourse units (EDUs) from Rhetorical Structure Theory (RST) (Mann", "acronyms": [[92, 96], [132, 135]], "long-forms": [[64, 90], [103, 130]], "ID": "676"}, {"text": "P (di|h(d1,..., di?1)) Using a neural network architecture called Simple Synchrony Networks (SSNs), the history representation h(d1,..., di?1) is incrementally computed from", "acronyms": [[93, 97]], "long-forms": [[73, 91]], "ID": "677"}, {"text": "2 As a matter of fact, Figure 1 only shows 8 columns, although  the CoNLL-X format includes two additional columns for the  projective head (PHEAD) and projective dependency relation  (PDEPREL), which have not been used in our work.", "acronyms": [[141, 146], [68, 75], [185, 192]], "long-forms": [[124, 139], [152, 182]], "ID": "678"}, {"text": "collection; however, to facilitate comparisons with prior work (e.g., McCarthy et al 2004a), all our experiments use the British National Corpus (BNC). In", "acronyms": [[146, 149]], "long-forms": [[121, 144]], "ID": "679"}, {"text": "MUC-6, 1995; Agirre, 2007). By contrast, gold  standard Named Entity (NE) annotations are easy  to produce; indeed, there are many NE annotated ", "acronyms": [[70, 72]], "long-forms": [[56, 68]], "ID": "680"}, {"text": " For each combination, we measure the attachment score (AS) and the exact match (EM). A signif-", "acronyms": [[81, 83], [56, 58]], "long-forms": [[68, 79], [38, 54]], "ID": "681"}, {"text": "treebank. In: Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24? ", "acronyms": [[86, 89]], "long-forms": [[51, 84]], "ID": "682"}, {"text": "/(cs + |N |?)  P (GR = gri|SCF = s) = (csgri +?) /(cs + |G|?)", "acronyms": [[18, 20]], "long-forms": [[23, 30]], "ID": "683"}, {"text": "The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. ", "acronyms": [[107, 110]], "long-forms": [[78, 105]], "ID": "684"}, {"text": "of-the-art in more detail.  The field of Information Extraction (IE) has been heavily influenced by the Information Retrieval (IR)", "acronyms": [[65, 67]], "long-forms": [[41, 63]], "ID": "685"}, {"text": "we maximize the log likelihood J(?) using a simple optimization technique called stochastic gradient descent (SGD). N,W", "acronyms": [[110, 113]], "long-forms": [[81, 108]], "ID": "686"}, {"text": "The classification step currently supports three machine learning algorithms from the Python scikit-learn4 package: Na??ve Bayes (NB), Maximum Entropy (MaxEnt), and Support Vector", "acronyms": [[130, 132], [152, 158]], "long-forms": [[116, 128], [135, 150]], "ID": "687"}, {"text": "2.1 Collection Tasks To collect our data, we used two different types of human intelligence tasks (HITs). In type 1, the", "acronyms": [[99, 103]], "long-forms": [[73, 97]], "ID": "688"}, {"text": "Since this is a binary classification task, we have 5 different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Beginning an Idiomatic chunk), I-I (Inside an Idiomatic chunk),", "acronyms": [[106, 109], [139, 142], [70, 73], [175, 178]], "long-forms": [[111, 130], [144, 166], [75, 97], [180, 199]], "ID": "689"}, {"text": "However, to the best of our knowledge only the model of Blunsom & Cohn (2006), which is based on a Conditional Random Field (CRF) (Lafferty et al, 2001), can compute word indices pairs?", "acronyms": [[125, 128]], "long-forms": [[99, 123]], "ID": "690"}, {"text": "the Natural Language Generation (NLG) component to create the textual form of the output and last, the Text To Speech (TTS) component to convert the text to spoken output.", "acronyms": [[119, 122], [33, 36]], "long-forms": [[103, 117], [4, 31]], "ID": "691"}, {"text": "formation retrieval resulted in creation of reusable test collections in large-scale evaluations such as the Text REtrieval Conference (TREC)1. Researchers in", "acronyms": [[136, 140]], "long-forms": [[109, 134]], "ID": "692"}, {"text": "P2E5N5S1, C W D I FAA Air Traffic Control Phraseology (FAA 2010) is a controlled language defined by the Federal Aviation Administration (FAA) and used for the communication in air traffic 135", "acronyms": [[138, 141], [0, 8], [55, 58], [18, 21]], "long-forms": [[105, 136]], "ID": "693"}, {"text": "tions. This model has three main steps including Local Term Weighting (LTW), Global Term Weighting (GTM), and Fuzzy Clustering (Algo-", "acronyms": [[71, 74]], "long-forms": [[49, 69]], "ID": "694"}, {"text": "to) Peet a friend?  Figure 10: An argument post particle phrase (PP) (upper) and an adjunct PP (lower).", "acronyms": [[65, 67], [92, 94]], "long-forms": [[48, 63]], "ID": "695"}, {"text": "are contained in a XML file and each query  consists of following elements: Topic  Number(NUM),Topic Title(TITLE),Topic  question(DESC),Topic Narrative(NARR) and ", "acronyms": [[90, 93], [19, 22], [107, 112], [130, 134], [152, 156]], "long-forms": [[83, 88], [101, 106], [142, 151]], "ID": "696"}, {"text": "ber of hours of domain-specific spontaneous speech used for AM adaptation, the number of titles used to construct the language model (LM), the type of LM, the type of grammar rules in the Phoenix book", "acronyms": [[134, 136], [151, 153], [60, 62]], "long-forms": [[118, 132]], "ID": "697"}, {"text": " 1 Introduction  Generation of referring expression (GRE) is an  important task in the field of Natural Language ", "acronyms": [[53, 56]], "long-forms": [[17, 51]], "ID": "698"}, {"text": "from standard formats to OpenNLP specific ones. We represented standard formats with EMF (an Ecore model for each one) and we created specific transformations using Java Emitter Templates (JET) 16", "acronyms": [[189, 192], [29, 32], [85, 88]], "long-forms": [[165, 187]], "ID": "699"}, {"text": "structural and behavioral parts have to be fully specified at this level.  2.3 Eclipse Modeling Framework (EMF) We chose to adopt EMF as the underlying modeling framework and tooling for our model-driven ap-", "acronyms": [[107, 110], [130, 133]], "long-forms": [[79, 105]], "ID": "700"}, {"text": "As suggested from the tables, the accuracy values of the component classifiers (Ccn and Cen) in CoTrain are almost always higher than those of the corresponding TSVM(CN) and TSVM(EN), based on any machine translation service.", "acronyms": [[166, 168], [161, 165], [174, 178], [179, 181], [80, 83], [88, 91]], "long-forms": [], "ID": "701"}, {"text": "a set of features in the Sentence Scoring phase.  The Maximal Marginal Relevance (MMR) algorithm is then used in the Sentence Re-ordering", "acronyms": [[82, 85]], "long-forms": [[54, 80]], "ID": "702"}, {"text": "CP = Relative Pronoun  +  IP  PP = Preposition  + NP AdjP = Adjective + NP", "acronyms": [[30, 32]], "long-forms": [[35, 46]], "ID": "703"}, {"text": "#and (a probabilistic AND) is used. Otherwise, the  probabilistic passage operator #UWn (unordered window)  is used.", "acronyms": [[84, 87], [22, 25]], "long-forms": [[89, 105]], "ID": "704"}, {"text": "Ill the last two  experimeuts a memory with the analysis of tile  most frequent word-forms (MFW) in Basque  was used, so that only word-forms not found ", "acronyms": [[92, 95]], "long-forms": [[66, 84]], "ID": "705"}, {"text": "strated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM cor-", "acronyms": [[114, 117], [166, 169]], "long-forms": [[93, 112]], "ID": "706"}, {"text": "interchange of LRs. It also demonstrate how MLI  can be applied to Asian Language Resource (ALR)  through making the results of collaborative en-", "acronyms": [[92, 95], [15, 18], [44, 47]], "long-forms": [[67, 90]], "ID": "707"}, {"text": "reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).  Entity coherence, which is based on the way the referents of noun phrases (NPs) relate subsequent clauses in the text, is an important aspect of textual organization.", "acronyms": [[134, 137]], "long-forms": [[120, 132]], "ID": "708"}, {"text": "tor. Together, these modifications reduce the BLEU score by 1.49 BLEU points (BP)9 at the largest training size.", "acronyms": [[78, 80], [46, 50]], "long-forms": [[65, 76]], "ID": "709"}, {"text": " The best method ASSVM outperforms other methods most clearly on METH (Method) category. Al-", "acronyms": [[65, 69], [17, 22]], "long-forms": [[71, 77]], "ID": "710"}, {"text": " 3.3 Bootstrapped Voting Experts The Bootstrapped Voting Experts (BVE) algorithm (Hewlett and Cohen, 2009) is an extension to VE.", "acronyms": [[66, 69], [126, 128]], "long-forms": [[37, 64]], "ID": "711"}, {"text": "  This approach has been employed in Augmentative  and Alternative Communication (AAC), in the  form of multimodal vocabularies in assistive de-", "acronyms": [[82, 85]], "long-forms": [[37, 80]], "ID": "712"}, {"text": "1Note: LM(language model); ME(maximum entropy).  Brand Name(BRA), Product Type(TYP), Product Name(PRO), and BRA and TYP are often embed-", "acronyms": [[60, 63], [7, 9], [27, 29], [79, 82], [98, 101], [108, 111], [116, 119]], "long-forms": [[49, 54], [10, 24], [30, 45], [66, 78], [85, 92]], "ID": "713"}, {"text": " Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify nounmodifier relations in the medical domain, using Medical Subject Headings (MeSH) and Unified Medical Language System (UMLS) as lexical resources for representing each", "acronyms": [[155, 159], [198, 202]], "long-forms": [[129, 153], [165, 196]], "ID": "714"}, {"text": "and in part by the TerraSwarm Research Center, one of six centers supported by the STARnet phase of the Focus Center Research Program (FCRP) a Semiconductor Research Corporation program sponsored by", "acronyms": [[135, 139], [83, 90]], "long-forms": [[104, 133]], "ID": "715"}, {"text": "Name Discrimination by Clustering Similar Contexts, Proceedings of the World Wide Web Conference (WWW). ", "acronyms": [[98, 101]], "long-forms": [[71, 85]], "ID": "716"}, {"text": "comparability between documents that are  chosen for exploitation in the current research,  are Mutual Infromation (MI) and Normalized  Mutual Infroamtion (NMI).", "acronyms": [[116, 118], [156, 159]], "long-forms": [[96, 114], [124, 154]], "ID": "717"}, {"text": "HG-ALN 0.266 0.359 Table 1: The Pk and WindowDiff scores of uniform segmentation (UNI), TextTiling (TT), baseline alignment (B-ALN), and alignment with hier-", "acronyms": [[100, 102], [82, 85], [0, 6], [125, 130]], "long-forms": [[88, 98], [60, 67], [105, 123]], "ID": "718"}, {"text": "not vacillate, vacillate is, line vacillate?  English Context(EC): shake/vacillate  Putting on Search Engine and getting counts:  ", "acronyms": [[62, 64]], "long-forms": [[46, 60]], "ID": "719"}, {"text": "represented in Table 3 based on the place (bilabial  (BL), lab-dental (LD), dental (DE), alveopalatal  (AP), velar (VL), uvular (UV) and glottal (GT))  and manner of articulation (stops (ST), fricatives ", "acronyms": [[146, 148], [54, 56], [71, 73], [84, 86], [104, 106], [116, 118], [129, 131], [187, 189]], "long-forms": [[137, 144], [43, 51], [59, 69], [76, 82], [89, 101], [109, 114], [121, 127], [180, 185]], "ID": "720"}, {"text": "pendency and constituency parsing.  2.4.1 On Dependency Parsing (DP) ?", "acronyms": [[65, 67]], "long-forms": [[45, 63]], "ID": "721"}, {"text": "cjlin/libsvm/ System P R F1 Schwartz & Hearst (SH) .978 .940 .959 SaRAD .891 .919 .905", "acronyms": [[47, 49]], "long-forms": [[28, 45]], "ID": "722"}, {"text": "targeted text. This type of question has been studied extensively in the Text Retrieval Conference Question Answering (QA) Track (Dang, Kelly, and Lin 2007). Using the", "acronyms": [[119, 121]], "long-forms": [[99, 117]], "ID": "723"}, {"text": "Table 5 shows the results when experimenting with various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We", "acronyms": [[217, 220], [113, 115], [181, 184], [248, 250]], "long-forms": [[193, 215], [101, 111], [127, 179], [232, 246]], "ID": "724"}, {"text": "as a statistical model of natural anguage, t and weak-  eus Jelinek et al's contention that \"in an ambiguous  but appropriately chosen probabilistic CFG (PCFG),  correct parses are Ifigh probability parses\" (p. 2).", "acronyms": [[154, 158]], "long-forms": [[135, 152]], "ID": "725"}, {"text": "each other.  Normalized common neighbors (NCN). Nor-", "acronyms": [[42, 45]], "long-forms": [[13, 40]], "ID": "726"}, {"text": "Figure 3: A Graphical Representation of the Infinite Tree Model than a simple Dirichlet process (DP)2 (Ferguson, 1973) is that we have to introduce coupling across", "acronyms": [[97, 99]], "long-forms": [[78, 95]], "ID": "727"}, {"text": "LOC(at. IN) The ACT (Actor) can be any noun in the subjective case (the abbreviation n), the PAT (Patient)", "acronyms": [[16, 19], [0, 3], [93, 96]], "long-forms": [[21, 26], [98, 105]], "ID": "728"}, {"text": "ing the following measures:   1. PrecisionCorrectTransliterations(PTrans)  2.", "acronyms": [[66, 72]], "long-forms": [[33, 64]], "ID": "729"}, {"text": "Other formats have been suggested for dictionary sharing,  notably those developed under the Text Encoding Initiative  using SGML (Standard Generalized Markup Language). We ", "acronyms": [[125, 129]], "long-forms": [[131, 167]], "ID": "730"}, {"text": "the global normalization of random field models,  and avoid the label bias problem that exists in  maximum entropy Markov models (MEMMs). ", "acronyms": [[130, 135]], "long-forms": [[99, 128]], "ID": "731"}, {"text": "= li), ? Hierarchical loss (H-Loss) function is defined as:", "acronyms": [[28, 34]], "long-forms": [[9, 26]], "ID": "732"}, {"text": "UniProt the protein sequence database managed by the Swiss Institute of Bioinformatics (SIB), the European Bioinformatics Institute (EBI) and the Protein Information Resource (PIR)", "acronyms": [[133, 136], [88, 91], [176, 179]], "long-forms": [[98, 131], [53, 85], [146, 174]], "ID": "733"}, {"text": "In ? Proceedings of  the Eighth Text REtrieval Conference (TREC-9)?, ", "acronyms": [[59, 65]], "long-forms": [[25, 57]], "ID": "734"}, {"text": "Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popu-", "acronyms": [[108, 114]], "long-forms": [[87, 106]], "ID": "735"}, {"text": "In Proceedings of the Eighteenth International Conference on Machine Learning (ICML), pages 282?289. ", "acronyms": [[79, 83]], "long-forms": [[33, 77]], "ID": "736"}, {"text": "input format. For instance, if the input is annotated with word and PoS (WP), so must be the translation model.", "acronyms": [[73, 75]], "long-forms": [[59, 71]], "ID": "737"}, {"text": "in their absence. If both people are aware of the event, we call it Interaction (INR): for example, one person is telling the other a story.", "acronyms": [[81, 84]], "long-forms": [[68, 79]], "ID": "738"}, {"text": "2,5 y1,5 Figure 1: The Partial Lattice MRF (PL-MRF) Model for a 5-word sentence and a 4-layer lattice.", "acronyms": [[44, 50]], "long-forms": [[23, 42]], "ID": "739"}, {"text": "several systems have been developed such as BITS (Bilingual Internet Test Search) (Ma and Liberman, 1999), PTMiner (Parallel Text Miner) (Chen and Nie, 2000), and STRAND (Structural Translation", "acronyms": [[107, 114], [44, 48], [163, 169]], "long-forms": [[116, 135], [50, 80], [171, 193]], "ID": "740"}, {"text": "583  Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 1?7, New York City, New York, June 2006.", "acronyms": [[68, 72], [86, 95]], "long-forms": [[24, 66]], "ID": "741"}, {"text": "Secondly, there is the group of speech particles which are not part of the core sentence construction,  yet pragmatically cannot stand on their own. These 'sentence external' (SE) elements can be subclassified into two classes.", "acronyms": [[176, 178]], "long-forms": [[156, 173]], "ID": "742"}, {"text": "various evidential features are proposed and  integrated effectively and efficiently through a  Hidden Markov Model (HMM). In addition, a ", "acronyms": [[117, 120]], "long-forms": [[96, 115]], "ID": "743"}, {"text": "thematic structure, and defined well formedness conditions on the thematic structure and on the relation between thematic structure (TH) and syntactic dominance (ID) structure.", "acronyms": [[133, 135], [162, 164]], "long-forms": [[113, 121], [0, 8]], "ID": "744"}, {"text": "A set of the hyponym candidates extracted from a single itemization or list is called a hyponym candidate set (HCS). For the itemization", "acronyms": [[111, 114]], "long-forms": [[88, 109]], "ID": "745"}, {"text": " 1 Introduction Question answering (QA) systems have received a great deal of attention because they provide both", "acronyms": [[36, 38]], "long-forms": [[16, 34]], "ID": "746"}, {"text": "Table 2: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer, McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary", "acronyms": [[138, 140], [73, 75], [94, 97], [182, 188], [205, 210], [245, 247], [280, 284]], "long-forms": [[141, 157], [76, 92], [98, 125], [172, 180], [189, 195], [211, 236], [248, 267], [285, 295]], "ID": "747"}, {"text": " 2.6 Adverb  Group (AdvG)   Adverb group (AdvG) is used in the realization  of several circumstantial functions given in Sec- ", "acronyms": [[42, 46], [20, 24]], "long-forms": [[28, 40]], "ID": "748"}, {"text": "The divergence of the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-) divergence (Cover and Thomas 1991): Fixednesssyn (v, n)", "acronyms": [[136, 139]], "long-forms": [[118, 134]], "ID": "749"}, {"text": "changes and there is a complement particle between complement constituents(COP)  and verbs. So in the information dictionary, the characteristics of {V(CHA), VP+COP}  should be described.", "acronyms": [[152, 155], [75, 78], [161, 164], [158, 160]], "long-forms": [[130, 145], [51, 73]], "ID": "750"}, {"text": "A List of POS-tags ADJ (adjectives), ADV (adverbs), CJ (conjunctions), CL (clitics), CN (common nouns), DA (definite articles), DEM (demonstratives),", "acronyms": [[71, 73], [85, 87], [10, 13], [19, 22], [37, 40], [52, 54], [104, 106], [128, 131]], "long-forms": [[75, 82], [89, 101], [24, 34], [42, 49], [56, 68], [108, 125], [133, 147]], "ID": "751"}, {"text": "3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classification task.", "acronyms": [[105, 108], [31, 40]], "long-forms": [[80, 103]], "ID": "752"}, {"text": "social media. In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM), pages 291?300.", "acronyms": [[96, 100]], "long-forms": [[68, 94]], "ID": "753"}, {"text": "of Excellence and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.", "acronyms": [[133, 137], [65, 70]], "long-forms": [[102, 131], [22, 63]], "ID": "754"}, {"text": "particular components - -  immediate dominance (ID) rules, meta-  rules, linear precedence (LP) statements, feature co-occurrence  restrictions (FCRs), and feature specification defaults (FSDs)  - -  and four universal components - - a theory of syntactic fea- ", "acronyms": [[188, 192], [48, 50], [92, 94], [145, 149]], "long-forms": [[156, 186], [27, 46], [73, 90], [108, 143]], "ID": "755"}, {"text": "we describe SCANMail, a system that employs automatic speech recognition (ASR), information retrieval (IR), information extraction (IE), and human computer interaction (HCI) technology to permit users to browse and search their voicemail messages by content", "acronyms": [[169, 172], [12, 20], [74, 77], [103, 105], [132, 134]], "long-forms": [[141, 167], [44, 72], [80, 101], [108, 130]], "ID": "756"}, {"text": "characterize the AAV functions mediating this effect, cloned AAV type 2 wild-type or mutant genomes were transfected into simian virus 40 (SV40)-transformed hamster cells together with the six HSV replication genes", "acronyms": [[139, 143], [17, 20], [61, 64], [193, 196]], "long-forms": [[122, 137]], "ID": "757"}, {"text": "grammatical features from a diachronic corpus of academic English, and visualise our extraction results with Structured Parallel Coordinates (SPC), a tool for the visualisation of structured multidi-", "acronyms": [[142, 145]], "long-forms": [[109, 140]], "ID": "758"}, {"text": "Abstract This paper reports on the participation of the TALP Research Center of the UPC (Universitat Polit?cnica de Catalunya) to the ACL WMT 2008 evaluation", "acronyms": [[84, 87], [56, 60], [134, 137], [138, 141]], "long-forms": [[89, 112]], "ID": "759"}, {"text": "lar, at the level of additional information (CR), we observed some differences in judgement in particular between restrictions (AR) and warnings (AA), and a few others between CSFH and CSFC whose", "acronyms": [[146, 148], [45, 47], [128, 130], [176, 180], [185, 189]], "long-forms": [], "ID": "760"}, {"text": "onomy using a Japanese-English bilingual dictionary as  a \"bridge\", in order to support semantic processing in a  knowledge-based machine translation (MT) system. ", "acronyms": [[151, 153]], "long-forms": [[130, 149]], "ID": "761"}, {"text": "tion string in discourse: PER, GPE, ORG, LOC,  FAC, NPER (NOMINAL PER), NGPE, NORG,  NLOC, NFAC, PPER (PROUNOUN PER),  PGPE, PORG, PLOC, and PFAC.", "acronyms": [[97, 101], [26, 29], [31, 34], [36, 39], [41, 44], [47, 50], [52, 56], [72, 76], [78, 82], [85, 89], [91, 95], [119, 123], [125, 129], [131, 135], [141, 145]], "long-forms": [[103, 115], [58, 69]], "ID": "762"}, {"text": "The unspecified role filler is not 'bound'  to a complement (i.e. any item on the SUBCAT list)  but is existentially quantified (EX-Q). The ergative ", "acronyms": [[129, 133], [82, 88]], "long-forms": [[103, 127]], "ID": "763"}, {"text": "Non-MonoClausal Verbs (NMCV), Passives  (Pass) and Auxiliary Construction (AC) that  are identified as compound verbs (CompVs). ", "acronyms": [[119, 125], [23, 27], [41, 45], [75, 77]], "long-forms": [[103, 117], [0, 21], [30, 38], [51, 73]], "ID": "764"}, {"text": "3.1 The Information Extraction Pipeline  The extraction task we are addressing is that of the  Automatic Content Extraction (ACE)1 evaluations. ", "acronyms": [[125, 128]], "long-forms": [[95, 123]], "ID": "765"}, {"text": "tions (Abe et al, 1996).  1.1 Question answering (QA) Unlike IR systems which return a list of documents", "acronyms": [[50, 52]], "long-forms": [[30, 48]], "ID": "766"}, {"text": "helpful in identifying the embedded words. However  some ambiguous segmentation strings(ASSs) and  unregistered words (i.e. the word that is not registered ", "acronyms": [[88, 92]], "long-forms": [[57, 87]], "ID": "767"}, {"text": " 2.2 Hierarchical Softmax Hierarchical Softmax (HSM) organizes the output vocabulary into a tree where the leaves are", "acronyms": [[48, 51]], "long-forms": [[26, 46]], "ID": "768"}, {"text": "In the next experiments, we run experiments across three different locales in Places domain: United Kingdom (GB), Australia (AU), and India (IN).", "acronyms": [[125, 127], [109, 111], [141, 143]], "long-forms": [[114, 123], [93, 107], [134, 139]], "ID": "769"}, {"text": "ing and understanding convolutional networks. In European Conference on Computer Vision (ECCV). ", "acronyms": [[89, 93]], "long-forms": [[49, 87]], "ID": "770"}, {"text": "tion (IE). Considerable volume of location data was imported in a knowledge base (KB) with entities of general importance used for seman-", "acronyms": [[82, 84], [6, 8]], "long-forms": [[66, 80]], "ID": "771"}, {"text": "multilingual MT system developed at the Laboratoire d'Analyse et de Technologie du Langage (LATL), University of Geneva. ", "acronyms": [[92, 96], [13, 15]], "long-forms": [[40, 90]], "ID": "772"}, {"text": "assignment. We use a generative model based on a Dirichlet Process (DP) defined over composed rules.", "acronyms": [[68, 70]], "long-forms": [[49, 66]], "ID": "773"}, {"text": " Because of data sparseness, we cannot reliably use a  maximum likelihood estimator (MLE) for bigram prob-  abilities.", "acronyms": [[85, 88]], "long-forms": [[55, 83]], "ID": "774"}, {"text": "Frustratingly easy domain adaptation.  In Association for Computational Linguistics (ACL). ", "acronyms": [[85, 88]], "long-forms": [[42, 83]], "ID": "775"}, {"text": "and the speech. The SU detection task is evaluated on both the reference human transcriptions (REF) and speech recognition outputs (STT).", "acronyms": [[95, 98], [20, 22], [132, 135]], "long-forms": [[63, 72], [104, 130]], "ID": "776"}, {"text": "Each corpus uses a different set of entity labels.  MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numeri-", "acronyms": [[73, 76], [94, 97], [52, 55], [119, 122]], "long-forms": [[62, 71], [79, 92], [103, 111]], "ID": "777"}, {"text": "Baselines and Evaluation We compare prediction results with a set of single-task baselines: a Support Vector Machine (SVM) using an RBF kernel with hyperparameters optimised via cross-", "acronyms": [[118, 121], [132, 135]], "long-forms": [[94, 116]], "ID": "778"}, {"text": "perform rather well because the combined recency  bias representation worked well on its own and be-  cause the restricted memory (RM) bias essentially  discards features that are distant from the relative ", "acronyms": [[131, 133]], "long-forms": [[112, 129]], "ID": "779"}, {"text": "that is neither terminal nor lexical. An interior node is  said to meet he foot condition (FC) iffeach foot feature  that it contains appears also on at least one daughter ", "acronyms": [[91, 93]], "long-forms": [[75, 89]], "ID": "780"}, {"text": "However, a study regarding maximal recall shows that we do not remove too many true positives (TPs) (more details in Section 4.1).", "acronyms": [[95, 98]], "long-forms": [[79, 93]], "ID": "781"}, {"text": "frequency pairs, Erk et al(2010) drew (R, t)  pairs from each of five frequency bands in the  entire British National Corpus (BNC):  50-100  occurrences; 101-200; 201-500; 500-1000; and ", "acronyms": [[126, 129]], "long-forms": [[101, 124]], "ID": "782"}, {"text": "details of this collection.  AECMA Simplified English (AECMA-SE) (AECMA 1986) was the predecessor of ASD Simplified Technical English.", "acronyms": [[55, 63], [66, 71], [101, 104]], "long-forms": [[29, 53]], "ID": "783"}, {"text": "an efficient bottom-up algorithm. The asymptotic time complexity of this search is O(SR) where S is the number of source nodes andR is the number", "acronyms": [[85, 87]], "long-forms": [[73, 79]], "ID": "784"}, {"text": "  2. PrecisionCorrectTransliteration  (PTrans)  The precision is going to be computed using the ", "acronyms": [[39, 45]], "long-forms": [[5, 36]], "ID": "785"}, {"text": "The parameters ? are estimated through the optimization of a Maximum Likelihood (ML) criterion using the Expectation-Maximization (EM) al-", "acronyms": [[81, 83], [131, 133]], "long-forms": [[61, 79], [105, 129]], "ID": "786"}, {"text": "2 We use a new related measure, which we call the overall percentage error reduction (OPER), that uses the entire area under the curves given by", "acronyms": [[86, 90]], "long-forms": [[50, 84]], "ID": "787"}, {"text": " 1 Introduction Massive Open Online Courses (MOOCs), run by organizations such as Coursera, have been among", "acronyms": [[45, 50]], "long-forms": [[16, 43]], "ID": "788"}, {"text": "of a multi-class document categorization. We introduce PRBEP (precision recall break even point) as a measure which is popular in the area of infor-", "acronyms": [[55, 60]], "long-forms": [[62, 95]], "ID": "789"}, {"text": "erature for further details.  Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sen-", "acronyms": [[54, 57]], "long-forms": [[30, 52]], "ID": "790"}, {"text": "\\[Harman, 1996\\] D. Harman. Overview of the Forth  Text RetrievalConference (TREC-4). In Proceedings ", "acronyms": [[77, 83]], "long-forms": [[44, 75]], "ID": "791"}, {"text": "languages.  Mutual Information (MI)  For the purpose of this experiment we use a ", "acronyms": [[32, 34]], "long-forms": [[12, 30]], "ID": "792"}, {"text": "Schafer and Graham, 2002) discussed several approaches such as case deletion, mean substitution, and recommended maximum likelihood (ML) and Bayesian multiple imputation (MI).", "acronyms": [[133, 135], [171, 173]], "long-forms": [[113, 131], [150, 169]], "ID": "793"}, {"text": "Figure 3: Dialogue system architecture    The Dialogue Manager (DM) uses sceneobject attributes, such as type, angle or interval ", "acronyms": [[64, 66]], "long-forms": [[46, 62]], "ID": "794"}, {"text": "of the first studies to investigate such constancy is Genzel and Charniak (2002), in which the authors proposed the Constant Entropy Rate (CER) hypothesis: in written text, the entropy per sig-", "acronyms": [[139, 142]], "long-forms": [[116, 137]], "ID": "795"}, {"text": " In this definition, WHEN has two formal parameters x and y; each of them refers to a  situation that occurs at the same point in time (PTIM). Any occurrence of the relation ", "acronyms": [[136, 140]], "long-forms": [[121, 134]], "ID": "796"}, {"text": "Central to the predictive dialogue is the topic representation for each scenario, which enables the population of a Predictive Dialogue Network (PDN). ", "acronyms": [[145, 148]], "long-forms": [[116, 143]], "ID": "797"}, {"text": "88 lated based on a co-occurrence relationship between i and w. Next, the semantic orientation (SO) of the phrase i is obtained by calculating the difference be-", "acronyms": [[96, 98]], "long-forms": [[74, 94]], "ID": "798"}, {"text": "These  probabilities could be estimated from training cases  with Maximum Likelihood Estimation (MLE). Let l ", "acronyms": [[97, 100]], "long-forms": [[66, 95]], "ID": "799"}, {"text": " 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality ma-", "acronyms": [[61, 63]], "long-forms": [[40, 59]], "ID": "800"}, {"text": "Semitic languages (in which vowels are not written), etc.  The problem of word sense disambiguation (WSD) has been described as \"AI-complete,\"  that is, a problem which can be solved only by first resolving all the difficult problems ", "acronyms": [[101, 104]], "long-forms": [[74, 99]], "ID": "801"}, {"text": "ajaynagesh@cse.iitb.ac.in Abstract Information Extraction (IE) has become an indispensable tool in our quest to handle the data", "acronyms": [[59, 61]], "long-forms": [[35, 57]], "ID": "802"}, {"text": "Because LSA is closely related to principle component analysis (PCA), extensions of PCA such as canonical correlation analysis (CCA) and oriented principle component analysis (OPCA) can leverage", "acronyms": [[128, 131], [8, 11], [64, 67], [84, 87], [176, 180]], "long-forms": [[96, 126], [34, 62], [137, 174]], "ID": "803"}, {"text": "Perhaps not surprisingly, therefore, few natural language understanding (NLU) systems use graphical presentational features to aid interpretation, and few natural language generation (NLG) systems attempt to render the output texts in a principled way.", "acronyms": [[184, 187], [73, 76]], "long-forms": [[155, 182], [41, 71]], "ID": "804"}, {"text": "The implementations of the oracles described in the first part of this work (sections 3 and 4) use the common formalism of finite state acceptors (FSA) over different semirings and are implemented us-", "acronyms": [[147, 150]], "long-forms": [[123, 145]], "ID": "805"}, {"text": " NB-B uses full Bayesian inference and NB-M uses Maximum a posteriori (MAP). ", "acronyms": [[71, 74]], "long-forms": [[49, 69]], "ID": "806"}, {"text": "The general idea of the main algorithm is to align phrase synsets from the Patty taxonomy with verb synsets in WordNet. To this end, we first construct a directed candidate alignment graph (CAG). Section", "acronyms": [[190, 193]], "long-forms": [[163, 188]], "ID": "807"}, {"text": "rently, a pure statistical MT system based on Pharaoh is developed by BPPT and National News  Agency (ANTARA) using 500K sentences pair,  expected to have better accuracy and robustness ", "acronyms": [[102, 108], [27, 29], [70, 74]], "long-forms": [[79, 100]], "ID": "808"}, {"text": "NP and selection of the head NP by the relative 1The following abbreviations are used in glosses: NOM = nominative, ACC = accusative, PRES = non-past and POT = potential. ( )", "acronyms": [[116, 119], [29, 31], [0, 2], [98, 101], [134, 138], [154, 157]], "long-forms": [[122, 132], [104, 114], [160, 169]], "ID": "809"}, {"text": "Entropy Guided Transformation Learning (ETL) is a new machine learning strategy that combines the advantages of Decision Trees (DT) and Transformation-Based Learning (TBL) (dos Santos", "acronyms": [[128, 130], [40, 43], [167, 170]], "long-forms": [[112, 126], [0, 38], [136, 165]], "ID": "810"}, {"text": "The TRIANGLE application  (TRIANGLE) and a new Windows 95  Accessible Graphing Calculator (ACG) both  developed by the SAP uses tone plots to ", "acronyms": [[91, 94], [27, 35], [119, 122]], "long-forms": [[59, 78], [4, 12]], "ID": "811"}, {"text": " Bidirectional CLSTM Graves and Schmidhuber (2005) proposed a Bidirectional LSTM (B-LSTM) model, which utilizes additional backward informa-", "acronyms": [[82, 88], [15, 20]], "long-forms": [[62, 80]], "ID": "812"}, {"text": "Run 3 100% 18 (9.0%) 38 (19.0%)  Table 7.  Effect of Translation (E-C)   ", "acronyms": [[66, 69]], "long-forms": [[43, 49]], "ID": "813"}, {"text": "UMichigan non-Tipster UNK X USouthern California non-Tipster SNAP X USussex (UK) non-Tipster SUSSEX X Table 1 .", "acronyms": [[77, 79], [22, 25], [61, 65], [93, 99]], "long-forms": [[68, 75]], "ID": "814"}, {"text": "Adverb Variation (AdvV) ? Modifier Variation (ModV) ?", "acronyms": [[46, 50]], "long-forms": [[26, 44]], "ID": "815"}, {"text": " ? WHNP_NN_IN  Syntactic Parse Trees (PT)  72", "acronyms": [[38, 40], [3, 13]], "long-forms": [[25, 36]], "ID": "816"}, {"text": "2. TREE ADJO IN ING GRAMMARS- -TAG's   We now introduce tree adjoining grammars (TAG's). TAG's ", "acronyms": [[81, 86], [31, 36], [89, 94]], "long-forms": [[56, 79]], "ID": "817"}, {"text": "proach to automatically recognize predicate  heads of Chinese sentences based on a preprocessing step for maximal noun phrases 1(MNPs). ", "acronyms": [[129, 133]], "long-forms": [[106, 126]], "ID": "818"}, {"text": "entire search process.  (b) EPN for the first optimum solution (EPN-F): The number of the expanded problems when", "acronyms": [[64, 69]], "long-forms": [[28, 45]], "ID": "819"}, {"text": "an ASR system. The main idea was to design a language model (LM) to combine the trigram language model probability with the translation", "acronyms": [[61, 63], [3, 6]], "long-forms": [[45, 59]], "ID": "820"}, {"text": "summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) features. ", "acronyms": [[103, 106]], "long-forms": [[89, 101]], "ID": "821"}, {"text": "city. City and state can then be used to select a local area language model (LM) for recognizing listing names.", "acronyms": [[77, 79]], "long-forms": [[61, 75]], "ID": "822"}, {"text": "We trained the UKP machine learning classifier originally developed for the Semantic Textual Similarity (STS) task at SemEval-2012 (B?r et al 2012) on the averaged binary and senary judge-", "acronyms": [[105, 108], [15, 18], [118, 130]], "long-forms": [[76, 103]], "ID": "823"}, {"text": "4.3 Classifiers All evaluation tests were performed using two classifiers, Maximum Entropy (MaxEnt) and Support Vector Machines (SVM).", "acronyms": [[92, 98], [129, 132]], "long-forms": [[75, 90], [104, 127]], "ID": "824"}, {"text": "375 that...). Extreme case formulations (ECF) are lexical patterns emphasizing extremeness (e.g., This is", "acronyms": [[41, 44]], "long-forms": [[14, 39]], "ID": "825"}, {"text": " NPHIL = Stray NP: Volume I: Syntax,  SUBJ = Subject: H__~e reads., ", "acronyms": [[38, 42]], "long-forms": [[45, 52]], "ID": "826"}, {"text": "In this paper, we describe an initial  implementation of a general spoken language interface, the  Carnegie Mellon Spoken Language Shell (CM-SLS) which  provides voice interface services to a variable number of applica- ", "acronyms": [[138, 144]], "long-forms": [[99, 136]], "ID": "827"}, {"text": "implicit (assuming a good enough coverage of the marker resource). The Annodis corpus lists rhetorical relations between elementary discourse units (EDUs), typically clauses, and complex discourse units (sets of EDUs) ; as a simplification we only consider EDUs, since the question of what is a main verb of", "acronyms": [[149, 153], [212, 216], [257, 261]], "long-forms": [[121, 147]], "ID": "828"}, {"text": "among other ways). We refer to cases in which the subject and object of the relation are contained within a phrase headed by a Noun as Relational NP?s (RNP). ", "acronyms": [[152, 155]], "long-forms": [[135, 150]], "ID": "829"}, {"text": "been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents.", "acronyms": [[160, 163]], "long-forms": [[125, 158]], "ID": "830"}, {"text": " ? Left Reveal (LRev) : Pop the top two nodes in the stack (left, right).", "acronyms": [[16, 20]], "long-forms": [[3, 14]], "ID": "831"}, {"text": " ? MEA+LexPageRank (MEALR) : This method applies the proposed mixture-event-aspect model to", "acronyms": [[20, 25]], "long-forms": [[3, 18]], "ID": "832"}, {"text": "To experiment on MAYA, we compute the  performance score as the Reciprocal Answer  Rank (RAR) of the first correct answer given by  each question.", "acronyms": [[89, 92], [17, 21]], "long-forms": [[83, 87]], "ID": "833"}, {"text": "  ? PT (parse tree)  ", "acronyms": [[4, 6]], "long-forms": [[8, 18]], "ID": "834"}, {"text": "gle word maze); B-M (beginning of multi-word 72 maze); I-M (in multi-word maze); and E-M (end of multi-word maze).", "acronyms": [[55, 58], [16, 19], [85, 88]], "long-forms": [[60, 68], [21, 39], [90, 103]], "ID": "835"}, {"text": "(wine) is localized to exterior locus (barrel) and crosses the intermediate locus IME(LOC) to be localized to the interior INT(LOC) (the_bottle). ", "acronyms": [[127, 130], [86, 89], [82, 85], [123, 126]], "long-forms": [[97, 106], [76, 81]], "ID": "836"}, {"text": " 2 System Overview Our system, named PML Tree Query (PML-TQ), consists of three main components (discussed fur-", "acronyms": [[53, 59]], "long-forms": [[37, 51]], "ID": "837"}, {"text": "Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al (2006) have shown that a shift-reduce parser can give", "acronyms": [[72, 75]], "long-forms": [[54, 70]], "ID": "838"}, {"text": "NEARING A PROJECT PROPOSAL TO OTTA BOARD  As the  Urns,,. C6ngress Office of Techdblogy Assessment (OTA) planning  sttidy on te lecomunica t ions ,  computers and information p o l i c i e s  approaches ", "acronyms": [[100, 103], [30, 34]], "long-forms": [[67, 98]], "ID": "839"}, {"text": "1 Introduction In this paper, we describe our submission to the Genia Event (GE) information extraction subtask of the BioNLP Shared Task.", "acronyms": [[77, 79], [119, 125]], "long-forms": [[64, 75]], "ID": "840"}, {"text": "1 N 79.4 80.6 1.89 46.2 74.5 2 Y 80.8 81.4 1.70 44.3 80.4 Table 1: Results for parsing section 0 ( \u0000 40 words) of the WSJ Penn Treebank: OP = overparsing, LP/LR = labelled precision/recall.", "acronyms": [[137, 139], [118, 121], [155, 160]], "long-forms": [[142, 153], [163, 188]], "ID": "841"}, {"text": "guished in Boxer: 1. Discourse Representation Structures (DRSs) 2.", "acronyms": [[58, 62]], "long-forms": [[21, 56]], "ID": "842"}, {"text": " 3.3 Linear-chain CRF for Extraction The alignment CRF (AlignCRF) model described in Section 3.1 is able to predict labels for a text", "acronyms": [[56, 64], [18, 21]], "long-forms": [[41, 54]], "ID": "843"}, {"text": "Table 2: The evaluation for each combination of agents. LB = ListenerBot; DB = DialogBot. ", "acronyms": [[56, 58], [74, 76]], "long-forms": [[61, 72], [79, 88]], "ID": "844"}, {"text": " 3.1 Annotating message-level data Amazon?s Mechanical Turk (MTurk) was used to annotate the 5,000 random English (American)", "acronyms": [[61, 66]], "long-forms": [[44, 59]], "ID": "845"}, {"text": "Abstract This paper introduces a tensor-based approach to semantic role labeling (SRL). The motiva-", "acronyms": [[82, 85]], "long-forms": [[58, 80]], "ID": "846"}, {"text": "1. INTRODUCTION  Hidden Markov Models (HMMs) have been used suc-  cessfully in a wide variety of recognition tasks ranging ", "acronyms": [[39, 43]], "long-forms": [[17, 37]], "ID": "847"}, {"text": "course structure. In Proceedings of the International Conference in Computational Linguistics (COLING), pages 43?49. ", "acronyms": [[95, 101]], "long-forms": [[68, 93]], "ID": "848"}, {"text": "namely, the set {? ( D)d |ad = a}, where ad is the author of document d. An alternative approach is LDA-S (LDA with a single document per author), where each author?s documents are concatenated into a single document in a preprocessing step, LDA is run", "acronyms": [[100, 105], [242, 245]], "long-forms": [[107, 124]], "ID": "849"}, {"text": "It has been shown in (Ando and Zhang, 2005a) that the optimization problem (3) has a simple solution using singular value decomposition (SVD) when we choose square regularization: r(f", "acronyms": [[137, 140]], "long-forms": [[107, 135]], "ID": "850"}, {"text": "complementary features ( Section 3.3).  3.2 Mining Labeled Sequential Patterns ( LSP ) Labeled Sequential Patterns (LSP).", "acronyms": [[81, 84], [115, 119]], "long-forms": [[51, 78], [87, 114]], "ID": "851"}, {"text": "class of mildly context sensitive grsmmars which we  230  call \"Ranked Node Rewriting Grammaxs\" (RNRG's). ", "acronyms": [[97, 103]], "long-forms": [[64, 94]], "ID": "852"}, {"text": "we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM (Brants, 2000) or a conditional Markov model (CMM) (Ratnaparkhi, 1996)).", "acronyms": [[148, 151], [98, 101]], "long-forms": [[122, 146]], "ID": "853"}, {"text": "To this end, we employ the  27 Adaptive Hierarchical Density Histograms (AHDH) as visual feature vectors, due to the fact that they  have shown discriminative power between binary complex drawings (Sidiropoulos et al.,", "acronyms": [[73, 77]], "long-forms": [[31, 71]], "ID": "854"}, {"text": "actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations eas-", "acronyms": [[75, 78]], "long-forms": [[50, 73]], "ID": "855"}, {"text": "with parts of the annotated logical form.  \u0001 ZC07 (Zettlemoyer and Collins 2007) extends ZC05 with extra (disharmonic) combinators to increase the expressive power of the model.", "acronyms": [[45, 49], [89, 93]], "long-forms": [[51, 79]], "ID": "856"}, {"text": " 1 Introduction Referring Expression Generation (REG) is a keytask in NLG, and the topic of the REG 2008 Chal-", "acronyms": [[49, 52], [70, 73], [96, 99]], "long-forms": [[16, 47]], "ID": "857"}, {"text": "Our data comes from English (ENG), Chinese (CHI), Portuguese (POR), and Kinyarwanda (KIN). ", "acronyms": [[85, 88], [29, 32], [44, 47], [62, 65]], "long-forms": [[72, 83], [20, 27], [35, 42], [50, 60]], "ID": "858"}, {"text": "Beijing University of Posts and Telecommunications (BUPT) ? ?  Beijing Institute of Technology (BIT) ?  ", "acronyms": [[96, 99], [52, 56]], "long-forms": [[63, 94], [0, 49]], "ID": "859"}, {"text": "the traditional k-nearest neighbor (kNN) algorithm.  Maximum a posteriori (MAP) principle is used to determine which emotion set is related to the giv-", "acronyms": [[75, 78], [36, 39]], "long-forms": [[53, 73], [16, 34]], "ID": "860"}, {"text": " 2.2 Recognizing subwords An automatic speech recognition (ASR) system (Jelinek, 1998) serves to recognize both queries", "acronyms": [[59, 62]], "long-forms": [[29, 57]], "ID": "861"}, {"text": "In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL), pages 440?447, Hong Kong, October.", "acronyms": [[86, 89]], "long-forms": [[43, 84]], "ID": "862"}, {"text": "ductive transfer learning. In Proceedings of the IEEE International Conference on Data Mining (ICDM) 2007 Workshop on Mining and Management of Bio-", "acronyms": [[95, 99], [49, 53]], "long-forms": [[54, 93]], "ID": "863"}, {"text": "srlrr(r** TRANSPORMATIONS IC*S**  SCAN CALLED AT 1 I  ANTEST CALLED F O R  5\"SYLLAB \" (AACC) ,SD= 6. RES= 11.", "acronyms": [[87, 91], [94, 96], [101, 104]], "long-forms": [[54, 67]], "ID": "864"}, {"text": " ? Self-training Segmenters (STS): two variant models were defined by the approach re-", "acronyms": [[29, 32]], "long-forms": [[17, 27]], "ID": "865"}, {"text": "Again, even the Table 5 Precision (PD), recall (RD), and F-measure (FD) for malapropism detection with five measures of semantic relatedness, varying the scope of the search for related words to 1, 3, or 5 paragraphs", "acronyms": [[35, 37], [48, 50], [68, 70]], "long-forms": [[24, 33], [40, 46], [57, 66]], "ID": "866"}, {"text": " 2.2 Human Intelligence Tasks A Human Intelligence Task (HIT) is a short paid task on MTurk.", "acronyms": [[57, 60]], "long-forms": [[32, 55]], "ID": "867"}, {"text": "A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proc. Conference on Empirical Methods in Natural Language Processing 2009, (EMNLP-09). David Yarowsky.", "acronyms": [[158, 166], [82, 86]], "long-forms": [[102, 155]], "ID": "868"}, {"text": "on terrorism in a matter of weeks. Since the terrorism  domain knowledge bases (KB's) were developed for English  for MUC-4 already, and since the KB's can be shared across ", "acronyms": [[80, 84], [118, 123], [147, 151]], "long-forms": [[63, 78]], "ID": "869"}, {"text": "augmented by a set of PRIDES-specific Common  Gateway Interfaces (CGIs), communicates with the  client via Hypertext Transport Protocol (HTTP). A ", "acronyms": [[137, 141], [22, 37], [66, 70]], "long-forms": [[107, 135], [38, 64]], "ID": "870"}, {"text": "In Proc. of Seventh Text REtrieval Conference (TREC-7). ", "acronyms": [[47, 53]], "long-forms": [[12, 45]], "ID": "871"}, {"text": " For this reason, NIST assessors not only marked  the segments shared between system units (SU)  and model units (MU), they also indicated the ", "acronyms": [[92, 94], [18, 22], [114, 116]], "long-forms": [[78, 90], [101, 112]], "ID": "872"}, {"text": " The implementation of our approach is a system called LetSum (Legal text Summarizer), which has been developed in Java and Perl.", "acronyms": [[55, 61]], "long-forms": [[63, 84]], "ID": "873"}, {"text": "model for sequence classification. In Proceedings of International Conference on Data Mining (ICDM). ", "acronyms": [[94, 98]], "long-forms": [[53, 92]], "ID": "874"}, {"text": "In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC). ", "acronyms": [[89, 93]], "long-forms": [[54, 72]], "ID": "875"}, {"text": "References TREC (Text REtrieval Conference) : http://trec.nist.gov/ NTCIR (NII-NACSIS Test Collection for IR Systems) project: http://research.nii.ac.jp/ntcir/index-en.html", "acronyms": [[75, 85], [11, 15], [68, 73], [106, 108]], "long-forms": [[17, 42]], "ID": "876"}, {"text": "As first step we try to map the linguistic triple  into an ontology triple, by using an adaptation of  Aqualog?s Relation Similarity Service (RSS).  ", "acronyms": [[142, 145]], "long-forms": [[113, 140]], "ID": "877"}, {"text": "baseline (BAS) system which is close to the system described in (Hu et al 2009), and three variants of our novel divide and conquer (DAC) system. Fea-", "acronyms": [[133, 136], [10, 13]], "long-forms": [[113, 131], [0, 8]], "ID": "878"}, {"text": "LS (List item marker) LS  MD (Modal) MD  NN (Noun, singular or mass) N  NNS (Noun, plural) N ", "acronyms": [[41, 43], [0, 2], [22, 24], [26, 28], [37, 39], [72, 75]], "long-forms": [[45, 49], [30, 35], [77, 81]], "ID": "879"}, {"text": "the toolkits. Sizes are given for the resulting transducers (VM = Verbmobil). ", "acronyms": [[61, 63]], "long-forms": [[66, 75]], "ID": "880"}, {"text": "tools for the Indian Languages. For Telugu, though a Part Of Speech(POS) Tagger for Telugu, is available, the accuracy is less when compared to English", "acronyms": [[68, 71]], "long-forms": [[53, 66]], "ID": "881"}, {"text": "determines the nature of such space.  For example, Syntactic Tree Kernel (STK) are used to model complete context free rules as in (Collins", "acronyms": [[74, 77]], "long-forms": [[51, 72]], "ID": "882"}, {"text": "summarization. During these intervening decades,  progress in Natural Language Processing (NLP), coupled  with great increases of computer memory and speed, ", "acronyms": [[91, 94]], "long-forms": [[62, 89]], "ID": "883"}, {"text": "c?2008 Association for Computational Linguistics Dialect Classification for online podcasts fusing Acoustic and Language based Structural and Semantic Information   Rahul Chitturi, John. H.L. Hansen1 Center for Robust Speech Systems(CRSS) Erik Jonsson School of Engineering and Computer Science University of Texas at Dallas Richardson, Texas 75080, U.S.A {rahul.ch@student, john.hansen@}utdallas.edu Abstract  The variation in speech due to dialect is a factor which significantly impacts speech system per-formance.", "acronyms": [[233, 237], [350, 355]], "long-forms": [[200, 231]], "ID": "884"}, {"text": "Sotelo, 2007).  4.3 Polarity Lexicon (LEX) We have built a polarity lexicon with both Positive", "acronyms": [[38, 41]], "long-forms": [[29, 36]], "ID": "885"}, {"text": "tend to conflate the distances between all possible sense pairs. Latent semantic analysis (LSA) (Landauer et al, 1998) has also been used to measure dis-", "acronyms": [[91, 94]], "long-forms": [[65, 89]], "ID": "886"}, {"text": "cal patterns and the impacts of different  constraints that are used to identify the  Complex Predicates (CPs). System ", "acronyms": [[106, 109]], "long-forms": [[86, 104]], "ID": "887"}, {"text": "semantically interpreted.  We apply conditional random fields (CRFs) to the task of SRL proposed by the CoNLL shared", "acronyms": [[63, 67]], "long-forms": [[36, 61]], "ID": "888"}, {"text": "category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department,  GI = Operative Gastrointestinal, RAD = Radiology and ", "acronyms": [[75, 77], [27, 29], [52, 56], [103, 105], [136, 139]], "long-forms": [[80, 100], [32, 49], [59, 73], [118, 134], [142, 151]], "ID": "889"}, {"text": "lected randomly from some reference corpus.  Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of", "acronyms": [[62, 64]], "long-forms": [[45, 60]], "ID": "890"}, {"text": "The described research is undertaken in the course of the development of human computer interfaces for natural interaction in Virtual Reality (VR). Conducting empirical inves-", "acronyms": [[143, 145]], "long-forms": [[126, 141]], "ID": "891"}, {"text": "In Proceedings of Workshop on Setting Standards for Searching Electronically Stored Information In Discovery Proceedings (DESI-4). ", "acronyms": [[122, 128]], "long-forms": [[99, 120]], "ID": "892"}, {"text": "given threshold, this binary feature fires.  4.3 Variant Feature (VAR) In the variant cipher, the plaintext is written into", "acronyms": [[66, 69]], "long-forms": [[49, 64]], "ID": "893"}, {"text": "The second and last step to generate qwn-ppv(s) consists of propagating over a WordNet graph to obtain a Personalized PageRanking Vector (PPV), one for each polarity.", "acronyms": [[138, 141], [37, 44]], "long-forms": [[105, 136]], "ID": "894"}, {"text": "This gave rise to a relatively new research area within the emerging field of Textto-Text Generation (TTG) called Multiple-Choice Test Item Generation (MCTIG).1", "acronyms": [[102, 105], [152, 157]], "long-forms": [[78, 100], [114, 150]], "ID": "895"}, {"text": "We extend the SPARSELDA (Yao et al, 2009) inference scheme for latent Dirichlet alocation (LDA) to tree-based topic models.", "acronyms": [[91, 94], [14, 23]], "long-forms": [[63, 89]], "ID": "896"}, {"text": "For each IDR triple  all the object grammar triples are generated whose CF-PS rules  conform with the linear precedence(LP) rules, the fourth rule set  of the metagrammar.", "acronyms": [[120, 122], [9, 12], [72, 77]], "long-forms": [[102, 118]], "ID": "897"}, {"text": "SVO = Subject-Verb-Object GE = General Event PE = Predefined Event Grammar Module", "acronyms": [[45, 47], [0, 3], [26, 28]], "long-forms": [[50, 66], [6, 25], [31, 44]], "ID": "898"}, {"text": " For all NER experiments, we use a sequential firstorder conditional random field (CRF) with a unit variance Normal prior, trained with L-BFGS until", "acronyms": [[83, 86], [9, 12], [136, 142]], "long-forms": [[57, 81]], "ID": "899"}, {"text": "the titles of the entity articles titles(e) to represent the entities in the query and two ranking functions, Recursive TFISF (R-TFISF) and LC, 3", "acronyms": [[127, 134], [140, 142]], "long-forms": [[110, 125]], "ID": "900"}, {"text": "Text5 holiday(0.432) humor(0.23) 0.099 blues(0.15) Table 2: Percent Agreement(PA) to manually extracted index terms", "acronyms": [[78, 80]], "long-forms": [[60, 76]], "ID": "901"}, {"text": "The score of an abstract based on extracted PICO elements, SPICO, is broken into individual components according to the following formula: SPICO = Sproblem + Spopulation + Sintervention + Soutcome (4) The first component in the equation, Sproblem, reflects a match between the primary", "acronyms": [[139, 144], [44, 48], [59, 64]], "long-forms": [[147, 196]], "ID": "902"}, {"text": " 3.3.1 Determinantal Point Processes Determinantal point processes (DPPs) are distributions over subsets that jointly prefer quality of", "acronyms": [[68, 72]], "long-forms": [[37, 66]], "ID": "903"}, {"text": " They use Only Word-Seg (OWS), Whole Layer Weight (WLW), SC (SC) and FeedBack mechanism (FB) separately.", "acronyms": [[51, 54], [25, 28], [57, 59], [61, 63], [89, 91]], "long-forms": [[43, 49], [10, 23], [69, 77]], "ID": "904"}, {"text": "In Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI-89), volume 2, pages 1511?1517.", "acronyms": [[86, 94]], "long-forms": [[27, 84]], "ID": "905"}, {"text": "Similarity function We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008).", "acronyms": [[121, 125]], "long-forms": [[101, 119]], "ID": "906"}, {"text": "wsj_1286)) In addition to the semantic roles described in the rolesets, verbs can take any of a set of general, adjunct-like arguments (ArgMs), distinguished by one of the function tags shown in Table 1.", "acronyms": [[136, 141]], "long-forms": [[125, 134]], "ID": "907"}, {"text": "Under the former, they include  personal pronouns, sentential \" i t ,\"  and null-comple-  ment anaphora, and under the latter, verb phrase (VP)  ellipsis, sluicing, gapping, and stripping.", "acronyms": [[140, 142]], "long-forms": [[127, 138]], "ID": "908"}, {"text": "4 Crowd-sourcing Multiple-choice Questions from Tables We use Amazon?s Mechanical Turk (MTurk) service to generate MCQs by imposing constraints", "acronyms": [[88, 93], [115, 119]], "long-forms": [[71, 86]], "ID": "909"}, {"text": "a directed acyclic graph, and a set of conditionnal probablities, each node being represented as a Random Variable (RV). Parametrizing the BN", "acronyms": [[116, 118], [139, 141]], "long-forms": [[99, 114]], "ID": "910"}, {"text": "                  Threshold is a function of Length(LR) and text  size. The basic idea is larger amount of length(LR)  or text size matches larger amount of Threshold.", "acronyms": [[114, 116]], "long-forms": [[90, 96]], "ID": "911"}, {"text": "mid method in DUC 2005. Proceedings of the 5th Document Understanding Conference (DUC). Van-", "acronyms": [[82, 85], [14, 17]], "long-forms": [[47, 80]], "ID": "912"}, {"text": "on the guidance of domain experts, who can devise pedagogically valuable reading lists that order docAutomatic Speech Recognition (ASR) with HMMs Noisy Channel Model Viterbi Decoding for ASR", "acronyms": [[131, 134], [187, 190]], "long-forms": [[101, 129]], "ID": "913"}, {"text": "ing (NLP) applications. In Information Retrieval (IR) and Question Answering (QA) it is typically termed query/question expansion (Moldovan and", "acronyms": [[78, 80], [5, 8], [50, 52]], "long-forms": [[58, 76], [27, 48]], "ID": "914"}, {"text": "In sum, the text-to-text similarity measure combined with our sentence-level sen-timent analysis algorithm helps us identify the representative rationales of diverse opinions in an online deliberation. An overview of our meth-od is shown in Figure 1.  We applied our method in analyzing Wikipe-dia Article for Deletion (AfD) deliberation con-tent. Next we discuss how this method is used to analyze the content.", "acronyms": [[320, 323]], "long-forms": [[298, 318]], "ID": "915"}, {"text": "computational devices for natural language processing.  called active production networks (APNs), and explore  how certain kinds of movement are handled.", "acronyms": [[91, 95]], "long-forms": [[63, 89]], "ID": "916"}, {"text": "Figure 1: Top-k Accuracy Level Configuration MRR 0 Baseline (BL) 0.6559 1", "acronyms": [[61, 63], [45, 48]], "long-forms": [[51, 59]], "ID": "917"}, {"text": "mental state labels that are highly similar to the context of the scene in a latent, conceptual vector space; and an information retrieval (IR) model that identifies labels commonly appearing in sentences", "acronyms": [[140, 142]], "long-forms": [[117, 138]], "ID": "918"}, {"text": "sentences (Sent.), the number of tokens (Tokens) and the unlabeled attachment score (UAS) of MST. ", "acronyms": [[85, 88], [11, 15], [41, 47], [93, 96]], "long-forms": [[57, 83], [0, 9], [33, 39]], "ID": "919"}, {"text": " We have three versions of reference summaries based on summarization ratio(SR): 10%, 15% and 20% respectively.", "acronyms": [[76, 78]], "long-forms": [[56, 75]], "ID": "920"}, {"text": "particularly helpful in parsing where the sequence  of words forming the MWE is treated as a single  word with a single part of speech (POS) tag. MWE ", "acronyms": [[136, 139], [73, 76], [146, 149]], "long-forms": [[120, 134]], "ID": "921"}, {"text": " 1 Introduction Machine translation (MT) systems have different strengths and weaknesses which can be exploited", "acronyms": [[37, 39]], "long-forms": [[16, 35]], "ID": "922"}, {"text": "To tackle this challenge, we incorporate multiple graphs probabilistic factorization with two alternatively designed combination strategies into collaborative topic regression (CTR). Experimental results on real dataset demonstrate", "acronyms": [[177, 180]], "long-forms": [[145, 175]], "ID": "923"}, {"text": "operating system.  For production deployment we  used Message Driven Beans (MDBs)using IBM  Websphere Application Server? (", "acronyms": [[76, 80], [87, 90]], "long-forms": [[54, 74]], "ID": "924"}, {"text": "But, it is observed that the identification of  lexical scopes of compound verbs (CompVs)  and conjunct verbs (ConjVs) from long sequence of successive Complex Predicates ", "acronyms": [[111, 117], [82, 88]], "long-forms": [[95, 109], [66, 80]], "ID": "925"}, {"text": "lowing rules are used in the detailed example.  if tense of el and of e2 is simple past (SP)  with perfective AP tben there is justification for ", "acronyms": [[89, 91], [110, 112]], "long-forms": [[76, 87]], "ID": "926"}, {"text": "3  Chinese NER Using CRFs Model Integrating Multiple Features  Besides the text feature(TXT), simplified part-ofspeech (POS) feature, and small-vocabulary-", "acronyms": [[88, 91], [11, 14], [21, 25], [120, 123]], "long-forms": [[75, 87], [105, 118]], "ID": "927"}, {"text": "In a second step, unmatched words are converted into stems or synonyms and then matched. A method that uses the concept of maximum matching string (MMS) is presented by Turian, Shen, and Melamed (2003).", "acronyms": [[148, 151]], "long-forms": [[123, 146]], "ID": "928"}, {"text": "LMs by perplexity (PPL). We use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB). ", "acronyms": [[88, 91], [0, 3], [19, 22], [57, 60]], "long-forms": [[73, 86], [7, 17], [36, 55]], "ID": "929"}, {"text": "We evaluate the following two search algorithms: ? beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000)", "acronyms": [[74, 76]], "long-forms": [[51, 62]], "ID": "930"}, {"text": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING?10), pages 617? ", "acronyms": [[82, 91]], "long-forms": [[55, 80]], "ID": "931"}, {"text": "Translation retrieval (TR) is a description of this process of selecting from the TM a set of translation records (TRecs) of maximum L1 similarity to a given input.", "acronyms": [[115, 120], [23, 25], [82, 84]], "long-forms": [[94, 113], [0, 21]], "ID": "932"}, {"text": " 2 Abstract Syntax Trees We describe abstract syntax trees (ASTs) using an example from CFR Section 610.11:", "acronyms": [[60, 64], [88, 91]], "long-forms": [[37, 58]], "ID": "933"}, {"text": " 1.1 Language Modeling Formally, a language model (LM) is a probability distribution over strings of a language:", "acronyms": [[51, 53]], "long-forms": [[35, 49]], "ID": "934"}, {"text": "   First Paragraph (FPAR): We examined several  hundred pages, and observed that a human could ", "acronyms": [[20, 24]], "long-forms": [[3, 18]], "ID": "935"}, {"text": "We used 1300 texts (DEV) as our training set, 200 texts (TST1+TST2) for tuning, and 200 texts (TST3+TST4) as a test set. All 1700 docu-", "acronyms": [[95, 104]], "long-forms": [[88, 93]], "ID": "936"}, {"text": "tion task which involves acquiring patterns in the distribution of opinion-bearing words and targets using machine learning (ML) techniques. In partic-", "acronyms": [[125, 127]], "long-forms": [[107, 123]], "ID": "937"}, {"text": "Given these restrictions the DAR problem becomes to find that value da of DA that maximises P ( DA = da | f1 = v1, . . . , fn = vn,", "acronyms": [[96, 98], [29, 32], [74, 76]], "long-forms": [[101, 103]], "ID": "938"}, {"text": " 1 Introduction Question Answering (QA) from structured data, such as DBPedia (Auer et al.,", "acronyms": [[36, 38], [70, 77]], "long-forms": [[16, 34]], "ID": "939"}, {"text": "They found that using a combination of all the features in a Support Vector Machine (SVM), they can obtain an accuracy of 80% in the classification of 5 differ-", "acronyms": [[85, 88]], "long-forms": [[61, 83]], "ID": "940"}, {"text": " One application area of increasing interest is  information extraction (IE) (see, e.g., Cowie and  Lehnert (1996)).", "acronyms": [[73, 75]], "long-forms": [[49, 71]], "ID": "941"}, {"text": " ? Reduce(RE): Pop the stack. ", "acronyms": [[10, 12]], "long-forms": [[3, 9]], "ID": "942"}, {"text": "PP Fu?r diese Behauptung has the grammatical function OAMOD, which indicates that it is a modifier (MOD) of a direct object (OA) elsewhere in the structure (in this case keinen", "acronyms": [[100, 103], [125, 127], [0, 2], [54, 59]], "long-forms": [[90, 98], [105, 109]], "ID": "943"}, {"text": "Pierre PN Note that a preposition (PR) with lemma de and a determiner (DT) with lemma le and the same gender and number as the common noun have been", "acronyms": [[71, 73], [7, 9], [35, 37]], "long-forms": [[59, 69], [22, 33]], "ID": "944"}, {"text": "AT&T Labs-Research Abstract Statistical Machine Translation (SMT) systems are heavily dependent on the qual-", "acronyms": [[61, 64], [0, 4]], "long-forms": [[28, 59]], "ID": "945"}, {"text": "template includes, for each sentence, syntactic Infor-  mation that Is represented in a tree whose nodes are  syntacti~ categories such as S (sentence), CL (clause),  SUBJECT or VERB.", "acronyms": [[153, 155]], "long-forms": [[157, 163], [142, 150]], "ID": "946"}, {"text": "Ph i lade lph ia ,  PA  191C4  .ABSTRACT  Tree Adjoining Grammar (TAG) is a formalism for natural  language grammars.", "acronyms": [[66, 69], [20, 22]], "long-forms": [[42, 64], [0, 16]], "ID": "947"}, {"text": "LL; it is calculated from contingency table information as follows: LO = log (O11 + 0.5)(O22 + 0.5)", "acronyms": [[68, 70]], "long-forms": [[73, 76]], "ID": "948"}, {"text": "Hyderabad, India Abstract Named Entity Recognition (NER) is the task of identifying and classifying all proper", "acronyms": [[52, 55]], "long-forms": [[26, 50]], "ID": "949"}, {"text": " ADEPT tags documents in a uniform fashion, using  Standard Generalized Markup (SGML) according to  OIR standards.", "acronyms": [[80, 84], [1, 6], [100, 103]], "long-forms": [[51, 78]], "ID": "950"}, {"text": "with one object sentences (IMP_VNP), V_NP_PP  agentless passive sentences (PAS_VNPP), V_NP bypassives (BYPAS_VN), and N_PP clauses (NPP) and  these are all decisions that happen in the realiser, ", "acronyms": [[132, 135], [27, 34], [37, 44], [75, 83], [103, 111]], "long-forms": [[118, 122], [86, 101], [46, 73]], "ID": "951"}, {"text": "of spoken dialogue systems is database retrieval.  Some IVR (interactive voice response) systems using the speech recognition technology are being put", "acronyms": [[56, 59]], "long-forms": [[61, 87]], "ID": "952"}, {"text": "the left hand side (LHS) of the rule, and ? the right hand side (RHS) of the rule.", "acronyms": [[65, 68], [20, 23]], "long-forms": [[48, 63], [4, 18]], "ID": "953"}, {"text": "provement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent", "acronyms": [[80, 86]], "long-forms": [[64, 78]], "ID": "954"}, {"text": "  ME Classification  ME (Maximum Entropy) classification is used here  to directly estimate the posterior probability for ", "acronyms": [[21, 23], [2, 4]], "long-forms": [[25, 40]], "ID": "955"}, {"text": "mentary and United Nations parallel corpora. The semantic phrase table (SPT) was extracted from the same corpora annotated with FreeLing (Carreras et", "acronyms": [[72, 75]], "long-forms": [[49, 70]], "ID": "956"}, {"text": "In particular, we use transition probability and emission probability in Hidden Markov Model (HMM) (Leek, 1997) to capture this dependency.", "acronyms": [[94, 97]], "long-forms": [[73, 92]], "ID": "957"}, {"text": " What we describe is part of a theory of language (knowledge and processing)  called Word Grammar (WG) (Hudson 1984; 1990). Section 2 introduces the knowledge ", "acronyms": [[99, 101]], "long-forms": [[85, 97]], "ID": "958"}, {"text": "er positions less than the penalty of reordering the first ranks.  iii):  Weighted Normalized Discounted Cumulative Gain (WNDCG): NDCG (Moffat and Zobel,  2008) normally compares the rankings of two lists.", "acronyms": [[122, 127], [130, 134]], "long-forms": [[74, 120]], "ID": "959"}, {"text": "consider in (14) five salient phases of the passage: at the beginning of the process, the parts of river are localized to the exterior EXT(LOC), then to the boundary FRO(LOC), they arrive in", "acronyms": [[139, 142], [135, 138], [166, 169], [170, 173]], "long-forms": [[109, 118]], "ID": "960"}, {"text": "and normalizing it with a view to discriminate the importance of words across documents and then approximating it using singular value decomposition(SVD) in R dimensions (Bellegarda, 2000).", "acronyms": [[149, 152]], "long-forms": [[120, 147]], "ID": "961"}, {"text": "2.85 0.001 8 29. ( VP (VBP ? NEED?) (", "acronyms": [[19, 21]], "long-forms": [[23, 26]], "ID": "962"}, {"text": "To estimate the weights ? i in formula (1), we  use Minimum Error Rate Training (MERT) algorithm, which is widely used for phrase-based ", "acronyms": [[81, 85]], "long-forms": [[52, 79]], "ID": "963"}, {"text": "al., 2013), containing essays written by students at the National University of Singapore (NUS) which have been manually corrected by English instruc-", "acronyms": [[91, 94]], "long-forms": [[57, 89]], "ID": "964"}, {"text": "3.4 The  NATO Research  Study  Group  on  Speech  Process ing   The North Atlantic Treaty Organization (NATO) Re-  search Study Group on Speech Processing (RSG10) \\[87\\], ", "acronyms": [[104, 108], [9, 13], [156, 161]], "long-forms": [[68, 102], [110, 154]], "ID": "965"}, {"text": "Abstract One of the most neglected areas of biomedical Text Mining (TM) is the development of systems based on carefully assessed user", "acronyms": [[68, 70]], "long-forms": [[55, 66]], "ID": "966"}, {"text": "question answering (QA). However, few models in ad hoc information retrieval (IR) use paths for document ranking due to", "acronyms": [[78, 80], [20, 22]], "long-forms": [[55, 76], [0, 18]], "ID": "967"}, {"text": " ? Adjuncts (AM-): General arguments that any verb may take optionally.", "acronyms": [[13, 16]], "long-forms": [[3, 11]], "ID": "968"}, {"text": "ing is how to identify a temporal relation between a pair of temporal entities such as events (EVENT) and time expressions (TIMEX) in a narrative. Af-", "acronyms": [[124, 129]], "long-forms": [[106, 122]], "ID": "969"}, {"text": " The obtained Japanese scores as compared to the  scores from the initial English experiment (E-E-E)  are shown in Figure 6.", "acronyms": [[94, 99]], "long-forms": [[74, 92]], "ID": "970"}, {"text": "word if it exceeds a certain threshold.  4.2 LDA Graph Method (LDA-GM) The LDA-GM algorithm creates a similarity graph", "acronyms": [[63, 69], [75, 81]], "long-forms": [[45, 61]], "ID": "971"}, {"text": "English using the frequency dictionary of Arabic (Buckwalter and Parkinson, 2011) and the Corpus of Contemporary American English (COCA) top 5,000 words (Davies, 2010).", "acronyms": [[131, 135]], "long-forms": [[100, 121]], "ID": "972"}, {"text": " 1 Introduction Grammatical Framework (GF) (Ranta, 2004) is a grammar formalism designed in particular to serve", "acronyms": [[39, 41]], "long-forms": [[16, 37]], "ID": "973"}, {"text": "Sentence) and McDonald (2006) on a sentence-by-sentence basis. Table 1 shows the compression rates (CompR) for the three systems and evaluates the quality of their output using grammatical relations F1.", "acronyms": [[100, 105]], "long-forms": [[81, 98]], "ID": "974"}, {"text": " A statistical classification technique based  on the use of Hidden Markov Models (HMM) was  used as a language discriminator.", "acronyms": [[83, 86]], "long-forms": [[61, 81]], "ID": "975"}, {"text": "and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used suc-", "acronyms": [[80, 84]], "long-forms": [[58, 78]], "ID": "976"}, {"text": "In this section, we define pomsets as a model for describing concurrency. A labelled partial order (LPO) is a 4 tuple (V, ?,", "acronyms": [[100, 103]], "long-forms": [[76, 98]], "ID": "977"}, {"text": "Since the bilingual corpus is only aligned at the document level, we performed sentence alignment using the Champollion Tool Kit (CTK).4 After removing sentences with no aligned sentence, a total", "acronyms": [[130, 133]], "long-forms": [[108, 128]], "ID": "978"}, {"text": "toolkit that contains a suit of modules for generic tasks in Natural Language Processing (NLP), Information Retrieval (IR), and Network Analysis (NA). ", "acronyms": [[146, 148]], "long-forms": [[128, 144]], "ID": "979"}, {"text": "based chunking; 3. MEMM-based word segmenter with Support Vector Machines (SVM)-based chunking.", "acronyms": [[75, 78], [19, 23]], "long-forms": [[50, 73]], "ID": "980"}, {"text": "2004). Accordingly, we define in-NE probability  to help delete and create named entities (NE). ", "acronyms": [[91, 93], [30, 35]], "long-forms": [[75, 89]], "ID": "981"}, {"text": "interpreters. In International Conference on Learning Representations (ICLR). ", "acronyms": [[71, 75]], "long-forms": [[17, 69]], "ID": "982"}, {"text": "Just as in Figure 2, the initial model is denoted with a bold symbol in the left part of the plot. Also for reference the relevant Lexicon 1 accuracy (LEX 1) is denoted with a ? at the far right.", "acronyms": [[151, 156]], "long-forms": [[131, 140]], "ID": "983"}, {"text": " (e.g., hand, heart, blood, DNA) Diseases and Symptoms (DisSym): Diseases and symptoms.", "acronyms": [[56, 62], [28, 31]], "long-forms": [[33, 54]], "ID": "984"}, {"text": "ning of ORG (B-O). Many of them are mislabeled as O and beginning of location (B-L), resulting low recall and low precision for ORG.", "acronyms": [[79, 82], [8, 11], [13, 16], [128, 131]], "long-forms": [[56, 77]], "ID": "985"}, {"text": "  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87?95, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[71, 76], [80, 84]], "long-forms": [[37, 69]], "ID": "986"}, {"text": "roles they typically enter: ACT (Actor), PAT (Patient), ADDR (Addressee), ORIG (Origin) and EFF (Effect). Syntactic criteria are used to identify", "acronyms": [[92, 95], [28, 31], [41, 44], [56, 60], [74, 78]], "long-forms": [[97, 103], [33, 38], [46, 53], [62, 71], [80, 86]], "ID": "987"}, {"text": "other models, such as vector space model (VSM), Okapi model (Robertson et al, 1994) or language model (LM). The pipeline meth-", "acronyms": [[103, 105], [42, 45]], "long-forms": [[87, 101], [22, 40]], "ID": "988"}, {"text": "the identified reading level. Text plans define  rules on Noun Phrase (NP) density and lexical  choice.", "acronyms": [[71, 73]], "long-forms": [[58, 69]], "ID": "989"}, {"text": "Not Available?.  OmegaWiki (OW) is a freely editable online dictionary like WKT.", "acronyms": [[28, 30], [76, 79]], "long-forms": [[17, 26]], "ID": "990"}, {"text": "We also include a simple baseline that selects the first document sentence as a caption and show the average caption length (AvgLen) for each model.", "acronyms": [[125, 131]], "long-forms": [[101, 123]], "ID": "991"}, {"text": "For instance, a problem-tagged entity is represented as a first word tagged B-P (begin problem) and other 59", "acronyms": [[76, 79]], "long-forms": [[81, 94]], "ID": "992"}, {"text": "All experiments carried out in this study are for the English (EN) - French (FR) language pair. ", "acronyms": [[77, 79], [63, 65]], "long-forms": [[69, 75], [54, 61]], "ID": "993"}, {"text": "pirical study for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).", "acronyms": [[123, 125], [155, 159]], "long-forms": [[111, 121], [131, 154]], "ID": "994"}, {"text": "James Clark. 1999. XSL transformations (XSLT). W3C Recommendation, 16 November.", "acronyms": [[40, 44]], "long-forms": [[19, 38]], "ID": "995"}, {"text": "class. Among these are: L (Larsen, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al, 2002), H (Meila, 2001), clustering F-measure", "acronyms": [[89, 91]], "long-forms": [[64, 87], [27, 33]], "ID": "996"}, {"text": " UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston and Burnard, 1998).", "acronyms": [[44, 46], [58, 60], [1, 3], [13, 15], [28, 30]], "long-forms": [[49, 56], [63, 79], [6, 11], [18, 26], [33, 42]], "ID": "997"}, {"text": " 1 Introduction Mental State Verbs (MSVs), such as think, know, and want, are very frequent in child-directed lan-", "acronyms": [[36, 40]], "long-forms": [[16, 34]], "ID": "998"}, {"text": "Acknowledgment This work is supported by the 6th Framework Research Program of the European Union (EU), LUNA Project, IST contract no 33549,www.ist-luna.eu", "acronyms": [[99, 101], [104, 108], [118, 121]], "long-forms": [[83, 97]], "ID": "999"}, {"text": "2010. The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results.", "acronyms": [[55, 62]], "long-forms": [[17, 53]], "ID": "1000"}, {"text": "This examination can be held by plotting values of recall, precision and F-measure during each step  of merging process. Figure 5 shows the fluctuation of positive recall(PR), positive preclsion(AP),  averaged recall(AR), averaged precision and F-measure (FM).", "acronyms": [[171, 173]], "long-forms": [[155, 169]], "ID": "1001"}, {"text": "(6) health issues (HI) trend other (7) personal issues (PI) trend decreasing (8) lectures attended (LA) trend other (9) revision (R) trend decreasing", "acronyms": [[100, 102], [19, 21], [56, 58]], "long-forms": [[81, 98], [4, 16], [39, 53], [120, 128]], "ID": "1002"}, {"text": "X at Y (verb phr:~se, noun phrase)  X a.m. (corot?rand word)  After step (I) is finished, steps (II)-(IV) are repeated  recursively.", "acronyms": [[97, 99]], "long-forms": [[77, 88]], "ID": "1003"}, {"text": "some freedom in the ordering of major phrasal categories like  NPs  and adverbial phrases - for example, in the linear order of  subject (SUB J), direct object (DOBJ), and indirect object (lOB J)  with respect o one another.", "acronyms": [[138, 143], [63, 66], [161, 165], [189, 195]], "long-forms": [[129, 136], [146, 159], [172, 187]], "ID": "1004"}, {"text": "1979). Most tools and resources developed for natural language processing (NLP) of Arabic are designed for MSA.", "acronyms": [[75, 78], [107, 110]], "long-forms": [[46, 73]], "ID": "1005"}, {"text": "means of |P (G)|. This guarantees that neither trees of maximum height (MHT) nor of maximum degree (MDT), i.e. trees which trivially", "acronyms": [[72, 75], [100, 103]], "long-forms": [[56, 70], [84, 98]], "ID": "1006"}, {"text": " Single-tokenization of compound verbs  and named entities (NE) provides significant gains over the baseline PB-SMT ", "acronyms": [[60, 62], [109, 115]], "long-forms": [[44, 58]], "ID": "1007"}, {"text": " 1 Introduction Information Extraction (IE) is a natural language processing task in which text documents are ana-", "acronyms": [[40, 42]], "long-forms": [[16, 38]], "ID": "1008"}, {"text": "call this set of features Feat-IV.  Table 3 demonstrates the word error rate(WER) improvement enabled by our binary subsampling", "acronyms": [[77, 80]], "long-forms": [[61, 75]], "ID": "1009"}, {"text": "tistical machine translation. In Proceedings of the Machine Translation Summit (MT-Summit). ", "acronyms": [[80, 89]], "long-forms": [[52, 78]], "ID": "1010"}, {"text": "Table 5: Sample Hindi complex predicates   5 Corpus and pre-processing  Basic Travel Expressions Corpus (BTEC)  containing travel conversations is used for ", "acronyms": [[105, 109]], "long-forms": [[72, 103]], "ID": "1011"}, {"text": "Finally, it is clear from Figure 6 that certain relations are particularly difficult for both parsers. For example, indirect object (IObj) dependents are low scoring nodes: this is because they are often attached to the correct head but are", "acronyms": [[133, 137]], "long-forms": [[116, 131]], "ID": "1012"}, {"text": "we have established direct contact with the south korean delegation through tribal elders .  Figure 2: Random sample of 5 items from study in Section 4: original Google translation (GT), results of targeted paraphrasing translation process (TP), and a human reference translation.", "acronyms": [[182, 184], [241, 243]], "long-forms": [[162, 180], [220, 239]], "ID": "1013"}, {"text": " 2 Changes to the AZ Scheme Argumentative Zoning II (AZ-II) is a new annotation scheme, which is an elaboration of the orig-", "acronyms": [[53, 58], [18, 20]], "long-forms": [[28, 51]], "ID": "1014"}, {"text": "its title  read{ARG0, ARG1}  Figure 2: A sentence parse tree with two predicative tree structures (PAST s) which is equal 1 if the target fi is rooted at node n", "acronyms": [[99, 105], [16, 20], [22, 26]], "long-forms": [[70, 97]], "ID": "1015"}, {"text": "We train a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel  corpus.", "acronyms": [[83, 89], [40, 46], [27, 31]], "long-forms": [[63, 81], [32, 38]], "ID": "1016"}, {"text": "Narrative Summarization. Journal Traitement automatique des langues (TAL): Special  issue  on Context:  Automatic Text  Summariza-", "acronyms": [[69, 72]], "long-forms": [[33, 67]], "ID": "1017"}, {"text": "This article describes the collaborative work on applying  the newly proposed ISO standard for dialogue act  annotation to the Switchboard Dialogue Act (SWBD-DA)  Corpus, as part of our on-going effort to promote ", "acronyms": [[153, 160], [78, 81]], "long-forms": [[127, 151]], "ID": "1018"}, {"text": "Note: in genera\\], the resultant segments,  such as Declarative Sentence(SDEC),  Noun Phrase(NP), Inf init ive  Phrase(INF), and Verb Phrase(VP), ", "acronyms": [[93, 95], [73, 77], [119, 122], [141, 143]], "long-forms": [[81, 91], [52, 72], [98, 110], [129, 140]], "ID": "1019"}, {"text": "Reject? validations for reliably deliberate (Rel) and unreliable (URel) subsets of the metaphor production data, given that the", "acronyms": [[66, 70], [45, 48]], "long-forms": [[54, 64], [24, 32]], "ID": "1020"}, {"text": " In integrating this approach into a dialog system, we see that the dialog manager (DM) no longer determines surface strings to send to the TTS system, as is often the case in current dialog systems.", "acronyms": [[84, 86], [140, 143]], "long-forms": [[68, 82]], "ID": "1021"}, {"text": "document is zero.  Agglomerative Hierarchical Clustering (AHC)  AHC is a bottom-up hierarchical clustering ", "acronyms": [[58, 61], [64, 67]], "long-forms": [[19, 56]], "ID": "1022"}, {"text": "Since we are going to be  concerned with definability, we first translate CFGs  into CFTs (Context Free Theories). The ", "acronyms": [[85, 89], [74, 78]], "long-forms": [[91, 112]], "ID": "1023"}, {"text": "described as operating within the same three-stage framework as STRAND.  Parallel Text Miner (PTMiner) (Chen and Nie 2000) exploits already-existing Web search engines to locate pages by querying for pages in a given language that contain", "acronyms": [[94, 101], [64, 70]], "long-forms": [[73, 92]], "ID": "1024"}, {"text": "proaches, we also report on supplying them with different reordering rule sets: a set that was learned on manually aligned data (MAN), and a set learned on the same data but with automatic", "acronyms": [[129, 132]], "long-forms": [[106, 114]], "ID": "1025"}, {"text": "VNMT 32.25 34.50++ 33.78++ 36.72?++ 30.92?++ 24.41?++ 32.07 Table 1: BLEU scores on the NIST Chinese-English translation task. AVG = average BLEU scores on test sets. We", "acronyms": [[141, 145], [0, 4], [69, 73], [88, 92], [127, 130]], "long-forms": [[133, 140]], "ID": "1026"}, {"text": "301 3. False Attributes (FA) - these are situations where a value for an attribute has been spec-", "acronyms": [[25, 27]], "long-forms": [[7, 23]], "ID": "1027"}, {"text": " Performance has been measured with both the question followed by an extension (Q+E), as well as the question followed by the target and then", "acronyms": [[80, 83]], "long-forms": [[45, 78]], "ID": "1028"}, {"text": " As the noun or adjective occur in the first slot  of conjunct verbs (ConjVs) construction, the  search starts from the point of noun or adjec-", "acronyms": [[70, 76]], "long-forms": [[54, 68]], "ID": "1029"}, {"text": "Taking into account the above strategies, we propose three concrete DS to CS conversions: Flat conversion with H auxiliary symbol (FlatH). ", "acronyms": [[131, 136], [68, 70], [74, 76]], "long-forms": [[90, 112]], "ID": "1030"}, {"text": "Moreover, in (Hahn et al, 2008a) two more models are applied to SLU: a Maximum Entropy (EM) model and a model coming from the Statistical Machine Translation (SMT) commu-", "acronyms": [[88, 90], [64, 67], [159, 162]], "long-forms": [[79, 86], [126, 157]], "ID": "1031"}, {"text": "5.2.1 Query Focused Rewards We have proposed an extension to both reward functions to allow for query focused (QF) summarization.", "acronyms": [[111, 113]], "long-forms": [[96, 109]], "ID": "1032"}, {"text": "Many different algorithms  have been used for this task, including some  machine learning (ML) algorithms, such as  Na?ve Bayesian model, decision trees, and ", "acronyms": [[91, 93]], "long-forms": [[73, 89]], "ID": "1033"}, {"text": "tence pairs were selected from the WMT Giga corpus if the perplexity of their French part with respect to a language model (LM) trained on French news data was below a given threshold.", "acronyms": [[124, 126], [35, 38]], "long-forms": [[108, 122]], "ID": "1034"}, {"text": "POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11 Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24 Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00 Table 1: Correlations between (mean-centered) predictors.", "acronyms": [[163, 167], [13, 17], [90, 94]], "long-forms": [[150, 161], [0, 11], [83, 88]], "ID": "1035"}, {"text": "As expected, the poor performance observed on the sniper text is mainly due to two reasons: the presence of out of vocabulary (OOV) words and the incorrect translations of terminological", "acronyms": [[127, 130]], "long-forms": [[108, 125]], "ID": "1036"}, {"text": "to pour) means that the entity (wine) is localized to exterior locus (barrel) and crosses the intermediate locus IME(LOC) to be localized to the interior INT(LOC) (the_bottle).", "acronyms": [[117, 120]], "long-forms": [[107, 112]], "ID": "1037"}, {"text": "the set of required domains. Various classification systems were considered, including the Dewey Decimal Classification (DDC) and Universal Decimal Classification (UDC). These schemes, however, are", "acronyms": [[164, 167], [121, 124]], "long-forms": [[130, 162], [91, 119]], "ID": "1038"}, {"text": "these paths. This corresponds to the Viterbi approximation i speech recognition or in  other related areas for which hidden Markov models (HMM's) are used. In all such ", "acronyms": [[139, 144]], "long-forms": [[117, 137]], "ID": "1039"}, {"text": "2.4 Dictionaries of  Bahasa Nusantara, Indonesian Linguistics Association (MLI)   Masyarakat Linguistik Indonesia (MLI) is a group  of institutions, organizations and corporation, ", "acronyms": [[115, 118], [75, 78]], "long-forms": [[82, 113]], "ID": "1040"}, {"text": "The Multi-Perspective Question-Answering (MPQA) newswire corpus (Wilson and Wiebe, 2005) and the J. D. Power & Associates (JDPA) automotive review blog post (Kessler et al, 2010)", "acronyms": [[123, 127], [42, 46]], "long-forms": [[97, 121], [4, 40]], "ID": "1041"}, {"text": "Each word is considered as an  instance. Maximum Entropy (MaxEnt) is used in  this paper.", "acronyms": [[58, 64]], "long-forms": [[41, 56]], "ID": "1042"}, {"text": "gradient method. In The Conference on Advances in Neural Information Processing Systems (NIPS). ", "acronyms": [[89, 93]], "long-forms": [[50, 87]], "ID": "1043"}, {"text": "When using the classifiers to predict the class of a test example, there are four possible outcomes; true positive (TP), true negative (TN), false positive (FP), and false nega-", "acronyms": [[116, 118], [136, 138], [157, 159]], "long-forms": [[101, 114], [121, 134], [141, 155]], "ID": "1044"}, {"text": "nym, Instance Hypernym, Part Holonym, Member Holonym, Substance Meronym, Entailment Table 1: Similarity features using WordNet (WN). ", "acronyms": [[128, 130]], "long-forms": [[119, 126]], "ID": "1045"}, {"text": "former networks for image recognition. Bulletin of the International Statistical Institute (ISI). ", "acronyms": [[92, 95]], "long-forms": [[55, 90]], "ID": "1046"}, {"text": "e-mail: lynet te@goldilocks.lcs.mit.edu  ABSTRACT  The Air Travel Information System (ATIS) domain serves as  the common task for DARPA spoken language system re- ", "acronyms": [[86, 90], [130, 135]], "long-forms": [[55, 84]], "ID": "1047"}, {"text": "Finance and economics (FAE) 100  Education (EDU) 100  Entertainment (ENT) 100  Computer (COM) 100 ", "acronyms": [[69, 72], [23, 26], [44, 47], [89, 92]], "long-forms": [[54, 67], [0, 20], [33, 42], [79, 87]], "ID": "1048"}, {"text": "ferent corpora, Academia Sinica (AS), City  University of Hong Kong (HK), Peking University (PK), and Microsoft Research Asia (MSR),  each of which has its own definition of a word.", "acronyms": [[127, 130], [33, 35], [69, 71], [93, 95]], "long-forms": [[102, 120], [16, 31], [58, 67], [74, 91]], "ID": "1049"}, {"text": "1434  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 82?86, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[75, 80], [84, 88]], "long-forms": [[41, 73]], "ID": "1050"}, {"text": "F is a frame name, E a frame element name, and t and s are sequences of word indices (t is for the target (FEE)) Using this measure of partial agreement, we now", "acronyms": [[107, 110]], "long-forms": [[91, 105], [7, 17], [29, 41]], "ID": "1051"}, {"text": "Note that the autoPS heuristic for ranking senses is a more precise estimator than the WordNet most?frequent?sense (MFS). ", "acronyms": [[116, 119], [14, 20]], "long-forms": [[95, 114]], "ID": "1052"}, {"text": " of the International Joint Conference on Artificial Intelligence (IJCAI). ", "acronyms": [[67, 72]], "long-forms": [[8, 65]], "ID": "1053"}, {"text": "We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU?s People?s Daily (PPD) in our experiments.", "acronyms": [[109, 112], [78, 81], [87, 92]], "long-forms": [[93, 107], [60, 76]], "ID": "1054"}, {"text": "a new father node. The following simple rule forms a  noun phrase (NP). ", "acronyms": [[67, 69]], "long-forms": [[54, 65]], "ID": "1055"}, {"text": " 2 Data Kinyarwanda (KIN) and Malagasy (MLG) are lowresource, KIN is morphologically rich, and English", "acronyms": [[21, 24], [40, 43], [62, 65]], "long-forms": [[8, 19], [30, 38]], "ID": "1056"}, {"text": "only limited discontinuities in each tree.  Generalized Multitext Grammar (GMTG) offers a way to synchronize Mildly Context-Sensitive", "acronyms": [[75, 79]], "long-forms": [[44, 73]], "ID": "1057"}, {"text": "To allow for portability, the SABA parser translates its natural  language input into an ~mtermediate\" s mantic network formalism  called SF (for \"Sentence Formalism'), presented in details in (Binot,  1984, 1985).", "acronyms": [[138, 140], [30, 34]], "long-forms": [[147, 165]], "ID": "1058"}, {"text": "ENTITY_A appos Figure 2: Dependency tree (DT) for the entity blinded sentence ?", "acronyms": [[42, 44]], "long-forms": [[25, 40]], "ID": "1059"}, {"text": " Classifier models. We used a first-order linear chain conditional random fields (CRF) model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as a", "acronyms": [[82, 85], [138, 144]], "long-forms": [[55, 80], [121, 136]], "ID": "1060"}, {"text": " A dialogue act is defined as a pair consisting of a communicative function (CF) and a semantic content (SC): a =< CF,SC >.", "acronyms": [[77, 79], [105, 107], [115, 117], [118, 120]], "long-forms": [[53, 75], [87, 103]], "ID": "1061"}, {"text": "The most  important forms of discourse of interest o  Natural Language Processing (NLP) are text  and dialogue.", "acronyms": [[83, 86]], "long-forms": [[54, 81]], "ID": "1062"}, {"text": "17 classes set in Sun et al(2008).  We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but", "acronyms": [[69, 73]], "long-forms": [[48, 67]], "ID": "1063"}, {"text": " MRS constraints have three kinds of literals, two kinds of elementary predications (EPs) in the first two lines and handle constraints in the third line:", "acronyms": [[85, 88], [1, 4]], "long-forms": [[60, 83]], "ID": "1064"}, {"text": "media, where evidences for both actions and ties are available. We begin by necessary description of preliminaries and notations, we then present the mutual latent random graphs (MLRGs) model, upon which both sources of evidence could be exploited simultaneously to capture their mutual influence.", "acronyms": [[179, 184]], "long-forms": [[150, 177]], "ID": "1065"}, {"text": "= Task Defined RAW SCORI~  ((! OMM(~OST x number of messages)  - (INFCOST x nnmber  o f  in fe rences)  ", "acronyms": [[31, 34]], "long-forms": [[36, 60]], "ID": "1066"}, {"text": "  ? suffixes (SUF), such as verb endings, nominal  cases, the nominal feminine ending -at, etc.;", "acronyms": [[14, 17]], "long-forms": [[4, 12]], "ID": "1067"}, {"text": "tors of a lower-dimensional space. LSI, which is based on Singular Value Decomposition (SVD) of matrices, has showed to have the ability to ex-", "acronyms": [[88, 91], [35, 38]], "long-forms": [[58, 86]], "ID": "1068"}, {"text": "Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent", "acronyms": [[109, 112], [65, 68]], "long-forms": [[88, 107]], "ID": "1069"}, {"text": "ing instance is created as during training, and then presented to the decision tree, which returns a confidence value (CF)2 indicating the likelihood that NPi is coreferential to NPj .", "acronyms": [[119, 121], [155, 158], [179, 182]], "long-forms": [[101, 111]], "ID": "1070"}, {"text": "otherwise be very limited annotated data. The resource, the Online Database of INterlinear text (ODIN), makes this data available and provides additional annotation", "acronyms": [[97, 101]], "long-forms": [[60, 90]], "ID": "1071"}, {"text": "this corpus is labelled with a set of seven classes: beneficial (BNF), direction (DIR), extent (EXT), location (LOC), manner (MNR), purpose (PRP), and temporal (TMP).", "acronyms": [[112, 115], [141, 144], [65, 68], [82, 85], [96, 99], [126, 129], [161, 164]], "long-forms": [[102, 110], [132, 139], [53, 63], [71, 80], [88, 94], [118, 124], [151, 159]], "ID": "1072"}, {"text": "sider this sentence is correctly tagged.  Test data set 1 (TDS 1): contains about 10%  of the sentences from the complete emotion-", "acronyms": [[59, 64]], "long-forms": [[42, 57]], "ID": "1073"}, {"text": "Syntactic features from SLA research (SLASYN) ? Mean length of clause (MLC) ?", "acronyms": [[71, 74], [24, 27], [38, 44]], "long-forms": [[48, 69]], "ID": "1074"}, {"text": "factoid ones - as well as new elements ? such as  expected polarity type (EPT). However, opi-", "acronyms": [[74, 77]], "long-forms": [[50, 72]], "ID": "1075"}, {"text": " 2.5 Noise Contrastive Estimation Noise contrastive estimation (NCE) is another sampling-based technique (Hyv?arinen, 2010;", "acronyms": [[64, 67]], "long-forms": [[34, 62]], "ID": "1076"}, {"text": "tions for this sentence.  Figure 5: The LF (left) and MRS (right) representations for the sentence ?", "acronyms": [[40, 42], [54, 57]], "long-forms": [[44, 48]], "ID": "1077"}, {"text": "cessing information from a structured database ? a natural language interface to a database (NLIDB) (Kapetanios et al, 2010).", "acronyms": [[93, 98]], "long-forms": [[51, 91]], "ID": "1078"}, {"text": "in cooperation between HU Berlin, U Frankfurt and U Jena, conducted in the wider context of the Deutsch Diachron Digital (DDD) initiative. The", "acronyms": [[122, 125], [23, 25]], "long-forms": [[96, 120]], "ID": "1079"}, {"text": "These interfaces stand to play a critical role in the  ongoing migration of interaction fi'oln the desktop  to wireless portable computing devices (PI)As, next-  generation phones) that offer limited screen real es- ", "acronyms": [[148, 150]], "long-forms": [[120, 146]], "ID": "1080"}, {"text": "As noted by Melamed (2000), the problem of finding the best set of links is the maximum-weighted bipartite matching (MWBM) problem: Given a weighted bipartite graph G = (V1 ? V2, E) with", "acronyms": [[117, 121]], "long-forms": [[107, 115], [159, 164]], "ID": "1081"}, {"text": "He et al (2010) measure the similarity between hypothesis and reference translation in terms of the Lexical Functional Grammar (LFG) representation.", "acronyms": [[128, 131]], "long-forms": [[100, 126]], "ID": "1082"}, {"text": "Tutor's Priming Ratio aggregated by task set (TS = Task Set)     Figure 3. Student's Priming Ratio aggregated by task set (TS = Task Set)  Several significant relationships emerged within the models. We discuss a subset of these here.", "acronyms": [[123, 125], [46, 48]], "long-forms": [[128, 136], [51, 59]], "ID": "1083"}, {"text": " 3.1 Classification Our evaluation was performed using the Maximum Entropy (MaxEnt) and Support Vector Machine (SVM) classifiers.", "acronyms": [[76, 82], [112, 115]], "long-forms": [[59, 74], [88, 110]], "ID": "1084"}, {"text": " Symptom name recognition rate (RRdet),  recognition error rate (RERdet) and recognition  F-Measure (RFMdet): these metrics are designed ", "acronyms": [[65, 71], [32, 37], [101, 107]], "long-forms": [[41, 63], [14, 30], [77, 99]], "ID": "1085"}, {"text": " Dictionary-based methods rely on some dictionary or lexical knowledge base (LKB) such as WordNet (Fellbaum and Miller, 1998) that con-", "acronyms": [[77, 80]], "long-forms": [[53, 75]], "ID": "1086"}, {"text": " The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the French Treebank (FTB) (Abeill?, Cl?ment, and Kinyon", "acronyms": [[118, 121], [67, 70]], "long-forms": [[101, 116], [45, 65]], "ID": "1087"}, {"text": "between interdependent ? E steps (as might arise for an  assumption such as ((ANB)?C)). It is straightforward ", "acronyms": [[78, 84]], "long-forms": [[57, 72]], "ID": "1088"}, {"text": "Table 3: Example of retrieved Wikipedia pages from the four different methods tested in this paper.  Results of diverse merging (DivM) appear to cover more topics relevant to the conversation fragment than other methods.", "acronyms": [[129, 133]], "long-forms": [[112, 127]], "ID": "1089"}, {"text": "line models: ? Character Segmenter (CS): this model simply divides Chinese sentences into sequences", "acronyms": [[36, 38]], "long-forms": [[15, 34]], "ID": "1090"}, {"text": "parser on merged development PTB/PRBK data (section 24). Legend of models: ST=Split Tags; EC=enhanced connectivity.", "acronyms": [[75, 77], [29, 37], [90, 92]], "long-forms": [[78, 88], [93, 114]], "ID": "1091"}, {"text": "Participants in this study included 39 children with typical development (TD) and 21 children with autism spectrum disorder (ASD). ASD was di-", "acronyms": [[125, 128], [74, 76], [131, 134]], "long-forms": [[99, 123], [53, 72]], "ID": "1092"}, {"text": "5.5.3 Corpus Statistics. For training of the question detection module, we used the manually annotated set of about 200,000 SWITCHBOARD speech acts14 (SAs);15 for training of the answer detection component, we used the eight English CALLHOME dia-", "acronyms": [[151, 154]], "long-forms": [[136, 149]], "ID": "1093"}, {"text": " IV. RETROSPECTIVE SSL (R-SSL). After", "acronyms": [[24, 29]], "long-forms": [[5, 22]], "ID": "1094"}, {"text": "October-2001 w/out 2000 SPA 30.88 \u0000 95.68 \u0000 0.08 \u0000 Table 2: Results for Identifying Speech-Act DATE tags in the October-2001 Communicator Corpus, (Dim = Dimension of Date used for output classification (SPA = Speech Act, Maj. Cl.", "acronyms": [[203, 206], [24, 27], [95, 99], [147, 150], [226, 228]], "long-forms": [[209, 219], [153, 162]], "ID": "1095"}, {"text": "We also used two Korean speech recognizers: a speech recognizer made by LG-Elite (LG Electronics Institute of Technology) and a Korean commercial speech recog-", "acronyms": [[72, 80]], "long-forms": [[82, 106]], "ID": "1096"}, {"text": " 2001. Linguistic Inquiry and Word Count (LIWC):  LIWC2001.", "acronyms": [[42, 46]], "long-forms": [[7, 40]], "ID": "1097"}, {"text": "ory (Mann and Thompson, 1988), or RST, represents text by labeled hierarchical structures called Discourse Trees (DTs), which can incorporate several layers of other linguistic information, e.g.,", "acronyms": [[114, 117], [34, 37]], "long-forms": [[97, 112]], "ID": "1098"}, {"text": "the Internet (Colbath, 2012). Social media poses three major computational challenges, dubbed by Gartner the 3Vs of big data: volume, velocity, and variety1. Natural Language Processing (NLP) methods, in particular, face further difficulties arising from the short, noisy, and strongly contextualised nature of social media. In order to address the 3Vs of social media, new language technologies have emerged, such as the identification and definition of users' language varieties and the translation to a different language, than the source.", "acronyms": [[187, 190]], "long-forms": [[158, 185]], "ID": "1099"}, {"text": "the feature structures which are associated with each node, which prohibit certain compositions, are not shown. Note also that this is not a lexicalised TAG (LTAG). This is somewhat unusual; we intend, as", "acronyms": [[158, 162]], "long-forms": [[141, 156]], "ID": "1100"}, {"text": "would also be possible.  Personalized PageRank similarity (PPR) (Agirre and Soroa, 2009) measures the semantic relatedness between two word senses s", "acronyms": [[59, 62]], "long-forms": [[25, 57]], "ID": "1101"}, {"text": " 5 Clustering Methods Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew", "acronyms": [[43, 47]], "long-forms": [[22, 41]], "ID": "1102"}, {"text": "  TGTM P=p,pk ,b   TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l  ", "acronyms": [[24, 26], [2, 6], [19, 23], [46, 50], [51, 53]], "long-forms": [[27, 29], [55, 58]], "ID": "1103"}, {"text": "S# for the count in non-speculative ones)  3 Methods  Conditional random fields (CRF) model was  firstly introduced by Lafferty et al (2001).", "acronyms": [[81, 84]], "long-forms": [[54, 79]], "ID": "1104"}, {"text": " (5) Rank Value:  i. Top Rank (T-Rank): The rank of snippet  that first contains the candidate.", "acronyms": [[31, 37]], "long-forms": [[21, 29]], "ID": "1105"}, {"text": "TH + DR + EG + LC 56.51 TH + EG + LC 56.50 Character Type (CT) 51.96 Word Familiarity (WF) 51.50", "acronyms": [[59, 61], [0, 2], [5, 7], [10, 12], [15, 17], [24, 26], [29, 31], [34, 36], [87, 89]], "long-forms": [[43, 57], [69, 85]], "ID": "1106"}, {"text": " 5 Conclusions The spoken language understanding (SLU) system discussed in this paper is entirely statistically based.", "acronyms": [[50, 53]], "long-forms": [[19, 48]], "ID": "1107"}, {"text": "Langman dictionary.  Maximum Entropy (MaxEnt) principle has been successfully applied in many classification and tagging tasks (Rat-", "acronyms": [[38, 44]], "long-forms": [[21, 36]], "ID": "1108"}, {"text": "generation of textual descriptions from visual data, robot navigation tasks, giving directional instructions, and geographical information systems (GIS). ", "acronyms": [[148, 151]], "long-forms": [[114, 146]], "ID": "1109"}, {"text": " 3.2 Formal basis: Lexical Resource Semantics Lexical Resource Semantics (LRS) (Richter and Sailer, 2003) is an underspecified semantic formal-", "acronyms": [[74, 77]], "long-forms": [[46, 72]], "ID": "1110"}, {"text": "Intersection (I), Union (U), (Koehn et al, 2003) Grow Diagonal Final (GDF), (Och and Ney, 2003) H refined heuristics and Power Mean (PMn) alignment sets where n = 5.", "acronyms": [[133, 136], [70, 73]], "long-forms": [[121, 131], [0, 12], [18, 23], [49, 68]], "ID": "1111"}, {"text": "{wangruibo,gaoyahui}@sxu.edu.cn Abstract In this paper, semantic role labeling(SRL) on Chinese FrameNet is divided into the", "acronyms": [[79, 82]], "long-forms": [[56, 78]], "ID": "1112"}, {"text": "at a supermarket 2 1 1 0 able unable 1 0 0 1 Table 1: Word-based Levenshtein distance (LD) feature and separated edit operations (D = deletions, I", "acronyms": [[87, 89]], "long-forms": [[65, 85]], "ID": "1113"}, {"text": "We use four groups of datasets. The first group comes from the English Web Treebank (EWT),4 also used in the Parsing the Web shared task", "acronyms": [[85, 88]], "long-forms": [[63, 83]], "ID": "1114"}, {"text": "Abstract  Most machine transliteration systems  transliterate out of vocabulary (OOV)  words through intermediate phonemic ", "acronyms": [[81, 84]], "long-forms": [[62, 79]], "ID": "1115"}, {"text": "We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems.", "acronyms": [[131, 134]], "long-forms": [[98, 129]], "ID": "1116"}, {"text": "leaves=29). The first branching, which corresponds to no AP (Absolute Position) and no C (Colour), assigns n to as many as 1571 instances", "acronyms": [[57, 59]], "long-forms": [[61, 78], [90, 96]], "ID": "1117"}, {"text": " 1 Introduct ion  Many of the Natural Language Generation (NLG)  systems that produce flexible output, i.e. sentences ", "acronyms": [[59, 62]], "long-forms": [[30, 57]], "ID": "1118"}, {"text": "We then build three pairwise comparison matrices: one comparing pairs of typically developing (TD) children; one comparing pairs of children with ASD; and a third com-", "acronyms": [[95, 97], [146, 149]], "long-forms": [[73, 93]], "ID": "1119"}, {"text": " 6 Over Segmentation  For wrongly spelled or OOV (out of vocabulary)  Urdu words, the system may forcibly break the ", "acronyms": [[45, 48]], "long-forms": [[50, 67]], "ID": "1120"}, {"text": "Verbs can also involve non-core modifier arguments, such as ArgMTMP (time), ArgM-LOC (location), ArgM-CAU (cause), etc.", "acronyms": [[81, 84], [60, 67], [97, 105]], "long-forms": [[86, 94], [69, 73], [107, 112]], "ID": "1121"}, {"text": " SYSTEM ARCHITECTURE The TIA used for MUC-3 was developed from the AD-TIA (Alternate Domain TIA) . This system, shown", "acronyms": [[67, 73], [38, 43], [25, 28]], "long-forms": [[75, 95]], "ID": "1122"}, {"text": " In this paper, we introduce a novel method called Random Manhattan Indexing (RMI). RMI", "acronyms": [[78, 81], [84, 87]], "long-forms": [[51, 76]], "ID": "1123"}, {"text": "Abstract Crowd-sourcing approaches such as Amazon?s Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of", "acronyms": [[69, 74]], "long-forms": [[52, 67]], "ID": "1124"}, {"text": "vestigate four factors: text length (TL), sentence length (SL), average number of words per sentence (WS), and average number of characters per word (CW). Since", "acronyms": [[150, 152], [37, 39], [59, 61], [102, 104]], "long-forms": [[129, 148], [24, 35], [42, 57], [82, 100]], "ID": "1125"}, {"text": "Our initial experiment includes language model (LM), word posterior probability (WPP), confusion network (CN), and word lexicon (WL) features for a total of 11", "acronyms": [[106, 108], [48, 50], [81, 84], [129, 131]], "long-forms": [[87, 104], [32, 46], [53, 79], [115, 127]], "ID": "1126"}, {"text": "Figure 5: Example GMM fitting 2. Gaussian mixture model (GMM)-based POI probability (prior) calculation", "acronyms": [[57, 60], [18, 21], [68, 71]], "long-forms": [[33, 55]], "ID": "1127"}, {"text": "other synthesis techniques.  The TD-PSOLA (Time Domain Pitch Synchronous Overlap-add) developed by CNET is a very simple  but ingenious method which assures high voice quality, the only disadvantage is that it is based on a time: ", "acronyms": [[33, 41], [99, 103]], "long-forms": [[43, 84]], "ID": "1128"}, {"text": "direct analogy to Sidner's \\[26\\] potential local foci,  and assumes only one temporal referent in the  temporal focus (TF). ", "acronyms": [[120, 122]], "long-forms": [[104, 118]], "ID": "1129"}, {"text": " ? Modifier substitution (M-Sub) :  t2 is a substitution of t~ if and only if : ", "acronyms": [[26, 31]], "long-forms": [[3, 24]], "ID": "1130"}, {"text": "the Tomita parser, which handles Japanese and Spanish as well as English . The parser output is grammatical structures called Functionally Labelled Templates (FLTs) which are built using a linguistic formalism that modifies and extends the f-structure of Lexical-Functional Grammar (LFG) .", "acronyms": [[159, 163], [283, 286]], "long-forms": [[126, 157], [255, 281]], "ID": "1131"}, {"text": "MT is one of the oldest and most important areas of Natural Language Processing (NLP) / Computational Linguistics (CL).2 From its beginnings we have witnessed some changes in the", "acronyms": [[115, 117], [0, 2], [81, 84]], "long-forms": [[88, 113], [52, 79]], "ID": "1132"}, {"text": "and round corners for processes. Lexical access is applied to the input string to produce  (nondeterministically) the extended lexical item (ELI) of each word. Its output is split ", "acronyms": [[141, 144]], "long-forms": [[118, 139]], "ID": "1133"}, {"text": " 1 Introduction Question answering(QA) system aims at finding exact answers to a natural language question.", "acronyms": [[35, 37]], "long-forms": [[16, 33]], "ID": "1134"}, {"text": "All words are labeled as basic or not basic according to Ogden?s Basic English 850 (BE850) list (Ogden, 1930).3 In order to measure the lexical complexity", "acronyms": [[84, 89]], "long-forms": [[65, 82]], "ID": "1135"}, {"text": "  For all the reasons listed above, a dictionary of  Turkish Language Association (TLA) is used in  this study.", "acronyms": [[83, 86]], "long-forms": [[53, 81]], "ID": "1136"}, {"text": "the classification of various verb groups (generic verbs versus specific verbs) based on the semantic distance with Latent Semantic Analysis (LSA) and Cluster Analysis.", "acronyms": [[142, 145]], "long-forms": [[116, 140]], "ID": "1137"}, {"text": "Recently, McDonald et al (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative", "acronyms": [[91, 94]], "long-forms": [[68, 89]], "ID": "1138"}, {"text": "SC is mainly used in  mainland China while TC is mainly used in Taiwan  and Hong Kong (HK). In this experiment, we further ", "acronyms": [[87, 89], [0, 2], [43, 45]], "long-forms": [[76, 85]], "ID": "1139"}, {"text": "1 Introduction 1.1 Background Back in 2004, ETV (Eenadu Television), Hyderabad, felt a need for a text editor to prepare news", "acronyms": [[44, 47]], "long-forms": [[49, 66]], "ID": "1140"}, {"text": "imated conversational characters from recordings of human performance. ACM Transactions on Graphics (TOG), 23(3):506?513.", "acronyms": [[101, 104], [71, 74]], "long-forms": [[75, 99]], "ID": "1141"}, {"text": "TEXT), DECL (Declarative), HON (Honorific), IMPER (Imperative), NOM (Nominative), ORTH (ORTHOGRAPHY), PST (Past), SYN (SYNTAX), SEM (SEMANTICS), RELS (RELATIONS), and POS (part of speech).", "acronyms": [[102, 105], [7, 11], [27, 30], [44, 49], [64, 67], [82, 86], [114, 117], [128, 131], [145, 149], [167, 170]], "long-forms": [[107, 111], [13, 24], [32, 41], [51, 61], [69, 79], [88, 99], [119, 125], [133, 142], [151, 160], [172, 186]], "ID": "1142"}, {"text": "that provides a good compression rate of the text.  3.2 Byte Pair Encoding (BPE) Byte Pair Encoding (BPE) (Gage, 1994) is a sim-", "acronyms": [[76, 79], [101, 104]], "long-forms": [[56, 74], [81, 99]], "ID": "1143"}, {"text": " 2.2 Thread-level analysis Next, we perform named entity recognition (NER) over each thread to identify entities such as package", "acronyms": [[70, 73]], "long-forms": [[44, 68]], "ID": "1144"}, {"text": "and tfidf of unigrams, bigrams, and trigrams.  DAL (Dictionary of Affect in Language) is a psycholinguistic resource to measure the emo-", "acronyms": [[47, 50]], "long-forms": [[52, 84]], "ID": "1145"}, {"text": "nuqaqa llakiy qhachqa p?achakunata churakurqani.  Abbreviations: AMLQ = Academia Mayor de la Lengua Quechua en Cusco, norm = normalized, span = Spanish orthography, boliv = (old) Bolivian orthography Table 1: Different Orthographies with Corresponding Standardized Version", "acronyms": [[65, 69]], "long-forms": [[72, 107]], "ID": "1146"}, {"text": "collected as follows: Positive Diccn: 3,730 Chinese positive terms (e.g., /good-looking, / lucky) were collected from the Chinese Vocabulary for Sentiment Analysis (VSA)20 released by HOWNET.", "acronyms": [[165, 168], [184, 190]], "long-forms": [[130, 163]], "ID": "1147"}, {"text": " The system includes four main stages: topic classification, named entity recognition (NER), disease/location detection, and visualization.", "acronyms": [[87, 90]], "long-forms": [[61, 85]], "ID": "1148"}, {"text": "based classifier is used to select the most informative examples for training an another type of  classifier based on multinomial na?ve Bayes (NB)  model (McCallum and Nigam, 1998b).", "acronyms": [[143, 145]], "long-forms": [[130, 141]], "ID": "1149"}, {"text": "bic Diacritics. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and the 44th Annual Meeting of the the Association", "acronyms": [[98, 104]], "long-forms": [[71, 96]], "ID": "1150"}, {"text": "any hypotheses between frames and events.  (2) SameFrame (SF) is the second baseline system, which applies H1 over the results from AN-", "acronyms": [[58, 60]], "long-forms": [[47, 56]], "ID": "1151"}, {"text": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL). ", "acronyms": [[92, 95]], "long-forms": [[49, 90]], "ID": "1152"}, {"text": "? similar count? score (SC) is calculated as the number of characters that match between two", "acronyms": [[24, 26]], "long-forms": [[17, 22]], "ID": "1153"}, {"text": "constituent boundary prediction algorithm, the  followiug measures were used:  1) The cost time(CT) of the kernal  functions(CPU: Celeron TM 366, RAM: 64M).", "acronyms": [[96, 98], [125, 128], [138, 140], [146, 149]], "long-forms": [[86, 94]], "ID": "1154"}, {"text": "   Simple Segmentation Algorithm(SSA):  1.", "acronyms": [[33, 36]], "long-forms": [[3, 31]], "ID": "1155"}, {"text": "  For example, the s t ruc ture  tn whtch  ad jec t ives  (ADJ) repeat  a rb i t ra ry  ttmes and a noun  (N) fo l lows  them tn Engl lsh ts expressed as ", "acronyms": [[59, 62]], "long-forms": [[43, 56], [100, 104]], "ID": "1156"}, {"text": "ALO ( in to )   RO (OST LOC)  PO (PRE)  ON (VO)) ", "acronyms": [[34, 37], [0, 3], [16, 18], [20, 23], [24, 27], [44, 46]], "long-forms": [[30, 32]], "ID": "1157"}, {"text": "be intuitively characterized as a way of trigger-  ing semantically related concepts which define for  each role the projective conclusion space (PCS). ", "acronyms": [[146, 149]], "long-forms": [[117, 144]], "ID": "1158"}, {"text": "September/NNP \\] \\[O ./. \\]  we can extract following chunk patterns:  NP=NULL 90 PRP 99 VBZ  VP=PRP 99 VBZ 99 DT ", "acronyms": [[74, 78], [10, 13], [71, 73], [82, 85], [89, 92], [94, 96], [97, 100], [104, 107], [111, 113]], "long-forms": [], "ID": "1159"}, {"text": "term t appears in position i around the entity.  Bigram Context (BCON): The bigram-based  context model was built in a similar way to UCON, ", "acronyms": [[65, 69], [134, 138]], "long-forms": [[49, 63]], "ID": "1160"}, {"text": "In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST), pages 25?32, Rochester, NY.", "acronyms": [[70, 74], [3, 8], [101, 103]], "long-forms": [[45, 68]], "ID": "1161"}, {"text": "search engine, e.g., Google, Bing, etc. and (ii) a database (DB) system for accessing previously resolved crossword puz-", "acronyms": [[61, 63]], "long-forms": [[51, 59]], "ID": "1162"}, {"text": "extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff,", "acronyms": [[121, 123], [12, 14]], "long-forms": [[97, 119]], "ID": "1163"}, {"text": "ISREM 1 iff the candidate occurs 2 or more sentences before the anaphor POSITION 1 iff the antecedent occurs before anaphor SEMANTIC ROLE LABELLING (SR) IVERB 1 iff the governing verb of the given candidate is an issue verb", "acronyms": [[149, 151], [0, 5], [72, 80], [153, 158]], "long-forms": [[124, 137]], "ID": "1164"}, {"text": "using the written and spoken language corpora.  Occurrence probabilities (OPs) of expressions in the written and spoken language corpora can be used to dis-", "acronyms": [[74, 77]], "long-forms": [[48, 72]], "ID": "1165"}, {"text": "We experiment with multiple ways to select a snippet: the first 50 words of the summary (START), the last 50 words (END) and 50 words starting at a randomly chosen sentence", "acronyms": [[89, 94], [116, 119]], "long-forms": [[80, 87]], "ID": "1166"}, {"text": "Pattern Pattern Patterns composed of high frequency words (HFWs) 4", "acronyms": [[59, 63]], "long-forms": [[37, 57]], "ID": "1167"}, {"text": "that combining additional knowledge sources, including lexical features (LX1) and the non-verbal features, prosody (PROS), motion (MOT), and context (CTXT), yields a further improvement (of 8.8%", "acronyms": [[116, 120], [131, 134], [73, 75], [150, 154]], "long-forms": [[107, 114], [123, 129], [55, 70], [141, 148]], "ID": "1168"}, {"text": "(ADV), cause (CAU), direction (DIR), extent (EXT), location (LOC), manner (MNR), and time (TMP), modal verbs (MOD), negative markers (NEG), and discourse connectives (DIS). ", "acronyms": [[134, 137], [1, 4], [14, 17], [31, 34], [45, 48], [61, 64], [75, 78], [91, 94], [110, 113], [167, 170]], "long-forms": [[116, 124], [7, 12], [20, 29], [37, 43], [51, 59], [67, 73], [85, 89], [97, 102], [144, 153]], "ID": "1169"}, {"text": "than three thousand five hundred days of imprisonment .  Figure 4: Example translation using the back-off and the continuous space language model (CSLM). ", "acronyms": [[147, 151]], "long-forms": [[114, 145]], "ID": "1170"}, {"text": "Inspired by the work of web search (Gao et al2010) and question retrieval in community question answer (Q&A) (Zhou et al2011), we assume the following generative", "acronyms": [[104, 107]], "long-forms": [[87, 102]], "ID": "1171"}, {"text": "As an extension, Zhang et al (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR).", "acronyms": [[95, 98], [132, 135]], "long-forms": [[67, 93], [104, 130]], "ID": "1172"}, {"text": "the MACH-III expert system, we should begin with  a brief description. Functional hierarchy (FH) is a  new paradigm for organizing expert system ", "acronyms": [[93, 95], [4, 12]], "long-forms": [[71, 91]], "ID": "1173"}, {"text": "In this paper we investigate the relation between positive and negative pairs in Textual Entailment (TE), in order to highlight the role of contradiction in TE", "acronyms": [[101, 103], [157, 159]], "long-forms": [[81, 99]], "ID": "1174"}, {"text": "typically expressed inTopic Statements.  The Subject Field Coder (SFCoder) uses an  establishet~ semantic oding scheme from the machine- ", "acronyms": [[66, 73]], "long-forms": [[45, 64]], "ID": "1175"}, {"text": "abbreviations. ACM Transactions on Information Systems (TOIS), 24(3):380?404. ", "acronyms": [[56, 60], [15, 18]], "long-forms": [[19, 54]], "ID": "1176"}, {"text": "The evaluation strategy follows the global standard as  Text Retrieval Conference (TREC)8 metrics. It ", "acronyms": [[83, 87]], "long-forms": [[56, 81]], "ID": "1177"}, {"text": "method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs). Figure 1", "acronyms": [[125, 129]], "long-forms": [[98, 123]], "ID": "1178"}, {"text": "for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data", "acronyms": [[87, 94], [4, 7], [78, 85], [102, 109]], "long-forms": [], "ID": "1179"}, {"text": " We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006; Crammer et al, 2005), for updating the weights.", "acronyms": [[43, 47]], "long-forms": [[49, 69]], "ID": "1180"}, {"text": "notation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using human-", "acronyms": [[62, 64], [86, 88]], "long-forms": [[41, 60], [66, 84]], "ID": "1181"}, {"text": "2011a.  Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011.", "acronyms": [[45, 47], [57, 63]], "long-forms": [[24, 43]], "ID": "1182"}, {"text": " ? Term Base eXchange (TBX): XML Terminology Exchange Standard.", "acronyms": [[23, 26]], "long-forms": [[3, 21]], "ID": "1183"}, {"text": "There were four data sources used in the  training set: the Wall Street Journal, Associated  Press, Federal Register (FR), and Department of  215 ", "acronyms": [[118, 120]], "long-forms": [[100, 116]], "ID": "1184"}, {"text": "resolution pipeline consisted primarily of the C&C parser and Boxer (Curran et al, 2007), which produce Discourse Representation Structures (DRSs). ", "acronyms": [[141, 145], [47, 50]], "long-forms": [[104, 139]], "ID": "1185"}, {"text": "\u0007\u0002 B \u0007\u001eE B \u0004\u001e\u0014\t\u0005E B \u0002\u0014 B \u0019\u0002F\u0013\u0003E\b\u0007 B AE EA B \u0017\u0015 B \u0005\u0013\u0003\u0003\u0006\b) B ,\u0002\u0014 B \tA\u0007E\u0014\b\t\u0007\u0006 EA\u0015 B \u0004E\u0014\u001a\u0002\u0014\u0003\u0006\b) B F\u0002\u0003\u0004\u0002\bE\b\u0007+$\u0006\u0005EB \u0003\u0013A\u0007\u0006\u0004A\u0006F\t\u0007\u0006\u0002\bH0 B \u0002\u001a B$\u0002\u0014\u0019 B  EF\u0007\u0002\u0014\u0005 B ,\u001bF\u001eI\u0007JE\" B /66B0& B(\u001e\u0006\u0005 B \u001e\t\u0005 B \u0007\u001eE B \t\u0019 \t\b\u0007\t)E B \u0007\u001e\t\u0007 B \u0006\u0007 B \u0006\u0005 B \u0006\u0003\u0003E\u0019\u0006\t\u0007EA\u0015B \t\u0004\u0004A\u0006F\t\u0017AEB\u0007\u0002B\t\b\u0015B\u0003\u0002\u0019EAB\u0006\bB$\u001e\u0006F\u001eB$\u0002\u0014\u0019B\u0003E\t\b\u0006\b)BF\t\bB\u0017EBED\u0004\u0014E\u0005\u0005E\u0019B\t\u0005B\tB EF\u0007\u0002\u0014&B(\u001eEB\u0004\u0014\u0006\bF\u0006\u0004AEBA\u0006\u0003\u0006\u0007\t\u0007\u0006\u0002\bB \u0002\u001aBF\u0002\u0013\u0014\u0005EB\u0006\u0005B\u0007\u001e\t\u0007B\u0007\u001e\u0006\u0005B\u0007\t'E\u0005B\b\u0002B\tFF\u0002\u0013\b\u0007B\u0002\u001aB$\u0002\u0014\u0019B\u0002\u0014\u0019E\u0014&B(\u001eE\u0014E\u001a\u0002\u0014E\"B$\u001e\u0006AEB\u0006\u0007B\u001e\t\u0005B\u0004\u0014\u0002 E\bBE\u001a\u001aEF\u0007\u0006 EB\u0006\bB\u0007\u001eEB", "acronyms": [[300, 307]], "long-forms": [[309, 326]], "ID": "1186"}, {"text": "(Schubert 1987; Maxwell & Schubert 1989), and fig-  ure 1 shows the dependency trees for this example,  cross-coded for translation units (TUs). Each ellipse ", "acronyms": [[139, 142]], "long-forms": [[120, 137]], "ID": "1187"}, {"text": "edges. The NEs includes personal name(PRE), location name(LOC) and organization name(ORG). ", "acronyms": [[85, 88], [11, 14], [38, 41], [58, 61]], "long-forms": [[67, 79], [24, 32], [44, 52]], "ID": "1188"}, {"text": "We  describe our approach towards building a Wordnet for Tunisian dialect (TD). We proceed, first-", "acronyms": [[75, 77]], "long-forms": [[57, 73]], "ID": "1189"}, {"text": "Note that for the purposes of Figure 1: Example Babytalk Input Data: Sensors HR = Heart Rate; TcPO2 = blood O2 level; TCPCO2 = blood CO2 level; SaO2 = oxygen saturation; T1 = chest", "acronyms": [[77, 79], [94, 99], [118, 124], [144, 148], [170, 172]], "long-forms": [[82, 92], [102, 110], [127, 136], [151, 168]], "ID": "1190"}, {"text": "3.1 Data and preprocessing All embeddings are trained on 22 million tokens from the the North American News Text (NANT) corpus (Graff, 1995).", "acronyms": [[114, 118]], "long-forms": [[88, 112]], "ID": "1191"}, {"text": "Extraction Abbreviations NE = Named Entity CE = Correlated Entity", "acronyms": [[25, 27], [43, 45]], "long-forms": [[30, 42], [48, 65]], "ID": "1192"}, {"text": "internal structure. This type of expression is an example  of what will be called a \"comp\\]ex basic expression\" (CBE). ", "acronyms": [[113, 116]], "long-forms": [[85, 110]], "ID": "1193"}, {"text": "bath?). The builder can choose to represent binaries as either relational noun phrases (RELNP) or generalized transitive verbs (VP/NP).", "acronyms": [[88, 93], [128, 133]], "long-forms": [[63, 86], [121, 126]], "ID": "1194"}, {"text": "the vertex of the Abox which it expresses. Our current model uses the Tree Adjoining Grammar (TAG) formalism, see Joshi (1987), and the Atree acts as a", "acronyms": [[94, 97]], "long-forms": [[70, 92]], "ID": "1195"}, {"text": "The generator  uses as its linguistic resource a lexicon encoded in a version  of Categorial Grammar (CG), the extension of which with  rules of function composition gives rise to a problem of ", "acronyms": [[102, 104]], "long-forms": [[82, 100]], "ID": "1196"}, {"text": "spectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews).", "acronyms": [[80, 85], [128, 133]], "long-forms": [[70, 78], [112, 126]], "ID": "1197"}, {"text": "25 Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are also orthogonal to our work.", "acronyms": [[65, 70], [34, 38]], "long-forms": [[51, 63], [19, 32]], "ID": "1198"}, {"text": " ? Domain communicat ion knowledge (DCK). This is knowledge about how to communi- ", "acronyms": [[36, 39]], "long-forms": [[3, 34]], "ID": "1199"}, {"text": "the first optimum solution is obtained.  (c) EPN for the last optimum solution (EPN-L): The number of the expanded problems when", "acronyms": [[80, 85]], "long-forms": [[45, 78]], "ID": "1200"}, {"text": "build a bridge between UNL and one of the  internal representations of ETAP, namely  Normalized Syntactic Structure (NormSS), and in  this way link UNL with all other levels of text ", "acronyms": [[117, 123], [23, 26], [71, 75], [148, 151]], "long-forms": [[85, 115]], "ID": "1201"}, {"text": "by ranking the set of utterances by our confidence that they contain the query word, a task known as Ranked Utterance Retrieval (RUR). In particular,", "acronyms": [[129, 132]], "long-forms": [[101, 127]], "ID": "1202"}, {"text": "non relevant texts has the lower expectation. Figure 1 describes the probability density function (PDF ) for domain frequency scores of the SPORT domain", "acronyms": [[99, 102], [140, 145]], "long-forms": [[69, 97]], "ID": "1203"}, {"text": " These features of Latin influenced the choice  of Dependency Grammars (DG)2 as the most  suitable grammar framework for building Latin ", "acronyms": [[72, 74]], "long-forms": [[51, 70]], "ID": "1204"}, {"text": "PER (PN), only PER candidates beginning with  the family name is considered. For PER (FN), a  candidate is generated only if all its composing ", "acronyms": [[86, 88], [0, 3], [5, 7], [15, 18]], "long-forms": [[77, 84]], "ID": "1205"}, {"text": "(BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models trained on the English Broadcast News (BN) corpus (430 hours of audio) provided to us by IBM (Chen et al, 2009).", "acronyms": [[113, 115], [1, 3], [163, 166]], "long-forms": [[97, 111]], "ID": "1206"}, {"text": "on a 1,000 TU, EN-IT test set. B=Basic, LI=language identification, QE=quality estimation, WE=word embedding. ", "acronyms": [[68, 70], [91, 93], [11, 13], [15, 20], [33, 38], [40, 42]], "long-forms": [[71, 89], [94, 108], [43, 66]], "ID": "1207"}, {"text": "605  NP- - - -NP:NP NP=S:N I '  VP~-VP:VP  S~---S:S  NP=NI ' :PP  NP~-PP :NP  VP=VP:NP  S=S:N I '   NP  ~.-~NP :VP  PP -~-PP :PP  VP=VP:P I  ) S~S:  ", "acronyms": [[56, 58], [5, 7], [14, 16], [17, 19], [20, 22], [32, 38], [39, 41], [53, 55], [62, 64], [66, 72], [74, 76], [78, 80], [100, 102], [112, 114], [116, 118], [126, 128], [130, 132]], "long-forms": [[59, 62], [81, 86], [90, 93], [133, 137]], "ID": "1208"}, {"text": "follows. NLC(:A): the analysis of concepts that play  a role in natural anguage; (NL)CA: the lattice the-  Karaphuis and Sarbo 205 Natural Language Concept Analysis ", "acronyms": [[82, 84], [9, 12], [85, 87]], "long-forms": [[64, 79]], "ID": "1209"}, {"text": "matical relations on the arcs).  Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corre-", "acronyms": [[55, 57]], "long-forms": [[33, 53]], "ID": "1210"}, {"text": "the interpolated and adapted models are compared.  For the Estonian task, letter error rate (LER) is also reported, since it tends to be a more indicative", "acronyms": [[93, 96]], "long-forms": [[74, 91]], "ID": "1211"}, {"text": "NE translation.   Although NE translation is less sophisticated than machine translation (MT) in general, to some extent, the issues in NE translation are similar to those in MT.", "acronyms": [[90, 92], [0, 2], [27, 29], [136, 138], [175, 177]], "long-forms": [[69, 88]], "ID": "1212"}, {"text": "OWL DL is a logical language that combines the expressivity of OWL2 with the favourable computational properties of Description Logics (DL), notably decidability and monotonicity (Baader et al, 2003).", "acronyms": [[136, 138], [0, 3], [4, 6], [63, 67]], "long-forms": [[116, 134]], "ID": "1213"}, {"text": " 2 Named Entity Extraction  Named Entity Recognition (NER) is useful in  NLP applications such as question answering, ", "acronyms": [[54, 57], [73, 76]], "long-forms": [[28, 52]], "ID": "1214"}, {"text": " Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996),", "acronyms": [[57, 59]], "long-forms": [[37, 55]], "ID": "1215"}, {"text": "Finally, Lampert, Dale, and Paris (2006) describe a statistical classifier trained on text-based features for automatically predicting eight different speech acts derived from a taxonomy called Verbal Response Modes (VRM). The experiments are conducted", "acronyms": [[217, 220]], "long-forms": [[194, 215]], "ID": "1216"}, {"text": "  4.1 Speech recognition   The automatic speech recognition module (ASR)  is based on the Sphinx 4 system (Lamere et al, ", "acronyms": [[68, 71]], "long-forms": [[31, 59]], "ID": "1217"}, {"text": "534 3.1 Cross Validation on the Training Queries Random Walk with Restart (RWR) (also called personalized PageRank (Haveliwala, 2002)) is a", "acronyms": [[75, 78]], "long-forms": [[49, 73]], "ID": "1218"}, {"text": "transcription is carried out by using dynamic  programming alignment on the recognizer?s  hypothesis (HYP) and the non-literal transcription  that is used as reference (REF).", "acronyms": [[102, 105], [169, 172]], "long-forms": [[90, 100], [158, 167]], "ID": "1219"}, {"text": " rio deal with unknown words is a big problem in  natural language processing(NLP) too. To recognize ", "acronyms": [[78, 81]], "long-forms": [[50, 76]], "ID": "1220"}, {"text": " 3.4.1 Arabic Named Entity Recognition Named Entity Recognition (NER) is a subtask of information extraction, where each proper name in", "acronyms": [[65, 68]], "long-forms": [[39, 63]], "ID": "1221"}, {"text": "University of Massachusetts-Boston  Abstract  Word sense disambiguation (WSD) is one of  the main challenges in Computational ", "acronyms": [[73, 76]], "long-forms": [[46, 71]], "ID": "1222"}, {"text": "To standardize the measures to have fixed bounds, (Strehl and Ghosh, 2003) defined the normalized Mutual Information (NMI) as: NMI(Cr,Cg) =", "acronyms": [[118, 121], [127, 130]], "long-forms": [[87, 116]], "ID": "1223"}, {"text": "These models, variously known as vector spaces, semantic spaces, word spaces, corpus-based semantic models, or, using the term we will adopt, distributional semantic models (DSMs), all rely on some version of the distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree", "acronyms": [[174, 178]], "long-forms": [[142, 172]], "ID": "1224"}, {"text": "with the overall metric of error per response fill  (ERR), overgeneration (OVG) does not correlate  with it, and substitution (SUB) correlates with it  only to a limited extent.", "acronyms": [[127, 130], [53, 56], [75, 78]], "long-forms": [[113, 125], [59, 73]], "ID": "1225"}, {"text": "Hajdinjak and Mihelic? The PARADISE Evaluation Framework Number of help messages (NHM) and help-message ratio (HMR), i.e., the number and the ratio of system?s help messages;", "acronyms": [[82, 85], [111, 114]], "long-forms": [[57, 80], [91, 109]], "ID": "1226"}, {"text": "for topic models. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI), pages 27?34.", "acronyms": [[99, 102]], "long-forms": [[59, 97]], "ID": "1227"}, {"text": "the predicted margin. Dredze and Crammer (2008a) showed how Confidence Weighted (CW) learning could be used to generate a more informative mea-", "acronyms": [[81, 83]], "long-forms": [[60, 79]], "ID": "1228"}, {"text": "In Proceedings of the 15th International Conference on Computational Linguistics (COLING?94), pages 1145?1150, Kyoto, Japan.", "acronyms": [[82, 91]], "long-forms": [[55, 80]], "ID": "1229"}, {"text": " 1 Introduction In statistical machine translation (SMT) reordering (also called distortion) refers to the order in", "acronyms": [[52, 55]], "long-forms": [[19, 50]], "ID": "1230"}, {"text": "use the two cluster class features. The other selected features and the chosen algorithms (CL) are displayed in Table 1.", "acronyms": [[91, 93]], "long-forms": [[72, 89]], "ID": "1231"}, {"text": "Section 2  introduces some relevant work in IR and  question answering (QA). Section 3 talks about ", "acronyms": [[72, 74], [44, 46]], "long-forms": [[52, 70]], "ID": "1232"}, {"text": "lar expressions. So the detection of factoid words  can be achieved by Finite State Automaton(FSA). ", "acronyms": [[94, 97]], "long-forms": [[71, 92]], "ID": "1233"}, {"text": "Following are the two broad types of social events that were annotated: Interaction event (INR): When both entities participating in an event are aware of each other and of", "acronyms": [[91, 94]], "long-forms": [[72, 83]], "ID": "1234"}, {"text": "67  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189?195, October 25-29, 2014, Doha, Qatar.", "acronyms": [[92, 97]], "long-forms": [[42, 90]], "ID": "1235"}, {"text": "lead to over-fitting. Therefore, we propose another method, Probabilistic Soft Logic (PSL) (Broecheler et al, 2010).", "acronyms": [[86, 89]], "long-forms": [[60, 84]], "ID": "1236"}, {"text": "joining grammars. In Proceedings of the 12 th International  Conference on Computational Linguistics (COLING'88),  Budapest, Hungary, August 1988.", "acronyms": [[102, 111]], "long-forms": [[61, 100]], "ID": "1237"}, {"text": "usually similar with that in word sense disambiguation (WSD), including bag of word lemmas  in the sentence, n-grams and parts of speech (POS)  in a window, etc.", "acronyms": [[138, 141], [56, 59]], "long-forms": [[121, 136], [29, 54]], "ID": "1238"}, {"text": "coverage of the language's grammar rules.  This paper introduces a hidden Markov model (HMM) which  has been developed for Japanese word segmentation.", "acronyms": [[88, 91]], "long-forms": [[67, 86]], "ID": "1239"}, {"text": "semantic tree setups are depicted as follows:  TP2TP1 (a) Bag Of Features(BOF) ENT", "acronyms": [[74, 77], [47, 53], [79, 82]], "long-forms": [[58, 72]], "ID": "1240"}, {"text": "  1. RecallCorrectTransliteration  (RTrans)  The recall was computed using the sample as ", "acronyms": [[36, 42]], "long-forms": [[5, 33]], "ID": "1241"}, {"text": "108   Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, page 1, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[75, 80], [84, 88]], "long-forms": [[41, 73]], "ID": "1242"}, {"text": "Table 1: Probabilities computed for each type of linguistic information. Error codes correspond to the five error types in the CoNLL 2013 shared task: ArtOrDet (article or determiner), Nn (noun number), Prep (prepositions), SVA (subject-verb agreement) and Vform (verb form).", "acronyms": [[151, 159], [224, 227], [127, 132], [185, 187]], "long-forms": [[161, 182], [229, 251], [189, 200]], "ID": "1243"}, {"text": "We applied 3 MCMC algorithms: Gibbs sampling (GS), MCSAT and Simulated Tempering (ST) for inference and the comparative NER results are shown in Table 1.", "acronyms": [[82, 84]], "long-forms": [[61, 80]], "ID": "1244"}, {"text": "(reduced dimensions). The general idea behind the  Pseudo Relevance Feedback (PRF) (Croft &  Harper, 1979) or its more recent variation called ", "acronyms": [[78, 81]], "long-forms": [[51, 76]], "ID": "1245"}, {"text": "Sparse Lexicalised features and Topic Adaptation for SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 268?275.", "acronyms": [[143, 148], [53, 56]], "long-forms": [[88, 141]], "ID": "1246"}, {"text": "is Tws, for \"Translator's Workstation.\" We also used the  C-based X11 toolkit called MOTIF (Motif, 1991) and its Com-  monLisp interface called CLM (Babatz et.", "acronyms": [[85, 90]], "long-forms": [[92, 97]], "ID": "1247"}, {"text": "subsumption hierarchy of Patty is very sparse. It contains only 8,000 hypernymy links between phrases, and the entire taxonomy is kind of fragmented into a many-rooted DAG (directed acyclic graph). More-", "acronyms": [[168, 171]], "long-forms": [[173, 195]], "ID": "1248"}, {"text": "REF = obj123 SIZE = sizesensorreading85 SHAPE = shapesensorreading62 COLOUR = coloursensorreadning78", "acronyms": [[40, 45], [0, 3], [13, 17], [69, 75]], "long-forms": [[48, 68], [20, 39], [78, 100], [6, 12]], "ID": "1249"}, {"text": "Proc. ACM Multimedia (MM), ACM, Florence, Italy. pp.", "acronyms": [[22, 24]], "long-forms": [[15, 20]], "ID": "1250"}, {"text": "validity of the large margin method is guaranteed by the theorems of Structural Risk Minimization (SRM) under Probably Approximately Correct (PAC) framework2; test error is related to training data error, number of training", "acronyms": [[142, 145], [99, 102]], "long-forms": [[110, 140], [69, 97]], "ID": "1251"}, {"text": "In addition, adding the soft joint-inference formula results in further gain, and our full system (FULL) attained an F1 of 55.5. ", "acronyms": [[99, 103]], "long-forms": [[86, 90]], "ID": "1252"}, {"text": "possession (PO)  process (PR)  quantity (QU)  relation (RE) ", "acronyms": [[41, 43], [12, 14], [26, 28], [56, 58]], "long-forms": [[31, 39], [0, 10], [17, 24], [46, 54]], "ID": "1253"}, {"text": "In contrast to standard 357 multi-class Word Sense Disambiguation (WSD), it uses a coarse-grained sense inventory that allows to", "acronyms": [[67, 70]], "long-forms": [[40, 65]], "ID": "1254"}, {"text": "using the distributional similarity metric described by Lin (1998). We use WordNet (WN) as our sense inventory.", "acronyms": [[84, 86]], "long-forms": [[75, 82]], "ID": "1255"}, {"text": "impulses are possibly found. It is realized with very  simple space management transition networks (SMTNs),  in which $EXP, a distinguished symbol on an arc, ", "acronyms": [[100, 105]], "long-forms": [[62, 98]], "ID": "1256"}, {"text": "our system; where we achieved 0.627 top1 accuracy for Japanese transliterated to Japanese Kanji(JJ), 0.713 for English-toChinese(E2C) and 0.510 for English-to-", "acronyms": [[96, 98], [129, 132]], "long-forms": [[81, 94], [111, 128]], "ID": "1257"}, {"text": "6 6   Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 79?83, Seoul, South Korea, 5-6 July 2012.", "acronyms": [[102, 109]], "long-forms": [[52, 100]], "ID": "1258"}, {"text": "After applying the linguistic phenomena  resolution algorithm we obtain a new slot  structure (SS) that will store both the anaphora  and their antecedents.", "acronyms": [[95, 97]], "long-forms": [[84, 93]], "ID": "1259"}, {"text": "They refer to  syntactical features of a constituent such as number  (NUM), gender (GEN) etc. and to grammatical functions ", "acronyms": [[84, 87]], "long-forms": [[76, 82]], "ID": "1260"}, {"text": " 1 Introduction Semantic Role Labeling (SRL), independently of the approach adopted, comprehends two steps be-", "acronyms": [[40, 43]], "long-forms": [[16, 38]], "ID": "1261"}, {"text": "3http://www.cjk.org 4https://translit.i2r.a-star.edu.sg/news2009/evaluation/ 5The six metrics are Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank", "acronyms": [[122, 125]], "long-forms": [[103, 111]], "ID": "1262"}, {"text": "assumption of which is the stratificational  approach to sentence analysis pursued by  Functional Sentence Perspective (FSP), a  linguistic theory developed by Jan Firbas in the ", "acronyms": [[120, 123]], "long-forms": [[87, 118]], "ID": "1263"}, {"text": "side were installed from 2003 to 2005.?  3.4 Sentence Reordering (RE) Some of the transformation operations results in", "acronyms": [[66, 68]], "long-forms": [[54, 64]], "ID": "1264"}, {"text": "Two categories of neural networks have been used for language modeling: 1) recurrent neural networks (RNN), and 2) feedfoward network (FFN):", "acronyms": [[102, 105], [135, 138]], "long-forms": [[75, 100], [115, 133]], "ID": "1265"}, {"text": "CCG, as well as others.  A combinatory categorial grammar (CCG) is a categorial grammar whose rule system consists of", "acronyms": [[59, 62], [0, 3]], "long-forms": [[27, 57]], "ID": "1266"}, {"text": " 3.3 Question Classification We look next at question classification (QC). ", "acronyms": [[70, 72]], "long-forms": [[45, 68]], "ID": "1267"}, {"text": "based on such formalisms include Generalized Phrase  Structure Grammar (GPSG) \\[Gazdar et al 1985\\],  Lexical Functional Grammar (LFG) \\[Bresnan 1982\\],  Functional Unification Grammar (bUG) \\[Kay 1984\\], ", "acronyms": [[130, 133], [72, 76], [186, 189]], "long-forms": [[102, 128], [33, 70], [165, 184]], "ID": "1268"}, {"text": "Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven", "acronyms": [[156, 159], [92, 96], [188, 192]], "long-forms": [[127, 154]], "ID": "1269"}, {"text": "sets for Chinese and English. For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted 2For replicability, a complete description of all features can", "acronyms": [[82, 86]], "long-forms": [[60, 80]], "ID": "1270"}, {"text": "This convexity given the n4 Discounted cumulative gain (DCG) is widely used in information retrieval learning-to-rank settings.", "acronyms": [[56, 59]], "long-forms": [[28, 54]], "ID": "1271"}, {"text": " The focus of this paper is a discussion of various methods used to create a set of acoustic models for  characterizing the PLU's used in large vocabulary recognition (LVR). The set of context independent ", "acronyms": [[168, 171], [124, 129]], "long-forms": [[138, 166]], "ID": "1272"}, {"text": " In Proc. Rich Text 2004 Fall Workshop (RT-04F). ", "acronyms": [[40, 46]], "long-forms": [[10, 29]], "ID": "1273"}, {"text": "In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics(ACL). ", "acronyms": [[91, 94]], "long-forms": [[49, 89]], "ID": "1274"}, {"text": "of Machine Translation and present an implemetation of a morphological analyser for Amharic using Xerox Finite State Tools (XFST). The different", "acronyms": [[124, 128]], "long-forms": [[98, 122]], "ID": "1275"}, {"text": "Abstract  This paper presents a new bootstrapping  approach to named entity (NE)  classification.", "acronyms": [[77, 79]], "long-forms": [[63, 75]], "ID": "1276"}, {"text": "supervised labels to train our user model.  We used Amazon Mechanical Turk (MTurk) to collect data.", "acronyms": [[76, 81]], "long-forms": [[59, 74]], "ID": "1277"}, {"text": "are the formal-language theoretic foundation for n-gram models (Garcia et al, 1990), which are widely used in natural language processing (NLP) in part because such distributions can be estimated", "acronyms": [[139, 142]], "long-forms": [[110, 137]], "ID": "1278"}, {"text": "functions(CPU: Celeron TM 366, RAM: 64M).  2) Prediction precision(PP) =  number of words with correct BPs(CortBP) ", "acronyms": [[67, 69], [10, 13], [23, 25], [103, 106], [107, 113], [31, 34]], "long-forms": [[46, 66]], "ID": "1279"}, {"text": "columbia, edu  Abstract  Concept To Speech (CTS) systems are  closely related to two other types of ", "acronyms": [[44, 47]], "long-forms": [[25, 42]], "ID": "1280"}, {"text": " The attribute of a node is one of part of speech (POS), lexical value (LEX), or dependency label (DEP), as for instance LEX(QUEUE0)", "acronyms": [[72, 75], [51, 54], [99, 102], [121, 124], [125, 131]], "long-forms": [[57, 64], [35, 49], [81, 91]], "ID": "1281"}, {"text": "for every language. All results in percent. LAS = labeled attachment score, UAS = unlabeled attachment score.", "acronyms": [[44, 47], [76, 79]], "long-forms": [[50, 74], [82, 108]], "ID": "1282"}, {"text": "PROJECT GOALS  This project involves the integration of speech and natural-  language processing for spoken language systems (SLS). The ", "acronyms": [[126, 129]], "long-forms": [[101, 124]], "ID": "1283"}, {"text": "tracting sentence plan construction rules from the only publicly available corpus of discourse trees, the RST Discourse Treebank (RST-DT) (Carlson et al, 2002).", "acronyms": [[130, 136]], "long-forms": [[106, 128]], "ID": "1284"}, {"text": "Thus, we  name our system for generating compressions the  Adjustable Rate Compressor (ARC).   ", "acronyms": [[87, 90]], "long-forms": [[59, 85]], "ID": "1285"}, {"text": "machine learning algorithm is used to discover the morph set of the language in question, using minimum description length (MDL) as an optimization criterion.", "acronyms": [[124, 127]], "long-forms": [[96, 122]], "ID": "1286"}, {"text": "slot-def SN subslot-of SN1 . . . SNn (SN v SN1) . . . ( SN v SNn)", "acronyms": [[33, 36], [9, 11]], "long-forms": [[38, 46]], "ID": "1287"}, {"text": "descriptions 3800 4247 Table 2: Properties of the annotated two subcorpora, genetics (GEN) and computational linguistics (CL)", "acronyms": [[86, 89], [122, 124]], "long-forms": [[76, 84], [95, 120]], "ID": "1288"}, {"text": "state%1:03:00; ? AB = abstraction%1:03:00 \\ (event%1:03:00 ? state%1:03:00).", "acronyms": [[17, 19]], "long-forms": [[22, 41]], "ID": "1289"}, {"text": "We require that the language I has an available Wordnet linked to the Princeton Wordnet (PWN) (Fellbaum, 1998). ", "acronyms": [[89, 92]], "long-forms": [[70, 87]], "ID": "1290"}, {"text": "that together with the BOW it yields higher accuracy. Their results show a significant 1 The reciprocal rank (RR) for a question is 1 divided by the rank ordinal of the highest ranked relevant answer.", "acronyms": [[110, 112], [23, 26]], "long-forms": [[93, 108]], "ID": "1291"}, {"text": "University of Brighton There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG.", "acronyms": [[154, 157], [284, 287]], "long-forms": [[125, 152]], "ID": "1292"}, {"text": "I .  INTRODUCTION  Preliminary research on machine translat ion (MT) started soon af ter  computers  became avai lable.", "acronyms": [[65, 67]], "long-forms": [[43, 59]], "ID": "1293"}, {"text": "Proc. of the IEEE International Conference on Data Mining (ICDM). ", "acronyms": [[59, 63]], "long-forms": [[54, 57]], "ID": "1294"}, {"text": "(AvgToRecipients) in emails sent by p, the percentage of emails p received in which he/she was in the To list (InToList%), boolean features denoting whether p added or removed people when", "acronyms": [[111, 120], [1, 16]], "long-forms": [[95, 109], [43, 53]], "ID": "1295"}, {"text": "regression model. For our regression task we use a Generalised Linear Model (GLM) via penalized maximum likelihood (Friedman et al, 2010).", "acronyms": [[77, 80]], "long-forms": [[51, 75]], "ID": "1296"}, {"text": " org/wiki/California?. It is distinct from named entity extraction (NEE) in that it identifies not the occurrence of names but their reference.", "acronyms": [[68, 71]], "long-forms": [[43, 66]], "ID": "1297"}, {"text": "take scope over another.  Those natural language processing (NLP) systems that have managed to provide some sort of account of quantifier scope preferences have done so by using a separate", "acronyms": [[61, 64]], "long-forms": [[32, 59]], "ID": "1298"}, {"text": "sual scenes. In their seminal work Dale and Reiter (1995) present the Incremental Algorithm (IA) for GRE.", "acronyms": [[93, 95], [101, 104]], "long-forms": [[70, 91]], "ID": "1299"}, {"text": "e i Algorithm 1 Sparse projection (SP) Require: v // Vocabulary: vector of n words", "acronyms": [[35, 37]], "long-forms": [[16, 33]], "ID": "1300"}, {"text": "three statistical models: Conditional Random  Fields(CRF), Maximum Entropy(ME), and  Support Vector Machine(SVM), which have  good performance and used widely in the ", "acronyms": [[108, 111], [53, 56], [75, 77]], "long-forms": [[85, 106], [26, 52], [59, 74]], "ID": "1301"}, {"text": "First, we rewrite equation 1 in a more detailed fashion as: A?R = argmax A", "acronyms": [[60, 63]], "long-forms": [[66, 72]], "ID": "1302"}, {"text": "outer: the perceived concrete or abstract source, goal, or  location of the action,event, or state  Correspondent (CAR):  inner: the entity perceived as being in correspondence with ", "acronyms": [[115, 118]], "long-forms": [[100, 113]], "ID": "1303"}, {"text": "! JIM: { Person67 / Person83 / Name18 / (TYPE=&Person, SEX=Male, NAME=NamelS) }  Finally, the association between grammatical functions and ", "acronyms": [[65, 69]], "long-forms": [[70, 76]], "ID": "1304"}, {"text": "(Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels.", "acronyms": [[78, 80]], "long-forms": [[52, 76]], "ID": "1305"}, {"text": "prove sentiment classification. In Annual Meeting of the Association for Computational Linguistics (ACL). ", "acronyms": [[100, 103]], "long-forms": [[57, 98]], "ID": "1306"}, {"text": "Deployment management, enabling rapid deployment of locally tested charac-ters to highly available web servers as well as review and data warehousing functions for both analytic and refinement purposes. The information model is implemented in a re-lational database that fully specifies, relates and allows inquiry and validation of authored infor-mation. Additionally, a complete web application programming interface (API) powers the Roundtable application, providing a transactional framework for data operations as well as user privilege enforcement, but which also allows application expansion. The information model also serves to decouple the authoring representation from the data struc-tures necessary to drive dialogue behavior at runtime.", "acronyms": [[420, 423]], "long-forms": [[385, 418]], "ID": "1307"}, {"text": "Approach for Arabic-English Named Entity Translation, Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages (ACL),  University of Michigan, Ann Arbor", "acronyms": [[136, 139], [73, 76]], "long-forms": [[103, 134]], "ID": "1308"}, {"text": "include other language skills such as listening and reading. These constitute the integrated (INT) items.", "acronyms": [[94, 97]], "long-forms": [[82, 92]], "ID": "1309"}, {"text": "[NP : [XNOUNS : MERINO'S (NOUN) HOME (NOUN)] ] [NP : [XNOUNS : MERINO'S (NOUN)] ] [VP : [VERB_GROUP : HOME (VERB)] ] [XPPS : [PP : IN (PREPOSITION )", "acronyms": [[102, 106], [48, 50], [83, 85], [126, 128], [7, 13], [54, 60], [118, 122]], "long-forms": [[89, 99], [1, 3]], "ID": "1310"}, {"text": "appear in the labeled training data. In this paper, we call their method the latent variable method (LVM). ", "acronyms": [[101, 104]], "long-forms": [[77, 99]], "ID": "1311"}, {"text": "                                                                  Barcelona, July 2004                                               Association for Computations Linguistics                        ACL Special Interest Group on Computational Phonology (SIGPHON)                                                     Proceedings of the Workshop of the", "acronyms": [[252, 259], [197, 200]], "long-forms": [[201, 250]], "ID": "1312"}, {"text": "However, Turkers are not trained to provide reliable annotations for natural language processing (NLP) tasks, and some Turkers attempt to game the system by submitting", "acronyms": [[98, 101]], "long-forms": [[69, 96]], "ID": "1313"}, {"text": "{xiaoluo,hraghav,vittorio,smaskey,raduf}@us.ibm.com Abstract In natural language question answering (QA) systems, questions often contain terms and", "acronyms": [[101, 103]], "long-forms": [[81, 99]], "ID": "1314"}, {"text": "We used four machine learning algorithms implemented in Mallet (McCallum, 2002): decision tree, Naive Bayes, maximum entropy (MaxEnt), and conditional random field (CRF).5 Table 4 shows the", "acronyms": [[126, 132], [165, 168]], "long-forms": [[109, 124], [139, 163]], "ID": "1315"}, {"text": "adv Adverbial words(RB, RBR, RBS)  adj Adjunct word(JJ,JJR,JJS)  advP Adverb phrase(ADVP)  punct Punctuation(,) ", "acronyms": [[84, 88]], "long-forms": [[70, 82]], "ID": "1316"}, {"text": "composition process.  4.1 Tag Guided RNN (TG-RNN) We propose Tag Guided RNN (TG-RNN) to re-", "acronyms": [[42, 48], [77, 83]], "long-forms": [[26, 40], [61, 75]], "ID": "1317"}, {"text": "S = Spanish  JV = Joint Venture ?????????? ME = Microelectronics  Brief History of the Message Understanding Conferences", "acronyms": [[43, 45], [13, 15]], "long-forms": [[48, 64], [4, 11], [18, 31]], "ID": "1318"}, {"text": "Learning dependency-based compositional semantics.  In Association for Computational Linguistics (ACL). ", "acronyms": [[98, 101]], "long-forms": [[55, 96]], "ID": "1319"}, {"text": "In STS, we encoded only similarity feature between the two sentences. Thus, we used two classes of kernels: (1) the syntactic/semantic class (SS) with the final kernel defined as K(p 1", "acronyms": [[142, 144], [3, 6]], "long-forms": [[126, 140]], "ID": "1320"}, {"text": "to prevent this class of mistakes. To put it another way, we hoped to exploit the correlation between named-entities and noun phrase (NP) boundaries. A", "acronyms": [[134, 136]], "long-forms": [[121, 132]], "ID": "1321"}, {"text": " 5 Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context in which it occurs. { 4,5}", "acronyms": [[29, 32]], "long-forms": [[3, 27]], "ID": "1322"}, {"text": " Conf. on Language Resources and Evaluation (LREC), pages 147?152, Las Palmas, Spain, May.", "acronyms": [[45, 49]], "long-forms": [[10, 28]], "ID": "1323"}, {"text": "Table 4: Reranking results (%BLEU on TEST).  Discriminative Word/Tag LMs (DISC): For each language pair, we generated 10,000-best lists for", "acronyms": [[74, 78], [69, 72], [37, 41], [29, 33]], "long-forms": [[45, 59]], "ID": "1324"}, {"text": "274  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1192?1202, October 25-29, 2014, Doha, Qatar.", "acronyms": [[93, 98]], "long-forms": [[43, 91]], "ID": "1325"}, {"text": "(Suchanek et al 2007) have been playing a pivotal role in many AI applications, such as relation extraction(RE), question answering(Q&A), etc. ", "acronyms": [[132, 135], [108, 110], [63, 65]], "long-forms": [[113, 130], [88, 107]], "ID": "1326"}, {"text": "1093  Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 1?9, Gothenburg, Sweden, April 26-30 2014.", "acronyms": [[91, 96], [25, 29]], "long-forms": [[47, 89]], "ID": "1327"}, {"text": " The machine receives natural language input (text)  with referring expressions (RE), and possibly other  input (e.g. mouse clicks on a screen) with pseudo- ", "acronyms": [[81, 83], [46, 50]], "long-forms": [[58, 79], [39, 44]], "ID": "1328"}, {"text": " 4 . i .1  Bagging (BAG)  From a training set of n examples, severaI sam- ", "acronyms": [[20, 23]], "long-forms": [[11, 18]], "ID": "1329"}, {"text": " In table 1, we present the accuracy of the model trained on the output of the joint inference (JOINT) against that of the self-training baseline (SELF).", "acronyms": [[96, 101], [147, 151]], "long-forms": [[79, 84], [123, 136]], "ID": "1330"}, {"text": "the earliest in the passage is returned. We used the selective gain computation (SGC) algorithm (Zhou et al, 2003) to select features and estimate", "acronyms": [[81, 84]], "long-forms": [[53, 79]], "ID": "1331"}, {"text": "Vector Machines (SVM) with radial basis kernel, Na??ve Bayes (NB), J48 Decision Trees (DT), and Neural Networks (NN) with back propagation. In", "acronyms": [[113, 115], [17, 20], [62, 64], [87, 89]], "long-forms": [[96, 111], [0, 15], [48, 60], [71, 85]], "ID": "1332"}, {"text": "Computing Center ,  Academy of Sc iences ,  Hosoow, USSR  1.  Personal  Computer Systems (POS) represent  nowadays a  s ign i f teaut  t rend  in  the professiona~, and amateur use of  ", "acronyms": [[90, 93], [52, 56]], "long-forms": [], "ID": "1333"}, {"text": "Networks A more similar model to the proposed larger-context recurrent language model is a hierarchical recurrent encoder decoder (HRED) proposed recently by Serban et al (2015).", "acronyms": [[131, 135]], "long-forms": [[91, 129]], "ID": "1334"}, {"text": "cessing. In Proceedings of the 2nd International Conference on Knowledge Capture(K-CAP). USA.", "acronyms": [[81, 86], [89, 92]], "long-forms": [[63, 80]], "ID": "1335"}, {"text": "an example of a low-pass filter. The concept of recursion is next introduced in order to pave the way for a discussion of IIR (Infinite Impulse Response) filters. High-, low-, and", "acronyms": [[122, 125]], "long-forms": [[127, 152]], "ID": "1336"}, {"text": "grammars. In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING-92), pages 426?432, Nantes, 1992.", "acronyms": [[98, 107]], "long-forms": [[57, 96]], "ID": "1337"}, {"text": "This method is much simpler than the ILP method, while it can achieve comparable result on the CLANG (Coach Language) and Query corpus. ", "acronyms": [[95, 100], [37, 40]], "long-forms": [[102, 116]], "ID": "1338"}, {"text": "AVERAGE 3.31 316 72.58% 78.02% 84.65% Table 2: Word sense disambiguation results, including two baselines (MFS = most frequent sense; LeskC = Lesk-corpus) and the word sense disam-", "acronyms": [[107, 110], [134, 139]], "long-forms": [[113, 132], [142, 153]], "ID": "1339"}, {"text": "ifications) to be the first on the COMPS list, and further assigns a positive value for an additional feature INV (inverted) on verbs. This feature may", "acronyms": [[110, 113], [35, 40]], "long-forms": [[115, 123]], "ID": "1340"}, {"text": "tion access tasks. Current approaches to AZ rely on supervised machine learning (ML). ", "acronyms": [[81, 83], [41, 43]], "long-forms": [[63, 79]], "ID": "1341"}, {"text": "(? 2.2), we propose an induction algorithm based on Integer Linear Programming (ILP). Figure 2", "acronyms": [[80, 83]], "long-forms": [[52, 78]], "ID": "1342"}, {"text": "portance of the edge features and the resultant largemargin constraint, we also compare against a standard binary Support Vector Machine (SVM) which uses node features alone to predict whether each", "acronyms": [[138, 141]], "long-forms": [[114, 136]], "ID": "1343"}, {"text": "In frame semantics, the meaning of words or word expressions, also called target words (TW), comprises aspects of conceptual structures, or frames, that de-", "acronyms": [[88, 90]], "long-forms": [[74, 86]], "ID": "1344"}, {"text": "maps: 2-d space-filling approach. ACM Transactions on Graphics (TOG), 11(1):92?99. ", "acronyms": [[64, 67], [34, 37]], "long-forms": [[38, 62]], "ID": "1345"}, {"text": "tion of predicate signs. Ramchand divides events into a maximum of three hierarchical phrases: an initiation phrase (InitP), a process phrase (ProcP), and a result phrase (ResP).", "acronyms": [[117, 122], [143, 148]], "long-forms": [[98, 115], [127, 141]], "ID": "1346"}, {"text": " (Choudhury et al, 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisy-", "acronyms": [[86, 89]], "long-forms": [[65, 84]], "ID": "1347"}, {"text": "parts of speech, and for different confidence levels. We compare our method to the Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin model described in Takamura, Inui, and Okumura (2005), the shortest path method", "acronyms": [[114, 120]], "long-forms": [[83, 112]], "ID": "1348"}, {"text": "[5] Corbett, J. C., M. B. Dwyer, J. Hatcliff, S. Laubach, C. S. Pasareanu, Robby and H. Zheng, Bandera: Extracting finite-state models from java source code, in: Proceedings of the International Conference on Software Engineering (ICSE), 2000. ", "acronyms": [[231, 235]], "long-forms": [[181, 229]], "ID": "1349"}, {"text": "Web Search 1 Figure 1: Architecture of the Multi-task Deep Neural Network (DNN) for Representation Learning: The lower layers are shared across all tasks, while top layers are task-specific.", "acronyms": [[75, 78]], "long-forms": [[54, 73]], "ID": "1350"}, {"text": "non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions.", "acronyms": [[86, 89]], "long-forms": [[64, 84]], "ID": "1351"}, {"text": " The motivation for that work is twofold: on the one hand it builds on the strength of the first sense heuristic in Word Sense Disambiguation (WSD) (i.e. the heuristic of choosing themost commonly used sense of a word, irrespective of the context in which", "acronyms": [[143, 146]], "long-forms": [[116, 141]], "ID": "1352"}, {"text": "F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which does not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-", "acronyms": [[154, 157]], "long-forms": [[132, 152]], "ID": "1353"}, {"text": "Linear-chain CRFs correspond to finite state machines, and can be roughly understood as conditionally-trained hidden Markov models (HMMs). This class of CRFs", "acronyms": [[132, 136]], "long-forms": [[110, 130]], "ID": "1354"}, {"text": " Opennlp maxent1, an implementation of Maximum Entropy (ME) modeling, is used as the classification tool.", "acronyms": [[56, 58]], "long-forms": [[39, 54]], "ID": "1355"}, {"text": "Introduction  The pro jec t  note  presents  the  computer  program  GECO (GEometry COnsu l te r ) ,  wh ich  generates   exp lanat ions  (descr ip t ions )  o f  geometr i ca l  ", "acronyms": [[69, 73]], "long-forms": [[75, 96]], "ID": "1356"}, {"text": "4.2 Proposed Model : PNB (vs. UM) Figure 1 shows the performances of our new model named Poisson naive Bayes(PNB) classifiers acTable 2: Performances of UM and PNB on the", "acronyms": [[109, 112], [21, 24], [153, 155], [160, 163], [30, 32]], "long-forms": [[89, 107]], "ID": "1357"}, {"text": "rately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers", "acronyms": [[135, 138]], "long-forms": [[106, 133]], "ID": "1358"}, {"text": "and Ripper, on the other hand, appear to take more advantage of some feature types than others. For the third task, lexical (LX) and discourse (DS) features apparently have more predictive power for both C4.5 and SVM than the other types.", "acronyms": [[125, 127], [144, 146], [213, 216]], "long-forms": [[116, 123], [133, 142]], "ID": "1359"}, {"text": "IGT-XML. At the heart of the model is a representation of interlinearized glossed text (IGT). Building", "acronyms": [[88, 91], [0, 7]], "long-forms": [[58, 86]], "ID": "1360"}, {"text": "son, 2012; Vlas and Robinson, 2011). Due to its expressiveness, natural language (NL) became a popular medium of communication between users and", "acronyms": [[82, 84]], "long-forms": [[64, 80]], "ID": "1361"}, {"text": "Knowledge-free induction of morphology using latent semantic analysis. In Proceedings of the Conference on Natural Language Learning 2000 (CoNLL-2000), pages 67?72, Lisbon, Portugal.", "acronyms": [[139, 149]], "long-forms": [[93, 137]], "ID": "1362"}, {"text": "We conducted experiments on a number of different datasets: (1) the English Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al, 1993) with standard POS", "acronyms": [[97, 100], [163, 166]], "long-forms": [[76, 95]], "ID": "1363"}, {"text": "tual distributions (Miller and Charles, 1991). In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,", "acronyms": [[92, 96]], "long-forms": [[60, 90]], "ID": "1364"}, {"text": "We have adapted the list from Rambow et al(2006) for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle", "acronyms": [[149, 151], [162, 164], [180, 182], [194, 197], [218, 221], [78, 84]], "long-forms": [[138, 147], [118, 122], [128, 132], [154, 160], [167, 178], [185, 192], [200, 216], [224, 235], [241, 252]], "ID": "1365"}, {"text": "lists are derived automatically from the training data.  Frequent Word List (FWL) This list consists of words that occur in more than 5 different documents.", "acronyms": [[77, 80]], "long-forms": [[57, 75]], "ID": "1366"}, {"text": "+ ??????????)  random Markov Clustering Algorithm (MCL)  (Dongen, 2000) ", "acronyms": [[51, 54]], "long-forms": [[22, 49]], "ID": "1367"}, {"text": "coord dep coord (c) Previous conjunct headed (PH) Je vois Jean , Paul et Marie", "acronyms": [[46, 48]], "long-forms": [[20, 44]], "ID": "1368"}, {"text": "TI = fTW; F; ADV; AUX; V A; V C; V V; Pg Each type of the indicators, e.g. TW , contains a set of words, such as TW = twlist = ftw", "acronyms": [[113, 115], [75, 77], [18, 21], [13, 16], [0, 2]], "long-forms": [[118, 124]], "ID": "1369"}, {"text": "The \u0002 grams in an utterance SSG can be extracted by converting it to a finite state transducer (FST), \f\u000e  .", "acronyms": [[96, 99], [28, 31]], "long-forms": [[71, 94]], "ID": "1370"}, {"text": " BIBLIOGRAPHY  ALPAC (Automatic Language Processing Advisory Committee)  1966 Lanquage and Machines - Computers i n  Translation and Linguistics ", "acronyms": [[15, 20]], "long-forms": [[22, 70]], "ID": "1371"}, {"text": "edge in Y .  The Wall Street Journal Penn Treebank (PTB) (Marcus et al, 1993) contains parsed constituency", "acronyms": [[52, 55]], "long-forms": [[37, 50]], "ID": "1372"}, {"text": "appropriate.  Terminology Data banks (TD) are the least ambitious  systems because access frequently is not made during a ", "acronyms": [[38, 40]], "long-forms": [[14, 30]], "ID": "1373"}, {"text": "constructed using only surface Alterf patterns; for the GLM and text versions, we can use either surface patterns, logical form (LF) patterns, or both. ", "acronyms": [[129, 131], [56, 59]], "long-forms": [[115, 127]], "ID": "1374"}, {"text": "timing. This flexibility is in contrast to speech output in spoken dialogue systems (SDSs) which typically generate, synthesize and deliver speech", "acronyms": [[85, 89]], "long-forms": [[60, 83]], "ID": "1375"}, {"text": "Abstract We present the design, preparation, results and analysis of the Cancer Genetics (CG) event extraction task, a main task of the", "acronyms": [[90, 92]], "long-forms": [[73, 88]], "ID": "1376"}, {"text": "ral probabilistic language model. In Advances in Neural Information Processing Systems (NIPS), 2000.", "acronyms": [[88, 92]], "long-forms": [[49, 86]], "ID": "1377"}, {"text": "tical problems in class. It might also be useful in massive open online courses (MOOCs). In this for-", "acronyms": [[81, 86]], "long-forms": [[52, 79]], "ID": "1378"}, {"text": "These 67? 30 candidate claims were annotated using Amazon?s Mechanical Turk (AMT). In each", "acronyms": [[77, 80]], "long-forms": [[51, 75]], "ID": "1379"}, {"text": "episodes in lexical processing. In Proceedings of SWAP (Spoken Word Access Processes), A. Cutler, J. M McQueen, and R. Zondervan, ed.,", "acronyms": [[50, 54]], "long-forms": [[56, 84]], "ID": "1380"}, {"text": "SR+LM+SB 0.57 0.75 0.65 0.75 0.68 0.71 0.69 0.92 0.87 0.88 0.87 0.85 0.75 0.87 0.81 0.95 A0 - - - 0.85 - - - 0.93 - - - N/A - - - 0.97 Table 5: Alignment results for all datasets and configurations: Using semantic relations (SR), monosemous links (LM) or both (SR+LM).", "acronyms": [[225, 227], [120, 123], [248, 250], [261, 266]], "long-forms": [[205, 223], [230, 246]], "ID": "1381"}, {"text": "Training uses balanced data (50:50). Testing uses two class distributions (C.D.): 50:50 (balanced) and Natural Distribution (N.D.). Improvements of our method are statistically significant with p<0.005 based on paired t-test.", "acronyms": [[125, 129], [75, 78]], "long-forms": [[103, 123], [54, 72]], "ID": "1382"}, {"text": "2007). Two 5 DOF sensors - TT (Tongue Tip)  and TB (Tongue Body Back) - were attached on  the midsagittal of the tongue.", "acronyms": [[48, 50], [27, 29], [13, 16]], "long-forms": [[52, 63], [31, 41]], "ID": "1383"}, {"text": "A Robust Algorithm for the Tree Edit Distance.  Proceedings of the VLDB Endowment (PVLDB), 5(4):334?345.", "acronyms": [[83, 88]], "long-forms": [[48, 71]], "ID": "1384"}, {"text": "mechanism for accurate named entity  (NE) translation in English?Chinese  question answering (QA). This mecha-", "acronyms": [[94, 96], [38, 40]], "long-forms": [[74, 92], [23, 35]], "ID": "1385"}, {"text": "637  Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 27?32, Sofia, Bulgaria, August 9, 2013.", "acronyms": [[70, 77]], "long-forms": [[36, 68]], "ID": "1386"}, {"text": "Table 1 shows the feature template sets.  For training, we used soft confidence weighted (SCW) (Wang et al., 2012).", "acronyms": [[90, 93]], "long-forms": [[64, 88]], "ID": "1387"}, {"text": " As with most modern simulators, DISs are controlled via  graphical user interfaces (GUIs). However, the simulation ", "acronyms": [[85, 89], [33, 37]], "long-forms": [[58, 83]], "ID": "1388"}, {"text": "feeling (FE)  food (FO)  group (GR)  location (LO) ", "acronyms": [[32, 34], [9, 11], [20, 22], [47, 49]], "long-forms": [[25, 30], [0, 7], [14, 18], [37, 45]], "ID": "1389"}, {"text": "match with a hypothesis. These weights are  confidence measures: Logical Sufficiency (LS)  and Logical Necessity (LN).", "acronyms": [[86, 88], [114, 116]], "long-forms": [[65, 84], [95, 112]], "ID": "1390"}, {"text": "Abstract  This paper describes a heuristic algorithm capable of automatically assigning a label to  each of the senses in a machine readable dictionary (MRD) for the purpose of acquiring a com-  putational-semantic lexicon for treatment of lexical ambiguity.", "acronyms": [[153, 156]], "long-forms": [[124, 151]], "ID": "1391"}, {"text": "Th~se de l'Universitt~ Joseph  Fourier, Grenoble I, Mars 1990  \\[8\\] TEl (Text Encoding Initiative), Guidelines for the  Encoding and lnterchange of Machine Readable Texts.", "acronyms": [[69, 72]], "long-forms": [[74, 98]], "ID": "1392"}, {"text": " Because we don't have any other useful resources except ChineseGigaword(CGW),We first computed mutual information for all 3-character words in two", "acronyms": [[73, 76]], "long-forms": [[57, 72]], "ID": "1393"}, {"text": " In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040?1047, Uppsala, Sweden, July.", "acronyms": [[93, 96]], "long-forms": [[50, 91]], "ID": "1394"}, {"text": "low navigational directions. In Proceedings of the Association for Computational Linguistics (ACL), 2010.", "acronyms": [[94, 97]], "long-forms": [[51, 92]], "ID": "1395"}, {"text": "on the gender of the user.  User ID: The user ID (UID) labels are inspired by research on Arabic Twitter showing that a consid-", "acronyms": [[50, 53]], "long-forms": [[41, 48]], "ID": "1396"}, {"text": "hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing.  The parser is able to parse 20 Wall Street Journal (WSJ) sentences per second on standard hardware, using our best-performing model, which compares very favorably with other", "acronyms": [[143, 146], [44, 47]], "long-forms": [[122, 141]], "ID": "1397"}, {"text": "markert@l3s.de Abstract Automatic timeline summarization (TLS) generates precise, dated overviews over", "acronyms": [[58, 61]], "long-forms": [[34, 56]], "ID": "1398"}, {"text": "We discuss related work in section 5, and conclude in section 6.  2 Index of Productive Syntax (IPSyn) The Index of Productive Syntax (Scarborough, 1990) evaluates a child?s linguistic development by ana-", "acronyms": [[96, 101]], "long-forms": [[68, 94]], "ID": "1399"}, {"text": "ings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). ", "acronyms": [[85, 88]], "long-forms": [[58, 83]], "ID": "1400"}, {"text": " 2 Extended typed  A-ca lcu lus   CU(\\] (Categorial U,,ificAtion (l:ra,nma,r) \\[8\\] is a,d-  vantageous, compared to other phrase structure ", "acronyms": [[34, 36]], "long-forms": [[41, 64]], "ID": "1401"}, {"text": "32  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 169?178, Seoul, South Korea, 5-6 July 2012.", "acronyms": [[100, 107]], "long-forms": [[50, 98]], "ID": "1402"}, {"text": "Note also that there is some overlap of infer-  marion between the Lexical Systems analysis and  the Brandeis analysis, such as SUISCAT(TRAN)  and DO.", "acronyms": [[136, 140], [147, 149], [128, 135]], "long-forms": [], "ID": "1403"}, {"text": "successful method presented by Bendersky and Croft (2008) for selection and weighting of query noun phrases (NPs). It also extends work for deter-", "acronyms": [[109, 112]], "long-forms": [[95, 107]], "ID": "1404"}, {"text": "In that case partial semantic mapping will take place where no Logical Form is being built and only referring expressions are asserted in the Discourse Model ? but see below.  3.2 Lexical Information The output of grammatical modules is then fed onto the Binding Module(BM) which activates an algorithm for anaphoric binding in LFG (see [13]) terms using f-structures as domains and grammatical functions as entry points into the structure. We show here below the architecture of the system.", "acronyms": [[270, 272], [328, 331]], "long-forms": [[255, 268]], "ID": "1405"}, {"text": "al. ( 1997)  and later Harabagiu and Maiorano (HM) (2000)  investigated the acquisition of the lexical concept ", "acronyms": [[47, 49]], "long-forms": [[23, 45]], "ID": "1406"}, {"text": "There are four goodness algorithms reviewed by Zhao and Kit (2008a). The algorithms, including Description Length Gain (DLG) (Kit and Wilks 1999), Accessor Variety (Feng et al 2004a, 2004b), and Branching Entropy (Tanaka-Ishii", "acronyms": [[120, 123]], "long-forms": [[95, 118]], "ID": "1407"}, {"text": "12  Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137?142, October 25, 2014, Doha, Qatar.", "acronyms": [[82, 86]], "long-forms": [[46, 80]], "ID": "1408"}, {"text": "In the SIGHAN Bakeoff 2007, there are five training corpus for word segmentation (WS) task: AS  (Academia Sinica), CityU (City University of  Hong Kong) are traditional Chinese corpus; CTB ", "acronyms": [[115, 120], [82, 84], [92, 94], [185, 188]], "long-forms": [[122, 137], [63, 80], [97, 112]], "ID": "1409"}, {"text": "a t tachment  (PP): the attachment ofa PP  in the sequence VP hip PP (VP = verb  phrase, 51P = noun phrase, PP = prepo-  sitional phrase).", "acronyms": [[108, 110], [15, 17], [39, 41], [59, 61], [66, 68], [70, 72]], "long-forms": [[113, 119], [75, 87]], "ID": "1410"}, {"text": " Unknown  1976 \"The Lexicography Informati on Sys tern (LEXIS) o f  the Bundeswher  Language Service,\" i n  Machine Assisted \"h-ansl ation i n  West ", "acronyms": [[56, 61]], "long-forms": [[20, 49]], "ID": "1411"}, {"text": "This research has been supported in part by DARPA (under contract number FA8750-13-2-0005), NIH (NICHD award 1R01HD07532801), Keck Foundation (DT123107), NSF (IIS0835797), and", "acronyms": [[92, 95], [44, 49], [73, 75], [143, 145], [154, 157]], "long-forms": [[97, 102]], "ID": "1412"}, {"text": "dependency analyzer, respectively.  (V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG =  ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP, ", "acronyms": [[62, 66], [85, 90], [144, 147], [224, 228], [248, 252]], "long-forms": [[69, 83], [93, 103], [56, 60], [41, 50], [151, 159], [164, 172], [231, 246], [255, 267]], "ID": "1413"}, {"text": "Hence, it seems plausible to  utilize a back-off mechanism for these sentences  via a combined system (COMB) incorporating  NB only for the sentences that fail to parse.", "acronyms": [[103, 107], [124, 126]], "long-forms": [[86, 94]], "ID": "1414"}, {"text": "but more often there is only one.  FN=false negative, etc.). I also consider micro- and", "acronyms": [[35, 37]], "long-forms": [[38, 52]], "ID": "1415"}, {"text": "Abstract Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising", "acronyms": [[153, 156]], "long-forms": [[124, 151]], "ID": "1416"}, {"text": " 2Regularized parses (henceforth, \"parse trees\") are  like F-structures of Lexical Ftmction Grammar (LFG),  except, hat a dependency structure is used.\"", "acronyms": [[101, 104]], "long-forms": [[75, 99]], "ID": "1417"}, {"text": "It is based on bilingual phrases, where a bilingual phrase (BP ) is simply two monolingual phrases (MP ) in which each one is supposed to be the translation of each", "acronyms": [[100, 102], [60, 62]], "long-forms": [[79, 98], [42, 58]], "ID": "1418"}, {"text": "actual object and \\[AI is the word that represents A. CS : = speaking A;yes refers A \\[A\\] ; yes  (111) Resource Situation(RS)  A resource situation is defined for each individual in a discourse; it ", "acronyms": [[123, 125], [54, 56]], "long-forms": [[104, 122]], "ID": "1419"}, {"text": "belief from the belief tracker in 6.3% of the dialogs.  The mean Word Error Rate (WER) per worker on the test set is 27.5%.", "acronyms": [[82, 85]], "long-forms": [[65, 80]], "ID": "1420"}, {"text": "To explore the impact of the quality of annotation resources, we also use a Chinese language analysis tool: Language Technology Platform (LTP) (Che et al, 2010).", "acronyms": [[138, 141]], "long-forms": [[108, 136]], "ID": "1421"}, {"text": "CrossT values). The regression model predicted  Figure 4: ST alignment crossings (CrossS), as generated  when checking the ST against the TT ", "acronyms": [[82, 88], [0, 6], [58, 60], [123, 125], [138, 140]], "long-forms": [[71, 80]], "ID": "1422"}, {"text": "translation evaluation metrics such as BLEU score (Papineni et al, 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002).", "acronyms": [[146, 149], [39, 43], [74, 78]], "long-forms": [[108, 144]], "ID": "1423"}, {"text": "given topic.  The Maximal Marginal Relevance (MMR) summarization method, which is based on a", "acronyms": [[46, 49]], "long-forms": [[18, 44]], "ID": "1424"}, {"text": "In Modern Standard Arabic (MSA), all nouns and adjectives have one of three cases: nominative (NOM), accusative (ACC), or genitive (GEN). What", "acronyms": [[113, 116], [132, 135], [27, 30], [95, 98]], "long-forms": [[101, 111], [122, 130], [3, 25], [83, 93]], "ID": "1425"}, {"text": "Figure 4: Change of global network properties with incremental addition of edges to the directed network of news genre. SCC = Strongly Connected Component, CC = Connected Component. By ?", "acronyms": [[120, 123], [156, 158]], "long-forms": [[126, 154], [161, 180]], "ID": "1426"}, {"text": " 1 LR  Parser  Generat ion   Tree Adjoining Grammars (TAGs) are tree rewrit-  ing systems which combine trees with the sin- ", "acronyms": [[54, 58], [3, 5]], "long-forms": [[29, 52]], "ID": "1427"}, {"text": "tions\" as per the gold standard.  D = True Negatives (TN) = Pairs that were identified as \"Incorrect Transliterations\" by the par-", "acronyms": [[54, 56]], "long-forms": [[38, 52]], "ID": "1428"}, {"text": " 3 Results and Discussions Chain frequency (CF) and chain length (CL) reflect the dyad?s tweeting behaviors.", "acronyms": [[44, 46], [66, 68]], "long-forms": [[27, 42], [52, 64]], "ID": "1429"}, {"text": "an NP node with the grammatical function (GF) of the pronoun. The GF of the pronoun, in turn, is replaced by HD (head). Such unary branching NPs are added on top of nouns", "acronyms": [[109, 111], [3, 5], [42, 44], [66, 68], [141, 144]], "long-forms": [[113, 117], [20, 40]], "ID": "1430"}, {"text": "the percentage of tokens that are assigned the correct head and dependency label, as well as the unlabeled attachment score (UAS), that is, the percentage of tokens with the correct head, and the label accuracy (LA), that is, the percentage of tokens with the correct dependency label.", "acronyms": [[212, 214], [125, 128]], "long-forms": [[196, 210], [97, 123]], "ID": "1431"}, {"text": "certainty factor equals to 0.6 (3/5). The general formula for the certainty factor (CF) is shown as follow: CFi = Total number of the answer elements at leaf node i", "acronyms": [[84, 86], [108, 110]], "long-forms": [[66, 82]], "ID": "1432"}, {"text": "edge of syntax and semantics.   In connection to conjunct verbs (ConjVs),  (Mohanty, 2010) defines two types of conjunct ", "acronyms": [[65, 71]], "long-forms": [[49, 63]], "ID": "1433"}, {"text": "To classify the NPs according to their type in biomedical terms, we have adopted the Sequence Ontology (SO)2 (Eilbeck and Lewis, 2004).", "acronyms": [[104, 106], [16, 19]], "long-forms": [[85, 102]], "ID": "1434"}, {"text": "2 Algorithms and Data  2.1 Task Definition and Data  The named entity (NE) task used for this  evaluation requires the system to identify all ", "acronyms": [[71, 73]], "long-forms": [[57, 69]], "ID": "1435"}, {"text": "Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF", "acronyms": [[105, 109], [84, 86], [130, 132], [173, 177]], "long-forms": [[91, 103]], "ID": "1436"}, {"text": "tion using a method similar to FOIL (Quin-  lan, 1990) and bottom-up generalization using  Least General Generalizations (LGG's). Ad- ", "acronyms": [[122, 127], [31, 35]], "long-forms": [[91, 120]], "ID": "1437"}, {"text": "TOP (PRP ? I?) ( VP (VBP ? NEED?) (", "acronyms": [[17, 19], [0, 3], [5, 8]], "long-forms": [[21, 24]], "ID": "1438"}, {"text": "calculating the posterior probabilities.  Active SVM with self-training (ASSVM) is an extension of ASVM where each round of training has", "acronyms": [[73, 78], [99, 103]], "long-forms": [[42, 52]], "ID": "1439"}, {"text": "2 Shared-task evaluation in HLT Over the past twenty years, virtually every field of research in human language technology (HLT) has introduced STECs.", "acronyms": [[124, 127], [28, 31], [144, 149]], "long-forms": [[97, 122]], "ID": "1440"}, {"text": "Y? >l LY?l, s.t. SYl = SY?l (1) 1", "acronyms": [[17, 20]], "long-forms": [[23, 27]], "ID": "1441"}, {"text": "given for each environment. For example, Q appeared eight  times in the context EH--EN ( E - n ,  and a check of the  reference list shows that all these occurrences were in the ", "acronyms": [[84, 86], [80, 82]], "long-forms": [[89, 94]], "ID": "1442"}, {"text": "or more previous turns in the dialogue. The third column shows the mean error and standard error (SE) predicted for the model specified by the first two columns. When", "acronyms": [[98, 100]], "long-forms": [[82, 96]], "ID": "1443"}, {"text": "3http://www.noslang.com 370 are the New York Times (NYT),4 SMS,5 and Twitter.6 The results are presented in Figure 1.", "acronyms": [[52, 55]], "long-forms": [[36, 50]], "ID": "1444"}, {"text": "2http://www.nist.gov/speech/tests/mt/ Table 1: Training, development and test data from Basic Travel Expression Corpus(BTEC) Japanese English", "acronyms": [[119, 123]], "long-forms": [[88, 117]], "ID": "1445"}, {"text": "tions. Following Bahdanau et al (2014), we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its capacity in capturing long-distance depen-", "acronyms": [[76, 79]], "long-forms": [[54, 74]], "ID": "1446"}, {"text": "The columns in the first section of the table represent different settings of the p# parameter, with highest performance for each adjusted count model shown in bold. p# values were selected to show a representative range of performance. P = phoneme model; OR = onset-rhyme model; S = syllable model; IR = iterative re-estimation; LM = local minimum strategy. The best performing local minimum model is shaded.", "acronyms": [[300, 302], [330, 332], [256, 258]], "long-forms": [[305, 317], [335, 348], [241, 248], [261, 272], [284, 292]], "ID": "1447"}, {"text": " We consider two resources for training the RCM term: the Paraphrase Database (PPDB) (Ganitkevitch et al, 2013) and WordNet (Fell-", "acronyms": [[79, 83]], "long-forms": [[58, 77]], "ID": "1448"}, {"text": "These are the second-order prepositional complement (PC) and directional complement (LD) relations, and the first-order direct object (OBJ1) and subject (SU) relations. Finally, the setting SU+OBJ1 joins words obtained from subject", "acronyms": [[154, 156], [53, 55], [85, 87], [135, 138]], "long-forms": [[145, 152], [27, 51], [127, 133]], "ID": "1449"}, {"text": "Natural Language Processing (NLP) techniques can be leveraged in detecting events from voluminous social media data. Events are associated with entities and NLP techniques can be applied to extract the entities that are mentioned in the text that defines an event. To perform Named Entity Recognition (NER) on tweets Ritter et. al. (", "acronyms": [[302, 305]], "long-forms": [[276, 300]], "ID": "1450"}, {"text": "10 ESA on senses and Wikipedia Link Measure (WLM) compute similarity on a sense-level, however, sim-", "acronyms": [[45, 48], [3, 6]], "long-forms": [[21, 43]], "ID": "1451"}, {"text": "Test Data Method Accuracy leave-one-out Minnen et al 83.58% Language Model (LM) 86.74% tenfold on development LM 84.72%", "acronyms": [[76, 78], [110, 112]], "long-forms": [[60, 74]], "ID": "1452"}, {"text": "(TEMPO1) and YOUTH-TIME (TEMPO2) where the choice is a secor~d  order Pa~t and for the pair YOUTH-TIME (TEMPO2) and BUILDING-  TIME (TEMPO3) where the choice is a third order F~Jture. As a ", "acronyms": [[133, 139]], "long-forms": [[127, 131]], "ID": "1453"}, {"text": "POS tag distribution. We also use features based on part of speech (POS). We tag using", "acronyms": [[68, 71], [0, 3]], "long-forms": [[52, 66]], "ID": "1454"}, {"text": " MEI is one of the four projects elected for  the Johns Hopkins University (JHU) Summer  Workshop 2000.1 Our research focus is on the ", "acronyms": [[76, 79], [1, 4]], "long-forms": [[50, 74]], "ID": "1455"}, {"text": "the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV).", "acronyms": [[105, 110], [34, 39], [44, 49], [145, 148]], "long-forms": [[91, 99], [128, 139]], "ID": "1456"}, {"text": "Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information", "acronyms": [[132, 134]], "long-forms": [[112, 130]], "ID": "1457"}, {"text": "here we only consider clusters which contain at least one ? Person (PER)? entity.", "acronyms": [[68, 71]], "long-forms": [[60, 66]], "ID": "1458"}, {"text": "Certain CT+ (factual) CT? ( counterfactual) CTu (certain but unknown output) Probable PR+ (probable) PR? ( not probable) [NA]", "acronyms": [[86, 89], [8, 11], [22, 24], [44, 47], [101, 104], [122, 125]], "long-forms": [[91, 99], [111, 119], [49, 75], [28, 42]], "ID": "1459"}, {"text": "There have been p,'evious VCl'sions of I,'1\" (l,cpage 1986)  The I,T llSed ill our wo!'k has been implemented on  MacilLtosh with CLOS (Common Lisp Oh.jeer System)  (l,afeurcade 1993) The realizalion is lmscd mainly on ", "acronyms": [[130, 134], [26, 29]], "long-forms": [[136, 162]], "ID": "1460"}, {"text": "tion database (OID) and provides the corresponding interface;  - Knowledge Retriever (KR) ? retrieves KSs from ", "acronyms": [[86, 88], [15, 18], [102, 105]], "long-forms": [[65, 84]], "ID": "1461"}, {"text": "noisy and potentially unreliable observations.  While scenario template creation (STC) is a difficult problem, its evaluation is arguably more dif-", "acronyms": [[82, 85]], "long-forms": [[54, 80]], "ID": "1462"}, {"text": "email: adam@itri.bton.ac.uk  People have been writing programs for auto-  matic Word Sense Disambiguation (WSD) for  forty years now, yet the validity of the task has ", "acronyms": [[107, 110]], "long-forms": [[80, 105]], "ID": "1463"}, {"text": "Extraction algorithms: ReV = REVERB; Comp = Compression; Data sets: NS = NewsSpike URLs; All = news 2008-2014.", "acronyms": [[83, 87], [23, 26], [37, 41]], "long-forms": [[73, 82], [29, 35], [44, 55]], "ID": "1464"}, {"text": "The dataset used in the CIPS-ParsEval-2010 evaluation is converted from the Tsinghua Chinese Treebank (TCT). There are two subtasks: (1)", "acronyms": [[103, 106], [24, 42]], "long-forms": [[93, 101]], "ID": "1465"}, {"text": "hardware) but also not highly specialized (implying that it is not limited too severely in scope of use). We believe  an architecture composed of a distributed set of processing elements (PE's), each containing local memory and high  speed DSP processors, with a limited interconnecfion a d communication capability may suit our needs.", "acronyms": [[188, 192], [240, 243]], "long-forms": [[167, 186]], "ID": "1466"}, {"text": "lates in the treebank; it is erroneous because close to wholesale needs another layer of structure, namely adjective phrase (ADJP) (Bies et al, 1995, p. 179). ", "acronyms": [[125, 129]], "long-forms": [[107, 123]], "ID": "1467"}, {"text": "of the label candidates. For the supervised method, we use a support vector regression (SVR) model (Joachims, 2006) over all of the features.", "acronyms": [[88, 91]], "long-forms": [[61, 86]], "ID": "1468"}, {"text": "Also,  the dependency framework is arguably closer to  semantics than the phrase structure grammar (PSG)  if the dependency relations are judiciously chosen.", "acronyms": [[100, 103]], "long-forms": [[74, 98]], "ID": "1469"}, {"text": "error criteria: ? WER (word error rate): The WER is computed as the minimum", "acronyms": [[18, 21], [45, 48]], "long-forms": [[23, 38]], "ID": "1470"}, {"text": " ? Fondazione Bruno Kessler (FBK-irst), Italy ?", "acronyms": [[29, 37]], "long-forms": [[3, 27]], "ID": "1471"}, {"text": "segment of a conversation. We will therefore use the  terms initiating conversational participant (ICP) and other  conversational participant(s) (OCP) to distinguish the initi- ", "acronyms": [[99, 102], [146, 149]], "long-forms": [[60, 97], [108, 141]], "ID": "1472"}, {"text": "training data (Surdeanu et al, 2008). We report purity (PU), collocation (CO), and their harmonic mean (F1) evaluated on gold arguments in two set-", "acronyms": [[56, 58], [74, 76]], "long-forms": [[48, 54], [61, 72]], "ID": "1473"}, {"text": "90 Table 1: Comparison of emotion corpora ordered by the amount of annotations (abbreviations: T=tokenization, POS=part-of-speech tagging, L=lemmatization, DP=dependency parsing, NER=Named Entity Recognition). ", "acronyms": [[156, 158], [179, 182], [111, 114]], "long-forms": [[159, 177], [183, 207], [97, 109], [115, 129], [141, 154]], "ID": "1474"}, {"text": " cs.uni-kassel.de/bibsonomy/dumps Content Relevance (CRM) model (Iwata et al, 2009) and Tag Allocation Model (TAM) (Si et al,", "acronyms": [[53, 56], [110, 113]], "long-forms": [[34, 51], [88, 108]], "ID": "1475"}, {"text": "triplet model since it is based on word triplets, is not trained discriminatively but uses the classical maximum likelihood approach (MLE) instead. ", "acronyms": [[134, 137]], "long-forms": [[105, 123]], "ID": "1476"}, {"text": "using the DSO corpus, which contains sentences drawn from two different corpora, namely Brown Corpus (BC) and Wall Street Journal (WSJ). They", "acronyms": [[131, 134], [10, 13], [102, 104]], "long-forms": [[110, 129], [88, 100]], "ID": "1477"}, {"text": "Table 3: Experimental Results (Microsoft?s Provided Train and Test Set) sorted the sentences pairs of the MSRP corpus according to the length difference ratio (LDR) defined in Section 3, and partitioned the sorted cor-", "acronyms": [[160, 163], [106, 110]], "long-forms": [[135, 158]], "ID": "1478"}, {"text": "341  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1070?1080, October 25-29, 2014, Doha, Qatar.", "acronyms": [[93, 98]], "long-forms": [[43, 91]], "ID": "1479"}, {"text": "Information Retrieval (CLIR). It is also important for Machine Translation (MT), especially when the languages do not use the same scripts.", "acronyms": [[76, 78], [23, 27]], "long-forms": [[55, 74], [0, 21]], "ID": "1480"}, {"text": "vantages while considering the consistency, we further propose a global decoding strategy using Integer Linear Programming(ILP). The constraints", "acronyms": [[123, 126]], "long-forms": [[96, 121]], "ID": "1481"}, {"text": "a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al, 1993) are converted into CCG derivations and a", "acronyms": [[118, 121], [18, 25], [163, 166]], "long-forms": [[103, 116]], "ID": "1482"}, {"text": " In Proceedings of the 14th International Conference on World Wide Web (WWW), pages 342?351, Chiba.", "acronyms": [[72, 75]], "long-forms": [[56, 70]], "ID": "1483"}, {"text": "We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven", "acronyms": [[60, 62]], "long-forms": [[35, 58]], "ID": "1484"}, {"text": "tongues. In Proc. of the Text Encoding Initiative 10th Anniversary User Conference (TEI-10). ", "acronyms": [[84, 90], [12, 16]], "long-forms": [[25, 54]], "ID": "1485"}, {"text": "In addition, for some nodes it is necessary to insist that adjunction is mandatory at  a node. In such a case, we say that the node has an Obligatory Adjoining (OA)  constraint.", "acronyms": [[161, 163]], "long-forms": [[139, 159]], "ID": "1486"}, {"text": " 2 Normalized Compression Distance Normalized compression distance (NCD) is a similarity measure based on the idea that a string x is", "acronyms": [[68, 71]], "long-forms": [[35, 66]], "ID": "1487"}, {"text": "UMLS Metathesaurus version 2003AC, the string mammectomy has been assigned the concept-unique identifier C0024881 (CUI), the lemma-unique identifier L0024669 (LUI), and the string-unique identifier S0059711 (SUI).", "acronyms": [[115, 118], [0, 4], [27, 33], [159, 162], [208, 211]], "long-forms": [[105, 113], [125, 157], [173, 206]], "ID": "1488"}, {"text": "........ ? ................... ? ............... + ................... +  CLS = Clause NP = Noun Phrase (BAR 2)  PROD = Predicate $SUBJ -- SUBJECT value of feature ~OL ", "acronyms": [[87, 89], [74, 77], [105, 108], [113, 117], [130, 135], [165, 167]], "long-forms": [[92, 103], [80, 86], [120, 129], [139, 146]], "ID": "1489"}, {"text": "For example, in (2) we find frames identifying baseform verbs (VB) (2a) and frames identifying cardinal numbers (CD) (2b), despite having a variety of context words.", "acronyms": [[113, 115], [63, 65]], "long-forms": [[95, 103], [56, 60]], "ID": "1490"}, {"text": "z that maps sentences x to logical expressions z. We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xi, zi)|i = 1 . . .", "acronyms": [[106, 110]], "long-forms": [[87, 104]], "ID": "1491"}, {"text": "contributions to sentence similarity. In most cases, the longer common sequence (LCS) the two sentences have, the higher similarity score the sentences", "acronyms": [[81, 84]], "long-forms": [[57, 79]], "ID": "1492"}, {"text": "1 Introduction  In this paper we address the event extraction task  defined in Automatic Content Extraction (ACE)1  program.", "acronyms": [[109, 112]], "long-forms": [[79, 107]], "ID": "1493"}, {"text": " Introduction  Categorial Grammars (CGs) consist of two compo-  nents: (i) a lexicon, which assigns syntactic types ", "acronyms": [[36, 39]], "long-forms": [[15, 34]], "ID": "1494"}, {"text": "rent participation week (Curr) and the second using data from the beginning participation week till the current week (TCurr). For the second setup,", "acronyms": [[118, 123]], "long-forms": [[100, 111]], "ID": "1495"}, {"text": "approaches. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 419?424. ", "acronyms": [[82, 86]], "long-forms": [[57, 80]], "ID": "1496"}, {"text": "use(USE) social(SOC) body(BOD)  phy_creation(PCR) mental_creation(MCR)  verbal_creagion (VCR)  These are mostly taken from the classifications ", "acronyms": [[89, 92], [4, 7], [16, 19], [26, 29], [45, 48], [66, 69]], "long-forms": [[72, 87], [0, 3], [9, 15], [21, 25], [32, 44], [50, 65]], "ID": "1497"}, {"text": "1  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 42?47, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[72, 77], [81, 85]], "long-forms": [[38, 70]], "ID": "1498"}, {"text": "4.2 Preprocessing Part?Whole Lexico-Syntactic Patterns Since our discovery procedure is based on the semantic information provided by WordNet, we need to preprocess the noun phrases (NPs) extracted by the three clusters considered and identify the potential part and the whole concepts.", "acronyms": [[183, 186]], "long-forms": [[169, 181]], "ID": "1499"}, {"text": "trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard En-", "acronyms": [[94, 98]], "long-forms": [[66, 92]], "ID": "1500"}, {"text": "based on the design the Princeton English Wordnet.  Arabic Wordnet (AWN) (Elkateb, 2006; Black and Fellbaum, 2006; Elkateb and Fellbaum, 2006) has", "acronyms": [[68, 71]], "long-forms": [[52, 66]], "ID": "1501"}, {"text": "That is, it learns some new knowledge.  Static Interactive Learning (SIL): Whenever the  system encounters a sentence out o f  i t s  processing ", "acronyms": [[69, 72]], "long-forms": [[40, 67]], "ID": "1502"}, {"text": "AJCL = AmericanJournal of Computational  Linguistics (1974-present)  SNLP = Studies in Natural Language Processin~  (Cambridge University Press Monograph ", "acronyms": [[69, 73], [0, 4]], "long-forms": [[76, 114], [7, 52]], "ID": "1503"}, {"text": "tl,...,tM i=2 i=N+I   (7)  which is a Nth-order Markovian chain for the language model (MLM). ", "acronyms": [[88, 91]], "long-forms": [[48, 86]], "ID": "1504"}, {"text": "scikit-learn python library 3 : Naive Bayes (NB), Nearest Neighbor (NN), Decision Tree (DT), Ran-", "acronyms": [[45, 47], [68, 70], [88, 90]], "long-forms": [[32, 43], [50, 66], [73, 86]], "ID": "1505"}, {"text": "  (a)  Support vector machine (SVM) (Vapnik,  1998) is a supervised learning algorithm proposed ", "acronyms": [[31, 34]], "long-forms": [[7, 29]], "ID": "1506"}, {"text": " 2.1 Modeling Votes Ideal point (IP) models are a mainstay in quantitative political science, often applied to voting records to", "acronyms": [[33, 35]], "long-forms": [[20, 31]], "ID": "1507"}, {"text": "1 ? Active Node List (ANL): a list that records all ?", "acronyms": [[22, 25]], "long-forms": [[4, 20]], "ID": "1508"}, {"text": "ple need access to information anywhere, anytime. The  Adaptive Information Management (AIM) service in the  FASiL VPA seeks to automatically prioritise and pre-", "acronyms": [[88, 91], [109, 114], [115, 118]], "long-forms": [[55, 86]], "ID": "1509"}, {"text": "tically justified.  Tree adjoining grammars (TAGs) are also a tree-based  system, ltowever, the major composition operation in ", "acronyms": [[45, 49]], "long-forms": [[20, 43]], "ID": "1510"}, {"text": "tence All the indexes dove ., in which All should be tagged as a predeterminer (PDT).10 Most occurrences of All, however, are as a determiner (DT, 106/135 vs", "acronyms": [[80, 83], [143, 145]], "long-forms": [[65, 78], [131, 141]], "ID": "1511"}, {"text": "The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). ", "acronyms": [[262, 268], [315, 321], [47, 50], [199, 203]], "long-forms": [[236, 253], [279, 313], [165, 180]], "ID": "1512"}, {"text": "would have higher perplexity.  Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally cor-", "acronyms": [[48, 50]], "long-forms": [[41, 46]], "ID": "1513"}, {"text": "Second, in many applications, an effective means of incorporating distributional semantics is Random Indexing (RI). Thus", "acronyms": [[111, 113]], "long-forms": [[94, 109]], "ID": "1514"}, {"text": "the sentences produced. Additionally, automated machine translation (MT) metrics are explored to quantify the amount of information missing from", "acronyms": [[69, 71]], "long-forms": [[48, 67]], "ID": "1515"}, {"text": " 2.1 Tree Substitution Grammars A tree substitution grammar (TSG) is a 4-tuple ?", "acronyms": [[61, 64]], "long-forms": [[34, 59]], "ID": "1516"}, {"text": "ability of reordering models to capture this tag sequence in system translations. Popovic et al (2006) use the relative difference between WER (word error rate) and PER (position independent word error rate) to indicate reordering errors.", "acronyms": [[139, 142], [165, 168]], "long-forms": [[144, 159], [170, 206]], "ID": "1517"}, {"text": "ofwi. Just like the statistical pproaches in many automatic POS tagging programs, our job is to select a  constituent boundary sequence B'with the highest score, P(BIS), from all possible sequences. ", "acronyms": [[164, 167], [60, 63]], "long-forms": [[136, 160]], "ID": "1518"}, {"text": "In Proceedings of the International Conference on World Wide Web (WWW), pages 641?650. ", "acronyms": [[66, 69]], "long-forms": [[50, 64]], "ID": "1519"}, {"text": "A Named Entity Labeler for German: Exploiting Wikipedia and Distributional Clusters. In Proceedings of the Conference on Language Resources and Evaluation (LREC), pages 552?556, La Valletta, Malta.", "acronyms": [[156, 160]], "long-forms": [[121, 139]], "ID": "1520"}, {"text": "     - French/English (fra-eng)    Seven Recognizing Textual Entailment (RTE)  evaluation tracks have already been held: RTE-1 ", "acronyms": [[73, 76]], "long-forms": [[41, 71]], "ID": "1521"}, {"text": "Dependency-Based Open Information Extraction Pablo Gamallo and Marcos Garcia Centro de Investigac?a?o sobre Tecnologias da Informac?a?o (CITIUS) Universidade de Santiago de Compostela", "acronyms": [[137, 143]], "long-forms": [[77, 119]], "ID": "1522"}, {"text": "Accept? validations for reliably deliberate (Rel) and unreliable (URel) subsets of the metaphor production data, given that the", "acronyms": [[66, 70], [45, 48]], "long-forms": [[54, 64], [24, 32]], "ID": "1523"}, {"text": "In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 417?422.", "acronyms": [[89, 93]], "long-forms": [[54, 72]], "ID": "1524"}, {"text": " The graph in figure 2 shows that as the number of relevant documents increases, average precision (AveP) after feedback increases considerably for each extra relevant", "acronyms": [[100, 104]], "long-forms": [[81, 98]], "ID": "1525"}, {"text": " More recently, Galley and Quirk (2011) have introduced linear programming MERT (LP-MERT) as an exact search algorithm that reaches the global op-", "acronyms": [[81, 88]], "long-forms": [[56, 79]], "ID": "1526"}, {"text": "eling approaches. Table 3 shows the corresponding letter error rates (LER). LERs are more compa-", "acronyms": [[70, 73], [76, 80]], "long-forms": [[50, 68]], "ID": "1527"}, {"text": "Winnow and voted-perceptrons (Zhang et al2002;  Collins, 2002), or by using the sequence labeling  models, such as Hidden Markov Models (HMMs)  (Molina and Pla, 2002) and Conditional Random ", "acronyms": [[137, 141]], "long-forms": [[115, 135]], "ID": "1528"}, {"text": "Exact  match rate(EMR), violate match rate(VMR), and  inside match rate(IMR) denote the ratio of three  types of bracketing numbers in all bracketing ", "acronyms": [[72, 75], [18, 21], [43, 46]], "long-forms": [[54, 70], [0, 17], [24, 42]], "ID": "1529"}, {"text": "sequence of ATN arcs which is matched against the  input string. A pattern arc (PAT) has been added to  the ATN formalism with a form similar to that of oth- ", "acronyms": [[80, 83], [108, 111]], "long-forms": [[67, 74], [12, 15]], "ID": "1530"}, {"text": "cific characteristics in form, meaning, function, and distribution. Each entry includes a free text definition, schematic structural description, definitions of construction elements (CEs) and annotated example sentences.", "acronyms": [[184, 187]], "long-forms": [[161, 182]], "ID": "1531"}, {"text": " 3 The V IT  Format   The VIT (short for Verbmobil Interface Term) was  designed as a common output format for the two ", "acronyms": [[26, 29], [7, 11]], "long-forms": [[41, 65]], "ID": "1532"}, {"text": " 2.1 Random Indexing Our first method is based on Random Indexing (RI), introduced by Kanerva (Kanerva, 1988).", "acronyms": [[67, 69]], "long-forms": [[50, 65]], "ID": "1533"}, {"text": " ? Max Similarity (MaxSim): For tuple ? in an", "acronyms": [[19, 25]], "long-forms": [[3, 17]], "ID": "1534"}, {"text": "vided for training data.  the use of well-motivated Lexical Structures (LS's)  to capture the presuppositional nd anaphoric as- ", "acronyms": [[72, 76]], "long-forms": [[52, 70]], "ID": "1535"}, {"text": " The CBDF similarity values between  100,000-word subsets of Original French (OF),  French translated from English (EF), from ", "acronyms": [[78, 80], [5, 9], [116, 118]], "long-forms": [[61, 76], [84, 114]], "ID": "1536"}, {"text": " 4.4 Bag of Words using Maximum Entropy (MaxEnt) Classifier We include Maximum Entropy classifier using sim-", "acronyms": [[41, 47]], "long-forms": [[24, 39]], "ID": "1537"}, {"text": " (Markov) that significantly differed from the output of the noisy channel model (NoisyC), which confirms our finding that Markovized models can pro-", "acronyms": [[82, 88]], "long-forms": [[61, 74]], "ID": "1538"}, {"text": "developing a ser ies of inc reas ing ly  soph is t icated natura l  language unders tand ing   systems which will serve as an in tegrated  in ter face  to severa l  faci l i t ies at the Pacif ic  F leet Command Center:  the In tegrated  Data Base (IDB), which conta ins  information  about  ships, the i r  read iness  s tates ,  the i r  capabi l i t ies,  etc.;", "acronyms": [[249, 252]], "long-forms": [[225, 247]], "ID": "1539"}, {"text": "= Europarl). TOOL = grammatical words, PCT/NB = punctuation and numbers, ADJ/ADV = adjectives and adverbs, NAM = proper name, NOM = noun,", "acronyms": [[39, 45], [73, 80], [107, 110], [126, 129]], "long-forms": [[48, 71], [83, 105], [120, 124], [132, 136]], "ID": "1540"}, {"text": "Table 4. Comparative results in AIMed. The number of positive instances (POS) and negative instances (NEG) and macro-averaged precision (ma-P), recall (ma-R) and F1-score (ma-F) are shown.", "acronyms": [[73, 76], [32, 37], [102, 105], [137, 141], [152, 156], [172, 176]], "long-forms": [[53, 71], [82, 100], [111, 135], [144, 150], [162, 170]], "ID": "1541"}, {"text": "fast method to train SVM. SMO breaks the large  quadratic programming (QP) optimization problem needed to be resolved in SVM into a series ", "acronyms": [[71, 73], [21, 24], [26, 29], [121, 124]], "long-forms": [[48, 69]], "ID": "1542"}, {"text": "each cross level text pair, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (SPh), Phrase to Word (Ph-W) and Word to Sense (W-Se).", "acronyms": [[105, 109], [57, 60], [83, 86], [130, 134]], "long-forms": [[89, 103], [34, 55], [63, 81], [115, 128]], "ID": "1543"}, {"text": "2012. Disclosure and Quality of Answers in Text and Voice Interviews on iPhones. 67th Annual Conference of the Ameri-can Association for Public Opinion Research (AAPOR) Orlando, FL. Jongho Shin, Shrikanth Narayanan, Laurie Gerber, Abe Kazemzadeh, and Dani Byrd.", "acronyms": [[162, 167], [178, 180]], "long-forms": [[121, 160]], "ID": "1544"}, {"text": "DUC 2007. In Proceedings of the Seventh Document Understanding Conference (DUC), Rochester, NY.", "acronyms": [[75, 78], [0, 3], [92, 94]], "long-forms": [[40, 73]], "ID": "1545"}, {"text": "  2 NII-Speech Resources Consortium  The National Institute of Informatics (NII) was  founded in Tokyo, Japan in April 2000 as an in-", "acronyms": [[76, 79], [4, 7]], "long-forms": [[41, 74]], "ID": "1546"}, {"text": " 5.2 Named Entities As standard named entity recognition (NER) systems do not capture categories that are relevant to", "acronyms": [[58, 61]], "long-forms": [[32, 56]], "ID": "1547"}, {"text": " P rev ious  Accomplishments  We have previously constructed a UNIX Consultant (UC), an intelligent NL-capable \"help\"  facility that allows naive users to learn about the UNIX operating system.", "acronyms": [[80, 82], [100, 102], [171, 175]], "long-forms": [[63, 78]], "ID": "1548"}, {"text": "ously capture the semantics of words and sentences, such as latent semantic analysis (LSA) or latent Dirichlet alocation (LDA). However, our", "acronyms": [[122, 125], [86, 89]], "long-forms": [[94, 120], [60, 84]], "ID": "1549"}, {"text": "son mari.  Les confessions (CO) is much most faithful to the content, yet, the translator has significantly departed", "acronyms": [[28, 30]], "long-forms": [[15, 26]], "ID": "1550"}, {"text": "Artola Xo (1993)o \"ilIZTSUA: lIiztegi-sistema urDrt.lc  a(lime~dunaren sorkuntza eta eraikuntza /Conccption  d'un syst~,me intelligent d'aide dictionnarialc (SIAl))\"  Ph.D. Thesis.", "acronyms": [[158, 162]], "long-forms": [[114, 156]], "ID": "1551"}, {"text": "HOO) pro-posed and initiated a shared task in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the problem by developing tools or techniques for the non-native speaker of English, which will automat-ically correct the English prose of the papers so that they can be accepted. This tools and tech-niques may also help native English speakers. This task is simply expressed as text-to-text generation or Natural language Generation (NLG). In the 2011 shared task, all possible errors were covered which made the task enormously huge.", "acronyms": [[441, 444], [0, 3]], "long-forms": [[412, 439]], "ID": "1552"}, {"text": "tools freely available for many languages. That is the case for morphosyntactic analyzers (MSA), but not yet for full or even shallow parsers.", "acronyms": [[91, 94]], "long-forms": [[64, 89]], "ID": "1553"}, {"text": "The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). ", "acronyms": [[129, 131], [28, 31], [104, 107]], "long-forms": [[113, 127], [4, 26], [74, 102]], "ID": "1554"}, {"text": "2003. MDA Guide Version 1.0.1. Technical report, Object Management Group (OMG). ", "acronyms": [[74, 77], [6, 9]], "long-forms": [[49, 72]], "ID": "1555"}, {"text": "models use a word embedding size of 200, whereas the hidden layer(s) size is fixed at 400, with all hidden units using the Rectified Linear Unit (ReLu) i.e., f(x) = max(0, x) as activation function.", "acronyms": [[146, 150], [165, 168]], "long-forms": [[123, 144]], "ID": "1556"}, {"text": "P2 = < ash, c >}.  3-3 Mixed String Adjunct Language (MAL)  We now have two different styles of rules in G, namely, the ", "acronyms": [[54, 57]], "long-forms": [[23, 52]], "ID": "1557"}, {"text": "known that Figure 1(a) can be represented by an equivalent hierarchical Chinese Restaurant Process (CRP) (Aldous, 1985) as in Figure 1(b). ", "acronyms": [[100, 103]], "long-forms": [[72, 98]], "ID": "1558"}, {"text": "The DM is bracketed between two other components, the Input Manager (IM) and the Output Manager (OM). The", "acronyms": [[97, 99], [4, 6], [69, 71]], "long-forms": [[81, 95], [54, 67]], "ID": "1559"}, {"text": "pears or no other attribute is included.  Surface Text (ST): To measure the effectiveness of the semantic analysis (attribute labels and ", "acronyms": [[56, 58]], "long-forms": [[42, 54]], "ID": "1560"}, {"text": "up in the bracketed terminal string as insertion o f \"  (* *) \" surround-  ing \" 1970 \".) Inverse noun phrase preposing (NPPREPOS), which  relates such surface pairs as \" GM's sales \" and \" the sales of GM \", ", "acronyms": [[121, 129], [171, 173], [203, 205]], "long-forms": [[98, 119]], "ID": "1561"}, {"text": "each frame from each camera view.  3.3 Dyadic Features (DF)  All of the features discussed above are low-level ", "acronyms": [[56, 58]], "long-forms": [[39, 54]], "ID": "1562"}, {"text": "This problem  is of practical interest for the design of various types  of natural anguage interfaces (NLI's) that make use of  different knowledge sources.", "acronyms": [[103, 108]], "long-forms": [[75, 101]], "ID": "1563"}, {"text": "course Connectives. Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT). ", "acronyms": [[93, 96]], "long-forms": [[58, 91]], "ID": "1564"}, {"text": "Univers i ty  of Vienna  The first part of this paper is dedicated to an overv iew  of the parser of the system VIE-LANG (Viennese Language  Understanding System).", "acronyms": [[112, 120]], "long-forms": [[122, 139]], "ID": "1565"}, {"text": "ZZ initial optional verb complements  statement(Q) -...  verb_complementsO(VC). ", "acronyms": [[75, 77], [0, 2]], "long-forms": [[57, 73]], "ID": "1566"}, {"text": "constructions. The notion of construction is similar to the one in Construction Grammar (CxG)4, as in (Goldberg, 1995), where:", "acronyms": [[89, 92]], "long-forms": [[67, 87]], "ID": "1567"}, {"text": "Errors are italicized and marked in red.  LDA with phrases (LDA-P): As aspect-sentiment phrases are often noun phrases, a basic approach", "acronyms": [[60, 65]], "long-forms": [[42, 58]], "ID": "1568"}, {"text": "They thus attract research from many different and beneficial perspectives. Dialog acts (DAs) (Searle, 1969), which reflect the functions that ut-", "acronyms": [[89, 92]], "long-forms": [[76, 87]], "ID": "1569"}, {"text": " 2. Computer  Aided Wri t ing (CAW)  A computer system for a writer is basically a ", "acronyms": [[31, 34]], "long-forms": [[4, 29]], "ID": "1570"}, {"text": "The most comparable tools to MT-EQuAl are PET (Aziz et al., 2012), COSTA (Chatzitheodorou and Chatzistamatis, 2013), TAUS DQF framework,", "acronyms": [[67, 72], [29, 36], [42, 45], [117, 121], [122, 125]], "long-forms": [[74, 108]], "ID": "1571"}, {"text": "7-3-1, Hongo, Bunkyo-ku  Tokyo 113, Japan  It is difficult for a natural language understanding system (NLUS) to deal  with ambiguities.", "acronyms": [[104, 108]], "long-forms": [[65, 102]], "ID": "1572"}, {"text": "clude substitution, splitting and merging statistics. Given  an input (ASCII) word, and the above statistics, candidate  (corrupted) words are generated based on simulating and pro- ", "acronyms": [[71, 76]], "long-forms": [[61, 69]], "ID": "1573"}, {"text": "  The project used the Linguamatics Interactive  Information Extraction (I2E) platform. This ", "acronyms": [[73, 76]], "long-forms": [[49, 71]], "ID": "1574"}, {"text": "example, the prepObject )f a LOCATION-PP must be a  PLACE-NOUN. A description of \"on AI\" (as in \"book on  AI\") as a LOCATION-PP c~Id  not be constructed since AI ", "acronyms": [[85, 88], [159, 161], [29, 40], [52, 62], [116, 127]], "long-forms": [], "ID": "1575"}, {"text": "fying the native language based on the manner of speaking and writing a second language is borrowed from Second Language Acquisition (SLA), where this is known as language transfer.", "acronyms": [[134, 137]], "long-forms": [[105, 132]], "ID": "1576"}, {"text": "Figure 2: Heat map of the relevance scores w s, j between the target domain Usenet (UN) with the other domains on ACE 2004 data set.", "acronyms": [[84, 86], [114, 117]], "long-forms": [[76, 82]], "ID": "1577"}, {"text": " Conf. on Language Resources and Evaluation (LREC). ", "acronyms": [[45, 49]], "long-forms": [[10, 28]], "ID": "1578"}, {"text": "pus (Mitchell et al, 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based par-", "acronyms": [[75, 80], [48, 53]], "long-forms": [[59, 73], [38, 46]], "ID": "1579"}, {"text": "Background (BKG) the background of the study Problem (PROB) the research problem Method (METH) the methods used Result (RES) the results achieved", "acronyms": [[89, 93], [12, 15], [54, 58], [120, 123]], "long-forms": [[81, 87], [0, 10], [45, 52], [112, 118]], "ID": "1580"}, {"text": "sets used in the fast Tree Kernel.  2.3 A Fast Tree Kernel (FTK) To compute the kernels defined in the previous", "acronyms": [[60, 63]], "long-forms": [[42, 58]], "ID": "1581"}, {"text": " 1 Introduction Coreference resolution (CoRe) is the process of finding markables (noun phrases) referring to the same", "acronyms": [[40, 44]], "long-forms": [[16, 38]], "ID": "1582"}, {"text": "retrieval with locality information using smart. In  Text retrieval conferenc (TREC-1) (pp. 59-72).", "acronyms": [[79, 85], [88, 90]], "long-forms": [[53, 77]], "ID": "1583"}, {"text": "A similar embedding method, called ? Global Vector (GloVe)?, was", "acronyms": [[52, 57]], "long-forms": [[37, 50]], "ID": "1584"}, {"text": "Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney", "acronyms": [[83, 87]], "long-forms": [[62, 81]], "ID": "1585"}, {"text": "we use three categories for the identically spelled words: (a) we use the term true equivalents (TE) to refer to the pairs that have the same", "acronyms": [[97, 99]], "long-forms": [[79, 95]], "ID": "1586"}, {"text": "unified into one model. We refer to this model as the Unified Transition(UT) model. ", "acronyms": [[73, 75]], "long-forms": [[54, 72]], "ID": "1587"}, {"text": "3 HCMUS 6L OpenNLP OpenNLP Dict Rules - Table 2: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer,", "acronyms": [[113, 115]], "long-forms": [[116, 132]], "ID": "1588"}, {"text": "mance figures are the standard measures used for this task: F-measure (harmonic mean of recall and precision) and slot error rate (SER), where separate type, extent and content error measures are averaged to get the reported result.", "acronyms": [[131, 134]], "long-forms": [[114, 129]], "ID": "1589"}, {"text": "SVO = Subject-Verb-Object GE = General Event PE = Predefined Event Rule-based", "acronyms": [[45, 47], [0, 3], [26, 28]], "long-forms": [[50, 66], [6, 25], [31, 44]], "ID": "1590"}, {"text": "with common sense knowledge.       Natural language processing (NLP) techniques  such as part of speech tagging and parse tree gen-", "acronyms": [[64, 67]], "long-forms": [[35, 62]], "ID": "1591"}, {"text": "ing Language Understanding Engine), and subsequently analyze its performance on the task?s shared texts. BLUE consists of a pipeline of a parser, logical form (LF) generator, an initial logic generator, and subsequent processing modules.", "acronyms": [[160, 162]], "long-forms": [[146, 158]], "ID": "1592"}, {"text": "grading.  The Content Assessment Module (CAM) presented in Bailey (2008) and Bailey and Meurers", "acronyms": [[41, 44]], "long-forms": [[14, 39]], "ID": "1593"}, {"text": "1991), can be characterized asknowledge-rich  in that they presuppose that known lexical  items possess Conceptual Dependence(CD)-  like descriptions.", "acronyms": [[126, 128]], "long-forms": [[104, 124]], "ID": "1594"}, {"text": " Entities On the level of entity extraction, Named Entities (NE) were defined as proper names and quantities of interest. ", "acronyms": [[61, 63]], "long-forms": [[45, 59]], "ID": "1595"}, {"text": "and Causal Relations. In proceedings of the Association for Computational Linguistics (ACL). ", "acronyms": [[87, 90]], "long-forms": [[50, 85]], "ID": "1596"}, {"text": "PLC = partition left context  (has been done)  PRC = partition right context  (yet to be done) ", "acronyms": [[47, 50], [0, 3]], "long-forms": [[53, 76], [6, 28]], "ID": "1597"}, {"text": "In our particular application, access to Getty?s Art and Architecture Thesaurus (AAT), to other museum and collection databases or online auction cata-", "acronyms": [[81, 84]], "long-forms": [[49, 79]], "ID": "1598"}, {"text": " 2.3 Named Entity Recognition Named Entity Recognition (NER) is the task of finding all instances of explicitly named entities", "acronyms": [[56, 59]], "long-forms": [[30, 54]], "ID": "1599"}, {"text": "37  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, page 97, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[73, 78]], "long-forms": [[39, 71]], "ID": "1600"}, {"text": "The idea is simply to count the mmlber of new  words introduced ow'x a moving interval and 1)roduce  what he calls a vocabulary managemenl profile (VMI'),  or lneasurements at intervals.", "acronyms": [[148, 152]], "long-forms": [[117, 146]], "ID": "1601"}, {"text": " In order to acquire labeled instances for training, we decompose the gold standard (GS) events into multiple events with single arguments.", "acronyms": [[85, 87]], "long-forms": [[70, 83]], "ID": "1602"}, {"text": " 310 X. Fan et al  Causality Na?ve Bayesian Classifier (CNB): For a document represented by a binary-valued vector d=(X1 ,X2 , ?,", "acronyms": [[56, 59]], "long-forms": [[19, 43]], "ID": "1603"}, {"text": "subject and object with the ground truth. ETS/ETO = Emotions towards subject/object, MAS=mean absolute error, and RMSE= root mean square error", "acronyms": [[85, 88], [42, 49], [114, 118]], "long-forms": [[89, 102], [52, 83], [120, 142]], "ID": "1604"}, {"text": "3 Experiments We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong", "acronyms": [[96, 98], [132, 135]], "long-forms": [[79, 94]], "ID": "1605"}, {"text": "following are the most frequently used ones: ? MS = Mel?c?uk style used in the MeaningText Theory (MTT): the first conjunct is the", "acronyms": [[47, 49]], "long-forms": [[52, 66]], "ID": "1606"}, {"text": "can be specified.  The 'source condition(SCND)' represents  conditions on variables in the 'source pattern.'", "acronyms": [[41, 45]], "long-forms": [[24, 39]], "ID": "1607"}, {"text": "Performing proper Arabic dialect identification may positively impact many Natural Language Processing (NLP) application.", "acronyms": [[104, 107]], "long-forms": [[75, 102]], "ID": "1608"}, {"text": "a baseline.  Language Model (LM): We model the semantic fit of a candidate substitute within the given context", "acronyms": [[29, 31]], "long-forms": [[13, 27]], "ID": "1609"}, {"text": "compared to the baseline and stem, respectively.  As for the Kirghiz to Chinese translation (KI-CH) task, improvements seem relative small compared", "acronyms": [[93, 98]], "long-forms": [[61, 79]], "ID": "1610"}, {"text": "are not very demanding on resources: Inverse Consultation (IC) (Tanaka and Umemura, 1994) and Distributional Similarity (DS) (Kaji et al, 2008), their strong points and weaknesses, and proposed", "acronyms": [[121, 123], [59, 61]], "long-forms": [[94, 119], [37, 57]], "ID": "1611"}, {"text": "work for sentence level feature extraction. In the Window Processing component, each token is further represented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2). Then, the", "acronyms": [[132, 134], [159, 161]], "long-forms": [[117, 130], [140, 157]], "ID": "1612"}, {"text": "The seed set is compiled from popular mental and emotional state dictionaries, including the Profile of Mood States (POMS) (McNair et al.,", "acronyms": [[117, 121]], "long-forms": [[93, 115]], "ID": "1613"}, {"text": ")( bwN We use the corpus provided by IR task of  NTCIR2 (NTCIR 2002) as the training set to  compute the mutual information of words.", "acronyms": [[49, 55], [37, 39]], "long-forms": [[57, 67]], "ID": "1614"}, {"text": "4.3 Nested Expressions No nested expressions will be marked. Even in cases where LOCATION (ENAMEX) expressions occur within  TIMEX and NUMEX expressions, they are not to be tagged.", "acronyms": [[91, 97], [125, 130], [135, 140]], "long-forms": [[61, 89]], "ID": "1615"}, {"text": " 3 Bridgeman Art Library Bridgeman Art Library (BAL)2 is one of the world?s top image libraries for art, culture and history.", "acronyms": [[48, 51]], "long-forms": [[25, 46]], "ID": "1616"}, {"text": "1 Introduction Newswire text has long been a primary target for natural language processing (NLP) techniques such as information extraction, summarization, and ques-", "acronyms": [[93, 96]], "long-forms": [[64, 91]], "ID": "1617"}, {"text": "In this example, only Midas can be chosen for the role of twit, but any member of the class PN (proper names) having the attributes male, brave and handsome can be", "acronyms": [[92, 94]], "long-forms": [[96, 108]], "ID": "1618"}, {"text": "cannot co-exist on nouns. Next comes the class of particle proclitics (PART+): +  l+ ?", "acronyms": [[71, 76]], "long-forms": [[50, 69]], "ID": "1619"}, {"text": "it as a model for compiling their own corpora.  The Russian National Corpus (RNC) has been released by the group of specialists from different organi-", "acronyms": [[77, 80]], "long-forms": [[52, 75]], "ID": "1620"}, {"text": "The alignments produced by MEBA were  compared to the ones produced by YAWA and  evaluated against the Gold Standard (GS)1 annotations used in the Word Alignment Shared ", "acronyms": [[118, 120], [27, 31], [71, 75]], "long-forms": [[103, 116]], "ID": "1621"}, {"text": "In recent decades, this idea appears in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. ", "acronyms": [[204, 207]], "long-forms": [[209, 238]], "ID": "1622"}, {"text": "2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (deter-", "acronyms": [[69, 72], [86, 89], [100, 104], [116, 119]], "long-forms": [[74, 83], [91, 97], [106, 113], [121, 126]], "ID": "1623"}, {"text": "ond, task B referred to as task of normalization involves the mapping of each disorder mention to a  UMLS concept unique identifier (CUI).The mapping was limited to UMLS CUI of SNOMED clin-", "acronyms": [[133, 136]], "long-forms": [[106, 131]], "ID": "1624"}, {"text": " We follow Curran (2004) and use two performance measures: direct matches (DIRECT) and inverse rank (INVR).", "acronyms": [[75, 81], [101, 105]], "long-forms": [[59, 73], [87, 99]], "ID": "1625"}, {"text": "We further assume that these documents have been tagged with Named Entities (NE)s with coreferences resolved 807", "acronyms": [[77, 79]], "long-forms": [[61, 75]], "ID": "1626"}, {"text": "where P is the Macro Precision and R is the Macro Recall. We also use tree induced error (TIE) in the experiments.", "acronyms": [[90, 93]], "long-forms": [[70, 88], [21, 30], [50, 56]], "ID": "1627"}, {"text": "Two criteria are used:  1. Overlapping Ambiguity Strings (OAS): the  reference segmentation and the segmenter ", "acronyms": [[58, 61]], "long-forms": [[27, 56]], "ID": "1628"}, {"text": "Note Table 3: Statistics of training and test corpus for the Canadian Hansards task (PP=perplexity). ", "acronyms": [[85, 87]], "long-forms": [[88, 98]], "ID": "1629"}, {"text": "ance improvements in our system.  The OOV recall rates (RRoov) showed in  Table 4 demonstrate that the OOV recognition ", "acronyms": [[56, 61], [38, 41], [103, 106]], "long-forms": [[42, 54]], "ID": "1630"}, {"text": " ? REL = relation; ARG = NP/VP/ADJ (6) ACADE?MIQUE = Qui manque d?originalite?,", "acronyms": [[3, 6], [19, 22], [39, 50]], "long-forms": [[9, 17], [25, 34], [53, 78]], "ID": "1631"}, {"text": "is still room for improvement.  2 The Index of Productive Syntax (IPSyn) The Index of Productive Syntax (Scarborough,", "acronyms": [[66, 71]], "long-forms": [[38, 64]], "ID": "1632"}, {"text": "  Parallel corpora Size of English texts (in  million words (MB))  Size of Chinese texts (in ", "acronyms": [[61, 63]], "long-forms": [[46, 59]], "ID": "1633"}, {"text": "Available for SRL tasks are efforts such as PropBank (Palmer et al, 2005) and FrameNet (Fillmore et al, 2003) that have developed semantic role labels (based on differing approaches) and have labeled large corpora for training and testing of SRL systems. PropBank (PB) identifies and labels the semantic arguments of the verb on a verb-by-verb basis, creating a separate frameset that includes verb specific semantic roles to account for each subcategorization frame of the verb. Much like PB, FrameNet (FN) identifies and labels semantic roles, known as Frame Elements, around a relational target, usually a verb.2 But unlike PB, Frame Elements less verb specific, but rather are defined in terms of semantic structures called frames evoked by the verb. That is, one or more verbs can be associated with a single semantic frame.", "acronyms": [[504, 506], [14, 17], [242, 245], [265, 267]], "long-forms": [[494, 502], [255, 263]], "ID": "1634"}, {"text": "A necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference (NLI): the task of determining whether a natural-language", "acronyms": [[138, 141]], "long-forms": [[110, 136]], "ID": "1635"}, {"text": "transcripts produced with Automatic Speech Recognition  (ASR) systems tend to contain many recognition errors,  leading to low Information Retrieval (IR) performance  (Oard et al, 2007).", "acronyms": [[150, 152], [57, 60]], "long-forms": [[127, 148], [26, 54]], "ID": "1636"}, {"text": "qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, np), qdet?1.0 DET(the),", "acronyms": [[52, 54], [8, 10], [30, 32], [76, 79]], "long-forms": [[56, 60]], "ID": "1637"}, {"text": "shown by Kusner and colleagues (2015), semantic representations such as Latent Semantic Indexing and Latent Dirichlet Allocation (LDA) can outperform a BoW representation.", "acronyms": [[130, 133], [152, 155]], "long-forms": [[101, 128]], "ID": "1638"}, {"text": "This usage is domain specific and the referent is fixed as a second person subject.  pora: 1) three transcripts of family conversation (FaCon) drawn from Australian English Corpus (Monash University 1996~1998) collecting family interviews about their past holidays (Nariyama 2004); 2) three 30-minute-TV Australian drama transcripts (TV) (Nariyama 2004); 3) Switchboard corpus consisting of telephone conversation on a variety of specified every day topics (Cote 1996).  Referent   FaCon TV dramas Switchboard I we you he/she it they ", "acronyms": [[136, 141], [482, 487], [334, 336], [301, 303]], "long-forms": [[115, 134]], "ID": "1639"}, {"text": "query, and their performance asymptotes by the time they get to the second query.  This effect is confirmed by an analysis of variance (ANOVA)8, which shows a highly significant effect of order of presentation (F = 9.8427; p< .0001).", "acronyms": [[136, 141]], "long-forms": [[114, 134]], "ID": "1640"}, {"text": " 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle", "acronyms": [[28, 30]], "long-forms": [[16, 26]], "ID": "1641"}, {"text": "text refer to the same entity in real world or not.  Noun Phrase CRR (NP-CRR) considers all noun  phrases as entities, while Named Entity CRR ", "acronyms": [[70, 76], [138, 141]], "long-forms": [[53, 68]], "ID": "1642"}, {"text": "RLP = rule  learner prediction. RS = Reference Standard   ", "acronyms": [[32, 34], [0, 3]], "long-forms": [[37, 55], [6, 30]], "ID": "1643"}, {"text": "follows:  1. If the topic is an individual constant (IC), establish  restrictions (using the Restricts link), if any, on the ", "acronyms": [[53, 55]], "long-forms": [[32, 51]], "ID": "1644"}, {"text": "{EVENT 2 {AND {SUBTYPE DIE} {PERSON  $foo}}}  2.4 Graphical User Interface (GUI)  For some applications such as database ", "acronyms": [[76, 79]], "long-forms": [[50, 74]], "ID": "1645"}, {"text": "The proceedings from CoNLL2004 and  CoNLL2005 detail a wide variety of approaches  to Semantic Role Labeling (SRL).  Many re-", "acronyms": [[110, 113], [21, 30], [36, 45]], "long-forms": [[86, 108]], "ID": "1646"}, {"text": "We use the same evaluation metrics as in (McDonald et al, 2005). Dependency accuracy (DA) is the proportion of non-root words that are assigned the", "acronyms": [[86, 88]], "long-forms": [[65, 84]], "ID": "1647"}, {"text": " 1 Overv iew o f  the  IPS  pro jec t   The IPS (Interactive Parsing System) research  project, at the Linguistics Departement of the ", "acronyms": [[44, 47], [23, 26]], "long-forms": [[49, 75]], "ID": "1648"}, {"text": "Abstract  This article focuses on the development of  Natural Language Processing (NLP) tools for  Computer Assisted Language Learning ", "acronyms": [[83, 86]], "long-forms": [[54, 81]], "ID": "1649"}, {"text": "are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from citation texts (CT). ", "acronyms": [[143, 145], [55, 57], [74, 76], [83, 85]], "long-forms": [[127, 141], [39, 53], [63, 72]], "ID": "1650"}, {"text": "1 Quantification resolution We are concerned with ambiguously quantified noun phrases (NPs) and their interpretation, as illustrated by the following examples:", "acronyms": [[87, 90]], "long-forms": [[73, 85]], "ID": "1651"}, {"text": "mantic relations between referents. This task has a long tradition in natural language processing (NLP) since the early days of artificial intelligence (Web-", "acronyms": [[99, 102]], "long-forms": [[70, 97]], "ID": "1652"}, {"text": "bbai@nec-labs.com Abstract We develop a recursive neural network (RNN) to extract answers to arbitrary natural language", "acronyms": [[66, 69]], "long-forms": [[40, 64]], "ID": "1653"}, {"text": "sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems. ", "acronyms": [[131, 133]], "long-forms": [[110, 129]], "ID": "1654"}, {"text": " We used Moses (Koehn et al 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al 2011) to trans-", "acronyms": [[91, 93], [109, 114]], "long-forms": [[75, 89]], "ID": "1655"}, {"text": "terms encountered.  Shallow Syntactic (SSyn) features consider the number and ratios of common part-of-speech", "acronyms": [[39, 43]], "long-forms": [[20, 37]], "ID": "1656"}, {"text": "the verb that contained in a subordinate clause.  We use semantic role labeling (SRL) to help  solve this problem in which the coordinated can ", "acronyms": [[81, 84]], "long-forms": [[57, 79]], "ID": "1657"}, {"text": "We performed the same experiments on three dif-  ferent corpora:  Corpus SN (Spanish Novel) train: 15Kw, test:  2Kw, tag set size: 70.", "acronyms": [[73, 75]], "long-forms": [[77, 90]], "ID": "1658"}, {"text": " NONHUMAN (animals), DNA, RNA, PROTEIN, CONTROL (control measures to contain the disease), BACTERIA, CHEMICAL and SYMPTOM.", "acronyms": [[40, 47], [21, 24], [26, 29], [31, 38], [91, 99], [101, 109], [114, 121], [1, 9]], "long-forms": [[49, 56], [11, 18]], "ID": "1659"}, {"text": "In particular, the work of (Pietra et al, 1997) is inspiring to us, but the improved iterative scaling (IIS) method for parameter estimation and the Gibbs sampler", "acronyms": [[104, 107]], "long-forms": [[76, 102]], "ID": "1660"}, {"text": " 3 Dataset & Experimental Setup We use the First Certificate in English (FCE) ESOL examination scripts2 (upper-intermediate level as-", "acronyms": [[73, 76]], "long-forms": [[43, 71]], "ID": "1661"}, {"text": "We also mark conjunct clauses with the feature nosubj if they are neither headed by an imperative nor contain a child node with the grammatical function SB (subject) or EP (expletive). This is useful in order to correctly parse", "acronyms": [[153, 155], [169, 171]], "long-forms": [[157, 164], [173, 182]], "ID": "1662"}, {"text": " 5.3   Learning Algorithm: Conditional Random Field  Conditional Random Fields (CRF) is a formalism well-suited for learning and prediction on sequential data.", "acronyms": [[80, 83]], "long-forms": [[53, 78]], "ID": "1663"}, {"text": "  Abstract  In an attempt to extend Penn Discourse Tree Bank (PDTB) / Turkish Discourse Bank (TDB)  style annotations to spoken Turkish, this paper presents the first attempt at annotating the explicit ", "acronyms": [[62, 66], [94, 97]], "long-forms": [[36, 60], [70, 92]], "ID": "1664"}, {"text": "The relative clause itself has the category S; the incoming edge is labeled RC (relative clause). ", "acronyms": [[76, 78]], "long-forms": [[80, 95]], "ID": "1665"}, {"text": "words. We model semantic relatedness between two words using the Information Content (IC) of the pair in a method similar to the one used by Lin", "acronyms": [[86, 88]], "long-forms": [[65, 84]], "ID": "1666"}, {"text": "ber of occurrence for this feature per patient narrative is obtained based on the frequency of the coordinating conjunction PoS tag (CC) detected in the parse tree structure.", "acronyms": [[133, 135]], "long-forms": [[112, 123]], "ID": "1667"}, {"text": "Economics neighborhood fbank  bank  Subject Code EC = Economies  account cheque money by ", "acronyms": [[49, 51]], "long-forms": [[54, 63]], "ID": "1668"}, {"text": "five folds and evaluate them on the fifth 29 Table 1: Results for passage retrieval for TREC-QA using disjoint windows (DW) and sliding windows (SW). ??", "acronyms": [[120, 122], [88, 95], [145, 147]], "long-forms": [[102, 118]], "ID": "1669"}, {"text": "Having presented the sequent parser, we now show its embedding in the learning algorithm GraSp (Grammar of Speech). ", "acronyms": [[89, 94]], "long-forms": [[96, 113]], "ID": "1670"}, {"text": "2007) from 4 domains, labeled positive or negative. We apply logistic regression (LR) and SVM using unigram and bigram features.", "acronyms": [[82, 84], [90, 93]], "long-forms": [[61, 80]], "ID": "1671"}, {"text": "\u0000 -movement (mostly wh-movement: WH), empty complementizers (COMP), empty units (UNIT), and traces representing pseudo-attachments", "acronyms": [[61, 65], [33, 35], [81, 85]], "long-forms": [[44, 59], [74, 78]], "ID": "1672"}, {"text": "They  ha 1 Personal Name (PN); Date or Time (DT); Location Name  (LN); Team Name (TN); Competition Title (CT); Personal ", "acronyms": [[26, 28], [45, 47], [66, 68], [82, 84], [106, 108]], "long-forms": [[11, 24], [31, 43], [50, 63], [71, 80], [87, 104]], "ID": "1673"}, {"text": "Topic 1 Other modules Greeting(GR) Keep silence(KS) Figure 2: Overview of the information navigation", "acronyms": [[31, 33], [48, 50]], "long-forms": [[22, 29], [35, 46]], "ID": "1674"}, {"text": "Proceedings of the Fourteenth  International Joint Conference in Artificial  Intelligence (IJCAI?95), pp. 1395 ?", "acronyms": [[91, 99]], "long-forms": [[77, 89]], "ID": "1675"}, {"text": "for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland. ", "acronyms": [[145, 149], [49, 52]], "long-forms": [[106, 143]], "ID": "1676"}, {"text": "Figure 1: Sample transcript from a TD child 3 Narrative Topic Analysis Using LDA Latent Dirichlet Allocation (LDA) (Blei et al 2003) has been used in NLP to model topics within", "acronyms": [[110, 113]], "long-forms": [[81, 108]], "ID": "1677"}, {"text": "of which are limited in scope. We here restrict inferables to the particular subset de-  fined by Hahn, Markert, and Strube (1996), which we call functional anaphora (FA). ", "acronyms": [[167, 169]], "long-forms": [[146, 165]], "ID": "1678"}, {"text": "sulting matrices be M1 and M2, respectively. In step (3), SentIDs (sentences where the two words appear with the specified relation) are obtained by", "acronyms": [[58, 65]], "long-forms": [[67, 96]], "ID": "1679"}, {"text": "One is manually typing the text transcriptions which we regarded as the Correct Recognition Result (CRR) transcription, and another is the ASR result which", "acronyms": [[100, 103], [139, 142]], "long-forms": [[72, 98]], "ID": "1680"}, {"text": "1985) in which Tree Adjoining Grammars fall. 3 Since the  set of Tree Adjoining Languages (TALs) is a strict super-  set of the set of Context Free Languages, in order to define ", "acronyms": [[91, 95]], "long-forms": [[65, 89]], "ID": "1681"}, {"text": " In our experiments, we have applied the COLLINS (Collins, 1999) parser to generate the syntactic tree of both pieces of text.", "acronyms": [[41, 48]], "long-forms": [[50, 57]], "ID": "1682"}, {"text": "The generator operates from a declarative know-  ledge base of linguistic knowledge, common to that used  by PHRAN (PHRasal ANalyzer; Wilensky and Arens,  1980).", "acronyms": [[109, 114]], "long-forms": [[116, 132]], "ID": "1683"}, {"text": "the method described in Section 3.2). We also present the number of linear equations (L.Eq.) used", "acronyms": [[86, 91]], "long-forms": [[68, 84]], "ID": "1684"}, {"text": "word penalty The 8 features have weights adjusted on the tuning data using minimum error rate training (MERT) (Och, 2003).", "acronyms": [[104, 108]], "long-forms": [[75, 102]], "ID": "1685"}, {"text": "5.1 Data  We have two kinds of training data from general  domain: Labeled Data (LD) and Unlabeled Data  (UD).", "acronyms": [[81, 83], [106, 108]], "long-forms": [[67, 79], [89, 103]], "ID": "1686"}, {"text": "known formalisms uch as Head Driven Phrase  Structure Grammar (HPSG), Lexical Functional  Grammar (LFG) or Slot Grammars (SG), because  SUG allows a modular and computational ", "acronyms": [[122, 124], [63, 67], [99, 102], [136, 139]], "long-forms": [[107, 120], [24, 61], [70, 97]], "ID": "1687"}, {"text": " The single product opinion summarizer we consider is the Sentiment Aspect Match model (SAM) described and evaluated in (Lerman et al, 2009).", "acronyms": [[88, 91]], "long-forms": [[58, 80]], "ID": "1688"}, {"text": "cognition(COG) feeling(FEEL)  motivation(MOT) abstraction(ABS)  time(TIME) space(SPA) attribute(ATT)  relation(REL) social_relation(SREL) ", "acronyms": [[96, 99], [10, 13], [23, 27], [41, 44], [58, 61], [69, 73], [81, 84], [111, 114], [132, 136]], "long-forms": [[86, 94], [0, 9], [15, 22], [30, 40], [46, 57], [64, 68], [75, 80], [102, 110], [116, 131]], "ID": "1689"}, {"text": "2008).  The semantic role labeler (SRL) consists of a pipeline of independent, local classifiers that iden-", "acronyms": [[35, 38]], "long-forms": [[12, 33]], "ID": "1690"}, {"text": "rameter set to decode WSMT 2006 Europal test set (TEST1) and used the second on WSMT news commentary test set 2007 (TEST2)6. Table 6 shows the", "acronyms": [[116, 121], [80, 84], [22, 26], [50, 55]], "long-forms": [[101, 114], [40, 48]], "ID": "1691"}, {"text": "strat1 40.1 24.4 15.0 strat2 38.2 22.5 14.5 Table 4: Word Error Rate (WER), Concept Error Rate (CER) and Interpretation Error Rate (IER) ac-", "acronyms": [[70, 73], [96, 99], [132, 135]], "long-forms": [[53, 68], [76, 94], [105, 130]], "ID": "1692"}, {"text": "using a set of data-driven terms.  We investigated how likely term frequency (TF) based RF is to discover HEWs.", "acronyms": [[78, 80], [88, 90], [106, 110]], "long-forms": [[62, 76]], "ID": "1693"}, {"text": "Way (1991) emphasizes the importance of this taxonomy by positing a central role for a dynamic type hierarchy (DTH) in metaphor, one that can create new and com-", "acronyms": [[111, 114]], "long-forms": [[87, 109]], "ID": "1694"}, {"text": "Multi-document person name resolution, Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Reference Resolution Workshop.", "acronyms": [[124, 127]], "long-forms": [[81, 122]], "ID": "1695"}, {"text": " In Proceedings of the 16th International Conference on Computational Linguistics (COLING), volume I, pages 466?471.", "acronyms": [[83, 89]], "long-forms": [[56, 81]], "ID": "1696"}, {"text": " PRECISE (Popescu, Etzioni, and Kautz 2003) 88.0 88.0 ZC05 (Zettlemoyer and Collins 2005) 79.3 ? ", "acronyms": [[54, 58]], "long-forms": [[60, 88]], "ID": "1697"}, {"text": "Using the alignments from HIER, we created phrase tables using model probabilities (MOD), and heuristic extraction on words (HEUR-W), blocks (HEUR-B), and minimal phrases (HEUR-P) as de-", "acronyms": [[125, 131], [26, 30], [84, 87], [142, 148], [172, 178]], "long-forms": [[94, 123]], "ID": "1698"}, {"text": "to the Markov Logic system. At each step, we compute both the maximum a posteriori (MAP) assignment of coreference relationships as well", "acronyms": [[84, 87]], "long-forms": [[62, 82]], "ID": "1699"}, {"text": "CORROLA TION  NP1 = (COR) (APP) (ADJ) (NC) N  NP2 = (NC) N  NP3 = N ", "acronyms": [[53, 55], [21, 24], [27, 30], [33, 36], [39, 41], [14, 17]], "long-forms": [[46, 51], [0, 12]], "ID": "1700"}, {"text": "by each of these strategies.  The four Word Sense (WS) disambiguation  strategies resolve sense ambiguity errors.", "acronyms": [[51, 53]], "long-forms": [[39, 49]], "ID": "1701"}, {"text": "These are selected from the LDC English Gigaword corpus. AFP = Agence France-Presse; AFW = Associated Press Worldstream; NYT = New York Times; XIN = Xinhua News Agency; and CNA = Central News Agency of Taiwan denote the sections of the LDC English Gigaword", "acronyms": [[121, 124], [143, 146], [28, 31], [57, 60], [85, 88], [173, 176], [236, 239]], "long-forms": [[127, 141], [149, 160], [63, 83], [91, 119], [179, 198]], "ID": "1702"}, {"text": "knowledge about the structuf.e of the world into account.  - The Data Base Language ( DBL ) , which contains conatants that correspond  to data base primitives . (", "acronyms": [[86, 89]], "long-forms": [[65, 83]], "ID": "1703"}, {"text": "Term Frequency-Inverse Document Frequency (TF-IDF) is a widely used similarity measure in Information Retrieval(IR). It has also been shown", "acronyms": [[112, 114], [43, 49]], "long-forms": [[90, 110], [0, 41]], "ID": "1704"}, {"text": "word-level features described in Section 5.  4.1 Ranking by regression (RR) The first ranking strategy is based on training a re-", "acronyms": [[72, 74]], "long-forms": [[49, 70]], "ID": "1705"}, {"text": "of the log-likelihood.     We used the tempered EM (TEM) as described  by Hofmann (1999).", "acronyms": [[52, 55]], "long-forms": [[39, 50]], "ID": "1706"}, {"text": "2.1 Knowledge Source We employ BabelNet 2.5.1 as our reference knowledge base (KB). BabelNet is a multilingual", "acronyms": [[79, 81]], "long-forms": [[63, 77]], "ID": "1707"}, {"text": "size ? is fixed to 0.0001. We refer to this model as Orthogonal Matrix Factorization (OrMF). ", "acronyms": [[86, 90]], "long-forms": [[53, 84]], "ID": "1708"}, {"text": "or certain part-of-speech tags (e.g., interjection).8 4.2.3 Performance of the formality classifier We trained a Maximum Entropy (MaxEnt) classifier in the Mallet package (McCallum, 2002).", "acronyms": [[130, 136]], "long-forms": [[113, 128]], "ID": "1709"}, {"text": "77 .73  Problem 2nd 1.45 3.00  GS = Group significant cannot pool by individual  DISCUSSION OF THE RESULTS ", "acronyms": [[31, 33]], "long-forms": [[36, 53]], "ID": "1710"}, {"text": "ear (lin) kernel, second degree polynomial kernel (d=2), and RBF kernel (rbf); SVM with transductive inference (TSVM) and linear (lin) kernel or second degree polynomial (d=2) ker-", "acronyms": [[112, 116]], "long-forms": [[88, 110]], "ID": "1711"}, {"text": "The word lattices of the HUB-1 corpus are directed acyclic graphs in the HTK Standard Lattice Format (SLF), consisting of a set of vertices and a set of edges.", "acronyms": [[102, 105], [25, 30], [73, 76]], "long-forms": [[77, 100]], "ID": "1712"}, {"text": "2011). The default loss function is expected error (EE) (Och, 2003; Cherry and Foster, 2012).", "acronyms": [[52, 54]], "long-forms": [[36, 50]], "ID": "1713"}, {"text": " 3.1.1 Pre-Processing For Feature Extraction Phrase Analysis(PA): Using basic syntactic analysis (shallow parsing), the PA module re-builds", "acronyms": [[61, 63], [120, 122]], "long-forms": [[45, 59]], "ID": "1714"}, {"text": "plains the text? Our approach is related to minimum description length (MDL). We formulate our", "acronyms": [[72, 75]], "long-forms": [[48, 70]], "ID": "1715"}, {"text": "pattern, up to mi example questions.  Question Pattern (QPi):  When do Q_PRN Q_MVerb Q_BNP?", "acronyms": [[56, 59], [71, 76], [77, 84], [85, 90]], "long-forms": [[38, 54]], "ID": "1716"}, {"text": "Sample DCR entries specifying enumerated values for SynFeatureName, etc. The specification uses the Ontology Web Language (OWL) to list valid values for objects of the defined class. ", "acronyms": [[123, 126], [7, 10]], "long-forms": [[100, 121]], "ID": "1717"}, {"text": "{FirstName.SecondName}@dfki.de Abstract The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system", "acronyms": [[121, 123], [44, 48]], "long-forms": [[97, 119]], "ID": "1718"}, {"text": "smoothing as the evaluation metric.  Best v.s. Rest (BR) To score the best hypothesis in the n-best set", "acronyms": [[53, 55]], "long-forms": [[37, 51]], "ID": "1719"}, {"text": "Translation  The first method compared is a transitive  translation using MT (machine translation). The ", "acronyms": [[74, 76]], "long-forms": [[78, 97]], "ID": "1720"}, {"text": "to inspect and easily modify discourse-planning specifications for rapid  iterative refinement. The Explanation Design Package (EDP) formalism  is a convenient, schema-like (McKeown 1985; Paris 1988) programming ", "acronyms": [[128, 131]], "long-forms": [[100, 126]], "ID": "1721"}, {"text": "function (Section 4.1).  Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of", "acronyms": [[55, 57]], "long-forms": [[45, 53]], "ID": "1722"}, {"text": " 3 Conditional Random Fields  Conditional Random Fields (CRFs) are a type of  discriminative probabilistic model proposed for ", "acronyms": [[57, 61]], "long-forms": [[30, 55]], "ID": "1723"}, {"text": "(pre-nominal or post-nominal) and predicative functions; (ii) a unigram distribution (level uni), independently encoding the parts of speech (POS) of the words preceding and following the adjective, respec-", "acronyms": [[142, 145]], "long-forms": [[125, 140]], "ID": "1724"}, {"text": "{yvchen, yww, anatoleg, air}@cs.cmu.edu Abstract Spoken dialogue systems (SDS) typically require a predefined semantic ontology", "acronyms": [[74, 77]], "long-forms": [[49, 72]], "ID": "1725"}, {"text": "this, in addition to the four actions of the NonInc algorithm, we introduce two new actions: Left Reveal (LRev) and Right Reveal (RRev). For this,", "acronyms": [[130, 134], [106, 110]], "long-forms": [[116, 128], [93, 104]], "ID": "1726"}, {"text": " 4.1 Lexical Sample Tasks We have evaluated our system on SensEval-2 (SE2) and SensEval-3 (SE3) lexical sample tasks and also", "acronyms": [[70, 73], [91, 94]], "long-forms": [[58, 68], [79, 89]], "ID": "1727"}, {"text": "all characters are included as features; full remove (P4), where all special Twitter features like user names, URLs, hashtags, retweet (RT ) tags, and emoticons are stripped; and replacing Twitter fea-", "acronyms": [[136, 138], [54, 56], [111, 115]], "long-forms": [[127, 134]], "ID": "1728"}, {"text": "sampling the outputs at random locations.  INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) describes the direction", "acronyms": [[54, 57]], "long-forms": [[59, 81]], "ID": "1729"}, {"text": "Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus. ", "acronyms": [[146, 148], [15, 17], [36, 39], [80, 85], [120, 128], [181, 183]], "long-forms": [[149, 168], [18, 34], [40, 67], [86, 111], [129, 137], [184, 186]], "ID": "1730"}, {"text": " respectively display the results obtained without and with the use of subcat features (SF). The sec-", "acronyms": [[88, 90]], "long-forms": [[71, 86]], "ID": "1731"}, {"text": "exposed through the feature HOOK to facilitate further composition. These properties include pointers to the local top handle (LTOP), the constituent?s primary index (INDEX), and the external argument, if any (XARG).", "acronyms": [[127, 131], [167, 172], [210, 214], [28, 32]], "long-forms": [[109, 118], [160, 165], [183, 199]], "ID": "1732"}, {"text": "for Answer Search (IYAS) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Hamad Bin Khalifa University and", "acronyms": [[131, 135], [19, 23], [81, 84]], "long-forms": [[95, 129], [51, 79], [0, 17]], "ID": "1733"}, {"text": "SEMANTICS. The main component of SeSyn is a rule system (the syntax) which transforms the Semantic Analysis  (SA) of any given sentence into a Surface Structure (SS) of that sentence. The SAs represent meanings ina higher order ", "acronyms": [[162, 164], [33, 38], [110, 113], [188, 191]], "long-forms": [[143, 160], [90, 107]], "ID": "1734"}, {"text": "  Daniel S. Leite1, Lucia H. M. Rino1, Thiago A. S. Pardo2, Maria das Gra?as V. Nunes2  N?cleo Interinstitucional de Ling??stica Computacional (NILC)  http://www.nilc.icmc.usp.br ", "acronyms": [[144, 148]], "long-forms": [[88, 142]], "ID": "1735"}, {"text": " 4 Classifier We used a linear support vector machine (SVM) classifier, which is standard for text data.", "acronyms": [[55, 58]], "long-forms": [[31, 53]], "ID": "1736"}, {"text": "2 Related Work There have been several studies on supervised dialogue act (DA) modeling. To the best of", "acronyms": [[75, 77]], "long-forms": [[61, 73]], "ID": "1737"}, {"text": "syntactically correct) to 1.0 (completely wrong).  ISER (information item semantic error rate): The test sentences are segmented into information items; for each of these items, the translation candidates", "acronyms": [[51, 55]], "long-forms": [[57, 93]], "ID": "1738"}, {"text": " 1 Introduct ion  Some natural anguage processing (NLP) tasks can  be performed with only coarse-grained semantic in- ", "acronyms": [[51, 54]], "long-forms": [[23, 49]], "ID": "1739"}, {"text": "ble. The model makes heavy use of single-category Ambiguity Classes (AC)3, which (being independent on the tagger?s intermediate decisions) can be", "acronyms": [[69, 71]], "long-forms": [[50, 67]], "ID": "1740"}, {"text": "Mauser et al 2009). One promising approach is the Discriminative Word Lexicon (DWL). In this", "acronyms": [[79, 82]], "long-forms": [[50, 77]], "ID": "1741"}, {"text": "identification. In Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 2585?2602, Mumbai, India.", "acronyms": [[98, 104]], "long-forms": [[71, 96]], "ID": "1742"}, {"text": " 2.2 W3C Semantic Web The World Wide Web (WWW) was once designed to be as simple, as decentralized and as interop-", "acronyms": [[42, 45], [5, 8]], "long-forms": [[26, 40]], "ID": "1743"}, {"text": "quency and its character string frequency is  less than or equal to 1%, it is a SWBS;  BMM-ASM (BMM ambiguity string mapping  table: the BMM-ASM table lists all the ", "acronyms": [[87, 94], [137, 144], [80, 84]], "long-forms": [[96, 124]], "ID": "1744"}, {"text": "lined previously, in that its target is entity extraction from raw news wires from the news agency Agence France Presse (AFP), and not only linking relying on gold NER annotations: the input", "acronyms": [[121, 124], [164, 167]], "long-forms": [[99, 119]], "ID": "1745"}, {"text": "is indicated by the dotted black line.  The receiver operating characteristic (ROC) curves in Figures 2 and 3 demonstrate perfor-", "acronyms": [[79, 82]], "long-forms": [[44, 77]], "ID": "1746"}, {"text": "tive of the gold standard data.  Finally, the alignment error rate (AER) is lower (and hence better) for English?French than Romanian?", "acronyms": [[68, 71]], "long-forms": [[46, 66]], "ID": "1747"}, {"text": " Parsed* Recall t Prec/Rec t MLP Prob t  Left Corner (LC) 21797 91.75 9000 .76399 .78156 .175928  LB o LC 53026 96.75 7865 .77815 .78056 .359828 ", "acronyms": [[54, 56], [29, 32], [98, 100], [103, 105]], "long-forms": [[41, 52]], "ID": "1748"}, {"text": "C = connector  TR = terse reply  FS = false start  E = echo ", "acronyms": [[33, 35], [15, 17]], "long-forms": [[38, 49], [4, 13], [20, 31], [55, 59]], "ID": "1749"}, {"text": "provided as input. The crawler generates the  Universal Resource Locator (URL) address for the  index (first) page of any particular date.", "acronyms": [[74, 77]], "long-forms": [[46, 72]], "ID": "1750"}, {"text": "vides brief details of each annotation dimension.   2.1 Knowledge Type (KT)  This dimension is responsible for capturing the ", "acronyms": [[72, 74]], "long-forms": [[56, 70]], "ID": "1751"}, {"text": "e-maih ide@cs,  vassar ,  edu   Abstract. The Text Encoding Initiative (TEl) is an  international project established in 1988 to develop ", "acronyms": [[72, 75]], "long-forms": [[42, 70]], "ID": "1752"}, {"text": "We developed three ensemble learning approaches for recognizing disorder entities and a Vector Space Model based method for encoding. Our approaches achieved top rank in both subtasks, with the best F measure of 0.813 for entity recognition and the best accuracy of 74.1% for encoding, indicating the proposed approaches are promising.  1 Introduction  In recent years, clinical natural language processing (NLP) has received great attention for its critical role in unlocking information embedded in clinical documents. Leveraging such information can facilitate the secondary1 use of electronic health record (EHR) data to                                                      This work is licensed under a Creative Commons Attribution 4.0 International Licence.", "acronyms": [[408, 411], [612, 615]], "long-forms": [[379, 406], [586, 610]], "ID": "1753"}, {"text": "Comprehension in 100 days,? published by  Chung Hwa Book Co., (H.K.) Ltd.  The  ChungHwa training set includes 100 English ", "acronyms": [[63, 67]], "long-forms": [[48, 56]], "ID": "1754"}, {"text": "In Proc. of the Association for Computational Linguistics (ACL), pages 523?530.", "acronyms": [[59, 62]], "long-forms": [[16, 57]], "ID": "1755"}, {"text": "these basic models for email conversations.  4.1 Latent Dirichlet Allocation (LDA) Our first model is the probabilistic LDA model", "acronyms": [[78, 81], [120, 123]], "long-forms": [[49, 76]], "ID": "1756"}, {"text": "expunged to meet United States HIPAA standards, (U.S. Health, 2002) and approved for release by the local Institutional Review Board (IRB); the sample must represent problems that medical records coders", "acronyms": [[134, 137], [49, 52], [31, 36]], "long-forms": [[106, 132]], "ID": "1757"}, {"text": "PrecisionCorrectTransliterations (PTrans)  2. RecallCorrectTransliteration (RTrans)  3.", "acronyms": [[76, 82], [34, 40]], "long-forms": [[46, 74], [0, 32]], "ID": "1758"}, {"text": "Each accuracy measure is shown in a column, including the segmentation F-score (SF ), the overall tagging 894", "acronyms": [[80, 82]], "long-forms": [[58, 72]], "ID": "1759"}, {"text": "while the NB+E extractor has the worst. Training the CRF with negative examples (CRF+E) gave better precision in extracted information then train-", "acronyms": [[81, 86], [10, 14]], "long-forms": [[53, 79]], "ID": "1760"}, {"text": "on Chinese FrameNet is divided into the subtasks of boundary identification(BI) and semantic role classification(SRC). ", "acronyms": [[113, 116], [76, 79]], "long-forms": [[84, 112], [52, 75]], "ID": "1761"}, {"text": "Learning attitudes and attributes from multi-aspect reviews. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020?1025.", "acronyms": [[77, 81], [89, 93]], "long-forms": [[61, 75]], "ID": "1762"}, {"text": "posed web-based semantic similarity measures: Jaccard, Dice, Overlap, PMI (Bollegala et al, 2007), Normalized Google Distance (NGD) (Cilibrasi and Vitanyi, 2007), Sahami and Heil-", "acronyms": [[127, 130], [70, 73]], "long-forms": [[99, 125]], "ID": "1763"}, {"text": "data with which to test research hypotheses. We de-  scribe the Air Travel Information System (ATIS) pilot  corpus, a corpus designed to measure progress in Spo- ", "acronyms": [[95, 99]], "long-forms": [[64, 93]], "ID": "1764"}, {"text": "EN 94,725 2.58 Table 2: Corpus statistics: SR=Serbian, SL=Slovene, EN=English, BG=Bulgarian Tagset The Multext-East corpus is manually an-", "acronyms": [[67, 69], [79, 81], [0, 2], [43, 45], [55, 57]], "long-forms": [[70, 77], [82, 91], [46, 53], [58, 65]], "ID": "1765"}, {"text": "http://www.ukp.tu-darmstadt.de Abstract In this paper, we present a machine learning approach for word sense alignment (WSA) which combines distances between senses in the graph representations of lexical-semantic resources", "acronyms": [[120, 123]], "long-forms": [[98, 118]], "ID": "1766"}, {"text": "through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06-", "acronyms": [[123, 130], [8, 11], [50, 53], [167, 170]], "long-forms": [[95, 121], [20, 48], [140, 165]], "ID": "1767"}, {"text": "prove SRL performance.  Template Generation (TG)  Our template generation (TG) algorithm extracts ", "acronyms": [[45, 47], [6, 9], [75, 77]], "long-forms": [[24, 43], [54, 73]], "ID": "1768"}, {"text": ".  Reattachment Heuristic (RH) targets nonargument head errors that occur if a TL argument", "acronyms": [[27, 29], [79, 81]], "long-forms": [[3, 25]], "ID": "1769"}, {"text": "I_tt is raining.,  OBJ = Object: He read a book., ", "acronyms": [[19, 22]], "long-forms": [[25, 31]], "ID": "1770"}, {"text": " 1 Introduction  When a natural language processing (NLP) system  is created in a modular fashion, it can be relatively ", "acronyms": [[53, 56]], "long-forms": [[24, 51]], "ID": "1771"}, {"text": "? A chunking rule:  PP = prep, NP#1, if (pythontest(#1)). ", "acronyms": [[20, 22]], "long-forms": [[25, 29]], "ID": "1772"}, {"text": "On Complexity of Word Order. Traitement Automatique des Langues (TAL), 41(1):273?300. ", "acronyms": [[65, 68]], "long-forms": [[29, 62]], "ID": "1773"}, {"text": "target translation: the gunman was killed by police .  The Penn English Treebank (PTB) (Marcus et al, 1993) is our source of syntactic information, largely", "acronyms": [[82, 85]], "long-forms": [[59, 80]], "ID": "1774"}, {"text": " Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is com-", "acronyms": [[88, 91]], "long-forms": [[59, 86]], "ID": "1775"}, {"text": "rule in a CFG. It can  therefore fill the ACity (ArrivalCity) or the  DCity (DepartureCity) slot, and instantiate a ", "acronyms": [[42, 47], [10, 13], [70, 75]], "long-forms": [[49, 60], [77, 90]], "ID": "1776"}, {"text": "of Data-to-Speech systems have been and are be-  ing developed on the basis of D2S. Examples are  the Dial Your Disc (DYD)-system, which presents  information in English about Mozart compositions ", "acronyms": [[118, 121], [79, 82]], "long-forms": [[102, 116]], "ID": "1777"}, {"text": "Experimental results on Europarl with different translation directions (BLEU% on WMT08).  RW=Random Walk. * indicates the results are significantly better than the baseline (p<0.05).", "acronyms": [[90, 92], [72, 77], [81, 86]], "long-forms": [[93, 104]], "ID": "1778"}, {"text": "l (CAUSER sally-l)  (OBJECT paint-l)  (PATH (path-1 (DESTINATION wall-l))))  Sally sprayed the wall with paint.", "acronyms": [[39, 43], [10, 17], [28, 35]], "long-forms": [[45, 51]], "ID": "1779"}, {"text": " 2.2 LNRE Nature  o f  the  Data   The LNRE (Large Number of Rare Events)  zone (Chitashvili & Baayen, 1993) is defined as ", "acronyms": [[39, 43], [5, 9]], "long-forms": [[45, 72]], "ID": "1780"}, {"text": "gls: the definition of the verb  They also defined two alternate search protocols: rich hierarchy exploration (RHE) with no  more than six links and shallow hierarchy explo-", "acronyms": [[111, 114]], "long-forms": [[83, 109]], "ID": "1781"}, {"text": "F6: \"TO PRODUCE GOLF CLUBS\"  (VP (AUX (TO \"TO\"))  (VP (V \"PRODUCE\")  (NP (N \"GOLF\") (N \"CLUBS\")))) ", "acronyms": [[51, 53], [30, 32], [34, 37], [70, 72]], "long-forms": [[55, 58]], "ID": "1782"}, {"text": "The current   representat ion fo r  t h a t  sentence i n  pur system would be:  Z V l  =Ncorn(elephant,X1) PI =P.P(size,X1 ,small)  & =Ncom(animal,X1) P2 =P(size ,XI ,large) ", "acronyms": [[108, 110], [89, 94], [81, 86], [136, 140], [112, 115], [152, 154], [164, 166], [104, 106]], "long-forms": [], "ID": "1783"}, {"text": " Association for Computational Linguistics.          ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,        Morphological and Phonological Learning: Proceedings of the 6th Workshop of the", "acronyms": [[108, 115]], "long-forms": [[57, 106]], "ID": "1784"}, {"text": " 50 missed OG events were labeled as Past (PA) while FU events were commonly mislabeled as both PA", "acronyms": [[43, 45], [11, 13], [53, 55], [96, 98]], "long-forms": [[37, 41]], "ID": "1785"}, {"text": "5 Conclusion We have presented an efficient extension of the posterior regularization (PR) framework to a more general class of penalty functions.", "acronyms": [[87, 89]], "long-forms": [[61, 85]], "ID": "1786"}, {"text": "2013 Association for Computational Linguistics FCG offers a similar grammar engineering framework that follows the principles of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG", "acronyms": [[151, 154], [47, 50], [202, 205]], "long-forms": [[129, 149]], "ID": "1787"}, {"text": "and Innovation action). This research is part of the Interactive sYstems for Answer Search (IYAS) project, conducted by the Arabic Language Tech-", "acronyms": [[92, 96]], "long-forms": [[53, 90]], "ID": "1788"}, {"text": "translation from one source language to multiple target languages, inspired by the recently proposed neural machine translation(NMT) framework proposed by Bahdanau et al (2014).", "acronyms": [[128, 131]], "long-forms": [[101, 126]], "ID": "1789"}, {"text": " VP  Figure 3: A tree with some of its partial trees (PTs). ", "acronyms": [[54, 57]], "long-forms": [[39, 52]], "ID": "1790"}, {"text": "are assigned the correct head and dependency type ? and unlabeled attachment score (UAS) ? the per-", "acronyms": [[84, 87]], "long-forms": [[56, 82]], "ID": "1791"}, {"text": "3 Estimation  We estimate a model?s distributions with  probabilistic decision trees (DTs).4 We build  decision trees using the WinMine toolkit ", "acronyms": [[86, 89]], "long-forms": [[70, 84]], "ID": "1792"}, {"text": "~eather(WEA) ingestion(ING)  use(USE) social(SOC) body(BOD)  phy_creation(PCR) mental_creation(MCR)  verbal_creagion (VCR) ", "acronyms": [[74, 77], [95, 98], [8, 11], [23, 26], [33, 36], [45, 48], [55, 58], [118, 121]], "long-forms": [[61, 72], [79, 93], [1, 7], [13, 22], [29, 32], [38, 44], [50, 54], [101, 116]], "ID": "1793"}, {"text": " 1 Introduction In this paper, we propose TroFi (Trope Finder), a nearly unsupervised clustering method for sep-", "acronyms": [[42, 47]], "long-forms": [[49, 61]], "ID": "1794"}, {"text": "In Proceedings of the International Conference on Data Engineering (ICDE). ", "acronyms": [[68, 72]], "long-forms": [[22, 66]], "ID": "1795"}, {"text": "At  the top level, >,sb,,~ denotes the basic relation for the  overall ranking of information structure (IS) patterns. ", "acronyms": [[105, 107]], "long-forms": [[82, 103]], "ID": "1796"}, {"text": "sented in the graphic. The first strategy can be applied when the data set contains a  functionally independent attribute (FIA) that is used as an organizing device or \"an-  chor\" for the entire graphic.", "acronyms": [[123, 126]], "long-forms": [[87, 121]], "ID": "1797"}, {"text": "5.2 Results We evaluate SO of words on three different sized corpora: Gigaword (GW) 6.2GB, GigaWord + 50% of web data (GW+WB1) 21.2GB and Gi-", "acronyms": [[80, 82], [24, 26], [119, 121], [122, 125]], "long-forms": [[70, 78]], "ID": "1798"}, {"text": "Table 5: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus.", "acronyms": [[94, 97], [73, 75], [138, 143], [178, 186], [204, 206], [239, 241]], "long-forms": [[98, 125], [76, 92], [144, 169], [187, 195], [242, 244], [207, 226]], "ID": "1799"}, {"text": "Implicit Incongruity (IMP) Boolean Incongruity of extracted implicit phrases (Rilof et.al, 2013) Explicit Incongruity (EXP) Integer Number of times a word follows a word of opposite polarity", "acronyms": [[119, 122], [22, 25]], "long-forms": [[97, 105]], "ID": "1800"}, {"text": "on Artificial Intelligence (KI2002), volume 2479 of Lecture Notes in Artificial Intelligence (LNAI), pages 18?32, Aachen, Germany, September. ", "acronyms": [[94, 98], [28, 34]], "long-forms": [[52, 92]], "ID": "1801"}, {"text": "One of  the most commonly used methods is the  Latent Semantic Analysis (LSA). In this ", "acronyms": [[73, 76]], "long-forms": [[47, 71]], "ID": "1802"}, {"text": "unable to touch the robot?s screen or to verbalize a speech command (e.g. after a stroke) is the brain computer interface (BCI) of the robot (Hintermu?ller et al, 2011).", "acronyms": [[123, 126]], "long-forms": [[97, 121]], "ID": "1803"}, {"text": "functions. The latter three correspond to the three Gene Ontology (GO) (Ashburner et al, 2000) toplevel sub-ontologies, and terms of these types were", "acronyms": [[67, 69]], "long-forms": [[52, 65]], "ID": "1804"}, {"text": "Noun I Nominalized verb(NIO  Determinative modifier ::= Adjective I Differentiable Adjective(DA) I Verb I Noun I  Location I String l Numeral + Classifier ", "acronyms": [[93, 95], [24, 27]], "long-forms": [[68, 91]], "ID": "1805"}, {"text": "and get close to the top level in several other tracks.  Recently, Maximum Entropy model(ME) and CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai", "acronyms": [[89, 91], [97, 101]], "long-forms": [[67, 82]], "ID": "1806"}, {"text": "new opportunity: part of Attempto Controlled English (ACE) was mapped to OWL (Kaljurand and Fuchs, 2007), and Processable English (PENG) evolved to Sydney OWL Syntax (SOS) (Cregan et", "acronyms": [[131, 135]], "long-forms": [[110, 129]], "ID": "1807"}, {"text": " verb. The third is end position (EP), after a predi-  cate.", "acronyms": [[34, 36]], "long-forms": [[20, 32]], "ID": "1808"}, {"text": "n - c-dow British English American English Table 1: Example of tags assigned with coarse-grained Universal Tagset (UT) and fine-grained lexical type tagset (LTT).", "acronyms": [[115, 117], [157, 160]], "long-forms": [[97, 113], [136, 155]], "ID": "1809"}, {"text": "user?s state in the given session. In this research, the support vector machine (SVM) is used as a classifier.", "acronyms": [[81, 84]], "long-forms": [[57, 79]], "ID": "1810"}, {"text": "on a questionnaire provided to them. And a Mean Opinion Score(MoS) of 62.27% was achieved.", "acronyms": [[62, 65]], "long-forms": [[43, 61]], "ID": "1811"}, {"text": "cf. Webber 1987b), representing the narrative's unfold-  ing contents, and the l inear text structure (LTS), whose  components are linked by rhetorical relations such as ", "acronyms": [[103, 106]], "long-forms": [[79, 101]], "ID": "1812"}, {"text": "PROP  VP  I PROP = proposition  These-fragments would match a locative object use of a p r epos~ .~ lu r~  arlu L H ~ .", "acronyms": [[12, 16]], "long-forms": [[19, 30]], "ID": "1813"}, {"text": "The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al, 2004) is adopted as the unsupervised segmentation crite-", "acronyms": [[112, 114]], "long-forms": [[94, 110]], "ID": "1814"}, {"text": "lowing three metrics are used in this experiment.  (a) EPN in total (EPN-T): The number of the expanded problems which are generated in the", "acronyms": [[69, 74]], "long-forms": [[55, 67]], "ID": "1815"}, {"text": "ENT E1 E2 (b) Feature Paired Tree(FPT) ENT", "acronyms": [[34, 37]], "long-forms": [[14, 32]], "ID": "1816"}, {"text": "most u n e x p a n d e d  node o f  TI: for  3 b t h i s  r e s u l t s  i n :   3 e .  (S ( V  m a i l )  (NP ( N P  ( N  B o x e s )  (N*)) P P * )  ( P P * ) ) .  ", "acronyms": [[108, 110], [142, 145]], "long-forms": [[113, 116]], "ID": "1817"}, {"text": "The methods for scoring the Template Element, Template Relation, Scenario Template, and Named Entity tasks are  very similar. From the standpoint of calculating scores, The template element (TE) task is the basic task of these four. ", "acronyms": [[191, 193]], "long-forms": [[173, 189]], "ID": "1818"}, {"text": " Experiments are performed on two datasets, the English Penn Treebank (PTB) dataset using the standard train, dev and test splits, and the ARK", "acronyms": [[71, 74]], "long-forms": [[56, 69]], "ID": "1819"}, {"text": "and (?) denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking.", "acronyms": [[87, 89], [100, 102], [121, 123]], "long-forms": [[90, 98], [103, 119], [124, 139]], "ID": "1820"}, {"text": "tempered EM (TEM) [8] algorithm, instead of a naive one, to avoid this problem. TEM algorithm is closely related to the deterministic annealing EM (DAEM) algorithm [17], and helps avoid local extrema by introducing inverse temperature ?.", "acronyms": [[148, 152], [13, 16], [80, 83]], "long-forms": [[120, 146], [0, 11]], "ID": "1821"}, {"text": "CRF) A wide range of contextual information, such as surrounding words (GREF), dependency or case structure (GTREF), and dependency path (GREF ), has been utilized for similarity calculation, and achieved considerable success.", "acronyms": [[138, 142], [0, 3], [72, 76]], "long-forms": [[109, 114]], "ID": "1822"}, {"text": "ceedin.qs, IEEE-1ECEJ-ASJ htternational Con-  ference on Acoustics, Speech, and Signal Process-  ing (ICASSP), 2bkyo, April 1986. ", "acronyms": [[102, 108], [11, 25]], "long-forms": [[26, 100]], "ID": "1823"}, {"text": "al., ( 2013) but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al.,", "acronyms": [[78, 81]], "long-forms": [[52, 76]], "ID": "1824"}, {"text": "We apply LDA on  the user-word matrix UW:  UW = UM * MW  , where UM is the user-hidden matrix, MW is the ", "acronyms": [[48, 50], [9, 12], [38, 40], [43, 45], [53, 55], [65, 67], [95, 97]], "long-forms": [[51, 52], [21, 37]], "ID": "1825"}, {"text": "The development of efficient estimation procedures for context-dependent acoustic models revolutionised the field of Automatic Speech Recognition (ASR) (Young et al.,", "acronyms": [[147, 150]], "long-forms": [[117, 145]], "ID": "1826"}, {"text": " Figure 1: Graphical representation of the phrase pair topic (PPT) model. ", "acronyms": [[62, 65]], "long-forms": [[43, 60]], "ID": "1827"}, {"text": "(O?Shaughnessy, 2000), lip aperture (LA) is the normalized Euclidean distance between the lips, and lip protrusion (LP) is the normalized 2nd principal component of the midpoint between the lips.", "acronyms": [[116, 118], [36, 39]], "long-forms": [[100, 114], [23, 35]], "ID": "1828"}, {"text": "For example, PropBank annotates 8,037 ARGM-MNR relations (10.7%) out of 74,980 adjunct-like arguments (ARGMs). There are verbs", "acronyms": [[103, 108], [38, 46]], "long-forms": [[92, 101]], "ID": "1829"}, {"text": "v2i = v3i ? a circular convolution model (CON) v1 ? v2 = v3", "acronyms": [[42, 45]], "long-forms": [[23, 34]], "ID": "1830"}, {"text": "CoTrain vs. BaseCN2 1.8E-07 0.00257 0.000182 CoTrain vs. BaseCN3 1.27E-06 0.00922 0.000765 CoTrain vs. LEX(CN) 6.09E-29 3.72E-21 1.61E-24 CoTrain vs. LEX(EN) 0.0018 0.0276 0.00329", "acronyms": [[107, 109], [103, 106], [150, 153], [154, 156]], "long-forms": [[91, 98]], "ID": "1831"}, {"text": " Definition 2.5  Given a grammar, G, define MCL(G) (Maximum Change in Length) as:  MCL(G) = max { m \\] A (.. q/1. . .", "acronyms": [[44, 47], [83, 86], [92, 95]], "long-forms": [[52, 76], [25, 32]], "ID": "1832"}, {"text": "steels@arti.vub.ac.be Abstract Fluid Construction Grammar (FCG) is a new linguistic formalism designed to ex-", "acronyms": [[59, 62]], "long-forms": [[31, 57]], "ID": "1833"}, {"text": "matical device for handling coordination in computa-  tional linguistics has been the SYSCONJ facility for  augmented transition networks (ATNs) (Woods 1973;  Bates 1978).", "acronyms": [[139, 143], [86, 93]], "long-forms": [[108, 137]], "ID": "1834"}, {"text": "Extended Markup Language (XML) is a pro-  posed standard (XML, 1997) specified by the World  Wide Web Consortium (W3C). In XML, tags and ", "acronyms": [[114, 117]], "long-forms": [[93, 112]], "ID": "1835"}, {"text": "de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment under an attitude predicate (say), the events in Examples (6a) and (6b) are assessed as certain (CT+), whereas the words highly confident in Example (6c) trigger PR+, and may in Example (6d) leads to PS+.", "acronyms": [[181, 184], [246, 249], [284, 287]], "long-forms": [[172, 179]], "ID": "1836"}, {"text": "Adobe website:2.03 Adobe Systems:1.82 Data mining (DM), also known as Knowledge-Discovery in Databases (KDD) or Knowledge-Discovery and Data Mining (KDD), is the process of automatically searching large volumes of data for patterns.", "acronyms": [[51, 53], [104, 107], [149, 152]], "long-forms": [[38, 49], [70, 101], [112, 140]], "ID": "1837"}, {"text": "Table 9 Number of times a core grammatical function was annotated more than once in the treebank (TRBK) by the model using gold morphology (GOLD-M), and by the model using predicted morphology (PRED-M).", "acronyms": [[140, 146], [98, 102], [194, 200]], "long-forms": [[123, 138], [88, 96], [172, 192]], "ID": "1838"}, {"text": "ciently or accurately than alternative approaches.  Constraint Programming (CP) is a field of research that develops algorithms and tools for", "acronyms": [[76, 78]], "long-forms": [[52, 74]], "ID": "1839"}, {"text": "121 domain adaptation algorithm mentioned in (Daume, 2007) based on Maximum Entropy model (MaxEnt) (Ratnaparkhi, 1996).", "acronyms": [[91, 97]], "long-forms": [[68, 83]], "ID": "1840"}, {"text": "Disco-En-Gold consists of 349 expressions divided into training (TrainD), validation (ValD), and test data (TestD) manually assigned scores from 0 to 100, indicating the level of compositionality (the", "acronyms": [[108, 113], [86, 90], [65, 71]], "long-forms": [[97, 106], [74, 84], [42, 63]], "ID": "1841"}, {"text": "Domains: HT = human transcription factors in blood cells, TCS = two-component systems, BB = bacteria biology, BS = Bacillus subtilis", "acronyms": [[87, 89], [9, 11], [58, 61], [110, 112]], "long-forms": [[92, 108], [14, 33], [64, 85], [115, 132]], "ID": "1842"}, {"text": "shown in Figure 2. It is observed that the numbers of instances of Conjunct Verb (ConjV),  Passives (Pass), Auxiliary Construction (AC) ", "acronyms": [[82, 87], [101, 105], [132, 134]], "long-forms": [[67, 80], [108, 130], [91, 99]], "ID": "1843"}, {"text": " 4.1.7 Doctors? Prescriptions (PRESC) Some of our food-health relations are also men-", "acronyms": [[31, 36]], "long-forms": [[16, 29]], "ID": "1844"}, {"text": "did. We used files 1-270, 400-554, and 600-931 as source domain training data (STrain), files 271300 as source domain testing data (STest) and files", "acronyms": [[79, 85], [132, 137]], "long-forms": [[50, 72], [104, 130]], "ID": "1845"}, {"text": "mantic representation is not so clear cut. Generalising only verbs to semantic files (SFv) was the best option in most of the experiments, particularly", "acronyms": [[86, 89]], "long-forms": [[70, 84]], "ID": "1846"}, {"text": "TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l   TGTM PW=pw, pkw, bw  Figure 26 ", "acronyms": [[62, 64], [0, 4], [5, 7], [27, 31], [32, 34], [57, 61]], "long-forms": [[65, 67], [8, 10], [36, 39]], "ID": "1847"}, {"text": " 2011b. Overview of the entity relations (REL) supporting task of BioNLP Shared Task 2011.", "acronyms": [[42, 45]], "long-forms": [[31, 40]], "ID": "1848"}, {"text": " 2.3 Tree Insertion Grammar Tree Insertion Grammars (TIGs) are a longstanding compromise between the intuitive expressivity", "acronyms": [[53, 57]], "long-forms": [[28, 51]], "ID": "1849"}, {"text": "translation quality include the ridge regression (RR) and support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Scho?lkopf, 2004).", "acronyms": [[95, 98], [50, 52], [85, 88]], "long-forms": [[100, 122], [32, 48], [58, 83]], "ID": "1850"}, {"text": "77 (a) README.txt file (d) RPM Spec PACKAGE section (metadata) Bean Scripting Framework (BSF) is a set of Java classes which provides an easy to use scripting language support", "acronyms": [[89, 92], [27, 30]], "long-forms": [[63, 87]], "ID": "1851"}, {"text": "a generalization of the Semitic root-and-template modeling. We use Egyptian Arabic (EGY), and German (GER) as our test languages.", "acronyms": [[84, 87], [102, 105]], "long-forms": [[67, 75], [94, 100]], "ID": "1852"}, {"text": "2013).  Currently the most active framenet research teams are working on Swedish FrameNet (SweFN) (Borin et al.,", "acronyms": [[91, 96]], "long-forms": [[73, 89]], "ID": "1853"}, {"text": "emoeion(ENO) perception(PER)  possession(POSS) stat ive(STA)  ~eather(WEA) ingestion(ING)  use(USE) social(SOC) body(BOD) ", "acronyms": [[85, 88], [8, 11], [24, 27], [41, 45], [56, 59], [70, 73], [85, 88], [95, 98], [107, 110], [117, 120]], "long-forms": [[75, 84], [0, 7], [13, 23], [30, 40], [47, 56], [63, 69], [91, 94], [100, 106], [112, 116]], "ID": "1854"}, {"text": "notation is illustrated in Figure 3.  3.3 Positional Unknown Model (PosUnk) The main weakness of the PosAll model is that", "acronyms": [[68, 74]], "long-forms": [[42, 60]], "ID": "1855"}, {"text": "1982, 1984; Clark 1992; Cremers 1996; Arts 2004). The present article will examine its consequences for the generation of referring expressions (GRE). In doing this, we dis-", "acronyms": [[145, 148]], "long-forms": [[108, 143]], "ID": "1856"}, {"text": "If we look at the permutations, we have in 2. to-  picalization, with OBJect NP in focus structure (FS) I in I. the grammatical relations of 25 are preserved ", "acronyms": [[100, 102], [77, 79]], "long-forms": [[83, 98]], "ID": "1857"}, {"text": " Data set We evaluate segmentation performance on the Penn Arabic Treebank (ATB).5 It consists of about 4,500 sentences of modern Arabic obtained", "acronyms": [[76, 79]], "long-forms": [[59, 74]], "ID": "1858"}, {"text": "was supported in part by JSPS Research Fellowships for Young Scientists and in part by CREST, JST (Japan Science and Technology Agency). ", "acronyms": [[94, 97], [25, 29], [87, 92]], "long-forms": [[99, 127]], "ID": "1859"}, {"text": "which predicts the aligned source positions for every target word, and (c) the Positional Unknown (PosUnk) ?", "acronyms": [[99, 105]], "long-forms": [[79, 97]], "ID": "1860"}, {"text": "semantic F1 of 85.63 for English.  Time Expression Identification (TEI) and Normalization (TEN): We use the time module", "acronyms": [[67, 70]], "long-forms": [[35, 65]], "ID": "1861"}, {"text": "mostly context-free, with some context-sensit ive and  some transformational  rules, written in a modif ied  Backus Normal Form (BNF). Each rule contains the ", "acronyms": [[129, 132]], "long-forms": [[109, 127]], "ID": "1862"}, {"text": "The  motivation for this work is presented in section 4. Unsupervised Morphology Learner (UML)  framework is presented in section 5.", "acronyms": [[90, 93]], "long-forms": [[57, 88]], "ID": "1863"}, {"text": "called D2S. D2S has been used as the foundation of a number of language-generating systems, including GOALGETTER, a system that generates soccer reports in Dutch.1 D2S consists of two modules: (1) a language generation module (LGM) and (2) a speech generation module (SGM) which turns the generated text into a speech signal.", "acronyms": [[227, 230], [12, 15], [7, 10], [164, 167], [268, 271]], "long-forms": [[199, 225], [242, 266]], "ID": "1864"}, {"text": "assignment for each annotator. We then performed an analysis of variance (ANOVA) on the outcomes of our experiment.", "acronyms": [[74, 79]], "long-forms": [[52, 72]], "ID": "1865"}, {"text": " We split annotated data into two parts: the BLOB (Binary Large OBject) and the XML annotations that refer to specific regions of the BLOB.", "acronyms": [[45, 49], [80, 83], [134, 138]], "long-forms": [[51, 70]], "ID": "1866"}, {"text": "5.1 Overall Results Table 4 shows the parsing results for the StateSplit (SP) PCFG, the Head-Driven (HD) PCFG and the Relational-Realizational (RR) PCFG", "acronyms": [[74, 76], [78, 82], [101, 103], [105, 109], [144, 146], [148, 152]], "long-forms": [[67, 72], [88, 99], [118, 142]], "ID": "1867"}, {"text": "amples in 3.2).  In section 4, we describe the  specification of Korean TimeML (KTimeML). ", "acronyms": [[80, 87]], "long-forms": [[65, 78]], "ID": "1868"}, {"text": "BlogSum-generated summary content using ROUGE and compared the results with the original candidate list (OList). The t-test re-", "acronyms": [[105, 110], [40, 45]], "long-forms": [[80, 103]], "ID": "1869"}, {"text": "(domain specific) region. The upper region of the on-  tology is called the Ontology Base (OB) and contains  approximately 400 items that represent generalizations ", "acronyms": [[91, 93]], "long-forms": [[76, 89]], "ID": "1870"}, {"text": "this results in minimum expected word error rate (WER) hypothesis (Mangu et al, 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim", "acronyms": [[122, 125]], "long-forms": [[102, 120]], "ID": "1871"}, {"text": "We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely FLORIAN (Florian et al., ", "acronyms": [[99, 106], [68, 73]], "long-forms": [[108, 115]], "ID": "1872"}, {"text": "3. Maximization of 1)arameters, A of at:tire fea-  tures 1)y I IS(hnproved Iterative Sealing) algo-  rithm.", "acronyms": [[63, 65]], "long-forms": [[75, 92]], "ID": "1873"}, {"text": "In  Proceedings of the 16th International Conference  on World Wide Web (WWW), pages 697-706. ", "acronyms": [[73, 76]], "long-forms": [[57, 71]], "ID": "1874"}, {"text": " 1  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random Sig+Reply", "acronyms": [[58, 60], [81, 84], [105, 108]], "long-forms": [[37, 56], [61, 67]], "ID": "1875"}, {"text": "pointing and lacks systematic evaluation.  This paper employs a label propagation (LP)  algorithm for global learning of NP anaphoricity.", "acronyms": [[83, 85], [121, 123]], "long-forms": [[64, 81]], "ID": "1876"}, {"text": "1998) as the reference. In Chinese FrameNet, the predicates, called lexical units (LU), evoke frames which roughly correspond to different", "acronyms": [[83, 85]], "long-forms": [[68, 81]], "ID": "1877"}, {"text": "see chapter 4.3.  WIV(1): Weighted Identity Value (with the weight 1):  see chapter 2.2.", "acronyms": [[18, 21]], "long-forms": [[26, 49]], "ID": "1878"}, {"text": "pick PRON up?, where PRON is the part of speech (POS) tag for pronouns.", "acronyms": [[49, 52], [21, 25], [5, 9]], "long-forms": [[33, 47]], "ID": "1879"}, {"text": "tor machines: learning with many relevant features. In European Conference on Machine Learning (ECML). ", "acronyms": [[96, 100]], "long-forms": [[55, 94]], "ID": "1880"}, {"text": "This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To", "acronyms": [[86, 90]], "long-forms": [[61, 84]], "ID": "1881"}, {"text": "for candidate summary sentence selection  by standard page rank algorithms used in  Information Retrieval (IR). As Bengali is ", "acronyms": [[107, 109]], "long-forms": [[84, 105]], "ID": "1882"}, {"text": "ous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine transla-", "acronyms": [[83, 86], [89, 92]], "long-forms": [[54, 81]], "ID": "1883"}, {"text": "or database .  Superficially, DEFT resembles a Natural language Understanding (NLUI) system ; however, there are key differences .", "acronyms": [[79, 83]], "long-forms": [[47, 77]], "ID": "1884"}, {"text": "J08b 97.74 93.37 N07 97.83 93.32 SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score. ", "acronyms": [[33, 35], [60, 62], [88, 91]], "long-forms": [[38, 52], [65, 70]], "ID": "1885"}, {"text": "each new domain and scenario, as discussed in the next section.  The lexical analysis module (LexAn) is responsible for splitting the document into sentences, and the sentences into tokens.", "acronyms": [[94, 99]], "long-forms": [[69, 85]], "ID": "1886"}, {"text": "ing different training methods. The effects of discriminative training (CRF) and extended feature sets (lower section) are more than additive.", "acronyms": [[72, 75]], "long-forms": [[50, 70]], "ID": "1887"}, {"text": "WordNet Domains (Magnini and Cavagli a`, 2000).  Conceptual Density (CD) is a measure of the correlation among the sense of a given word and its", "acronyms": [[69, 71]], "long-forms": [[49, 67]], "ID": "1888"}, {"text": "Chinese Semantic Dictionary (CSD) for  Chinese-English machine translation, the  Chinese Concept Dictionary (CCD) for  cross-language text processing, the multi-level ", "acronyms": [[109, 112], [29, 32]], "long-forms": [[81, 107], [0, 27]], "ID": "1889"}, {"text": "tagging, lemmatization, etc.). For corpus query, we employ the Corpus Query Processor (CQP) (CWB; Evert, 2004) which works on the basis of", "acronyms": [[87, 90]], "long-forms": [[63, 85]], "ID": "1890"}, {"text": "partially completed subproof or function of the system. The implementation f this  was the IPSIM (Interruptible Prolog SIMulator) theorem prover, which can maintain  a set of partially completed proofs and jump to the appropriate one as dialog pro- ", "acronyms": [[91, 96]], "long-forms": [[98, 128]], "ID": "1891"}, {"text": " Introduction  The DARPA ATIS Spoken Language System (SLS) task  represents ignificant new challenges for speech and natural ", "acronyms": [[54, 57], [19, 24], [25, 29]], "long-forms": [[30, 52]], "ID": "1892"}, {"text": "applied this formula to a vocabulary of single terms.  Subiect Field Code (SFC). This system applies a ", "acronyms": [[75, 78]], "long-forms": [[55, 73]], "ID": "1893"}, {"text": "of Electrical and Computer Engineering Pohang University of Science and Technology (POSTECH) Advanced Information Technology Research Center (AITrc) San 31, Hyoja-Dong, Pohang, Republic of Korea, 790-784", "acronyms": [[142, 147], [84, 91]], "long-forms": [[93, 140], [39, 82]], "ID": "1894"}, {"text": " Another syntactic phenomena crucial to the parser is known as the complex NP  Constraint (CNPC) (Radford 1981); i.e., no transformation rule can move any element  out of a complex NP, where a complex NP (CNP) is an NP containing a relative clause.", "acronyms": [[91, 95], [181, 183], [201, 203], [205, 208], [216, 218], [75, 77]], "long-forms": [[79, 89]], "ID": "1895"}, {"text": "otherwise as uniform as possible (Berger et al, 1996). maximum entropy model (MaxEnt) is known to easily combine diverse features and", "acronyms": [[78, 84]], "long-forms": [[55, 70]], "ID": "1896"}, {"text": "al., 2007) use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambigua-", "acronyms": [[85, 88]], "long-forms": [[56, 83]], "ID": "1897"}, {"text": "n?5\u0001WZWZ7V?Zo+Y#?ZWA<\u0007E  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216?1224, October 25-29, 2014, Doha, Qatar.", "acronyms": [[113, 118]], "long-forms": [[63, 111]], "ID": "1898"}, {"text": "In this paper, we propose  methods to detect and correct WOEs in Chinese sentences. Conditional random fields (CRFs)  based WOEs detection models identify the sentence segments containing WOEs.", "acronyms": [[111, 115], [57, 61], [124, 128], [188, 192]], "long-forms": [[84, 109]], "ID": "1899"}, {"text": " ? The Match Rate(MR): The match rate is the match number normalized", "acronyms": [[18, 20]], "long-forms": [[7, 16]], "ID": "1900"}, {"text": "Abstract In this paper we present results from the METER (MEasuring TExt Reuse) project whose aim is to explore issues", "acronyms": [[51, 56]], "long-forms": [[58, 78]], "ID": "1901"}, {"text": "less than linear is the sample size, m. We formalize  this as a variant of \"Set Cover\" problem which we call  \"Weighted Set Cover~(WSC), and prove the existence of  an approximation algorithm with a performance guar- ", "acronyms": [[131, 134]], "long-forms": [[111, 129]], "ID": "1902"}, {"text": "ping observed sequences to possible ground truth sequences.  We do not use the Character Error Rate (CER) metric, since for almost all NLP applications the unit of", "acronyms": [[101, 104], [135, 138]], "long-forms": [[79, 99]], "ID": "1903"}, {"text": "3.3  Parameter  es t imat ion   In supervised lcarning~ the simpliest parameter  estimation is the maximum likelihood(ML) cs-  t imation(Duda et al, 1973) which lnaximizes ", "acronyms": [[118, 120]], "long-forms": [[99, 116]], "ID": "1904"}, {"text": " 1 Introduction Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context", "acronyms": [[42, 45]], "long-forms": [[16, 41]], "ID": "1905"}, {"text": "NEDcost = EDcost/length (4) ? The Match Number(MN): The match number is the number of words", "acronyms": [[47, 49], [0, 7], [10, 16]], "long-forms": [[34, 45]], "ID": "1906"}, {"text": "son to visit Udaipur.  Parse: September to March is [NP (np the  best season) [SBAR [S (dcP to visit Udaipur)]]] .", "acronyms": [[53, 55], [79, 83]], "long-forms": [[57, 59]], "ID": "1907"}, {"text": "stood statistical models?statistical dependency parsers, probabilistic context-free grammars (PCFGs), and word translation models (TMs)?can be effectively combined into a unified framework that jointly searches for the best", "acronyms": [[131, 134], [94, 99]], "long-forms": [[111, 129], [57, 92]], "ID": "1908"}, {"text": "A workaround is to restrict the possible tag candidates per position by using either morphological analyzers (MAs), dictionaries or heuristics (Hajic?,", "acronyms": [[110, 113]], "long-forms": [[85, 108]], "ID": "1909"}, {"text": "In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL), pages 50?57, Stroudsburg, PA.", "acronyms": [[100, 103], [132, 134]], "long-forms": [], "ID": "1910"}, {"text": "is that of the closest centroid.  The Naive Bayes (NB) classifier is based on a probabilistic model which assumes conditional in-", "acronyms": [[51, 53]], "long-forms": [[38, 49]], "ID": "1911"}, {"text": "Concepts across categories Hilke Reckman and Crit Cremers Leiden University Centre for Linguistics (LUCL) Leiden, Netherlands", "acronyms": [[100, 104]], "long-forms": [[58, 98]], "ID": "1912"}, {"text": "translators with the help of computer-aided translation tools (CAT), (3) rule-based MT systems (RBMT) and (4) statistical MT systems (SMT). ", "acronyms": [[134, 137], [63, 66], [96, 100]], "long-forms": [[110, 132], [29, 55], [73, 86]], "ID": "1913"}, {"text": "and documents created by three or four New York Times columnists (TF = Thomas Friedman, PK = Paul Krugman, MD = Maureeen Dowd, GC = Gail Collins).", "acronyms": [[88, 90], [66, 68], [107, 109], [127, 129]], "long-forms": [[93, 105], [71, 86], [112, 125], [132, 144]], "ID": "1914"}, {"text": "A closer, more detailed, look at the  LOCATION data suggests that he high payoff indicated  by the average REC and precision (PRE) scores was  achieved because most of the data were listable.", "acronyms": [[126, 129], [107, 110]], "long-forms": [[115, 124]], "ID": "1915"}, {"text": "various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (", "acronyms": [[126, 128], [69, 72]], "long-forms": [[108, 124], [44, 67]], "ID": "1916"}, {"text": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2670?2676. ", "acronyms": [[86, 94]], "long-forms": [[27, 84]], "ID": "1917"}, {"text": "discourse analysis phase, the situation  frame is interpreted, resulting in one or  more instantiated knowledge base (KB)  objects, which are state or event ", "acronyms": [[118, 120]], "long-forms": [[102, 116]], "ID": "1918"}, {"text": "method of ADN. ADN is constructed by  Restricted Boltzmann Machines (RBM)  with unsupervised learning using labeled ", "acronyms": [[69, 72], [10, 13], [15, 18]], "long-forms": [[38, 67]], "ID": "1919"}, {"text": "stituent category labels expressing adverbials (RB), coordinations (CC), various types of interjections (UH, INTJ) and adverbial phrases (ADVP). We may", "acronyms": [[138, 142], [48, 50], [68, 70], [105, 107], [109, 113]], "long-forms": [[119, 136], [53, 66], [25, 46], [90, 103]], "ID": "1920"}, {"text": "They are not constrained  by features of any previous utterance in the  discourse segment (DS), and the elements of Cf(Un)  are partially ordered to reflect relative prominence ", "acronyms": [[91, 93], [119, 121], [116, 118]], "long-forms": [[72, 89]], "ID": "1921"}, {"text": "set we trained on both glosses and statistical MT data, for the OnWN and FNWN test sets we trained on glosses only (OnWN), and for the SMT test set we trained on statistical MT data only (MTnews and", "acronyms": [[116, 120], [47, 49], [64, 68], [73, 77], [135, 138], [174, 176], [188, 194]], "long-forms": [[99, 114]], "ID": "1922"}, {"text": "For 1)reprocessing the dictionary definitions, we  have experimented with two ditDrent Caggers: the Xe-  rox PAR(J part-of-speech tagger \\[8\\], and the Chop-  per \\[9\\], an optimizing finit, e state luachine-hased tag- ", "acronyms": [[109, 112], [78, 86]], "long-forms": [[115, 119]], "ID": "1923"}, {"text": "number of correcVi'abeled-constituents in proposed parse  number of correct matched constituent inproposed parse  6) Sentence parsing ratio(SPg) =  number\" of sentences having a proposed parse by parser ", "acronyms": [[140, 143]], "long-forms": [[117, 133]], "ID": "1924"}, {"text": " 2. Effort of Association (EA): a mc~sure of the effort  required to associate some entity with lira description ", "acronyms": [[27, 29]], "long-forms": [[4, 25]], "ID": "1925"}, {"text": "Argument Filtering Argument  Boundary Detection (ABD) module ?)???????", "acronyms": [[49, 52]], "long-forms": [[29, 47]], "ID": "1926"}, {"text": "Reverse Gap 0.072 0.033 Table 1: Percentage of reordering patterns ` reverse gap (RG): The two source phrases are not adjacent, and are in the reverse order as", "acronyms": [[82, 84]], "long-forms": [[69, 80]], "ID": "1927"}, {"text": "Text REtrieval Conference (TREC)1. The TREC 1The Text REtrieval Conference (TREC) is a series of evaluations of fully automatic Q/A systems", "acronyms": [[76, 80], [27, 31], [39, 43], [128, 131]], "long-forms": [[49, 74]], "ID": "1928"}, {"text": "ror rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly.", "acronyms": [[102, 105], [10, 14], [70, 72]], "long-forms": [[85, 100]], "ID": "1929"}, {"text": "(Bikel et al, 1997), Decision Trees (Sekine, 1998), Maximum Entropy Models (Borthwick and Sterling, 1998), Support Vector Machines (SVM) (Asahara and Matsumoto, 2003), and also semi-supervised", "acronyms": [[132, 135]], "long-forms": [[107, 130]], "ID": "1930"}, {"text": "Since by default we return up to RT=100  search engine results to user, we will extract the  top RQ=RT/(#newQuery+1) entries from results of  each new query and original query.", "acronyms": [[97, 99], [33, 35]], "long-forms": [[100, 115]], "ID": "1931"}, {"text": "The IE results are stored in a database which  is the basis for IE-related applications like QA,  BR (Browsing, threading and visualization) and  AS (Automatic Summarization).", "acronyms": [[98, 100], [4, 6], [64, 66], [93, 95], [146, 148]], "long-forms": [[102, 110], [150, 173]], "ID": "1932"}, {"text": "TDP (target only) 62.60 33.04 Table 2: Results Generalized average precision (GAP) is a more precise measure than P", "acronyms": [[78, 81], [0, 3]], "long-forms": [[47, 76]], "ID": "1933"}, {"text": "1 Introduction Linguistics studies have shown that action verbs often denote some change of state (CoS) as the result of an action, where the change of state of-", "acronyms": [[99, 102]], "long-forms": [[82, 97]], "ID": "1934"}, {"text": "2007). The practical NLP application based evaluations are automatic speech recognition (ASR), information retrieval (IR) and statistical machine", "acronyms": [[89, 92], [21, 24], [118, 120]], "long-forms": [[59, 87], [95, 116]], "ID": "1935"}, {"text": "contribution: Functional GENDER and NUMBER features contribute more than their form-based counterparts, in both gold and predicted conditions; rationality (RAT) as a single feature on top of the POS tag set helps in gold (and with Easy-First Parser, also in predicted conditions)?but when used in combination with", "acronyms": [[156, 159], [195, 198]], "long-forms": [[143, 154]], "ID": "1936"}, {"text": "this mode\\]., the linguistic facts that pertain solely  to the source language (SL) are supposed to be  clearly separated from the facts that pertain solely ", "acronyms": [[80, 82]], "long-forms": [[63, 78]], "ID": "1937"}, {"text": "We set aside the blind TEST set for evaluating the final performance of our named entity recognition (NER) and relation extraction (RE) 2http://code.google.com/apis/ajaxsearch", "acronyms": [[132, 134], [102, 105]], "long-forms": [[111, 130], [76, 100]], "ID": "1938"}, {"text": "systems that learn new representations for opendomain NLP using latent-variable language models like Hidden Markov Models (HMMs). In POS-", "acronyms": [[123, 127], [54, 57], [133, 136]], "long-forms": [[101, 121]], "ID": "1939"}, {"text": "      The system integrates both dependency parse  tree pattern and semantic role labeler (SRL) results  of each input sentence when extracting the triples.", "acronyms": [[91, 94]], "long-forms": [[68, 89]], "ID": "1940"}, {"text": " 6 Experiments and Results We use the Wall Street Journal (WSJ) section of the Penn Treebank as our labeled source domain", "acronyms": [[59, 62]], "long-forms": [[38, 57]], "ID": "1941"}, {"text": " 2.2 CoSeC CoSeC (Comparing Semantics in Context) performs meaning comparison on the basis of an underspec-", "acronyms": [[11, 16]], "long-forms": [[18, 37]], "ID": "1942"}, {"text": "from Si.  Feature Causality Diagram (FCD): CNB allows each feature Y, which occurs in a  given document, to have a Feature Causality Diagram (FCD).", "acronyms": [[37, 40], [43, 46], [142, 145]], "long-forms": [[10, 35], [115, 140]], "ID": "1943"}, {"text": " 658     We investigate the effect of thyroid transcription factor 1 (TTF-1) ...x: a", "acronyms": [[70, 75]], "long-forms": [[46, 68]], "ID": "1944"}, {"text": "represented in an n ? n matrix of objects by a  multidimensional scaling (MDS) of the distance  between each object.", "acronyms": [[74, 77]], "long-forms": [[48, 72]], "ID": "1945"}, {"text": "nutcracker 155 22 62 312 449 0.0467 0.8342 39.5% (60% w/o B.O.) srl 0 487 437 63 13 0.9740 0.1260 55.0% Table 1: Results of the three systems on the SSI-testsuite ( TN = true negatives, FN = false negatives, TP = true positives, FP = false positives, N = TN + FP, P = TP + FN, Prec = Precision, ERROR: no", "acronyms": [[165, 167], [186, 188], [208, 210], [229, 231], [255, 257], [260, 262], [268, 270], [273, 275], [277, 281]], "long-forms": [[170, 184], [191, 206], [213, 227], [234, 249], [284, 293]], "ID": "1946"}, {"text": "of the Annual Meeting of the ACL and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP). ", "acronyms": [[117, 120], [29, 32], [110, 115]], "long-forms": [[120, 121]], "ID": "1947"}, {"text": "knowledge that we can get from these examples the required information to parse a new input sentence .  In our  approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema  where each SSTC describes a sentence, a representation tree as well as the correspondence b tween substrhzgs in ", "acronyms": [[193, 197], [229, 233]], "long-forms": [[154, 191]], "ID": "1948"}, {"text": "removed for expository reasons.  rewrites into an (optional) sentence adjunct (SA), a  subject, a verbphrase and subject's right adjunct ", "acronyms": [[79, 81]], "long-forms": [[61, 77]], "ID": "1949"}, {"text": "derivation decoding (Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and minimum Bayes risk system combination (MBR-SC) combining three systems.", "acronyms": [[137, 143], [26, 29], [89, 92]], "long-forms": [[98, 135], [55, 73]], "ID": "1950"}, {"text": "The straight case is the one mentioned above, treating all elements on the PARTS list equally (EQUAL). As a second op-", "acronyms": [[95, 100]], "long-forms": [[86, 93]], "ID": "1951"}, {"text": "(:all it as word accuracy(W.A.). We use one  more measure, called character accuracy(C.A.)  that measures the character edit distance be- ", "acronyms": [[85, 89], [26, 29]], "long-forms": [[66, 83], [12, 25]], "ID": "1952"}, {"text": "tance Metric from Relative Comparisons. Advances in Neural Information Processing Systems (NIPS).. J. Weeds, D. Weir and D. McCarthy.", "acronyms": [[91, 95]], "long-forms": [[52, 89]], "ID": "1953"}, {"text": " 1 Introduction  Spoken language translation (SLT) has become  more important due to globalization.", "acronyms": [[46, 49]], "long-forms": [[17, 44]], "ID": "1954"}, {"text": "\\[ Class (Tag) Kernel Nouns  act (AC)  an~ (AN)  art~fact (AR) ", "acronyms": [[44, 46]], "long-forms": [[39, 42]], "ID": "1955"}, {"text": " These algorithms are now getting keen atten-  tion from the natural anguage processing (NLP)  research community since the huge text corpus ", "acronyms": [[89, 92]], "long-forms": [[61, 87]], "ID": "1956"}, {"text": "s+trsl Alhnd qmrA<STnAEyA <lY Almryx ? India will send a satellite to Mars [in 2013]?. In every tree node, the terms above the line arepart of the CATiB annotations: the word, POS (VRB = verb, PRT = particle, PROP = proper noun, NOM = nominal)and relation (MOD = modifier, SBJ = subject, OBJ = object). The terms under the line are the Buckwalter POS tag, thelemma and the gloss, respectively.", "acronyms": [[181, 184], [193, 196], [209, 213], [229, 232], [257, 260], [273, 276], [288, 291], [176, 179], [147, 152], [347, 350]], "long-forms": [[187, 191], [199, 207], [216, 222], [235, 246], [263, 271], [279, 286], [294, 300]], "ID": "1957"}, {"text": "Hidden topic markov models. In Artificial Intelligence and Statistics (AISTATS), San Juan, Puerto Rico. ", "acronyms": [[71, 78]], "long-forms": [[31, 69]], "ID": "1958"}, {"text": " Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful", "acronyms": [[72, 75], [104, 106]], "long-forms": [[51, 70], [81, 96]], "ID": "1959"}, {"text": " Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly better result.", "acronyms": [[66, 69]], "long-forms": [[34, 64]], "ID": "1960"}, {"text": "There are four basic phrases in Korean: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP). Thus, chunking by rules is", "acronyms": [[122, 124], [53, 55], [71, 73], [91, 95]], "long-forms": [[102, 120], [40, 51], [58, 69], [76, 89]], "ID": "1961"}, {"text": "Natural Language Generation (NLG). For example, STOP is a Natural Language Generation (NLG) system that generates tailored smoking cessation let-", "acronyms": [[87, 90], [29, 32]], "long-forms": [[58, 85], [0, 27]], "ID": "1962"}, {"text": "matic and paradigmatic associations on the results of the clustering step. We conduct two experiments on SemEval-2012 task 2 and Scholastic Assessment Test (SAT) analogy quizzes to measure relational similarity to evaluate our model.", "acronyms": [[157, 160]], "long-forms": [[129, 155]], "ID": "1963"}, {"text": "Expanding  on a suggestion of Nfichieis (1982), we classify verbs  as subject equi (SEqui), object equi (OEqul), sub-  ject raising (SRals ing)  or object raising (ORuls ing) ", "acronyms": [[84, 89], [105, 110], [164, 173], [133, 142]], "long-forms": [[70, 82], [92, 103], [148, 162], [113, 131]], "ID": "1964"}, {"text": "AT(- +)  Stems to which the suffixes +ation and +ative may  attach are marked as (AT +), while those taking the  corresponding forms +ion and +ive are (AT -).", "acronyms": [[82, 86], [152, 154], [0, 2]], "long-forms": [[60, 66]], "ID": "1965"}, {"text": " 3.4 MAP Inference Maximum a posteriori (MAP) inference seeks the solution to", "acronyms": [[41, 44], [5, 8]], "long-forms": [[19, 39]], "ID": "1966"}, {"text": " BBLT Input Screen      We originally developed BBLT for ourselves as machine translation (MT) developers and evaluators, to rapidly see the meanings of Arabic strings", "acronyms": [[91, 93], [1, 5], [48, 52]], "long-forms": [[70, 89]], "ID": "1967"}, {"text": "(2) dobj? det:DT NN prep:IN 7DT/det=determiner, NN=noun, IN/prep=preposition, dobj=direct object", "acronyms": [[48, 50], [14, 16], [17, 19], [60, 64], [78, 82], [28, 35], [4, 8], [20, 24]], "long-forms": [[51, 55], [65, 76], [83, 96], [36, 46]], "ID": "1968"}, {"text": "PropBank defines core roles ARG0 through ARG5, which receive different interpretations for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional), as shown in Figure 2(a).", "acronyms": [[154, 162], [28, 32], [41, 45], [139, 145], [178, 186]], "long-forms": [[164, 172], [188, 199]], "ID": "1969"}, {"text": "1 (TICCL) we gradually developed in prior projects is now TICCLops (TICCL online processing system). TICCLops is a fully", "acronyms": [[58, 66], [3, 8], [101, 109]], "long-forms": [[68, 98]], "ID": "1970"}, {"text": "3.1.1 Directed Acyclic Graph The general SPRITE model can be thought of as a dense directed acyclic graph (DAG), where every document or topic is connected to every compo-", "acronyms": [[107, 110]], "long-forms": [[83, 105]], "ID": "1971"}, {"text": "ining both BLEU and NIST scores? relationship to their Unlabeled Accuracy Score(UAS). ", "acronyms": [[80, 83], [11, 15], [20, 24]], "long-forms": [[55, 79]], "ID": "1972"}, {"text": "automatique, GDR I3 ATALA, Paris, November 1999.  Tang E.K., Natural languages Analysis in machine translation (MT) based on the STCG, PhD thesis, Sains Malaysia University, Penang, March 1994", "acronyms": [[112, 114], [13, 16], [20, 25], [129, 133], [135, 138], [55, 58]], "long-forms": [[91, 110]], "ID": "1973"}, {"text": "~  = unrecognized input token.)  (ABC) = (A(BC)) = ((AB)C). Tim singletonbidi- ", "acronyms": [[53, 57]], "long-forms": [[42, 46]], "ID": "1974"}, {"text": "  Here the parameters are set using an algorithm  whose uniform resource name (URN),  xyz.edu/algo-1, is declared as an attribute of the ", "acronyms": [[79, 82]], "long-forms": [[56, 77]], "ID": "1975"}, {"text": "3.2 To resolve gapping under serial verb construction Serial verb construction (SVC) (Baker, 1989) is construction in which a sequence of verbs appears in", "acronyms": [[80, 83]], "long-forms": [[54, 78]], "ID": "1976"}, {"text": "We also wanted to determine if information about 6http://www.isi.edu/?ravichan/YASMET.html dialog acts (DA) helps the ranking task. If we", "acronyms": [[104, 106]], "long-forms": [[91, 102]], "ID": "1977"}, {"text": " 2 Question Classification We define Question Classification(QC) here to be the task that, given a question, maps it to one of", "acronyms": [[61, 63]], "long-forms": [[37, 59]], "ID": "1978"}, {"text": "  MTI. The original Medical Text Indexer (MTI)  system, shown in Figure 1, consists of an infra-", "acronyms": [[42, 45], [2, 5]], "long-forms": [[20, 40]], "ID": "1979"}, {"text": "nese kanji and words. The currently available  JWAD Version 1 (JWAD-V1) consists of  104,800 free word association responses col-", "acronyms": [[63, 70]], "long-forms": [[47, 61]], "ID": "1980"}, {"text": "For attribute selection on the composed vector, we use two methods we found to perform best in Hartung and Frank (2010): Entropy Selection (ESel) and Most Prominent Component (MPC).", "acronyms": [[140, 144], [176, 179]], "long-forms": [[121, 138], [150, 174]], "ID": "1981"}, {"text": "we used dialog features derived from manual annotations ? dialog acts (DA) and overt displays of power (ODP) ?", "acronyms": [[71, 73], [104, 107]], "long-forms": [[58, 69], [79, 102]], "ID": "1982"}, {"text": "not enter into speech recognition. Tillmann, Vogel, Ney, and Zubiaga (1997) proposes a dynamic programming (DP)?based search algorithm for statistical MT that monotonically translates the input sentence from left to right.", "acronyms": [[108, 110], [151, 153]], "long-forms": [[87, 106]], "ID": "1983"}, {"text": "All the words were categorized  into three types: Lexicon words (LWs), Factoid  words (FTs), Named Entity (NEs). Accordingly, ", "acronyms": [[107, 110], [65, 68], [87, 90]], "long-forms": [[93, 105], [50, 63], [71, 85]], "ID": "1984"}, {"text": "task yet to be tackled by TM but identified as an important potential application for it (Lewin et al 2008): Cancer Risk Assessment (CRA). Over the", "acronyms": [[133, 136], [26, 28]], "long-forms": [[109, 131]], "ID": "1985"}, {"text": " After detecting a new indefinite description (as ETA(x) : unlversity(x)) ReP  creates a new \"referential object'\" (RefO). During the discours6 (after the ", "acronyms": [[116, 120], [50, 56], [74, 77]], "long-forms": [[94, 112]], "ID": "1986"}, {"text": "only from the corresponding source language segment. We use the Moses statistical MT (SMT) toolkit (Koehn et al.,", "acronyms": [[86, 89]], "long-forms": [[70, 84]], "ID": "1987"}, {"text": "large and is often simplified.  Because we use belief propagation (BP) as baseline to compare to, and as a subroutine in our pro-", "acronyms": [[67, 69]], "long-forms": [[47, 65]], "ID": "1988"}, {"text": "proach of (Liu et al, 2004), in which IDs (category seeds) and instances are represented by vectors in a usual IR-style Vector Space Model (VSM), and similarity is measured by the cosine function:", "acronyms": [[140, 143], [111, 113]], "long-forms": [[120, 138]], "ID": "1989"}, {"text": "The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practice, a sequence", "acronyms": [[135, 137], [4, 7]], "long-forms": [[109, 133]], "ID": "1990"}, {"text": "2 Symmetrical Tversky?s Ratio Model In the field of mathematical psychology Tversky proposed the ratio model (TRM) (Tversky, 1977) motivated by the imbalance that humans have on", "acronyms": [[110, 113]], "long-forms": [[93, 108]], "ID": "1991"}, {"text": "1 Introduction As they are normally conceived, many tasks relevant to Computational Linguistics (CL), such as text categorization, clustering, and information retrieval, ignore the con-", "acronyms": [[97, 99]], "long-forms": [[70, 95]], "ID": "1992"}, {"text": "(Figure 2). Argviz is a web-based application, built using Google Web Toolkit (GWT),4 which allows users to visualize and manipulate SITS?s outputs en-", "acronyms": [[79, 82], [133, 138]], "long-forms": [[59, 77]], "ID": "1993"}, {"text": "of the reparandum coincides with the termination of  the fluent portion of the utterance, which we term the  INTERRUPTION SITE (IS). The DISFLUENCY INTERVAL ", "acronyms": [[128, 130]], "long-forms": [[109, 126]], "ID": "1994"}, {"text": "pseudo-terms?. We also discuss the use of Hidden Markov Models (HMMs) to capture contextual information.", "acronyms": [[64, 68]], "long-forms": [[42, 62]], "ID": "1995"}, {"text": "Afterward we name the TD composed of words from gold training set and tagged test set and as Na??ve TD (NTD) for its unbalanced coverage in training and test set.", "acronyms": [[104, 107], [22, 24]], "long-forms": [[93, 102]], "ID": "1996"}, {"text": "819 location (LO) of the incident (e.g. airport name), and the country (CO) where the incident occurred. ", "acronyms": [[72, 74], [14, 16]], "long-forms": [[63, 70], [4, 12]], "ID": "1997"}, {"text": "~  I n  recent  years  the  prob lem o f  man 'mach ine  communicat ion  by  means   o f  natura l  language (NL) i s  becoming  a pract i ca l  one .  And the  ", "acronyms": [[110, 112]], "long-forms": [[90, 108]], "ID": "1998"}, {"text": "phrase structure grammar (PSG) as thc tagging  formalisms(Lecch & Garside 1991), and some  adopt dependency grammar(DG) 1993, Komatsu,  Jin, & Yasuhara, 1993).", "acronyms": [[116, 118], [26, 29]], "long-forms": [[97, 114], [0, 24]], "ID": "1999"}, {"text": "ian.fletcher, peter.maguire@cs.man.ac.uk Abstract Dialogue Acts (DAs) which explicitly ensure mutual understanding are frequent", "acronyms": [[65, 68]], "long-forms": [[50, 63]], "ID": "2000"}, {"text": "Instead of using graph-based consensus  confidence as features in the log-linear model, we  perform structured label propagation (Struct-LP) to  re-rank the n-best list directly, and the similarity ", "acronyms": [[130, 139]], "long-forms": [[100, 128]], "ID": "2001"}, {"text": "1 In t roduct ion   Finding base noun phrases is a sensible first step  for many natural anguage processing (NLP) tasks:  Accurate identification of base noun phrases is ar- ", "acronyms": [[109, 112]], "long-forms": [[81, 107]], "ID": "2002"}, {"text": "The paper first provides a brief overview of Lexical Functional Grammar, and the Penn Arabic Treebank (ATB). The next section presents", "acronyms": [[103, 106]], "long-forms": [[86, 101]], "ID": "2003"}, {"text": "0.467 (+126%)?  Total Document Reciprocal Rank (TDRR) PubMed 0.495 0.137 0.038 0.331", "acronyms": [[48, 52]], "long-forms": [[16, 46]], "ID": "2004"}, {"text": "before the start of the current utterance.  Overlapping label (OL) an utterance on another channel with a particular DA tag overlaps the", "acronyms": [[63, 65], [117, 119]], "long-forms": [[44, 61]], "ID": "2005"}, {"text": "The most common and obvious way to  deal with disjunctive constraints i to expand the grammat-  ical description to disjunctive normal form (DNF) during a  pre-processing step, thereby eliminating disjunction from the ", "acronyms": [[141, 144]], "long-forms": [[116, 139]], "ID": "2006"}, {"text": " ? System integration, through SGML (the Standard Generalized Markup Language), both at the leve l of meaning analysis and at the overall application level .", "acronyms": [[31, 35]], "long-forms": [[41, 77]], "ID": "2007"}, {"text": " To overcome this problem, Gliozzo et al (2005) introduced the domain model (DM) and show how to define a domain VSM in which texts and terms", "acronyms": [[77, 79], [113, 116]], "long-forms": [[63, 75]], "ID": "2008"}, {"text": " Most of lexical networks, as networks extracted from real world, are small worlds (SW) networks.", "acronyms": [[84, 86]], "long-forms": [[70, 82]], "ID": "2009"}, {"text": "We encode the target state in the  similar way. Like the Vector Space Model(VSM),  we use a label matrix to represent each class as in ", "acronyms": [[76, 79]], "long-forms": [[57, 74]], "ID": "2010"}, {"text": "a wordbreak (WB). In other words, we model Chinese word segmentation as wordbreak (WB) identification which takes all CB?s as candidates and", "acronyms": [[83, 85], [13, 15], [118, 122]], "long-forms": [[72, 81], [2, 11]], "ID": "2011"}, {"text": "The last column lists the Spearman rank order correlation (?) of the rankings with the Berlin and Kay (B&K) ranks. ", "acronyms": [[103, 106]], "long-forms": [[87, 101]], "ID": "2012"}, {"text": "Then the lexicon Chinese Semantic  Dictionary (CSD) containing sense descriptions  and the corpus Chinese Senses Pool (CSP) annotated with senses are built interactively, simulta-", "acronyms": [[119, 122], [47, 50]], "long-forms": [[98, 117], [17, 45]], "ID": "2013"}, {"text": "ferent setups with this parameter. We compare the following setups: (1) The majority baseline (BL) i.e., choosing the most frequent label (SR). (", "acronyms": [[95, 97], [139, 141]], "long-forms": [[85, 93]], "ID": "2014"}, {"text": "the ratio of system?s moves stating that the requested information is not available; Number of abandoned requests (NAR) and abandoned-request ratio (ARR), i.e., the number and the ratio of the information-providing games", "acronyms": [[115, 118]], "long-forms": [[85, 113]], "ID": "2015"}, {"text": "have been opened.  Named entity recognition (NER) is one of the  many fields of NLP that rely on machine learn?", "acronyms": [[45, 48], [80, 83]], "long-forms": [[19, 43]], "ID": "2016"}, {"text": "For exam-  ple, an analysis of the texts using Mann and Thomp-  son's (1987) Rhetorical Structure Theory (RST) would  result primarily in the relations sequence  and jo in t  ", "acronyms": [[106, 109]], "long-forms": [[77, 104]], "ID": "2017"}, {"text": "It  is embedded to the C-value approach for  automatic term recognition (ATR), in the  form of weights constructed from statisti- ", "acronyms": [[73, 76], [23, 30]], "long-forms": [[45, 71]], "ID": "2018"}, {"text": "paradigm (Berners-Lee, 2006), which requires the use of uniform resource identifiers (URIs), the hypertext transfer protocol (HTTP), standard representation formats (such as RDF) and links to", "acronyms": [[126, 130], [86, 90], [174, 177]], "long-forms": [[97, 124], [56, 84]], "ID": "2019"}, {"text": " 1 Introduction Electroencephalography (EEG) and magnetoencephalography (MEG) are similar methods for", "acronyms": [[40, 43], [73, 76]], "long-forms": [[16, 38], [49, 71]], "ID": "2020"}, {"text": "is necessary to train sentence prediction models, a third approach that uses labeled comment data for training (CTr) but sentences for testing (STe) is included in the CTR/STE row.", "acronyms": [[144, 147], [112, 115], [168, 175]], "long-forms": [[121, 142], [85, 110]], "ID": "2021"}, {"text": " Therefore it makes sense to also extract data from machine readable dictionaries (MRDs). ", "acronyms": [[83, 87]], "long-forms": [[52, 81]], "ID": "2022"}, {"text": "Clear And Simple English (CASE) Caterpillar Fundamental English (CFE) Caterpillar Technical English (CTE) Diebold Controlled English (DCE)", "acronyms": [[101, 104], [26, 30], [65, 68], [134, 137]], "long-forms": [[70, 99], [0, 24], [32, 63], [106, 132]], "ID": "2023"}, {"text": "Table 1: First five SentiWordNet entries for cold#a In our experiments we use two different versions of SWN: SentiWordNet 1.0 (SWN1), the first release of SWN, and its updated version SentiWord-", "acronyms": [[127, 131], [104, 107], [155, 158]], "long-forms": [[109, 125]], "ID": "2024"}, {"text": "there is a link to the next node).  Prompts (PT) occur when the tutor attempts to elicit a meaningful contribution from the student.", "acronyms": [[45, 47]], "long-forms": [[36, 43]], "ID": "2025"}, {"text": "Section 7 concludes this article.  2 Automatic Speech Recognition (ASR)  Thai ASR research focused on two major topics.", "acronyms": [[67, 70], [78, 81]], "long-forms": [[37, 65]], "ID": "2026"}, {"text": "string is a false positive (FP). Each gold standard gene mention is counted as a false negative (FN) if it is not identified by the approach.", "acronyms": [[97, 99], [28, 30]], "long-forms": [[81, 95], [12, 26]], "ID": "2027"}, {"text": " 1 Introduction Todays natural user interfaces (NUI) for applications running on smart devices, e.g, phones (SIRI,", "acronyms": [[48, 51], [109, 113]], "long-forms": [[23, 46]], "ID": "2028"}, {"text": "3.2 Coordination Structures Among the most controversial annotation schemes are those of coordination structures (CS), which are groups of two or more tokens that are in coordina-", "acronyms": [[114, 116]], "long-forms": [[89, 112]], "ID": "2029"}, {"text": "For WSD evaluation, three measures are used: (1) Jaccard Index (JI), which measures the degree of overlap between the induced senses and the gold", "acronyms": [[64, 66], [4, 7]], "long-forms": [[49, 62]], "ID": "2030"}, {"text": " 1 Introduction Medical relation (MR) classification, an information extraction task in the clinical domain that was recently defined in the 2010 i2b2/VA Challenge (Uzuner et al.,", "acronyms": [[34, 36]], "long-forms": [[16, 32]], "ID": "2031"}, {"text": "for a comprehensive comparison: ? Mean absolute error (MAE) measures how closely predictions resemble their observed", "acronyms": [[55, 58]], "long-forms": [[34, 53]], "ID": "2032"}, {"text": "a user interface for the production of wordlevel annotations for an opinion-mining task in the information technology (IT) domain. ", "acronyms": [[119, 121]], "long-forms": [[95, 117]], "ID": "2033"}, {"text": "Litkowski, K. C.: Syntactic Clues and Lexical Resources in Question-Answering. In E. M. Voorhees & D. K. Harman (eds.), The Ninth Text Retrieval Conference (TREC-9). ( 2001) 157-166 10.", "acronyms": [[157, 163]], "long-forms": [[120, 155]], "ID": "2034"}, {"text": "ptishes a rather inconsequential change with respect o a  previously non-existent link or with respect to a link  No impairment (NI) Q Confusion (C)  Q Mislearning (ML) Q Insufficient Learning (IL) ", "acronyms": [[129, 131], [165, 167], [194, 196]], "long-forms": [[114, 127], [152, 163], [171, 192], [135, 144]], "ID": "2035"}, {"text": "five different linear classifiers to extract PPI from AIMed: L2-SVM, 1-norm soft-margin SVM (L1-SVM), logistic regression (LR) (Fan et al, 2008), averaged perceptron (AP) (Collins,", "acronyms": [[123, 125], [45, 48], [61, 67], [88, 91], [93, 99], [167, 169]], "long-forms": [[102, 121], [146, 165]], "ID": "2036"}, {"text": "974   Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875?879, October 25-29, 2014, Doha, Qatar.", "acronyms": [[94, 99]], "long-forms": [[44, 92]], "ID": "2037"}, {"text": "11  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675, October 25-29, 2014, Doha, Qatar.", "acronyms": [[92, 97]], "long-forms": [[42, 90]], "ID": "2038"}, {"text": "6. Demonstrative pronoun labels are collapsed to DEM PRON (person and number information is easily recovered)", "acronyms": [[53, 57], [49, 52]], "long-forms": [[59, 65]], "ID": "2039"}, {"text": "al., 2005; Joachims et al, 2009) formulation, as shown in Optimization Problem 1 (OP1), to learn a weight vector w.", "acronyms": [[82, 85]], "long-forms": [[58, 80]], "ID": "2040"}, {"text": "every lost arc translates to a set of lost parts, we can avoid repeating computations by storing the partial loss of every arc in a data structure (DS): e ?? ", "acronyms": [[148, 150]], "long-forms": [[132, 146]], "ID": "2041"}, {"text": "We conduct an extrinsic evaluation to compare  the different versions of ArSenL on the task of  subjectivity and sentiment analysis (SSA). We ", "acronyms": [[133, 136], [73, 79]], "long-forms": [[96, 131]], "ID": "2042"}, {"text": "For example, both the terms chiaroscuro and collage are classified under picture, image, icon in WordNet, but in the Art & Architecture Thesaurus (AA&T)16 chiaroscuro is categorized under perspective and shading techniques whereas collage is classified under image-making processes and", "acronyms": [[147, 151]], "long-forms": [[117, 145]], "ID": "2043"}, {"text": "Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, Mike Rosner, & Daniel Tapias, 310?314, Valletta, Malta. European Language Resources Association (ELRA).Ferra?ndez, Oscar, Michael Ellsworth, Rafael Mun?oz, & Collin F. Baker. 2010b.", "acronyms": [[183, 187]], "long-forms": [[142, 181]], "ID": "2044"}, {"text": "2 Pivot Translation Pivot translation is a translation from a source language (SRC) to a target language (TRG) through an intermediate pivot (or bridging) language (PVT).", "acronyms": [[106, 109], [79, 82], [165, 168]], "long-forms": [[89, 104], [62, 77], [135, 163]], "ID": "2045"}, {"text": "92  NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 28?36, Montre?al, Canada, June 7?8, 2012.", "acronyms": [[90, 95]], "long-forms": [[31, 88]], "ID": "2046"}, {"text": "Figure 3: The system architecture.   CA = communicative act. ", "acronyms": [[37, 39]], "long-forms": [[42, 59]], "ID": "2047"}, {"text": "programming alignment on the recognizer?s  hypothesis (HYP) and the non-literal transcription  that is used as reference (REF).  The alignment ", "acronyms": [[122, 125], [55, 58]], "long-forms": [[111, 120], [43, 53]], "ID": "2048"}, {"text": "volved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.", "acronyms": [[85, 88]], "long-forms": [[61, 83]], "ID": "2049"}, {"text": " To overcome the difficulty, we build a new Multilayer Search Mechanism (MSM). Different", "acronyms": [[73, 76]], "long-forms": [[44, 71]], "ID": "2050"}, {"text": "Abstract We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essen-", "acronyms": [[92, 95]], "long-forms": [[62, 90]], "ID": "2051"}, {"text": "The third is end position (EP), after a predi-  cate. Pre position (PreP) and post position (PostP)  are provided for adverbs as modifiers.", "acronyms": [[68, 72], [93, 98], [27, 29]], "long-forms": [[54, 66], [78, 91], [13, 25]], "ID": "2052"}, {"text": "constituents. For example, discussing the possible adaptation of Phillips' algorithm to incremental gener-  ation, Lager and Black (1994) point out that some versions of Categorial Grammar (CG) would make the  generator more talkative, by giving rise to \"a more generous notion of constituency\".", "acronyms": [[190, 192]], "long-forms": [[170, 188]], "ID": "2053"}, {"text": "sic and Young, 2011; Williams, 2010; Young et al., 2010) and Bayesian network (BN)-based methods (Raux and Ma, 2011; Thomson and Young,", "acronyms": [[79, 81]], "long-forms": [[61, 77]], "ID": "2054"}, {"text": "is placed sixth out of seventeen systems according to Mean Absolute Error (MAE) and third according to Root Mean Squared Error (RMSE). The", "acronyms": [[128, 132], [75, 78]], "long-forms": [[103, 126], [54, 73]], "ID": "2055"}, {"text": "{zhongzhi, nght}@comp.nus.edu.sg Abstract Word sense disambiguation (WSD) systems based on supervised learning", "acronyms": [[69, 72]], "long-forms": [[42, 67]], "ID": "2056"}, {"text": "DEP = dependency type ? MOR = morphological features (set) ?", "acronyms": [[24, 27], [0, 3]], "long-forms": [[30, 43], [6, 16]], "ID": "2057"}, {"text": "TF (Term Frequency)  is the word frequency within a document;  IDF (Inverse Document Frequency) is the  logarithm of the ratio of the total number of ", "acronyms": [[63, 66], [0, 2]], "long-forms": [[68, 94], [4, 18]], "ID": "2058"}, {"text": "of conditional random fields. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 870?878.", "acronyms": [[98, 101]], "long-forms": [[55, 96]], "ID": "2059"}, {"text": "Setting P0.1 P0.25 P0.33 P0.5 Best-F1 ContextSim (CS) 42.9 69.6 60.7 58.7 49.6 SpellingSim (SS) 90.5 74.2 69.9 64.6 50.9 (a) from baseline models", "acronyms": [[92, 94], [50, 52]], "long-forms": [[79, 90], [38, 48]], "ID": "2060"}, {"text": "participate in the interpretation f the CLS (e.g., elements bearing the thematic roles assigned by  the predicate, etc.). DPSs (DP structures) semantically characterize noun phrases. They consist ", "acronyms": [[122, 126], [40, 43]], "long-forms": [[128, 141]], "ID": "2061"}, {"text": "V is the vocabulary size.  The question difficulty estimation (QDE) task aims to automatically learn the question difficul-", "acronyms": [[63, 66]], "long-forms": [[31, 61]], "ID": "2062"}, {"text": "Constituents are tagged with IsA class labels from a large, automatically extracted lexicon, using a probabilistic context free grammar (PCFG). ", "acronyms": [[137, 141]], "long-forms": [[101, 135]], "ID": "2063"}, {"text": "CONN =  nil;  konj( KONJ )  FUNDF = fundf n( NOMINAL ); /* No nil */ ", "acronyms": [[20, 24], [0, 4], [28, 33]], "long-forms": [[14, 18]], "ID": "2064"}, {"text": "on three official testsets.  NIST 2008 Open Machine Translation (OpenMT) Evaluation9 has distributed test data from 2 domains: Newswire and Web.", "acronyms": [[65, 71]], "long-forms": [[39, 63]], "ID": "2065"}, {"text": "by adapting the baseline model to four adaptation domains. In particular, we attempt to interpret the results given in terms of the character error rate (CER) by  correlating them with the characteristics of the adaptation domain measured us-", "acronyms": [[154, 157]], "long-forms": [[132, 152]], "ID": "2066"}, {"text": "In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 1?5. ", "acronyms": [[101, 111]], "long-forms": [[22, 99]], "ID": "2067"}, {"text": "comparison with SMO-n) 6 Conclusions Automatic Text Simplification (ATS) aims to convert complex texts into a simpler form, which is more accessible to a wider audience.", "acronyms": [[68, 71], [16, 21]], "long-forms": [[37, 66]], "ID": "2068"}, {"text": "defined linguistic context, the task is to predict the class of a token. Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model.", "acronyms": [[98, 102]], "long-forms": [[73, 96]], "ID": "2069"}, {"text": "the exploration and verbalization history; and (4) it then sends semantic representations in the form of preverbal messages (PVMs) to the Formulation & Articulation components.", "acronyms": [[125, 129]], "long-forms": [[105, 123]], "ID": "2070"}, {"text": " In Proceedings of the International Conference on Computational Linguistics (COLING-04). ", "acronyms": [[78, 87]], "long-forms": [[51, 76]], "ID": "2071"}, {"text": "Branching quantification in DTS has partially been discussed in [7] and [8], in which we compared DTS with First Order Logic (FOL). However, FOL is limited in that it allows to", "acronyms": [[126, 129], [28, 31], [98, 101], [141, 144]], "long-forms": [[107, 124]], "ID": "2072"}, {"text": "Tables 79 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets.", "acronyms": [[98, 100]], "long-forms": [[82, 96]], "ID": "2073"}, {"text": "validation. This is carried out for the tweet text (TEXT), user-declared location (MB-LOC) and user-declared time zone (MB-TZ).", "acronyms": [[52, 56], [83, 89], [120, 125]], "long-forms": [[46, 50], [59, 81], [95, 118]], "ID": "2074"}, {"text": "2004. Evaluation of a Deidentification (De-Id) Software Engine to Share Pathology Reports and Clinical Documents for", "acronyms": [[40, 45]], "long-forms": [[22, 38]], "ID": "2075"}, {"text": "Norm = Normalisation of input prior to tagging. SUC = Subset of Stockholm-Umea? ", "acronyms": [[48, 51]], "long-forms": [[54, 73]], "ID": "2076"}, {"text": "Abstract In this paper, we address the problem of converting Dialectal Arabic (DA) text that is written in the Latin script (called", "acronyms": [[79, 81]], "long-forms": [[61, 77]], "ID": "2077"}, {"text": "76 4 Multi-media Information Networks A Multimedia Information Network (MINet) is a structured collection made up of a set of multimedia documents (e.g., texts and images) and links between these documents.", "acronyms": [[72, 77]], "long-forms": [[40, 70]], "ID": "2078"}, {"text": "20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M List Y 9.0 3.3 0.9 13.3 16030 Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%), D = deletions (%), I = insertions (%), T = total WER (%).", "acronyms": [[137, 139], [99, 104], [227, 230]], "long-forms": [[142, 153], [182, 191], [201, 211], [221, 226]], "ID": "2079"}, {"text": "in the lexicon to the following categories: protesters : NP seized : (S\\NP )/NP several : NP/NP", "acronyms": [[70, 74], [57, 59], [77, 79], [90, 95]], "long-forms": [[60, 68]], "ID": "2080"}, {"text": "ious learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas.", "acronyms": [[76, 79], [117, 120]], "long-forms": [[51, 74]], "ID": "2081"}, {"text": " 1 Introduction Information retrieval (IR) has been studied since an earlier stage [e.g., (Menzel, 1966)] and sev-", "acronyms": [[39, 41]], "long-forms": [[16, 37]], "ID": "2082"}, {"text": "FA8750-09-C-0181. The second author also thanks the Vietnam Education Foundation (VEF) for its sponsorship.", "acronyms": [[82, 85]], "long-forms": [[52, 80]], "ID": "2083"}, {"text": "special right category before COO > (5) the common left coordination category > (6) the other special right category > (7) the free cross-clause clausal category (IC) > (8) the common left cross-clause category > (9) the free cross-clause punctuations (PUS). ", "acronyms": [[253, 256], [30, 33], [163, 165]], "long-forms": [[239, 251], [56, 68]], "ID": "2084"}, {"text": "adapted to a new domain.  Word sense disambiguation (WSD), on the other hand, is the closely related task of assigning a sense", "acronyms": [[53, 56]], "long-forms": [[26, 51]], "ID": "2085"}, {"text": "The method seems to be a simple pattern  matching technique in a left-to-right fashion  but it helps in case of conjunct verbs (ConjVs). ", "acronyms": [[128, 134]], "long-forms": [[112, 126]], "ID": "2086"}, {"text": "Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The per-", "acronyms": [[111, 114]], "long-forms": [[84, 109]], "ID": "2087"}, {"text": "DEPICTION (DPC) TOPIC (TPC) SYNONYMY-NAME (SYN) CAUSALITY (CSL) PART-WHOLE (PW) MANNER (MNR) ANTONYMY (ANT) JUSTIFICATION (JST) HYPERNYMY (ISA) MEANS (MNS) PROBABILITY OF EXISTENCE (PRB) GOAL (GOL) ENTAIL (ENT) ACCOMPANIMENT (ACC) POSSIBILITY (PSB) BELIEF (BLF)", "acronyms": [[182, 185], [43, 46], [11, 14], [23, 26], [193, 196], [206, 209], [226, 229], [244, 247], [257, 260], [59, 62], [76, 78], [88, 91], [103, 106], [123, 126], [151, 154], [139, 142]], "long-forms": [[156, 167], [28, 36], [0, 9], [16, 21], [187, 191], [198, 204], [211, 224], [231, 242], [249, 255], [48, 57], [64, 74], [80, 86], [93, 101], [108, 121], [144, 149]], "ID": "2088"}, {"text": "Internet: chris@lsi .com NLP OBJECTIVES LSI's overall natural language processing (NLP) objective is the development of a broad coverage, reusable system which is readily transportable to additional domains, applications, and sublanguages in English, as well as", "acronyms": [[83, 86], [25, 28], [40, 43]], "long-forms": [[54, 81]], "ID": "2089"}, {"text": "argument structure agreement (Das, 2009), the  analysis of Non-MonoClausal Verb (NMCV) or  Serial Verb, Control Construction (CC),  Modal Control Construction (MCC), Passives ", "acronyms": [[126, 128], [81, 85], [160, 163]], "long-forms": [[104, 124], [59, 79], [132, 158]], "ID": "2090"}, {"text": "PP) MDI Missed Samples (MS) Bigram Missed Samples (MS) Figure 4: Values of PP and MS for automata for ad-hoc automata", "acronyms": [[51, 53], [0, 2], [4, 7], [24, 26], [82, 84], [75, 77]], "long-forms": [[35, 49], [8, 22]], "ID": "2091"}, {"text": "dominates the other.  Marcu?s Nuclearity Principle (NP) Marcu 1996 provides an alternative to the immediate interpretation and", "acronyms": [[52, 54]], "long-forms": [[30, 50]], "ID": "2092"}, {"text": "we will describe in detail in Section 3. They then  introduced a ClueWordSummarizer (CWS), a  graph-based unsupervised summarization ap-", "acronyms": [[85, 88]], "long-forms": [[65, 83]], "ID": "2093"}, {"text": "The lexical features used are word bigrams. The Part of Speech (PoS) of the target word and its neighbors make up the the syntactic", "acronyms": [[64, 67]], "long-forms": [[48, 62]], "ID": "2094"}, {"text": "6 Scope Resolution One way of dealing with scope ambiguities is by using underspecified representations (URs). A", "acronyms": [[105, 108]], "long-forms": [[73, 103]], "ID": "2095"}, {"text": "Most common approaches to language model adaptation, such as count merging and model interpolation, are special cases of maximum a posteriori (MAP) estimation (Bacchiani and Roark, 2003).", "acronyms": [[143, 146]], "long-forms": [[121, 141]], "ID": "2096"}, {"text": " 2 CFN and Its SRL task Chinese FrameNet(CFN) (You et al, 2005) is a research project that has been developed by Shanxi", "acronyms": [[41, 44], [3, 6], [15, 18]], "long-forms": [[24, 39]], "ID": "2097"}, {"text": "were computed for each scenario: bilingual evaluation under study (BLEU), position independent error rate (PER) and word error rate (WER). ", "acronyms": [[133, 136], [67, 71], [107, 110]], "long-forms": [[116, 131], [33, 59], [74, 105]], "ID": "2098"}, {"text": "patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction pat-", "acronyms": [[80, 83]], "long-forms": [[54, 78]], "ID": "2099"}, {"text": "propose a new inference method ? collective iterative classification (CIC), to find the maximum a posteriori (MAP) assignments for both entities", "acronyms": [[70, 73], [110, 113]], "long-forms": [[33, 68], [88, 108]], "ID": "2100"}, {"text": "EUD1.2 has the added benefit of being natively annotated with gold-standard Universal Dependencies (UD) parses (Nivre et al, 2015).", "acronyms": [[100, 102], [0, 3]], "long-forms": [[76, 98]], "ID": "2101"}, {"text": "tegrating more linguistic and structural knowledge with modern statistical techniques, and in particular, for syntax-based machine translation (MT) systems. ", "acronyms": [[144, 146]], "long-forms": [[123, 142]], "ID": "2102"}, {"text": "representation, a Partial-Lattice Markov Random Field (PL-MRF), which is a tractable variation of a Factorial Hidden Markov Model (HMM) for language modeling.", "acronyms": [[131, 134], [55, 61]], "long-forms": [[110, 129], [18, 53]], "ID": "2103"}, {"text": "namely the overall accuracy (Total-A) and the recall with respect to in-vocabulary words (IV-R),  OOV words (OOV-R) or multi-POS words (MTR).", "acronyms": [[109, 114], [90, 94], [125, 128], [136, 139]], "long-forms": [[98, 107], [69, 82]], "ID": "2104"}, {"text": "573   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 51?59, Sofia, Bulgaria, August 9, 2013.", "acronyms": [[71, 78]], "long-forms": [[37, 69]], "ID": "2105"}, {"text": " ? Argument Similarity (ArgSim): This baseline computes the cosine similarity of the vectors for", "acronyms": [[24, 30]], "long-forms": [[3, 22]], "ID": "2106"}, {"text": "Instead to measure topic coherence we follow (Newman et al, 2009) to compute the Pointwise Mutual Information (PMI) of topic words w.r.t wikipedia articles.", "acronyms": [[111, 114]], "long-forms": [[81, 109]], "ID": "2107"}, {"text": "rules, along with a few lexical rules involving a list of stop phrases, discourse cue phrases and wordlevel parts of speech (POS) tags. First, paragraph", "acronyms": [[125, 128]], "long-forms": [[108, 123]], "ID": "2108"}, {"text": "The projection  of the root node on the active leaves is referred to  as the M-BDU (Main BDU). Only syntactic infor-", "acronyms": [[77, 82]], "long-forms": [[84, 92]], "ID": "2109"}, {"text": " A project that is based on a roughly similar notion of text meaning representation (TMR) concepts is the ?", "acronyms": [[85, 88]], "long-forms": [[56, 83]], "ID": "2110"}, {"text": "Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien", "acronyms": [[139, 144], [64, 67], [170, 177]], "long-forms": [[126, 137]], "ID": "2111"}, {"text": "In this paper, we describe a mechanism which gen-  erates rebuttals to such rejoinders in the context of  arguments generated from Bayesian etworks (BNs)  (Pearl, 1988).", "acronyms": [[149, 152]], "long-forms": [[131, 147]], "ID": "2112"}, {"text": "Nearly 2,500 sets of related words in  LLOCE are organized according to 14 subjects and 129 topics (TOP). Cross references (REF) between sets,  topics, and subjects are also given to show various inter-sense r lations not captured within the same topic.", "acronyms": [[124, 127]], "long-forms": [[112, 122]], "ID": "2113"}, {"text": " 4.1 BRAT The brat rapid annotation tool (BRAT) is an opensource web-based annotation tool that supports a", "acronyms": [[42, 46]], "long-forms": [[14, 40]], "ID": "2114"}, {"text": "ity. In Proceedings of Treebanks and Linguistic Theories (TLT) 2003, Vaxjo, Sweden. ", "acronyms": [[58, 61], [71, 74]], "long-forms": [[23, 56]], "ID": "2115"}, {"text": " We focus on the following languages: German (DE), French (FR), Italian (IT), and Dutch (NL).", "acronyms": [[59, 61], [73, 75], [46, 48], [89, 91]], "long-forms": [[51, 57], [64, 71], [38, 44], [82, 87]], "ID": "2116"}, {"text": "context dependence and mutual information. Yamamoto and Church (2001) experiment with both mutual information and residual inverse document frequency (RIDF)1 as criteria for deciding Japanese words, and their main contribution is in affording", "acronyms": [[151, 155]], "long-forms": [[114, 149]], "ID": "2117"}, {"text": "representative popular heterogeneous corpora, i.e. 232 Penn Chinese Treebank (CTB) and PKU?s People?s Daily (PPD).", "acronyms": [[78, 81], [109, 112]], "long-forms": [[60, 76], [87, 107]], "ID": "2118"}, {"text": " 260 SentiWordNet(SWN) (Baccianella et al., ", "acronyms": [[18, 21]], "long-forms": [[5, 16]], "ID": "2119"}, {"text": "Discourse Relations (DR) 48.04 Entity Grid (EG) 67.74 Lexical Cohesion (LC) 61.63 Document Length 69.40", "acronyms": [[72, 74], [21, 23], [44, 46]], "long-forms": [[54, 70], [0, 19], [31, 42]], "ID": "2120"}, {"text": "For this task we train and test three different statistical models: an n-gram language model, a maximum entropy model (MaxEnt) and a (linear) support vector machine (SVM).", "acronyms": [[119, 125], [166, 169], [71, 77]], "long-forms": [[96, 111], [142, 164]], "ID": "2121"}, {"text": "R5   95 7 Antecedent Contained Deletion(ACD)  Further evidence for the proposed analysis comes ", "acronyms": [[40, 43]], "long-forms": [[10, 39]], "ID": "2122"}, {"text": "Figure 2: Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammatical data: Gr = Grammatical, AG = Agreement, RW = Real-Word, EW = Extra Word, MW = Missing", "acronyms": [[126, 128], [108, 110], [142, 144], [158, 160], [175, 177]], "long-forms": [[131, 140], [113, 124], [147, 156], [163, 173], [180, 187]], "ID": "2123"}, {"text": "the sentence. The segmentation model is a chain LVM (latent variable model) that aims to maximize a linear objective defined by:", "acronyms": [[48, 51]], "long-forms": [[53, 74]], "ID": "2124"}, {"text": "(Roelofs, 2004), experiments on priming (Schvaneveldt et al., 1976) or the tip of the tongue problem (TOT) (Brown and McNeill, 1996).", "acronyms": [[102, 105]], "long-forms": [[75, 92]], "ID": "2125"}, {"text": "For the nouns, 31 basic types are selected from  WordNet top categories (unique beginners): 2  entity(ENT) life~orm(LIF)  causal_agent(AGT) human(HUN) ", "acronyms": [[102, 105]], "long-forms": [[95, 100]], "ID": "2126"}, {"text": "bilities. Experience has shown that this kind of  full-fledged question answering (QA) over texts  from a wide range of domains is so difficult for ", "acronyms": [[83, 85]], "long-forms": [[63, 81]], "ID": "2127"}, {"text": "4  At the highest level, the text is a request addressed to CCC  members to vote against making the nuclear freeze initiative (NFI)  one of the issues about which CCC actively lobbies and promotes ", "acronyms": [[127, 130], [60, 63], [163, 166]], "long-forms": [[100, 125]], "ID": "2128"}, {"text": " ? New York Times (NYT) archive: a set of around 1.8 million news article from the archives", "acronyms": [[19, 22]], "long-forms": [[3, 17]], "ID": "2129"}, {"text": " 8. Adjacent Variety(AV) of the candidate. We", "acronyms": [[21, 23]], "long-forms": [[4, 19]], "ID": "2130"}, {"text": "3 Polylingual Topic Model The polylingual topic model (PLTM) is an extension of latent Dirichlet alocation (LDA) (Blei et al.,", "acronyms": [[108, 111], [55, 59]], "long-forms": [[80, 106]], "ID": "2131"}, {"text": "morphologically very rich. Different suffixes  may be attached to a Light Verb (LVs) (in this  case [YYY]) depending on the various features ", "acronyms": [[80, 83]], "long-forms": [[68, 78]], "ID": "2132"}, {"text": "for si ? Bm do Use Breadth First Search (BFS) to check if ?", "acronyms": [[41, 44]], "long-forms": [[19, 39]], "ID": "2133"}, {"text": "scheme that includes: (1) a pre-annotation that segments the dialogue into turns which are further segmented into Elementary Discourse Units (EDUs) with the author of each turn automatically given;", "acronyms": [[142, 146]], "long-forms": [[114, 140]], "ID": "2134"}, {"text": "2.3 Approach BUAP-RUN-3: Random Indexing and Bag of Concepts The vector space model (VSM) for document representation supporting search is probably the most", "acronyms": [[85, 88], [13, 23]], "long-forms": [[65, 83]], "ID": "2135"}, {"text": "few open source programs. Since we are interested in a fully supervised WSD tool, IMS (It Makes Sense) (Zhong and Ng, 2010) is selected in our", "acronyms": [[82, 85], [72, 75]], "long-forms": [[87, 95]], "ID": "2136"}, {"text": "Measuring and estimating post-editing effort is therefore a growing concern addressed by Confidence Estimation (CE) (Specia, 2011). ", "acronyms": [[112, 114]], "long-forms": [[89, 110]], "ID": "2137"}, {"text": " 246  AO = all objects  MO = matched objects ", "acronyms": [[6, 8], [24, 26]], "long-forms": [[11, 22], [29, 43]], "ID": "2138"}, {"text": "validate the performance of our method:  1. Precision@N (P@N). P@N measures how ", "acronyms": [[57, 60], [63, 66]], "long-forms": [[44, 55]], "ID": "2139"}, {"text": "as source domain training data (STrain), files 271300 as source domain testing data (STest) and files 590-596 as target domain testing data (TTest). We", "acronyms": [[141, 146], [32, 38], [85, 90]], "long-forms": [[113, 139], [3, 30], [57, 83]], "ID": "2140"}, {"text": "716 Figure 5: The NUMBERS System Architecture (CA = communicative act) The module network topology of the system is", "acronyms": [[47, 49], [18, 25]], "long-forms": [[52, 69]], "ID": "2141"}, {"text": "y) 7. Mutual dependency (MD) log P (xy)2P (x?)P (? y)", "acronyms": [[25, 27]], "long-forms": [[6, 23]], "ID": "2142"}, {"text": "/NN ?? /NN ]   Input: wi: word index (ID) in a given sentence. ", "acronyms": [[38, 40]], "long-forms": [[31, 36]], "ID": "2143"}, {"text": "\u0000 EFF (effect): We made her the secretary.  \u0000 ORIG (origin): She made a cake from apples. ", "acronyms": [[46, 50], [2, 5]], "long-forms": [[52, 58], [7, 13]], "ID": "2144"}, {"text": "Rule 3: Question word followed immediately by a verb (Example (3)).   Qp = question word + headword in the following Verb Phrase(VP) or NP chunk  Rule 4: Question word followed by a passive VP (Example (4)).", "acronyms": [[129, 131], [136, 138], [190, 192]], "long-forms": [[117, 127]], "ID": "2145"}, {"text": "1 Introduction In the past few years, a number of studies have focused on verbal semantic role labeling (SRL). ", "acronyms": [[105, 108]], "long-forms": [[81, 103]], "ID": "2146"}, {"text": "Above all, our goal is to integrate cross-media inference and create the linkage among the information extracted from those heterogenous data. Our novel Multi-media Information Networks (MiNets) representation initializes our idea about a basic ontology of the ranking system.", "acronyms": [[187, 193]], "long-forms": [[159, 185]], "ID": "2147"}, {"text": "apply shallow semantic (selectlonal) constraints, to filter out semantically anomalous parses, in a  second experiment. This procedure used PUNDIT's Selection Pattern Query and Response (SPQR)  component ~Lang1988\\].", "acronyms": [[187, 191]], "long-forms": [[149, 185]], "ID": "2148"}, {"text": "tives falls in the middle range and what causes the large and small divergence of the document collection pairs with different topics (DT) and the same topic (ST) or perspective (SP), respectively.", "acronyms": [[135, 137], [159, 161], [179, 181]], "long-forms": [[117, 133], [147, 157], [166, 177]], "ID": "2149"}, {"text": "The application of the program is demonstrated using the Aberdeen Report Judgment Scales (ARJS; Sporer, 2004) with a set of 72 deceptive and true accounts of a driving examination. Data on different types of inter-coder reliabilities are presented and implications for future research with computer-assisted qualitative coding procedures as well as training of coders are outlined. Credits This research has been supported by a grant from the German Science Foundation (Deutsche Forschungsgemeinschaft (DFG): Sp262/3-2) to the present author. The author would like to thank Edda Niederstadt and Nina F. Petermann for the coding of the data, and to Jaume Masip, Valerie Hauch, and Sarah Treiber for comments on an earlier version of this manuscript.", "acronyms": [[503, 506], [90, 94]], "long-forms": [[470, 501], [57, 88]], "ID": "2150"}, {"text": "gathered training data from parallel texts for the set of most frequently occurring noun, adjective, and verb types in the Brown Corpus (BC). These word", "acronyms": [[137, 139]], "long-forms": [[123, 135]], "ID": "2151"}, {"text": "Tipster (ADEPT) Program is a demonstration project  aimed at alleviating problems currently being faced by  the Office of Information Resources (OIR). OIR has ", "acronyms": [[145, 148], [9, 14], [151, 154]], "long-forms": [[112, 143]], "ID": "2152"}, {"text": "most blogged about articles? of the New York Times (NYT)1. ", "acronyms": [[52, 55]], "long-forms": [[36, 50]], "ID": "2153"}, {"text": "outer: the perceived external frame or point of reference for  the action, event, or state as a whole  Means (MNS):  inner: the perceived immediate affeetor or effeetor of the ", "acronyms": [[110, 113]], "long-forms": [[103, 108]], "ID": "2154"}, {"text": "Ihe maohine translation problem has recently been replaced  by much narrower goals and computer processing of language has  become part df artificial intelligence (AI), speech recognition,  and structural pattern recognition.", "acronyms": [[164, 166]], "long-forms": [[139, 162]], "ID": "2155"}, {"text": "ers; (ii) to design an initial policy for reinforcement learning of multimodal clarifications.4 We use the Nite XML Toolkit (NXT) (Carletta et al, 2003) to represent and browse the data and to de-", "acronyms": [[125, 128]], "long-forms": [[107, 123]], "ID": "2156"}, {"text": "guided learning. The approach taken has been to en-  code an artificial neural network (ANN) in a genome  which stores its architecture and learning rules.", "acronyms": [[88, 91]], "long-forms": [[61, 86]], "ID": "2157"}, {"text": " 5Note that this is a recursive lexical rule, which  Adjunct Extraposition Lexical Rule (AELR)  \"r,oc \\[\\] ICATIHEAD nou,~ Vverb\\] ", "acronyms": [[89, 93]], "long-forms": [[53, 87]], "ID": "2158"}, {"text": "observed in Dutch. Dutch shows a pattern in which  an arbitrary number of noun phrases (NP's) may be  followed by a finite verb and an arbitrary number ", "acronyms": [[88, 92]], "long-forms": [[74, 86]], "ID": "2159"}, {"text": "stem of JUMP = <jump>.   sense of JUMP = jumping. ", "acronyms": [[34, 38], [8, 12]], "long-forms": [[41, 48], [16, 20]], "ID": "2160"}, {"text": "For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M paral-", "acronyms": [[124, 126], [24, 27]], "long-forms": [[114, 122]], "ID": "2161"}, {"text": " 4.3 Counting and Calculation The SRI Language Modelling Toolkit (SRILM) (Stolcke and others, 2002) is used to count the frequencies in our work.", "acronyms": [[66, 71]], "long-forms": [[34, 56]], "ID": "2162"}, {"text": "Center for Language Technology. After accomplishing the task concerning named entity (NE)  identification, we go on studying identification ", "acronyms": [[86, 88]], "long-forms": [[72, 84]], "ID": "2163"}, {"text": "ical relations may be at the head of multiple arcs.  For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ).", "acronyms": [[87, 92], [140, 145]], "long-forms": [[70, 85], [124, 138]], "ID": "2164"}, {"text": "tailment pairs to train an entailment classifier, our model was only trained on the 800 text-hypothesis pairs found in the RTE-3 Development Set (DevSet). ", "acronyms": [[146, 152], [123, 128]], "long-forms": [[129, 144]], "ID": "2165"}, {"text": "crimination information (MDI) estimation criterion for speech recognition and notes an improvement in terms of perplexity and word error rate (WER). ", "acronyms": [[143, 146], [25, 29]], "long-forms": [[126, 141]], "ID": "2166"}, {"text": "sions to identify stylistic shifts in paraphrase, allowing us to differentiate stylistic properties in the Paraphrase Database (PPDB) with high accuracy. Sec-", "acronyms": [[128, 132]], "long-forms": [[89, 126]], "ID": "2167"}, {"text": "Jiang, Hua. Xu} @uth.tmc.edu tangbuzhou@gmail.com yukun.chen@Vanderbilt. Edu      Abstract This work describes the participation of the University of Texas Health Science Center at Houston (UTHealth) team on the SemEval 2014 ? Task 7 analysis of clinical text challenge.", "acronyms": [[190, 198]], "long-forms": [[136, 188]], "ID": "2168"}, {"text": "making procedures.  Latent semantic analysis (LSA) (Deerwester et al.,", "acronyms": [[46, 49]], "long-forms": [[20, 44]], "ID": "2169"}, {"text": "(? baseline?) and MT (Madnani et al, 2012). RAE", "acronyms": [[18, 20], [44, 47]], "long-forms": [[22, 32]], "ID": "2170"}, {"text": "t have also been used.  2.2 EasyAdapt (EA) In this section, we give a brief overview of", "acronyms": [[39, 41]], "long-forms": [[28, 37]], "ID": "2171"}, {"text": "  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 120?125, Boulder, Colorado, June 2009.", "acronyms": [[87, 92]], "long-forms": [[46, 85]], "ID": "2172"}, {"text": "In addition, we also  experimented with different combinations of  translation models (TM), phrase-based and  factor-based, trained on various datasets to ", "acronyms": [[87, 89]], "long-forms": [[67, 85]], "ID": "2173"}, {"text": "ules, distributed on two computers.  The graphical user interface (GUI) has a close link to the dialogue manager since it integrates sev-", "acronyms": [[67, 70]], "long-forms": [[41, 65]], "ID": "2174"}, {"text": "Within Acquilex IP Project, a unification framework  based on typed feature structures \\[4\\] was ddveloped, the  LKB (Lexical Knowledge Base), in order to represent  conceptual units corresponding to lexieal senses, lexical ", "acronyms": [[113, 116], [16, 18]], "long-forms": [[118, 140]], "ID": "2175"}, {"text": "     1 Introduction  Most of the natural language generation (NLG)  components in current dialog systems are imple-", "acronyms": [[62, 65]], "long-forms": [[33, 60]], "ID": "2176"}, {"text": "structure by first computing the similarity of each proposition to the others using a Latent Dirichlet Allocation (LDA) model. LDA is a genera-", "acronyms": [[115, 118], [127, 130]], "long-forms": [[86, 113]], "ID": "2177"}, {"text": "In E.M. Voorhees and  D.K. Harman, editors, The 3d Text RE-  trieval Conference (TREC-3). ", "acronyms": [[81, 87], [3, 6], [22, 25]], "long-forms": [[48, 79]], "ID": "2178"}, {"text": "System and Datasets We use the Moses phrasebased MT system (Koehn et al, 2007) and consider Urdu?English (UR?EN), Chinese?English (ZH?EN) translation, and Arabic?English", "acronyms": [[106, 111]], "long-forms": [[92, 104]], "ID": "2179"}, {"text": "have to be induced from parallel corpora.  An inversion transduction grammar (ITG) strikes a good balance between STGs and SDTGs,", "acronyms": [[78, 81], [114, 118], [123, 128]], "long-forms": [[46, 76]], "ID": "2180"}, {"text": "? P2E1N3S2, C S D G ASD Simplified Technical English (ASD-STE) (ASD 2013), often abbreviated to Simplified Technical English (STE) or just Simplified English, is a CNL for the aerospace", "acronyms": [[54, 61], [0, 10], [126, 129], [164, 167], [64, 67]], "long-forms": [[20, 52], [96, 124]], "ID": "2181"}, {"text": "domain-oriented semantics of the GENIA event corpus, and suggests a factor for utilizing NLP techniques for Text Mining (TM) in the bio-medical domain.", "acronyms": [[121, 123], [33, 38], [89, 92]], "long-forms": [[108, 119]], "ID": "2182"}, {"text": "trieval process. ( Zhou and Wade, 2009b) proposed a Latent Dirichlet Allocation (LDA)based method to model the latent structure of ", "acronyms": [[81, 84]], "long-forms": [[52, 79]], "ID": "2183"}, {"text": "                                                    2  In our system, we define ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone  number, and WWW.", "acronyms": [[148, 151], [114, 118], [190, 193]], "long-forms": [[140, 146], [108, 112]], "ID": "2184"}, {"text": "The contextual information about social status and  sentence-external individuals can bc included in the  attribute CONTEXT (CONX). Ill order to see values the ", "acronyms": [[125, 129]], "long-forms": [[116, 123]], "ID": "2185"}, {"text": "1 Motivation  Question Answering has emerged as a key area in  natural language processing (NLP) to apply question parsing, information extraction, summariza-", "acronyms": [[92, 95]], "long-forms": [[63, 90]], "ID": "2186"}, {"text": "2.4 Optimisation and Sampling from a WCFG Optimisation in a weighted CFG (WCFG)3, that is, finding the maximum derivation, is well stud-", "acronyms": [[74, 78], [37, 41]], "long-forms": [[60, 72]], "ID": "2187"}, {"text": "recording  information pertinent to treatment of a patient that consists of a number of subsections such as Chief Complaint (CC), History of Present Illness (HPI),", "acronyms": [[125, 127], [158, 161]], "long-forms": [[108, 123], [130, 156]], "ID": "2188"}, {"text": "get node and another node on the dependency parsed tree: ANC (ancestor), DES (descendant), SIB (sibling), and TARGET (target word). Figure 5 shows", "acronyms": [[110, 116], [57, 60], [73, 76], [91, 94]], "long-forms": [[118, 124], [62, 70], [78, 88], [96, 103]], "ID": "2189"}, {"text": " 1 Introduction Many problems in natural language processing (NLP) involve optimizing some objective function over a set of", "acronyms": [[62, 65]], "long-forms": [[33, 60]], "ID": "2190"}, {"text": "Table 3: Statistics of training and test corpus for the Canadian Hansards task (PP=perplexity, SL=sentence length). ", "acronyms": [[95, 97]], "long-forms": [[98, 113]], "ID": "2191"}, {"text": " 1 Introduction  Relation Extraction (RE) aims to identify a set of  predefined relations between pairs of entities in ", "acronyms": [[38, 40]], "long-forms": [[17, 36]], "ID": "2192"}, {"text": "2 Methods In this IRB-approved study, we obtained the Shared Annotated Resource (ShARe) corpus originally generated from the Beth Israel Dea-", "acronyms": [[81, 86]], "long-forms": [[54, 79]], "ID": "2193"}, {"text": "2 Data In this study, we use a collection of blog posts from five blogs: Carpetbagger(CB)1, Daily Kos(DK)2, Matthew Yglesias(MY)3, Red State(RS)4, and Right", "acronyms": [[86, 88], [102, 104], [125, 127], [141, 143]], "long-forms": [[73, 84], [92, 100], [108, 124], [131, 140]], "ID": "2194"}, {"text": "it can therefore mean ? prostrated on the threshold  and respectfully (AD) paid visits three times? or ", "acronyms": [[71, 73]], "long-forms": [[53, 56]], "ID": "2195"}, {"text": "logical structure of the text is a boundary between two logical segments (see Figure 1).  The method is called Logical TextTiling (LTT), due to some similarities with the TextTiling algorithm for topic shifts detection (Hearst, 1997).", "acronyms": [[131, 134]], "long-forms": [[111, 129]], "ID": "2196"}, {"text": "We evaluated our Chinese word segmenter in the open track, on all 4 corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and", "acronyms": [[101, 103], [136, 141], [164, 167]], "long-forms": [[84, 99], [106, 121], [144, 162]], "ID": "2197"}, {"text": "Both Russian and Czech have relatively free word order, so it may seem an odd choice to use a Markov model (MM) tagger. Why should second order", "acronyms": [[108, 110]], "long-forms": [[101, 106]], "ID": "2198"}, {"text": "In this paper we present our entry to the WMT?13 shared task: Quality Estimation (QE) for machine translation (MT). ", "acronyms": [[111, 113], [42, 48], [82, 84]], "long-forms": [[90, 109], [62, 80]], "ID": "2199"}, {"text": "tant of these is the controlled vocabulary terms assigned by human indexers. NLM?s controlled vocabulary thesaurus, Medical Subject Headings (MeSH),2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000", "acronyms": [[142, 146], [77, 82]], "long-forms": [[116, 140]], "ID": "2200"}, {"text": " Table 1: Classifier features in predicate disambiguation (PredDis), argument identification (ArgId), and argument labeling (ArgLab).", "acronyms": [[94, 99], [59, 66], [125, 131]], "long-forms": [[69, 92], [33, 57], [106, 123]], "ID": "2201"}, {"text": "by using multiple learners and a label integrator.  We have developed a forward (FR) and a backward relationship (BR) learner to learn relation-", "acronyms": [[81, 83], [114, 116]], "long-forms": [[72, 79], [91, 112]], "ID": "2202"}, {"text": " 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mainly", "acronyms": [[41, 44], [7, 10]], "long-forms": [[11, 39]], "ID": "2203"}, {"text": "duction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original", "acronyms": [[87, 90]], "long-forms": [[57, 85]], "ID": "2204"}, {"text": "ambiguous verb structure in a garden-path in two ways; one is as a subordinate clause (MV), the other is a Reduced Relative (RR). He defined", "acronyms": [[125, 127], [87, 89]], "long-forms": [[107, 123]], "ID": "2205"}, {"text": " Based on the statistics shown in Table 3, the likelihood ratio tests (LRT) model captures the statistical association between a pattern p and a word", "acronyms": [[71, 74]], "long-forms": [[47, 69]], "ID": "2206"}, {"text": "relates only loosely to the semantics of natural language. The work we present in  this paper differs from all previous work in natural anguage processing (NLP) in at  least two respects.", "acronyms": [[156, 159]], "long-forms": [[128, 154]], "ID": "2207"}, {"text": "news headlines (headlines); mapping of lexical resources from Ontonotes to Wordnet (OnWN) and from FrameNet to WordNet (FNWN); and evaluation of machine translation (SMT).", "acronyms": [[120, 124]], "long-forms": [[99, 118]], "ID": "2208"}, {"text": "various subsets of the documents in the English Gigaword corpus, chiefly drawn from New York Times (NYT) and Agence France Presse (AFP).1 2.1 Are Discounts Constant?", "acronyms": [[131, 134], [100, 103]], "long-forms": [[109, 129], [84, 98]], "ID": "2209"}, {"text": "based on a probabilistic model. We investigate two methods using Latent Dirichlet Allocation (LDA) (Blei, 2003) in ?", "acronyms": [[94, 97]], "long-forms": [[65, 92]], "ID": "2210"}, {"text": "tagging. These errors in the training corpora affects badly to the machine learning (ML) based models. ", "acronyms": [[85, 87]], "long-forms": [[67, 83]], "ID": "2211"}, {"text": "Where is that??).  Finally, the task features (TASK) reflect conflicting instructions in the domain.", "acronyms": [[47, 51]], "long-forms": [[32, 36]], "ID": "2212"}, {"text": "3.3 Aspect term extraction Our approach for aspect term extraction is based on Conditional Random Fields (CRF). The choice", "acronyms": [[106, 109]], "long-forms": [[79, 104]], "ID": "2213"}, {"text": "tourist resources was made by the Direcc?a?o Geral de Turismo (DGT) and afterwards the Inventory of Tourist Resources (IRT) emerged. ", "acronyms": [[119, 122], [63, 66]], "long-forms": [[87, 117], [45, 61]], "ID": "2214"}, {"text": " OOV Handling Techniques and their Combination We compare our baseline system (BASELINE) to each of our basic techniques and their full combi-", "acronyms": [[79, 87], [1, 4]], "long-forms": [[62, 77]], "ID": "2215"}, {"text": " 2.3 IR Similarity Measures (IR) The information retrieval?based features (IR) were based on a dump of English Wikipedia from Novem-", "acronyms": [[75, 77], [5, 7], [29, 31]], "long-forms": [[37, 58]], "ID": "2216"}, {"text": "for re-ranking in the context of name tagging.  Maximum Entropy modeling (MaxEnt) has  been extremely successful for many NLP classifi-", "acronyms": [[74, 80], [122, 125]], "long-forms": [[48, 63]], "ID": "2217"}, {"text": "their semantic deviation values. The result is a  list of pairs called the ICS (Initial Cluster Set). ", "acronyms": [[75, 78]], "long-forms": [[80, 99]], "ID": "2218"}, {"text": "  1 Introduction  INTERA (Integrated European language data  Repository Area, Contract 22076Y2C2DMAL2) is ", "acronyms": [[18, 24]], "long-forms": [[26, 76]], "ID": "2219"}, {"text": "Table 1: Mean relative frequencies and standard deviation for each class (A(nimate) vs. I(nanimate)) from feature extraction (SUBJ=Transitive Subject, OBJ=Object, GEN=Genitive -s, PASS=Passive byphrase, ANAAN=Anaphoric reference by animate pronoun, ANAIN=Anaphoric reference by inanimate pronoun, REFL=Anaphoric reference by reflexive pronoun).", "acronyms": [[203, 208], [126, 130], [151, 154], [163, 166], [180, 184], [249, 254], [297, 301]], "long-forms": [[209, 239], [155, 161], [167, 175], [185, 192], [255, 287], [325, 334]], "ID": "2220"}, {"text": " 3 The TMop framework TMop (Translation Memory open-source purifier) is an open-source TM cleaning software written", "acronyms": [[22, 26], [87, 89]], "long-forms": [[28, 46]], "ID": "2221"}, {"text": "cabbage 9 chou 1 chou blossom 25 fleur 73 commande carpet 39 tapis 1 tapis bitter 59 amer 1 amer hammer 67 marteau 1 marteau bread 82 pain 1 pain citizen 115 citoyen 1 citoyen bath 178 bain 1 bain butterfly 201 papillon 1 papillon eat 208 manger 1 manger butter 220 beurre 59 terre eagle 282 aigle 1 aigle cheese 527 fromage 1 fromage cold 539 froid 1 froid deep 585 profond 1 profond cottage 624 cabanon 1 cabanon earth 702 terre 53 tabac child 735 enfant 1 enfant bed 806 lit 2 table beautiful 923 beau 1 beau care 1267 soin 1 soin hand 1810 main 2 main city 2610 ville 1 ville girl 2673 fille 1 fille green 2861 vert 1 vert blue 2914 bleu 1 bleu hard 3615 dur 1 dur black 9626 noir 1 noir Bible 17791 Bible 1 Bible foot 23548 pied 8 siffler chair 24027 chaise 1 chaise fruit 38544 fruit 1 fruit  Table 2: Results for the language pair English ? French. The meaning of the columns is as follows: ESW = English source word; CF = corpus frequency of English source word; ET = expected translation according to gold standard; RE = computed rank of expected translation; CT = computed translation. ", "acronyms": [[925, 927], [1069, 1071], [898, 901], [971, 973], [1025, 1027]], "long-forms": [[930, 946], [1074, 1094], [904, 923], [976, 996], [1039, 1055]], "ID": "2222"}, {"text": "from Association. ACM Transactions on Information Systems (TOIS) 21:315-346. ", "acronyms": [[59, 63], [18, 21]], "long-forms": [[22, 57]], "ID": "2223"}, {"text": "2   TASK A: Question Generation from Paragraphs  1.1   Task Definition  The Question Generation from Paragraphs (QGP) task challenges participants to  generate a list of 6 questions from a given input paragraph.", "acronyms": [[113, 116]], "long-forms": [[76, 111]], "ID": "2224"}, {"text": "The number of sentences with product features  ? Word level (WL)  ?", "acronyms": [[61, 63]], "long-forms": [[49, 59]], "ID": "2225"}, {"text": "2 Description of the France Telecom 3000 Voice Agency corpus The France Telecom 3000 (FT3000) Voice Agency service, the first deployed vocal service at France", "acronyms": [[86, 92]], "long-forms": [[65, 84]], "ID": "2226"}, {"text": " 2 Sign language phenomena Sign Languages (SLs) involve simultaneous manual and non-manual components for conveying mean-", "acronyms": [[43, 46]], "long-forms": [[27, 41]], "ID": "2227"}, {"text": "(DE), Greek (EL), English (EN), Spanish (ES), French (FR), Italian (IT), Korean (KO), Dutch (NL), Portugese (PT), Russian (RU), Swedish (SV) and Chinese (ZH) ?", "acronyms": [[123, 125], [1, 3], [13, 15], [27, 29], [41, 43], [54, 56], [68, 70], [81, 83], [93, 95], [109, 111], [137, 139], [154, 156]], "long-forms": [[114, 121], [18, 25], [32, 39], [46, 52], [59, 66], [73, 79], [86, 91], [98, 107], [128, 135], [145, 152]], "ID": "2228"}, {"text": "pute probability scores of word sequences. The general conversational language model (LM) is based on data from the SWITCHBOARD corpus and a small", "acronyms": [[86, 88]], "long-forms": [[70, 84]], "ID": "2229"}, {"text": "tic models, which in this case are hidden Markov models (HMM), and described in terms of wellknown Mel frequency cepstral coefficients (MFCCs) (Benesty et al, 2008).", "acronyms": [[136, 141], [57, 60]], "long-forms": [[99, 134], [35, 54]], "ID": "2230"}, {"text": "con using label propagation and ground truth EPA values (POS= part-of-speech, W= the number of the induced words, MAS=mean absolute error, and RMSE= root mean squared error", "acronyms": [[114, 117], [45, 48], [57, 60], [143, 147]], "long-forms": [[118, 131], [62, 76], [107, 112], [149, 172]], "ID": "2231"}, {"text": "Nincc NIA ~ \\]laS started moving from toy  problems to ,'eal applications one of the biggest  difficully has been Knowledge Acquisition (KA)  of different lypes (lexical, grammatical, domain ", "acronyms": [[137, 139]], "long-forms": [[114, 135]], "ID": "2232"}, {"text": "lingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents", "acronyms": [[100, 103], [50, 53], [68, 71], [73, 77]], "long-forms": [[90, 98]], "ID": "2233"}, {"text": "We chose Gaussian distributions. If the parents of node X are Y, P (X|Y ) = N(m + W ?", "acronyms": [[68, 71]], "long-forms": [[56, 63]], "ID": "2234"}, {"text": "4.0 release. This was also the training data used in the experiments in the Parsing the Web (PTW) shared task at NAACL 2012.2 In the shared task", "acronyms": [[93, 96], [113, 118]], "long-forms": [[76, 91]], "ID": "2235"}, {"text": "processing applications, su ch as larg e-vocab u lary speech recog nition (L V CS R), statistical machine translation (S M T ) and information retrieval (IR), is the morpholog ical analy sis of w ords.", "acronyms": [[154, 156], [119, 124], [75, 83]], "long-forms": [[131, 152], [86, 117], [34, 73]], "ID": "2236"}, {"text": "project.  In section 2 we provide an overview of the automatic compound processing (AuCoPro) project, which forms the background of this research.", "acronyms": [[84, 91]], "long-forms": [[53, 82]], "ID": "2237"}, {"text": "Table 5: Final test accuracies for Chinese. UAS = unlabeled attachment score; UEM = unlabeled exact match; LAS = labeled attachment score.", "acronyms": [[78, 81]], "long-forms": [[84, 105]], "ID": "2238"}, {"text": "Q-based Intersection PTSD stands for posttraumatic stress disorder and is a psychological disorder.  Generic Union Posttraumatic stress disorder (PTSD) is a psychological disorder, which is classified as an anxiety disorder in the DSM-IV, caused by a mental trauma (also called psychotrauma) that", "acronyms": [[146, 150], [21, 25], [231, 237]], "long-forms": [[115, 144]], "ID": "2239"}, {"text": " 1 Introduction Using natural language processing (NLP) techniques to mine software corpora such as code com-", "acronyms": [[51, 54]], "long-forms": [[22, 49]], "ID": "2240"}, {"text": " 3 Background Slot filling (SF) is a query-oriented relation extraction (RE) task in the Knowledge Base Popu-", "acronyms": [[28, 30], [73, 75]], "long-forms": [[14, 26], [52, 71]], "ID": "2241"}, {"text": "3.2 Result of Chinese NER We evaluated our named entity recognizer on the SIGHAN Microsoft Research Asia(MSRA) corpus in both closed and open track.", "acronyms": [[105, 109], [22, 25], [74, 80]], "long-forms": [[81, 104]], "ID": "2242"}, {"text": "  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 67?68, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[71, 76]], "long-forms": [[37, 69]], "ID": "2243"}, {"text": "functions to SGML-mark the input.  Fast Partial Parser (FPP) .  The ultimate ", "acronyms": [[56, 59], [13, 17]], "long-forms": [[35, 54]], "ID": "2244"}, {"text": "contrast, L/RMC = left/right-most char,  L/RMW = left/right-most word, VS = vowel  sequences, HYPH = hyphenation, CASE =  case, PM = parenthesized material.", "acronyms": [[94, 98], [10, 15], [41, 46], [71, 73], [114, 118], [128, 130]], "long-forms": [[101, 112], [18, 38], [49, 69], [76, 92], [122, 126], [133, 155]], "ID": "2245"}, {"text": "The next-to-last  column shows the precision (PRE)--the true positives divided by all verbs that Lerner  judged to be +S. The final column shows the recall (REC)--the true positives divided  by all verbs that were judged +S by hand.", "acronyms": [[157, 160], [46, 49]], "long-forms": [[149, 155], [35, 44]], "ID": "2246"}, {"text": "Transactions of the Association for Computational Linguistics (TACL), 1:25?36. ", "acronyms": [[63, 67]], "long-forms": [[0, 61]], "ID": "2247"}, {"text": " The improvement of PAS analysis would benefit many natural language processing (NLP) applications, such as information extraction, summariza-", "acronyms": [[81, 84], [20, 23]], "long-forms": [[52, 79]], "ID": "2248"}, {"text": "we use the DUC2002 and DUC2004 data sets, both of which are open benchmark data sets from Document Understanding Conference (DUC) for generic automatic summarization evaluation.", "acronyms": [[125, 128], [11, 14], [23, 26]], "long-forms": [[90, 123]], "ID": "2249"}, {"text": "characters ? are often referred to by pronouns or definite noun phrases (NPs) instead of explicit repetition. ", "acronyms": [[73, 76]], "long-forms": [[59, 71]], "ID": "2250"}, {"text": " The two-level analysis of the cited forms ap-  pears below ST = sm'face tape, PT -- pattern  tape, 115.\\[' -- root tape, VT : vocal{sin tat)e , and ", "acronyms": [[60, 62]], "long-forms": [[65, 77]], "ID": "2251"}, {"text": "exploited in (Goldberg and Zhu, 2006), which seeks document sentiments as an output of an optimisation problem (OPTIM) and the algorithm adopted by (Wu et al2009), that uses ranking", "acronyms": [[112, 117]], "long-forms": [[90, 110]], "ID": "2252"}, {"text": " 4KEY: E1S=singular first person ergative, INC=incompletive, PART=particle, PREP=preposition, PRON=pronoun, NEG=negation, 37", "acronyms": [[61, 65], [76, 80], [94, 98], [108, 111], [7, 10], [43, 46]], "long-forms": [[66, 74], [81, 92], [99, 106], [112, 120], [47, 59], [11, 41]], "ID": "2253"}, {"text": "and above 95% for determiners (DT). In addition, subjects (SS) have a score above 90%. In all these", "acronyms": [[59, 61], [31, 33]], "long-forms": [[49, 57], [18, 28]], "ID": "2254"}, {"text": "2.4.1 English Gigaword We created large-scale n-gram language models using English Gigaword Second Edition6 (EGW). ", "acronyms": [[109, 112]], "long-forms": [[75, 91]], "ID": "2255"}, {"text": "For POS tagging, the three main error categories are the confusion between adverbs (AD) and verbs with an  adverbial force, between measure words (M) and ", "acronyms": [[84, 86], [4, 7]], "long-forms": [[75, 82], [132, 139]], "ID": "2256"}, {"text": "three stages. A source-language input string is rewritten to form an information retrieval (IR) query.", "acronyms": [[92, 94]], "long-forms": [[69, 90]], "ID": "2257"}, {"text": "The open class  word (host) tends to be uninflected, and only  the light verb (LV) carries tense, agreement  and aspect markers.", "acronyms": [[79, 81]], "long-forms": [[67, 77]], "ID": "2258"}, {"text": "Table 1. Coverage and accuracy of each derived feature for RTE3 revised development collection  (RTE3Devmt), the RTE3 Test collection (RTE3Test ) and the entire RTE2 collection (RTE2All). ", "acronyms": [[135, 143], [59, 63], [97, 106], [161, 165], [178, 185]], "long-forms": [[113, 133]], "ID": "2259"}, {"text": "1.2 The Binding Module The output of grammatical modules is then fed onto the Binding Module(BM) which activates an algorithm for anaphoric binding in LFG terms", "acronyms": [[93, 95], [151, 154]], "long-forms": [[78, 92]], "ID": "2260"}, {"text": "followed by a verbal suffix\". This is to cover general  verb inflection, for both auxiliaries (AUX +) and main  verbs (AUX -).", "acronyms": [[95, 100], [119, 122]], "long-forms": [[82, 93]], "ID": "2261"}, {"text": "directed approach according to (Ephraim and Malah, 1985) based on two different noise estimation schemes, i.e. the minimum statistics approach (MS) as described in (Martin, 2001) and the minimum", "acronyms": [[144, 146]], "long-forms": [[115, 133]], "ID": "2262"}, {"text": "Recently, a larger set of word relatedness judgments was obtained by (Finkelstein et al, 2002) in the WordSimilarity-353 (WS-353) collection. Despite the", "acronyms": [[122, 128]], "long-forms": [[102, 120]], "ID": "2263"}, {"text": "ferent levels. For each text pair on four cross levels, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (S-Ph), Phrase to Word (Ph-W) and Word", "acronyms": [[85, 88], [111, 115], [134, 138]], "long-forms": [[62, 83], [91, 109], [118, 132]], "ID": "2264"}, {"text": " Acknowledgments This research was supported by the Deutsche Forschungsgemeinschaft (DFG) in the Center of Excellence in ?", "acronyms": [[85, 88]], "long-forms": [[52, 83]], "ID": "2265"}, {"text": "5 Conclusions We have presented a sequential semantic role labeling system for the Semeval-2007 task 17 (SRL). ", "acronyms": [[105, 108]], "long-forms": [[68, 95]], "ID": "2266"}, {"text": "mentary ASR systems, a technique first proposed in the context of NIST?s ROVER system (Fiscus, 1997) with a 12% relative error reduction (RER), and subsequently widely employed in many ASR", "acronyms": [[138, 141], [8, 11], [185, 188]], "long-forms": [[112, 136]], "ID": "2267"}, {"text": " The application will eventually be deployed using a Software as a Service (SaaS) model. It will", "acronyms": [[76, 80]], "long-forms": [[53, 74]], "ID": "2268"}, {"text": "Collins et al. ( 2008) proposed simple exponentiated gradient (EG) algorithm for Conditional Random Feild (CRF).", "acronyms": [[63, 65], [107, 110]], "long-forms": [[39, 61], [81, 105]], "ID": "2269"}, {"text": "The meaning of the abbreviations is as follows (for definitions see Section 1): Incr = Incrementality; DP = Discriminatory Power; Train = Trainability; Type = Hardwired Type Selection; Hum = Human Preference Modelling; FB = Full Brevity .", "acronyms": [[219, 221], [103, 105], [130, 135], [185, 188], [169, 173]], "long-forms": [[224, 236], [108, 128], [138, 150], [191, 196], [152, 156]], "ID": "2270"}, {"text": "FEDERAL DATA ENCRYPTION STANDARD APPROVED BY COMMERCE DEPARTMENT  A data encryption algorithm, designed to protect digital information, was  approved in November ad a Federal .Information Processing Standard (FIPS)  by the Department of Commerce.", "acronyms": [[209, 213]], "long-forms": [[167, 207]], "ID": "2271"}, {"text": "Free word associations are the words people spontaneously come up with in re-sponse to a stimulus word. Such informa-tion has been collected from test persons and stored in databases.  A well known example is the Edinburgh Associative Thesaurus (EAT). We will show in this paper that this kind of knowledge can be acquired automatically from corpora, en-abling the computer to produce similar associative responses as people do.", "acronyms": [[246, 249]], "long-forms": [[213, 244]], "ID": "2272"}, {"text": "(Joachims, 1999) software). In it, we implemented: the String Kernel (SK), the Syntactic Tree Kernel (STK), the Shallow Semantic Tree Kernel", "acronyms": [[70, 72]], "long-forms": [[55, 68]], "ID": "2273"}, {"text": "storing our storage. We built a pattern matching  system based on Finite State Automata(FSA). After ", "acronyms": [[88, 91]], "long-forms": [[66, 87]], "ID": "2274"}, {"text": "The target set is built using the ? 88-?89 Wall Street Journal Corpus (WSJ) tagged using the (Ratnaparkhi, 1996) tagger and", "acronyms": [[71, 74]], "long-forms": [[43, 62]], "ID": "2275"}, {"text": "lexicon tool, with a classification phase based on  Featured-Based kernel such as SL kernel and TreeBased kernel such as Dependency tree (DT) kernel  (Culotta and Sorensen, 2004) and Phrase Structure ", "acronyms": [[138, 140], [82, 84]], "long-forms": [[121, 136]], "ID": "2276"}, {"text": "Symbol Descriptor Example  If. I Simple connection between (IMPEDANCE) GA TAF~I  I|.2 ", "acronyms": [[60, 69]], "long-forms": [[31, 58]], "ID": "2277"}, {"text": " 1 Introduction Synchronous contex free grammars (SCFGs) generalize traditional context-free grammars to generate", "acronyms": [[50, 55]], "long-forms": [[16, 48]], "ID": "2278"}, {"text": "Association measure Following the aforementioned studies, we implemented these popular measures: pointwise mutual information (PMI), log-likelihood ratio (LL) and chi-square (?", "acronyms": [[127, 130], [155, 157]], "long-forms": [[97, 125], [133, 147]], "ID": "2279"}, {"text": "Given an input pair (q,a), where q is a question and a is a candidate answer, first we retrieve the word embeddings (WEs) of both q and a. Then, we separately apply a", "acronyms": [[117, 120]], "long-forms": [[100, 115]], "ID": "2280"}, {"text": "Therefore, the standard metrics widely used in sequential topic segmentation for monologues and dialogs, such as Pk and WindowDiff(WD), are also not applicable.", "acronyms": [[131, 133], [113, 115]], "long-forms": [[120, 129]], "ID": "2281"}, {"text": " English For English dataset, we follow the standard splits of Penn Treebank (PTB), using sections 2-21 for training, section 22 as de-", "acronyms": [[78, 81]], "long-forms": [[63, 76]], "ID": "2282"}, {"text": "1 Introduction For the past three decades, there has been a great deal of work on the automatic identification (ID) of languages from the speech signal alone.", "acronyms": [[112, 114]], "long-forms": [[96, 110]], "ID": "2283"}, {"text": "2004. On clusterings: Good, bad and spectral. Journal of the ACM (JACM), 51(3):497?515.", "acronyms": [[66, 70]], "long-forms": [[46, 64]], "ID": "2284"}, {"text": "Broadcast News (BN), Newswire (NW), Broadcast Conversation (BC), Telephone Conversation (TC), Web Blog (WB) and Magazine (MZ). ", "acronyms": [[104, 106], [122, 124]], "long-forms": [[94, 102], [112, 120]], "ID": "2285"}, {"text": "IDF Approach. Proceedings of International Conference on Language Resources and Evaluation (LREC) 2012, European Language Resources Association", "acronyms": [[92, 96], [0, 3]], "long-forms": [[57, 75]], "ID": "2286"}, {"text": "This paper presents the UNITOR system that participated in the *SEM 2013 shared task on Semantic Textual Similarity (STS). The task is", "acronyms": [[117, 120], [24, 30], [64, 67]], "long-forms": [[88, 115]], "ID": "2287"}, {"text": "figure 1. To avoid confusion, we refer to this basic  unit throughout as a Temporal Unit (TU). ", "acronyms": [[90, 92]], "long-forms": [[75, 88]], "ID": "2288"}, {"text": "(Koehn, 2004a). Furthermore, they extendedWSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a).", "acronyms": [[78, 81]], "long-forms": [[49, 76]], "ID": "2289"}, {"text": "in a fully automatic fashion. Again, this is an exciting possibility that would solve the current bottleneck of supervised word sense disambiguation (WSD) methods (namely, that sense-tagged corpora are very costly to acquire).", "acronyms": [[150, 153]], "long-forms": [[123, 148]], "ID": "2290"}, {"text": "Hypernym Hyponyms Co-Hyponyms Figure 2: The proposed semantic word embedding (SWE) learning framework (The left part denotes the state-of-the-art skip-gram model; The right part represents the semantic constraints).", "acronyms": [[78, 81]], "long-forms": [[53, 76]], "ID": "2291"}, {"text": "Stephanie Strassel and Zhiyi Song and Joe Ellis University of Pennsylvania Linguistic Data Consortium (LDC) Philadelphia, PA, USA", "acronyms": [[103, 106], [126, 129], [122, 124]], "long-forms": [[75, 101]], "ID": "2292"}, {"text": "CoTrain vs. BaseCN2 0.000144 4.77E-05 0.000247 CoTrain vs. BaseCN3 0.0009 0.000287 0.00139 CoTrain vs. LEX(CN) 9.53E-10 7.15E-11 1.17E-09 CoTrain vs. LEX(EN) 1.87E-05 1.64E-05 8.92E-07", "acronyms": [[107, 109], [150, 153], [154, 156]], "long-forms": [[91, 98]], "ID": "2293"}, {"text": "Patrizia Paggio University of Copenhagen Centre for Language Technology (CST) Njalsgade 140, 2300-DK Copenhagen", "acronyms": [[73, 76], [93, 100]], "long-forms": [[41, 71]], "ID": "2294"}, {"text": " ? Named entity (NE) representation in KBs poses another NED challenge.", "acronyms": [[17, 19], [39, 42], [57, 60]], "long-forms": [[3, 15]], "ID": "2295"}, {"text": "post-process of the internal diacritization task  using the same machine learning approach that  was trained on Base phrase (BP)-Chunk as well  as POS features of individual tokens with correct ", "acronyms": [[125, 127], [147, 150]], "long-forms": [[112, 123]], "ID": "2296"}, {"text": "ODQA system, SPIQA. The system derives disambiguating queries (DQs) that draw out additional information.", "acronyms": [[63, 66], [0, 4], [13, 18]], "long-forms": [[39, 61]], "ID": "2297"}, {"text": "initions of state cannot be sensitive to (sometimes critical) aspects of the dialogue context, such as the user?s last dialogue move (DM) (e.g. requesthelp) unless that move directly affects the status of", "acronyms": [[134, 136]], "long-forms": [[119, 132]], "ID": "2298"}, {"text": "English?German 45.59 43.72 Automatically aligned corpora average 47.99?4.20 45.75?3.64 Table 1: The grammatical coverage (GC) of NF-ITG for different corpora dependent on the interpretation of word alignments: contiguous Translation Equivalence or discontiguous Translation Equivalence", "acronyms": [[122, 124], [129, 135]], "long-forms": [[100, 120]], "ID": "2299"}, {"text": "   grandmother. CL.1SG.GEN ALL ART=airport  my grandmother to the airport ", "acronyms": [[31, 34], [16, 26]], "long-forms": [[35, 42]], "ID": "2300"}, {"text": "of the set of terminal symbols) or empty strings. A Phrase Structure Tree (PST) is a tree in which all and only the leaf nodes are labeled with words or", "acronyms": [[75, 78]], "long-forms": [[52, 73]], "ID": "2301"}, {"text": "the literature. In a recent Japanese NE workshop, a maximum entropy (ME) system outperformed decision tree sys-", "acronyms": [[69, 71]], "long-forms": [[52, 67]], "ID": "2302"}, {"text": " The pattern we used consists of a mix between the  part of speech (POS) tags and the mention tags for  the words in the training instance.", "acronyms": [[68, 71]], "long-forms": [[52, 66]], "ID": "2303"}, {"text": "the impact of various kinds of physical degradation that pages may endure before they are scanned and processed using optical character recognition (OCR) software. ", "acronyms": [[149, 152]], "long-forms": [[118, 147]], "ID": "2304"}, {"text": "2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al 2005).  Most semantic role labeling (SRL) systems to date conceptualize the task as a supervised learning problem and rely on role-annotated data for model training.", "acronyms": [[117, 120]], "long-forms": [[93, 115]], "ID": "2305"}, {"text": "Elfardy H. and Diab M. 2012. Simplified guidelines for the creation of large scale dialectal arabic annotations. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey. Elfardy H. and Diab M. 2013.", "acronyms": [[202, 206]], "long-forms": [[167, 185]], "ID": "2306"}, {"text": "tial state for HMM, then experiment with different inference algorithms such as ExpectationMaximization (EM), Variational Bayers (VB) or Gibbs sampling (GS).5 Gao and Johnson (2008) compare", "acronyms": [[130, 132], [15, 18], [105, 107], [153, 155]], "long-forms": [[110, 128], [80, 103], [137, 151]], "ID": "2307"}, {"text": "sub-tasks: ? Multimedia Information Network (MiNet) Construction: Construct MiNet from cross-media and cross-genre information (i.e. tweets, images, sentences of web doc-", "acronyms": [[45, 50], [76, 81]], "long-forms": [[13, 43]], "ID": "2308"}, {"text": "rithm to handle this setting. To do so, we use dynamic programming (DP) together with greedy search.", "acronyms": [[68, 70]], "long-forms": [[47, 66]], "ID": "2309"}, {"text": "ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrSF-12 27.03 30.20 21.62 24.81 28.57 34.01SF-123 26.83 30.34 21.34 24.64 28.77 34.24SF-NE 26.51 29.86 21.42 24.55 27.96 33.26SF-GD 26.22 29.48 20.33 23.72 28.30 33.83Gold 27.92 31.15 22.56 25.69 29.65 35.08 Table 5: Results with supervised selection of catenae with specified length (SF-12, SF-123) are more effective than combinations of SFeat with heuristic NomEnd (SF-NE) or GovDep (SF-GD). ", "acronyms": [[412, 417], [430, 435]], "long-forms": [[383, 410]], "ID": "2310"}, {"text": "On the basis of these specifications, a mapping between VAML and Concrete AML (CAML) can be made. CAML", "acronyms": [[79, 83], [56, 60], [98, 102]], "long-forms": [[65, 77]], "ID": "2311"}, {"text": " Phrasometer ? The phrasometer feature (PM) is the summed log-likelihood of all n-grams the word", "acronyms": [[40, 42]], "long-forms": [[19, 30]], "ID": "2312"}, {"text": "accessing semantic information represented in  input specifications, written in the form of the  Sentence Plan Language (SPL) (Kasper, 1989;  Bateman, 1997a), and in the knowledge base of ", "acronyms": [[121, 124]], "long-forms": [[97, 119]], "ID": "2313"}, {"text": "4.1 Compositional Neural Language Model (C-NLM) Compositional Neural Language Model (C-NLM) is a combination of a word representation learning", "acronyms": [[85, 90], [41, 46]], "long-forms": [[48, 83], [4, 39]], "ID": "2314"}, {"text": "system architecture  The data is stored in one central Resource  Repository (RR). As training data may change (for ", "acronyms": [[77, 79]], "long-forms": [[55, 75]], "ID": "2315"}, {"text": " At the shallowest level of attachment we find the conjunctions (CONJ+) +\u0000 w+ ? and?", "acronyms": [[65, 70]], "long-forms": [[51, 63]], "ID": "2316"}, {"text": "oleary@cs.umd.edu Abstract The Text Analysis Conference (TAC) ranks summarization systems by their average score", "acronyms": [[57, 60]], "long-forms": [[31, 55]], "ID": "2317"}, {"text": "Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association (ELRA). ", "acronyms": [[203, 207], [127, 134]], "long-forms": [[162, 201], [92, 125]], "ID": "2318"}, {"text": "adaptation led to a considerable improvement of +4.1 BLEU and large improvements in terms of METEOR and Translation Edit Rate (TER). We", "acronyms": [[127, 130], [53, 57], [93, 99]], "long-forms": [[104, 125]], "ID": "2319"}, {"text": "EOPAS (PARADISEC tool) for text interlinear text and media analysis  2. NLTK (Natural Language Toolkit) for text analytics with linguistic data (Bird, Klein, & Loper, 2009)  3.", "acronyms": [[72, 76], [0, 5], [7, 16]], "long-forms": [[78, 102]], "ID": "2320"}, {"text": "tract syntactic features for FB-LTAG.  We use with Sejong Treebank (SJTree) which  contains 32 054 eojeols (the unity of segmenta-", "acronyms": [[68, 74]], "long-forms": [[51, 66]], "ID": "2321"}, {"text": "lem, and our system adopted a supervised learning approach with Maximum Entropy classifier, which is widely used in natural language processing(NLP). ", "acronyms": [[144, 147]], "long-forms": [[116, 142]], "ID": "2322"}, {"text": "In Proc. of the Conference on Computational Natural Language Learning (CoNLL), 7.", "acronyms": [[71, 76]], "long-forms": [[30, 69]], "ID": "2323"}, {"text": "there are usually three kinds of named entities (NEs) to be dealt with: names of persons (PER) , locations (LOC) and organizations (ORG).", "acronyms": [[108, 111], [49, 52], [90, 93], [132, 135]], "long-forms": [[97, 106], [33, 47], [81, 87], [117, 129]], "ID": "2324"}, {"text": "2011.  Icelandic Parsed Historical Corpus (IcePaHC). ", "acronyms": [[43, 50]], "long-forms": [[7, 41]], "ID": "2325"}, {"text": "easily inspectable. The generalizing ability of the evolutionary reinforcement learning (RL) algorithm, XCS, can dramatically reduce the size of the opti-", "acronyms": [[89, 91], [104, 107]], "long-forms": [[65, 87]], "ID": "2326"}, {"text": " 1 Introduction  Lexical Acquisition (LA) processes strongly rely on  basic assumptions embodied by the source informa- ", "acronyms": [[38, 40]], "long-forms": [[17, 36]], "ID": "2327"}, {"text": "simardm@iro.umontreal.ca Abstract The term translation spotting (TS) refers to the task of identifying the target-language (TL)", "acronyms": [[65, 67]], "long-forms": [[43, 63], [107, 122]], "ID": "2328"}, {"text": "Theoretically, the expressive power of converting the cospecs of a GLS into DCG parse rules i s equivalent to the power of a Lexicalized Tree Adjoining Grammar with collocations (Shieber[14]) , what we have termed Hyper Lexicalized Tree Adjoining Grammars (HTAGs) (Pustejovsky[13]) . ", "acronyms": [[257, 262], [67, 70], [76, 79]], "long-forms": [[214, 255]], "ID": "2329"}, {"text": "7 8  Figure 2: The Sense Distribution  the help of a graphical user intefface(GUI) scans a  parsed sample article and indicates a series of se- ", "acronyms": [[78, 81]], "long-forms": [[53, 76]], "ID": "2330"}, {"text": "tice to both characteristics mentioned above. The central construct in this framework is  that of context factor (CF). A CF is defined by a scope, which is a collection of individ- ", "acronyms": [[114, 116], [121, 123]], "long-forms": [[98, 112]], "ID": "2331"}, {"text": "2003) ? Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set, target", "acronyms": [[37, 41]], "long-forms": [[8, 35]], "ID": "2332"}, {"text": "ring Expression Generation Task (TUNA-REG) organised by Albert Gatt, Anja Belz and Eric Kow; the two GREC Challenges, GREC Main Subject Reference Generation (GREC-MSR) and GREC Named Entity Generation (GREC-NEG), organised by Anja Belz, Eric Kow, Jette Viethen and Albert Gatt; and the Giving Instructions in", "acronyms": [[202, 210], [33, 41], [101, 105], [118, 122], [158, 166]], "long-forms": [[172, 200]], "ID": "2333"}, {"text": "velopment and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al, 2005); articles 001-270 and 440-", "acronyms": [[90, 93]], "long-forms": [[72, 88]], "ID": "2334"}, {"text": " These classifiers are based on a discriminative model: Support Vector Machine (SVM)6 (Vapnik, 1995).", "acronyms": [[80, 83]], "long-forms": [[56, 78]], "ID": "2335"}, {"text": "There is no person boiling noodles A woman is boiling noodles in water Example 9051 (ENTAILMENT) A pair of kids are sticking out blue and green colored tongues", "acronyms": [[85, 95]], "long-forms": [[71, 83]], "ID": "2336"}, {"text": "qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, qnp), qdet?1.0 DET(the),", "acronyms": [[52, 54], [77, 80], [30, 32], [8, 10]], "long-forms": [[56, 60]], "ID": "2337"}, {"text": "WordNet  In another related effort, SRI performed experiments  in utilizing WordNet (WN) as a knowledge source for  IE.", "acronyms": [[85, 87], [36, 39], [116, 118]], "long-forms": [[76, 83]], "ID": "2338"}, {"text": " One suggestion is is to use as a natural anguage  grammar the Core Language Engine (CLE)  (Alshawi 1992).", "acronyms": [[85, 88]], "long-forms": [[63, 83]], "ID": "2339"}, {"text": "variance distribution. In the feature space, a support vector machine (SVM) classifier (Vapnik, 1995) is used to determine the likelihoods", "acronyms": [[71, 74]], "long-forms": [[47, 69]], "ID": "2340"}, {"text": " 1 I n t roduct ion   The development of Natural Language (NL) systems  for data retrieval has been a central issue in NL Pro- ", "acronyms": [[59, 61], [119, 121]], "long-forms": [[41, 57]], "ID": "2341"}, {"text": "O-ADVL = Object Adverbial: lie ran two miles.  APP = Apposition: Helsinki, the capital of Finland,  N = Title: King George and Mr. ", "acronyms": [[47, 50], [0, 6]], "long-forms": [[53, 63], [9, 25], [104, 109]], "ID": "2342"}, {"text": "learning is straightforward.  Very Reduced Regular Expression (VRRE):  Given a finite alphabet E,  the set of very ", "acronyms": [[63, 67]], "long-forms": [[30, 61]], "ID": "2343"}, {"text": "pendency and constituency tracks are shown in table 1. The label attachment score (LAS) was used by the organizer for evaluating the dependency versions,", "acronyms": [[83, 86]], "long-forms": [[59, 81]], "ID": "2344"}, {"text": "2.1 Download Initial Collection        The Yahoo Full Coverage Collection (YFCC) was  downloaded from http://fullcoverage.yahoo.com during ", "acronyms": [[75, 79]], "long-forms": [[43, 73]], "ID": "2345"}, {"text": "processing. In Proceedings of the Conference on  Knowledge Capture (K-CAP), pages 70-77, 2003. ", "acronyms": [[68, 73]], "long-forms": [[49, 66]], "ID": "2346"}, {"text": " 2 In Pisa two dictionaries of Italian, ,are being used: the  Nuovo Dizionario Garzanti (GRZ) and the Dlzionario-  Macchina dell'ltaliano (DMI), a MRD mainly based on the ", "acronyms": [[89, 92], [139, 142], [147, 150]], "long-forms": [[79, 87], [102, 137]], "ID": "2347"}, {"text": "knowledge from a corpus\\[2\\]\\[6\\]\\[91.  Machine translation (MT) systems are no  exception.", "acronyms": [[61, 63]], "long-forms": [[40, 59]], "ID": "2348"}, {"text": "lines and web-gathered word lists. Theses grammars are represented by Finite State Machines (FSMs) (thanks to the AT&T GRM/FSM toolkit (Allauzen et", "acronyms": [[93, 97], [114, 118], [119, 126]], "long-forms": [[70, 91]], "ID": "2349"}, {"text": " This named-entity tagger program is based on a first order Maximum Entropy Markov Model (MEMM) and is described in Yoshida and Tsujii (2007).", "acronyms": [[90, 94]], "long-forms": [[60, 88]], "ID": "2350"}, {"text": "the first reference in this study. ( 3) a small dataset of Wikipedia articles (WIKI) to extend our corpus and metric evaluation to topics beyond the", "acronyms": [[79, 83]], "long-forms": [[59, 77]], "ID": "2351"}, {"text": "2008; Metzler and Cai, 2011).  Work in Content Based Image Retrieval (CBIR) (Datta et al., 2008) has progressed from systems that", "acronyms": [[70, 74]], "long-forms": [[39, 68]], "ID": "2352"}, {"text": "There are two machine learning tasks in our problem. The first is Dialogue Act (DA) Tagging, in which we assign DAs to every Dialogue Func-", "acronyms": [[80, 82], [112, 115]], "long-forms": [[66, 78]], "ID": "2353"}, {"text": "1 This work has been developed in the project KFr-FAST (KIT = Kilnstliche  Intelligenz und Textverstehen (Artificial Intelligence and Text  Understanding); FAST = Functor Argument S ructure for Translation), which  constitutes the Berlin component of the complementary research project of ", "acronyms": [[156, 160]], "long-forms": [[163, 189]], "ID": "2354"}, {"text": " The obtained Spanish scores as compared to the  scores from the initial English experiment (E-E-E)  are shown in figure 5.", "acronyms": [[93, 98]], "long-forms": [[73, 91]], "ID": "2355"}, {"text": "(approx. 68,000 sentences, 1.4 million tokens), (2) Brown Corpus (BROWN) (approx. 60,000", "acronyms": [[66, 71]], "long-forms": [[52, 57]], "ID": "2356"}, {"text": "This makes it hard to find particular information of in-terest (say, a mention of a particular company in a set of thousands of YouTube comments), or to un-derstand the gist of the discussion at a high-level. Our goal in this work was to create a simple tool which would allow people to rapidly ingest useful information contained in large community-created comment threads, where the volume of data precludes manual inspection. To this end, we created CoFi (Comment Filter), a language-independent, web-based interactive browser for single comment threads. 2 How CoFi works For a given set of comments, we create a distinct CoFi instance.", "acronyms": [[453, 457], [625, 629], [564, 568]], "long-forms": [[459, 473]], "ID": "2357"}, {"text": "a mistake when we generate the final output that results in a lower score of 56.31% in term of Labeled Attachment Score (LAS), reported by organizers.", "acronyms": [[121, 124]], "long-forms": [[95, 119]], "ID": "2358"}, {"text": "and embedded phrase levels: ? Object reordering (ObjR), in which the objects and their dependents are moved in front", "acronyms": [[49, 53]], "long-forms": [[30, 47]], "ID": "2359"}, {"text": "Part of FrameNet is also a corpus of 135,000 annotated example sentences from the British National Corpus (BNC). ", "acronyms": [[107, 110]], "long-forms": [[82, 105]], "ID": "2360"}, {"text": "and tests 2 and 3 are open tests performed on different test data. DM (i.e., Default Model) assigns all  incoming cases with the most likely class and it is ", "acronyms": [[67, 69]], "long-forms": [[77, 90]], "ID": "2361"}, {"text": "task. As reported in (Liu et al 2013a), we used a genetic algorithm (GA) (Cormen et al 2001) to au80", "acronyms": [[69, 71]], "long-forms": [[50, 67]], "ID": "2362"}, {"text": "for the task: a training dataset (TrainSet) with 9675 messages directly retrieved from Twitter; a development dataset (DevSet), with 1654 messages; the testing dataset from 2013 run, which", "acronyms": [[119, 125], [34, 42]], "long-forms": [[98, 117], [16, 32]], "ID": "2363"}, {"text": "Suffixes (S): able, est, ful, ic, ing, ive, ness etc.  Word Sentiment Polarity (SP): POS, NEG, NEU Pivoting on the head aspect, we look forward and", "acronyms": [[80, 82]], "long-forms": [[60, 78]], "ID": "2364"}, {"text": "2.2 Keystroke Ratio (KSR) In addition to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric (Och et al, 2003).", "acronyms": [[127, 130], [21, 24]], "long-forms": [[110, 125], [4, 19]], "ID": "2365"}, {"text": "tagger (Cutting et al 1992) and LT POS tagger  (Mikheev 1997). Maximum Entropy (MaxEnt)  based taggers also seem to perform very well        ", "acronyms": [[80, 86], [32, 34], [35, 38]], "long-forms": [[63, 78]], "ID": "2366"}, {"text": "This paper presents the UNL graph matching method for the Semantic Textual Similarity(STS) task. ", "acronyms": [[86, 89], [24, 27]], "long-forms": [[58, 84]], "ID": "2367"}, {"text": "tends the comparison set to players of AS Roma.  Prepositional phrases (PPs), gerunds, and relative clauses introduce additional complexity.", "acronyms": [[72, 75], [39, 41]], "long-forms": [[49, 70]], "ID": "2368"}, {"text": "Score(E), where E is an example of Pat To improve ranking, we also try to find the longest similar subsequence (LSS) between the user input, Sent and retrieved example, Exm", "acronyms": [[112, 115]], "long-forms": [[83, 110]], "ID": "2369"}, {"text": "exp (14)    Short word difference penalty (SWDP): a  good translation should have roughly the same ", "acronyms": [[43, 47]], "long-forms": [[12, 41]], "ID": "2370"}, {"text": "DSTG = Adverb s t r i r ig   mRTOVO = For + Subject -+ to -+ Object  NA = N t- Adjective  NASOBJBE - N + as - t- Object  of be -", "acronyms": [[69, 71], [0, 4], [29, 35], [90, 98]], "long-forms": [[74, 88]], "ID": "2371"}, {"text": " 1 Introduction Traditionally, Information Retrieval (IR) and Statistical Natural Language Processing (NLP) applica-", "acronyms": [[54, 56], [103, 106]], "long-forms": [[31, 52], [74, 101]], "ID": "2372"}, {"text": "This paper explores the use of the homotopy method for training a semi-supervised Hidden Markov Model (HMM) used for sequence labeling.", "acronyms": [[103, 106]], "long-forms": [[82, 101]], "ID": "2373"}, {"text": "from the output of the parser we adopt a uniform meaning representation which is a structured Logical Form(LF). In other words we map our f-", "acronyms": [[107, 109]], "long-forms": [[94, 106]], "ID": "2374"}, {"text": "The resulting unit denominates a concept which belongs to the language for special purposes (LSP). ", "acronyms": [[93, 96]], "long-forms": [[62, 91]], "ID": "2375"}, {"text": "2011.  Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011.", "acronyms": [[44, 46], [56, 62]], "long-forms": [[23, 42]], "ID": "2376"}, {"text": "1425  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 75?81, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[75, 80], [84, 88]], "long-forms": [[41, 73]], "ID": "2377"}, {"text": "Var. 47.9 60.7 67.9 70.8 75.0 77.3 Pronunciation (PHL) with Pron. Var.", "acronyms": [[50, 53], [66, 69], [60, 64]], "long-forms": [[35, 48]], "ID": "2378"}, {"text": "the reference than the rest. Considering this, we use a Longest Common Subsequence(LCS) based criterion to calculate s(x, y).", "acronyms": [[83, 86]], "long-forms": [[56, 81]], "ID": "2379"}, {"text": " 1 Introduction Statistical machine translation (SMT) starts from sequence-based models.", "acronyms": [[49, 52]], "long-forms": [[16, 47]], "ID": "2380"}, {"text": "Upsala College  INTRODUCTION  A computerized conference (CC) is a form of co~znunica-  tion in which participants type into and read frc~ a ", "acronyms": [[57, 59]], "long-forms": [[32, 55]], "ID": "2381"}, {"text": "redundancy at low and medium allophonic complexities, estimated by the Jaccard indices between their false positives (FP) and false negatives (FN). ", "acronyms": [[118, 120], [143, 145]], "long-forms": [[101, 116], [126, 141]], "ID": "2382"}, {"text": "entries in an existing knowledge base is called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang,", "acronyms": [[131, 134], [175, 178]], "long-forms": [[104, 129], [149, 173]], "ID": "2383"}, {"text": "left AV and right AV. For a string s with length l, we define the left accessor variety (LAV) as the types of distinct characters preceding s in", "acronyms": [[89, 92], [5, 7], [18, 20]], "long-forms": [[66, 87]], "ID": "2384"}, {"text": "ACL 2006 paper (see experiments). Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities", "acronyms": [[62, 65], [0, 3]], "long-forms": [[34, 60]], "ID": "2385"}, {"text": " ? Backward Looking (BL)/Forward Looking (FL) features (14 to 22) are mostly extracted from ut-", "acronyms": [[42, 44], [21, 23]], "long-forms": [[25, 40], [3, 19]], "ID": "2386"}, {"text": "  We have also made a preliminary attempt to transfer a thesaurus entry from the Collins Thesaurus (CT) into  Italian by means of the English-Italian and Italian- ", "acronyms": [[100, 102]], "long-forms": [[81, 98]], "ID": "2387"}, {"text": " Temporal Types Possible Values (tags) Timeline (TL) past, present, future Day of Week (DOW) Mon, Tue, . . . ,", "acronyms": [[49, 51], [88, 91]], "long-forms": [[39, 47], [75, 86]], "ID": "2388"}, {"text": "There are three options: French (FR), Spanish (SP), or, Merged languages (ML), where the results are obtained by merging the English output of FR", "acronyms": [[74, 76], [33, 35], [47, 49], [143, 145]], "long-forms": [[56, 72], [25, 31], [38, 45]], "ID": "2389"}, {"text": "4-gram + LSA using linear interpolation  with ? LSA = 0.11 (LI). ", "acronyms": [[60, 62], [9, 12]], "long-forms": [[48, 51], [19, 39]], "ID": "2390"}, {"text": "experiments can be listed as follows.  Head Word (HW.) The predicate?s head word as", "acronyms": [[50, 53]], "long-forms": [[39, 48]], "ID": "2391"}, {"text": "The two main Modern Standard Arabic dependency treebanks currently available are the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009) and the Prague Arabic Dependency", "acronyms": [[111, 116]], "long-forms": [[85, 109]], "ID": "2392"}, {"text": "Many researchers have attempted several tech-  niques to deal with extragrammatical sentences  such as Augmented Transition Network(ATN)  (Kwasny and Sondheimer, 1981), network-based ", "acronyms": [[132, 135]], "long-forms": [[103, 130]], "ID": "2393"}, {"text": ">60 ICE ICE ICE ICE 10C 1~ 10C I0C  >.Y0 ICE ICE ICE ICE ICE IOC I(E 1~  >40 IO(J ICE ICE lO0 ICE ICE 1~ ICE  >35 ICE lO(l ICE ICE lie lOC IOC lOC ", "acronyms": [[77, 79]], "long-forms": [[82, 93]], "ID": "2394"}, {"text": " IHMM-based: He et al (2008) propose an  indirect hidden Markov model (IHMM) for hypothesis alignment.", "acronyms": [[71, 75], [1, 5]], "long-forms": [[41, 69]], "ID": "2395"}, {"text": "We apply the combination patterns to the training corpus, and count pairs of True Positives (TP) and False Positives (FP). The scores are calculated", "acronyms": [[118, 120], [93, 95]], "long-forms": [[101, 116], [77, 91]], "ID": "2396"}, {"text": "ing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL). ", "acronyms": [[93, 96]], "long-forms": [[50, 91]], "ID": "2397"}, {"text": "1125 VNC token expressions (CFS07 has 1180).  We then split them into a development (DEV) set and a test (TEST) set.", "acronyms": [[85, 88], [5, 8], [28, 33], [106, 110]], "long-forms": [[72, 83], [100, 104]], "ID": "2398"}, {"text": "Although there is a modest cost associated with annotating data, we show that a reduction of 40% relative in alignment error (AER) is possible over the GIZA++ aligner (Och and Ney, 2003).", "acronyms": [[126, 129], [152, 158]], "long-forms": [[109, 124]], "ID": "2399"}, {"text": "At the semantic level, we have included three different families which operate using named entities (NE), semantic roles (SR), and discourse representations (DR).", "acronyms": [[101, 103], [122, 124], [158, 160]], "long-forms": [[85, 99], [106, 120], [131, 156]], "ID": "2400"}, {"text": "and GL.  GL = GR or GL unspec. CC", "acronyms": [[14, 16], [4, 6], [9, 11], [31, 33]], "long-forms": [[17, 19]], "ID": "2401"}, {"text": "We use the following label set: S-O (not in maze); S-M (single word maze); B-M (beginning of multi-word 72", "acronyms": [[75, 78], [32, 35], [51, 54]], "long-forms": [[80, 98], [56, 72]], "ID": "2402"}, {"text": "In (Raymond and Riccardi, 2007), the SFST-based model is compared with Support Vector Machines (SVM) (Vapnik, 1998) and Conditional Random Fields (CRF) (Laf-", "acronyms": [[96, 99], [37, 41], [147, 150]], "long-forms": [[71, 94], [120, 145]], "ID": "2403"}, {"text": "We used five retrieval systems to generate  relevance scores for query-document pairs:  Fuzzy Boolean (FB). This system translates a query ", "acronyms": [[103, 105]], "long-forms": [[88, 101]], "ID": "2404"}, {"text": "C, N, X, Y } (V = verb, AV = auxiliary verb, EV = verb with Ersatzinfinitiv, Vfin = finite verb, Vinf", "acronyms": [[24, 26], [45, 47], [77, 81]], "long-forms": [[29, 43], [50, 75], [84, 95], [18, 22]], "ID": "2405"}, {"text": "directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech", "acronyms": [[85, 88]], "long-forms": [[63, 83]], "ID": "2406"}, {"text": "fornls of words. For instance, the rule  \\[ S= ion I=  (NNS VBZ) R= (NN) M=8\\]  says if by deleting the suffix \"ion\" from a word ", "acronyms": [[60, 63], [69, 71]], "long-forms": [[56, 59]], "ID": "2407"}, {"text": "evaluate the quality of the paraphrase collection.  In parcitular, Amazon?s Mechanical Turk1 (MTurk) provides a way to pay people small amounts of", "acronyms": [[94, 99]], "long-forms": [[76, 92]], "ID": "2408"}, {"text": "we sample polysemous words from wide-domain {French,Chinese}-English corpora, and use Amazon?s Mechanical Turk (MTurk) to annotate word sense on the English side.", "acronyms": [[112, 117]], "long-forms": [[95, 110]], "ID": "2409"}, {"text": "scribe the relation R. Most previous systems perform these steps by first using named entity recognition (NER) to identify possible arguments and then using a simple string match, but this crude", "acronyms": [[106, 109]], "long-forms": [[80, 104]], "ID": "2410"}, {"text": " 1 The following abbreviations are used POSS = possessive prefix/suffix; LOC = locative suffix; OBV = obviative suffix;", "acronyms": [[40, 44], [73, 76], [96, 99]], "long-forms": [[47, 57], [79, 87], [102, 111]], "ID": "2411"}, {"text": "and include them in the training data for the SMT.  Corpus Combination (CComb) The easiest method is to use these n newly created paral-", "acronyms": [[72, 77]], "long-forms": [[52, 70]], "ID": "2412"}, {"text": "Our systems use both corpus-based and knowledge-based approaches: Maximum Entropy(ME) (Lau et al, 1993; Berger et al, 1996; Ratnaparkhi, 1998) is", "acronyms": [[82, 84]], "long-forms": [[66, 81]], "ID": "2413"}, {"text": "fn is tfi, and foov is fq in Feature (1).  v. Df_Rank (D-Rank): It is similar to SRank and computed based on Rank(i)= ", "acronyms": [[55, 61], [81, 86]], "long-forms": [[46, 53]], "ID": "2414"}, {"text": "Associated Press Worldstream English Service (APW) ? The New York Times Newswire Service (NYT) ? The Xinhua News Agency English Service (XIE) For each source, Gigaword articles are classified into several types, including newswire advisories, etc.  We restricted our investigations to actual news stories.", "acronyms": [[137, 140], [46, 49], [90, 93]], "long-forms": [[101, 135], [0, 28], [57, 71]], "ID": "2415"}, {"text": " The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al, 2012).", "acronyms": [[86, 89]], "long-forms": [[57, 84]], "ID": "2416"}, {"text": "dresses such conflicting constraints. In this method, the owner of the TM generates a Phrase Table (PT) from it, and makes it accessible to the user following", "acronyms": [[100, 102], [71, 73]], "long-forms": [[86, 98]], "ID": "2417"}, {"text": "approaches (Gasic and Young, 2011; Lee and  Eskenazi, 2012; Williams, 2010; Young et al  2010) and Bayesian network (BN)-based  methods (Raux and Ma, 2011; Thomson and ", "acronyms": [[117, 119]], "long-forms": [[99, 115]], "ID": "2418"}, {"text": " In addition, NE alignment can be very useful for  Statistical Machine Translation (SMT) and CrossLanguage Information Retrieval (CLIR).", "acronyms": [[84, 87], [14, 16], [130, 134]], "long-forms": [[51, 82], [93, 128]], "ID": "2419"}, {"text": "The tag B-X (Begin) represents the first word of a named entity of type X, for example, PER (Person) or LOC (Location). The tag I-X (In-", "acronyms": [[88, 91], [104, 107]], "long-forms": [[93, 99], [109, 117]], "ID": "2420"}, {"text": "and Buckley, 1988), ResidualIDF(RIDF), Variance, Burstiness and Gain, are based on derivations from term frequency (TF) and document frequency (DF). ", "acronyms": [[116, 118], [144, 146], [32, 36]], "long-forms": [[100, 114], [124, 142], [20, 31]], "ID": "2421"}, {"text": "Amongst the various learning algorithms available in QUEST, to make our results comparable we selected SVR with radial basis function (RBF) kernel, which has been shown to perform very well", "acronyms": [[135, 138], [53, 58], [103, 106]], "long-forms": [[112, 133]], "ID": "2422"}, {"text": "from the remaining pool of data.  The intrinsic stopping criterion (ISC) we propose here focuses on the latter aspect of the ideal stop-", "acronyms": [[68, 71]], "long-forms": [[38, 66]], "ID": "2423"}, {"text": " Snow et al (2008) explored the use of the Amazon Mechanical Turk (MTurk) web service for gathering annotations for a variety of natural lan-", "acronyms": [[67, 72]], "long-forms": [[50, 65]], "ID": "2424"}, {"text": "The results of dependency (ZPar-eager, Ours-standard, Ours-PS and Mate-tools) and constituent parsers (BerkeleyParser and ZPar-con) are measured by the unlabeled accuracy score (UAS), labeled accuracy score (LAS) and bracketing f-measure (BF), respectively. ", "acronyms": [[208, 211], [239, 241], [54, 61], [178, 181], [122, 130], [27, 37]], "long-forms": [[184, 206], [217, 229], [152, 176]], "ID": "2425"}, {"text": "(A~.TG, ~ROC)I ^  (SUBS, SUSU, VT),  (ARTG, PR.OC)I ^  (SUBS, SUSU, VT)2  , ARTG = article g~n&al  SUBS = substantif  compl~ment ", "acronyms": [[76, 80], [99, 103], [4, 6], [9, 12], [19, 23], [25, 29], [31, 33], [38, 42], [44, 49], [56, 60], [62, 66], [68, 70]], "long-forms": [[83, 97], [106, 116]], "ID": "2426"}, {"text": "grammatical frameworks (HLG). Combinatory  Categorial Grammars (CCG) (Steedman, 1987;  Steedman, 1996; Steedman, 1998; Steedman and ", "acronyms": [[64, 67], [24, 27]], "long-forms": [[30, 62]], "ID": "2427"}, {"text": " 1 Introduction Coreference resolution (CR) ? the task of determin-", "acronyms": [[40, 42]], "long-forms": [[16, 38]], "ID": "2428"}, {"text": "sures for each portion of the results. One is a relevance score (RS) with the target document \u0001 \u0002", "acronyms": [[65, 67]], "long-forms": [[48, 63]], "ID": "2429"}, {"text": "be compared, for any section of the corpus. The tool also calculates the majority tag (MajTag). Av-", "acronyms": [[87, 93]], "long-forms": [[73, 85]], "ID": "2430"}, {"text": "planes, the \"READ\"-units by AND-planes. The flip-  flops (FF) are simple register units and the shift  register is a simple PLA network of well  known ", "acronyms": [[58, 60], [124, 127]], "long-forms": [[44, 56]], "ID": "2431"}, {"text": "future research which are suggested by some af the techniques used in this program.  The SFRAME (semantic frame) concept. in which a sernantirl interpretation ", "acronyms": [[89, 95]], "long-forms": [[97, 111]], "ID": "2432"}, {"text": "guage varieties: the language of native speakers (N), the language of advanced, highly fluent nonnative speakers (NN), and translationese (T). We", "acronyms": [[114, 116], [50, 52], [139, 141]], "long-forms": [[97, 103], [33, 39], [123, 137]], "ID": "2433"}, {"text": "First, we provide background on Open IE and how it relates to Semantic Role Labeling (SRL). Section 3 de-", "acronyms": [[86, 89], [37, 39]], "long-forms": [[62, 84]], "ID": "2434"}, {"text": "didate substitutes, as described below.  Lexical Baseline (LB): In this approach we use the pre-existing lexical resources to provide a rank-", "acronyms": [[59, 61]], "long-forms": [[41, 57]], "ID": "2435"}, {"text": "Understanding (NLU) framework (see below), while ASR includes features such as speech/nonspeech (SNS) detection and automatic gain control (AGC).", "acronyms": [[97, 100], [15, 18], [49, 52], [140, 143]], "long-forms": [[79, 95], [116, 138]], "ID": "2436"}, {"text": "can formulate natural language-like patterns as exploratory queries for relations against a text corpus.  We draw inspiration from the information seeking paradigm of Exploratory Search (ES) (Marchionini, 2006; White and Roth, 2009), where users start with a vaguely defined information need and - with a mix", "acronyms": [[187, 189]], "long-forms": [[167, 185]], "ID": "2437"}, {"text": "The language is defined by about 50 simple grammar rules. ? P5E2N3S4, F W A Standard Language (SLANG). See Section 4.1. ?", "acronyms": [[95, 100]], "long-forms": [[76, 93]], "ID": "2438"}, {"text": "Metonymy Often a sentence relates entities in a way inconsistent with the target ontology. For example, with the Component Library (CLib) ontology,movement properties (e.g., speed, acceleration) are defined as properties of the movement events, rather", "acronyms": [[132, 136]], "long-forms": [[113, 130]], "ID": "2439"}, {"text": "stable functional definition across languages. These categories include NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition,", "acronyms": [[84, 87], [101, 104], [115, 118], [129, 132]], "long-forms": [[90, 99], [107, 113], [121, 127], [135, 145]], "ID": "2440"}, {"text": "cal Dirichlet Process (HDP) (Teh et al, 2006), a Bayesian nonparametric variant of Latent Dirichlet Allocation (LDA), to automatically infer the number of topics.", "acronyms": [[112, 115], [23, 26]], "long-forms": [[83, 110], [0, 21]], "ID": "2441"}, {"text": "Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al, 2004).", "acronyms": [[90, 93]], "long-forms": [[66, 88]], "ID": "2442"}, {"text": "From every argument to the predicate, we extract all child  noun phrases (NP) and adjectival phrases (ADJP)  as candidate gaps as well.", "acronyms": [[74, 76], [102, 106]], "long-forms": [[60, 72], [82, 100]], "ID": "2443"}, {"text": " 2004. Document understanding conferences (DUC). ", "acronyms": [[43, 46]], "long-forms": [[7, 41]], "ID": "2444"}, {"text": " 3.1 LSTMs for sequence generation A Recurrent Neural Network (RNN) is a generalization of feed forward neural networks to se-", "acronyms": [[63, 66], [5, 10]], "long-forms": [[37, 61]], "ID": "2445"}, {"text": "con. In Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci). ", "acronyms": [[84, 90]], "long-forms": [[57, 82]], "ID": "2446"}, {"text": "audiences have little trouble mapping a collection  of noun phrases onto the same entity, this task of  noun phrase (NP) coreference r solution can present  a formidable challenge to an NLP system.", "acronyms": [[117, 119], [186, 189]], "long-forms": [[104, 115]], "ID": "2447"}, {"text": "its right neighbour. Hence, the tree in Figure 2 is assumed to have the structure ((AB)C). ", "acronyms": [[84, 88]], "long-forms": [[52, 81]], "ID": "2448"}, {"text": "4.4 Word Sense Induction In this section, we present an evaluation of our model on the word sense induction (WSI) tasks. The", "acronyms": [[109, 112]], "long-forms": [[87, 107]], "ID": "2449"}, {"text": "response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Sch?olkopf, 2004), with the complexity parame-", "acronyms": [[89, 92]], "long-forms": [[62, 87]], "ID": "2450"}, {"text": "Table 1: Labelled attachment score on the two test sets of the best single parse, blended with weights set to PoS labelled attachment score (LAS) and blended with learned weights.", "acronyms": [[141, 144], [110, 113]], "long-forms": [[114, 139]], "ID": "2451"}, {"text": "AO = all objects  MO = matched objects  TF = text filtering  FM = F-measures ", "acronyms": [[40, 42], [0, 2], [18, 20], [61, 63]], "long-forms": [[45, 59], [5, 16], [23, 38], [66, 76]], "ID": "2452"}, {"text": "6.1 Corpus The training and test data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference", "acronyms": [[81, 84]], "long-forms": [[65, 79]], "ID": "2453"}, {"text": " 2004. The Automatic Content Extraction (ACE) Program?Tasks, Data, and Evaluation.", "acronyms": [[41, 44]], "long-forms": [[11, 39]], "ID": "2454"}, {"text": " We use LibSVM (Chang and Lin, 2011), an implementation of Support Vector Machines (SVM) (Cortes and Vapnik, 1995), as the underlying tech-", "acronyms": [[84, 87], [8, 14]], "long-forms": [[59, 82]], "ID": "2455"}, {"text": "Typically, the weights of the log-linear combination in Equation 3 are optimised by means of Minimum Error Rate Training (MERT) (Och, 2003).", "acronyms": [[122, 126]], "long-forms": [[93, 120]], "ID": "2456"}, {"text": "O N S  *****  SCAN CALLED AT 1 I  ANTEST CALL'EC FOR 'I \"REDVO U (AACC) ,SD= 2 .  RES= 6.", "acronyms": [[49, 52], [66, 70], [73, 75], [82, 85]], "long-forms": [[34, 48]], "ID": "2457"}, {"text": "One part of the work is directed towards developing computational methods to facilitate the manual construction of SweFN. We have so far focused on three tasks: (1) semantic role labeling (SRL) (Johansson et al.,", "acronyms": [[189, 192], [115, 120]], "long-forms": [[165, 187]], "ID": "2458"}, {"text": "The SCBD structure.  prepositional phrases (PPs), verb phrases (VPs), and adverbial phrases  (APs).", "acronyms": [[44, 47], [64, 67], [4, 8], [94, 97]], "long-forms": [[21, 42], [50, 62], [74, 91]], "ID": "2459"}, {"text": "SN = Sa-inflection oun (nominal verb)  CM = case marker (-nom/-acc argument)  PT = particle (other arguments)  VB = verb ", "acronyms": [[78, 80], [0, 2], [39, 41], [111, 113]], "long-forms": [[83, 91], [5, 22], [44, 55], [116, 120]], "ID": "2460"}, {"text": "evidences, making the coupled approach efficient enough to be applied to themore complex task of joint word segmentation (WS) and POS tagging for the first time.", "acronyms": [[122, 124], [130, 133]], "long-forms": [[103, 120]], "ID": "2461"}, {"text": "Researchers have ex-plored the topic of CLI in the areas of lexical style (Jarvis et al 2012a), lexical n-grams (Jarvis & Paquot, 2012), character n-grams (Tsur & Rappo-prot, 2007), using variables related to cohesion, lexical sophistication, syntactic complexity and conceptual knowledge (Crossley & McNamara, 2012), error patterns (Bestgen, et al 2012; Wong & Dras, 2009), and a combination of these ap-proaches (Jarvis et al 2012b; Koppel et al 2005; Mayfield Tomokiyo & Jones, 2001, Wong & Dras, 2009).  Such studies have demonstrated relatively strong success rates for classifying an L2 writing sample based on the L1 of the writer. For instance, Jarvis and Paquot (2012), using 1-4-grams as pre-dictor variables on a subset of argumentative es-says included in the International Corpus of Learner English (ICLE) (Granger et al 2009) achieved a 53.6% classification accuracy for 12 groups of L1s. Crossley and McNamara (2012) used features related to cohesion, lexical sophisti-cation, syntactic complexity, and conceptual knowledge taken from the computational tool Coh-", "acronyms": [[813, 817], [40, 43], [590, 592], [621, 623]], "long-forms": [[772, 811]], "ID": "2462"}, {"text": "any parser; the third requirement is not easily met  in all languages, but even in those languages where  nonrestrictives are not easily identifiable, (II)  works reasonable well.", "acronyms": [[152, 154]], "long-forms": [[137, 149]], "ID": "2463"}, {"text": "The effectiveness of customer care in the email channel is measured using two competing metrics: Average Handling Time (AHT) and Customer Experience Evaluation (CEE).", "acronyms": [[120, 123], [161, 164]], "long-forms": [[97, 118], [129, 159]], "ID": "2464"}, {"text": "2004. A maxi-mum-entropy Chinese parser augmented by transformation-based learning. ACM Transactions on Asian Language Information Processing (TALIP) 3(2): 159-168. Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu.", "acronyms": [[143, 148]], "long-forms": [[88, 141]], "ID": "2465"}, {"text": "higher indicating better.  We used Amazon?s Mechanical Turk (MTurk)5 to collect the human judgements.", "acronyms": [[61, 66]], "long-forms": [[44, 59]], "ID": "2466"}, {"text": " We excluded only punctuation; we did no filtering for part of speech (POS). Each word was actually", "acronyms": [[71, 74]], "long-forms": [[55, 69]], "ID": "2467"}, {"text": "information. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 317?325, Columbus, USA.", "acronyms": [[100, 103], [131, 134]], "long-forms": [[57, 98]], "ID": "2468"}, {"text": "tially freely-available sources: Family Practitioner Inquiry Network (FPIN)2, Parkhurst Exchange Forum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4 were used to design and develop the presented test", "acronyms": [[137, 143], [70, 74], [104, 106]], "long-forms": [[114, 135], [33, 68], [78, 102]], "ID": "2469"}, {"text": " It considers all pronouns (PRP, PRP$), noun phrases (NP) and heads of verb phrases (VP) as potential mentions.", "acronyms": [[85, 87], [28, 31], [54, 56], [33, 37]], "long-forms": [[71, 83], [40, 51]], "ID": "2470"}, {"text": "3 Bayes ian  networks   A Bayes ian  network  (Pearl, 1988), or  Bayesian 1)el|el nel;work (BBN),  eonsisi;s of a sol;  of var iab les  and a sel; of d i rec ted  edges  (:on- ", "acronyms": [[92, 95]], "long-forms": [[65, 90]], "ID": "2471"}, {"text": "Saccade Length (SL) Real Sum of saccade lengths (measured by number of words) divided by word count Simple Regression Count (REG) Real Total number of gaze regressions Gaze Skip count (SKIP) Real Number of words skipped divided by total word count", "acronyms": [[125, 128], [16, 18], [185, 189]], "long-forms": [[107, 117], [0, 14]], "ID": "2472"}, {"text": "to find words with the same meanings. We use a simple approach called the Direct Reversal (DR) approach in (Lam and Kalita, 2013) to create", "acronyms": [[91, 93]], "long-forms": [[74, 89]], "ID": "2473"}, {"text": " 0.7  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random CoNLL-03", "acronyms": [[60, 62], [107, 110], [118, 126], [83, 86]], "long-forms": [[39, 58], [63, 69]], "ID": "2474"}, {"text": "1. Introduction  Named entity(NE) recognition is important for recent  sophisticated information service such as question answering ", "acronyms": [[30, 32]], "long-forms": [[17, 28]], "ID": "2475"}, {"text": "monolingual applications and have been used in commercial grammar checkers.1 These parsers produce a logical form (LF) representation that is compatible across multiple languages (see", "acronyms": [[115, 117]], "long-forms": [[101, 113]], "ID": "2476"}, {"text": "Learning\". In Proceedings of the 3 rd ACL  Workshop on Very Large Corpora (WVLC95). ", "acronyms": [[75, 81]], "long-forms": [[43, 73]], "ID": "2477"}, {"text": "ceedings of the 10th International Conference on Text, Speech, and Dialogue (TSD-2007), Lecture Notes in Computer Science (LNCS), Springer-Verlag.", "acronyms": [[123, 127], [77, 80]], "long-forms": [[88, 121]], "ID": "2478"}, {"text": "cos(d, c).  Candidate Rank (CR) The features described so far disambiguate every surface form s ?", "acronyms": [[28, 30]], "long-forms": [[12, 26]], "ID": "2479"}, {"text": "4 Learning Algorithms We evaluated four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and de-", "acronyms": [[97, 100], [153, 155], [134, 137]], "long-forms": [[72, 95], [140, 150], [103, 111]], "ID": "2480"}, {"text": "1998. Protein folding in the hydrophobic-hydrophilic(HP) model is NPcomplete.", "acronyms": [[53, 55], [66, 68]], "long-forms": [[29, 52]], "ID": "2481"}, {"text": "Suppose that the feature f2 is an agreement feature and that a local  tree t which is a projection of this ID rule has been constructed, then  the Agreement Principle (AP) forces X = Y = Z and therefore the  AP has to consider three cases 6: ", "acronyms": [[168, 170], [107, 109], [208, 210]], "long-forms": [[147, 166]], "ID": "2482"}, {"text": "900  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48?57, Gothenburg, Sweden, April 27, 2014.", "acronyms": [[74, 79], [83, 87]], "long-forms": [[40, 72]], "ID": "2483"}, {"text": "a r e  the fo l lowing  -- Performer,  Object, Goal, Source,  Locat ion,   Means, Cause, and Enabler  -- and (2) s t r u c t u r a l  c a s e s ,  which a r e   R E E L  ( r e l a t i v e  c l a u s e )  and COMP (compound). I w i l l  not  e x p l a i n  ", "acronyms": [[208, 212]], "long-forms": [[214, 222]], "ID": "2484"}, {"text": "and the neural language model (NLM), for each phrase combination: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj). For each", "acronyms": [[100, 103], [122, 127], [31, 34], [82, 87]], "long-forms": [[90, 98], [109, 120], [8, 29], [66, 80]], "ID": "2485"}, {"text": "to be explained by a set of unobserved (latent) topics. Hidden Markov Model LDA (HMM-LDA) (Griffiths et al, 2005) is a topic model that simul-", "acronyms": [[81, 88]], "long-forms": [[56, 79]], "ID": "2486"}, {"text": "SELF = talking to oneself  TQ = terse question  TI = terse information  INT = interrupted ", "acronyms": [[48, 50], [0, 4], [27, 29], [72, 75]], "long-forms": [[53, 70], [7, 25], [32, 46], [78, 89]], "ID": "2487"}, {"text": "gorithm used belongs to the family of algorithms described by Covington (2001), and the classifiers are trained using support vector machines (SVM) (Vapnik, 1995).", "acronyms": [[143, 146]], "long-forms": [[118, 141]], "ID": "2488"}, {"text": "mvzaanen@uvt.nl Gerhard van Huyssteen Centre for Text Technology (CTexT) North-West University", "acronyms": [[66, 71]], "long-forms": [[38, 64]], "ID": "2489"}, {"text": "capability [and t o  go] interregional without involving the private sector.  The General Services Administratioq (GSA) l a s t  month amended its M v a o y  Gu--&de-  Zines adding privacy a d  security considerations for use i n  ADP o r  tqlecom- ", "acronyms": [[115, 118], [231, 234]], "long-forms": [[82, 113]], "ID": "2490"}, {"text": "-4, -12, and -109 are all disjoint speaker sets.)  (Codes: SD=speaker dependent (2400 training sentences for RM2), MS=multi-speaker, SI=speaker independent, -4=all 4  RM2 speakers combined, -12=all 12 RM1 SD speakers combined, -109=109 RM1 SI training speakers, SDG=SD Gaussians, ", "acronyms": [[59, 61], [133, 135], [109, 111], [115, 117], [167, 169], [201, 203], [205, 207], [236, 238], [240, 242], [262, 265], [266, 268]], "long-forms": [[62, 79], [136, 155], [118, 131]], "ID": "2491"}, {"text": "A mobile real-time speech-to-speech translation (S2ST) device is one of the grand challenges in natural language processing (NLP). It involves", "acronyms": [[125, 128], [49, 53]], "long-forms": [[96, 123], [19, 47]], "ID": "2492"}, {"text": " ? ALGN (alignment-based): We ran a sentence alignment algorithm (Gale and Church, 1993)", "acronyms": [[3, 7]], "long-forms": [[9, 18], [45, 54]], "ID": "2493"}, {"text": "Abstract This paper explores the use of set expansion (SE) to improve question answering (QA) when the expected answer is a list of entities", "acronyms": [[90, 92], [55, 57]], "long-forms": [[70, 88], [40, 53]], "ID": "2494"}, {"text": "back. We define a currency for annotation cost as Annotation cost Units (AUs). For an annotation bud-", "acronyms": [[73, 76]], "long-forms": [[50, 71]], "ID": "2495"}, {"text": "sible classes we show the accuracy of the correct hashtag being amongst the top 1,5 or 50 hashtags as well as the Mean Reciprocal Rank (MRR). The", "acronyms": [[136, 139]], "long-forms": [[114, 134]], "ID": "2496"}, {"text": "54  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 325?335, October 25-29, 2014, Doha, Qatar.", "acronyms": [[92, 97]], "long-forms": [[42, 90]], "ID": "2497"}, {"text": " 3 Data The RST Discourse Treebank (RST-DT) (Carlson et al, 2002) was used for training and testing.", "acronyms": [[36, 42]], "long-forms": [[12, 34]], "ID": "2498"}, {"text": "When ready and mature,  technology and language processing techniques will be  incorporated into Foreign Broadcast Information Service (FBIS)  processing.", "acronyms": [[136, 140]], "long-forms": [[97, 134]], "ID": "2499"}, {"text": "3.2 Formalizing Paradigmatic Relations with Lexical Functions Lexical functions (LF) are a formal tool designed to describe all types of genuine lexical relations", "acronyms": [[81, 83]], "long-forms": [[62, 79]], "ID": "2500"}, {"text": "Evaluating the quality of language output tasks such as Machine Translation (MT) and Automatic Summarisation (AS) is a challenging topic in Natural Language Processing", "acronyms": [[110, 112], [77, 79]], "long-forms": [[85, 108], [56, 75]], "ID": "2501"}, {"text": "tent both in their living rooms and on their mobile devices. Digital video recorders (DVRs) allow people to record TV programs from hundreds of chan-", "acronyms": [[86, 90], [115, 117]], "long-forms": [[61, 84]], "ID": "2502"}, {"text": " 2.1 Named Entity Recognition We regard named entity recognition (NER) as a standalone task, independent of language identification.", "acronyms": [[66, 69]], "long-forms": [[40, 64]], "ID": "2503"}, {"text": "Context-Free Grammars 2.1 Minimalist Grammars A Minimalist Grammar (MG) (Stabler and Keenan, 2003)1 is a five-tuple", "acronyms": [[68, 70]], "long-forms": [[48, 66]], "ID": "2504"}, {"text": "As shown in the table, all models perform equally well on identification, which is determined by the frame matcher (FM); i.e., any extracted argument receiving one or more candidate roles is ?", "acronyms": [[116, 118]], "long-forms": [[101, 114]], "ID": "2505"}, {"text": "The CoreSC scheme consists of three layers; the first layer corresponds to eleven concepts (Background (BAC), Hypothesis (HYP), Motivation (MOT), Goal (GOA), Object (OBJ), Method (MET), Model", "acronyms": [[122, 125], [140, 143], [4, 10], [104, 107], [152, 155], [166, 169], [180, 183]], "long-forms": [[110, 120], [128, 138], [92, 102], [146, 150], [158, 164], [172, 178]], "ID": "2506"}, {"text": "Feature F1,344 d Automated Readability Index (ARI) 0.187 0.047 Average Sentence Length (ASL) 3.870 0.213 Sentence Complexity (COM) 10.93 0.357", "acronyms": [[88, 91], [46, 49], [126, 129]], "long-forms": [[63, 86], [17, 44], [114, 124]], "ID": "2507"}, {"text": "to do the testing on real emotions. The Berlin Emotional Database (EMO-DB) contains the set of emotions from the MPEG-4 standard (anger,", "acronyms": [[67, 73], [113, 119]], "long-forms": [[47, 65]], "ID": "2508"}, {"text": "Queue to for user intervention.  4.2,  Document Processor (DP)   The DP identifies and extracts all SGML tags de- ", "acronyms": [[59, 61], [100, 104]], "long-forms": [[39, 57]], "ID": "2509"}, {"text": "systematic way.  The MIME (Managing Information in Medical Emergencies)1 project is developing technology to", "acronyms": [[21, 25]], "long-forms": [[27, 70]], "ID": "2510"}, {"text": "07 a Single 0.71 1.00 0.83 0.00 0.00 0.00 0.42 1.39 9  Table 9  Constituent parsing evaluation results of Task 2-2 (Close Track), ranked with Tot-F1  (S_S=simple sentence, C_S=complex sentence)  ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A Rank", "acronyms": [[151, 154], [172, 175], [253, 258], [227, 234], [198, 204]], "long-forms": [[155, 170], [176, 192]], "ID": "2511"}, {"text": "2214  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 1?9, Gothenburg, Sweden, April 26-30 2014.", "acronyms": [[74, 76], [32, 36]], "long-forms": [[54, 72]], "ID": "2512"}, {"text": "In Proceedings of the Conference of the Pacific Association for Computational Linguistics (PACLING), pages 120?128. ", "acronyms": [[91, 98]], "long-forms": [[40, 89]], "ID": "2513"}, {"text": "2 Long Short-Term Memory Networks 2.1 Overview Recurrent neural networks (RNNs) are able to process input sequences of arbitrary length via the re-", "acronyms": [[74, 78]], "long-forms": [[47, 72]], "ID": "2514"}, {"text": "determine the appropriate xpressional form. Hovy's text structurer (Hovy 1988b),  for example, uses rhetorical relations as defined in Rhetorical Structure Theory (RST)  (Mann and Thompson 1987) to order a set of propositions to be expressed.", "acronyms": [[164, 167]], "long-forms": [[135, 162]], "ID": "2515"}, {"text": "seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) procedure.", "acronyms": [[131, 133]], "long-forms": [[105, 129]], "ID": "2516"}, {"text": "vincent.claveau@irisa.fr christian.raymond@irisa.fr R?SUM?Dans cet article, nous d?crivons notre participation au D?fi Fouille de Texte (DeFT) 2012. Ced?fi consistait en l?attribution automatique de mots-cl?s ?", "acronyms": [[137, 141], [52, 57]], "long-forms": [[81, 135]], "ID": "2517"}, {"text": "EUROTYP + + + + + + Leipzig Glossing Rules + + + + + + + + Penn Treebank (POS) + + + + + + + + STTS + + + + + + +", "acronyms": [[74, 77], [0, 7], [95, 99]], "long-forms": [[59, 72]], "ID": "2518"}, {"text": "We first experiment with the separately trained supertagger and parser, which are then combined using belief propagation (BP) and dual decomposition (DD).", "acronyms": [[122, 124], [150, 152]], "long-forms": [[102, 120], [130, 148]], "ID": "2519"}, {"text": "We show that languageindependent features can be used for regression with Support Vector Machines (SVMs) and the Margin-Infused Relaxed Algorithm (MIRA), and", "acronyms": [[99, 103], [147, 151]], "long-forms": [[74, 97], [113, 145]], "ID": "2520"}, {"text": "tional words and notional words. In  the field  of Natural Language Processing(NLP), many  studies on text computing or word meaning ", "acronyms": [[79, 82]], "long-forms": [[51, 77]], "ID": "2521"}, {"text": "wards a Shared Task for Multiword Expressions. ACL Special Interest Group on the Lexicon (SIGLEX), Marrakech.", "acronyms": [[90, 96], [47, 50]], "long-forms": [[51, 88]], "ID": "2522"}, {"text": "kept on a ventilator for medical reasons.     Change of state (COS) is most often understood  as an aspectual difference that is reflected in verb ", "acronyms": [[63, 66]], "long-forms": [[46, 61]], "ID": "2523"}, {"text": ", with different projection and SRL training methods. SP=Supplement; OW=Overwrite. ", "acronyms": [[54, 56], [32, 35], [69, 71]], "long-forms": [[72, 81], [57, 67]], "ID": "2524"}, {"text": "NER model was shown in Table 4. We use the Peking University (PKU) named entity corpus to train the models.", "acronyms": [[62, 65], [0, 3]], "long-forms": [[43, 60]], "ID": "2525"}, {"text": "pre-rendered animations.  The Natural Language Understanding (NLU) module needs to cope with both chat and military", "acronyms": [[62, 65]], "long-forms": [[30, 60]], "ID": "2526"}, {"text": "Interpreting news requires identifying its constituent events. Information extraction (IE) makes this feasible by considering only events of a specified type,", "acronyms": [[87, 89]], "long-forms": [[63, 85]], "ID": "2527"}, {"text": "must be stored at each step of the decoding algorithm.  This information includes: the current score (SCORE),  the pointer to the previous lexical item (BPO) on the best ", "acronyms": [[102, 107], [153, 156]], "long-forms": [[95, 100]], "ID": "2528"}, {"text": "tities e1, e2, ei,.., eE on translated English documents through aforementioned step, meanwhile, we consider all noun phrases(NP) in original Chinese documents and generate mention candidates", "acronyms": [[126, 128]], "long-forms": [[113, 124]], "ID": "2529"}, {"text": "Japanese Translated SemEval 2007 Test Corpus (in %)  Before Morphology [After Morphology]  Emotion Score (ES) ? 0 Emotion Score (ES) ?", "acronyms": [[106, 108], [129, 131]], "long-forms": [[91, 104], [114, 127]], "ID": "2530"}, {"text": "2.3.3 Name List Generated using Double Propagation We implement the Double Propagation (DP) algorithm described in Qiu et al. (", "acronyms": [[88, 90]], "long-forms": [[68, 86]], "ID": "2531"}, {"text": "the (Penn Treebank) annotation style, (3) the (LexTract) extraction tool, (4) possible unsuitability of the (TAG) model, and (5) annotation errors. We", "acronyms": [[109, 112], [47, 55]], "long-forms": [], "ID": "2532"}, {"text": "The ISO 639-3 language codes for our eight languages are as follows: Urdu (URD), Thai (THA), Bengali (BEN), Tamil (TAM), Punjabi (PAN), Tagalog (TGL), Pashto", "acronyms": [[75, 78], [102, 105], [4, 7], [87, 90], [115, 118], [130, 133], [145, 148]], "long-forms": [[69, 73], [93, 100], [81, 85], [108, 113], [121, 128], [136, 143]], "ID": "2533"}, {"text": "icantly better performance than GIZA++. We also evaluated Support Vector Machines (SVM) classifiers on the same first order feature space and", "acronyms": [[83, 86], [32, 38]], "long-forms": [[58, 81]], "ID": "2534"}, {"text": "user gender (GEN), the user identity (UID) (e.g. the user could be a person or an organization), and the source document ID (DID). We also mark the lan-", "acronyms": [[125, 128], [13, 16], [38, 41]], "long-forms": [[112, 123], [5, 11], [23, 36]], "ID": "2535"}, {"text": "In this work, we apply Dirichlet Process Mixture Models (DPMMs) to a learning task in natural language processing (NLP): lexical-semantic verb clustering.", "acronyms": [[115, 118], [57, 62]], "long-forms": [[86, 113], [23, 55]], "ID": "2536"}, {"text": "190  troductory phase (GREET-INTRODUCE-TOPIC), the  negotiation phase (NEGOTIATE) and the closing  phase (FINISH).", "acronyms": [[71, 80], [106, 112]], "long-forms": [[52, 69], [90, 104]], "ID": "2537"}, {"text": "In Proceedings of the NAACL/AMTA Workshop on Syntax and Structure in Statistical Translation (SSST), pages 33?40, Rochester, NY.", "acronyms": [[94, 98], [22, 32], [125, 127]], "long-forms": [[45, 92]], "ID": "2538"}, {"text": "fered to punched cards. Abbreviated alphabetical symbols  are used for the syntactic analysis (AP=adJective phrase)  because of the program's 24unlt search limitation.", "acronyms": [[95, 97]], "long-forms": [[98, 114]], "ID": "2539"}, {"text": "relative-resource?, i.e.  EuroWordNet (EWN).1   In this paper we start by briefly recalling the ", "acronyms": [[39, 42]], "long-forms": [[26, 37]], "ID": "2540"}, {"text": "They are the One-error Loss (O-Loss) function, the Symmetric Loss (S-Loss) function, and the Hierarchical Loss (H-Loss) function: ?", "acronyms": [[112, 118], [29, 35], [67, 73]], "long-forms": [[93, 110], [13, 27], [51, 65]], "ID": "2541"}, {"text": "other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis", "acronyms": [[124, 127], [68, 71]], "long-forms": [[96, 122], [42, 66]], "ID": "2542"}, {"text": "   FIGURE 1: Community of Inquiry (CoI) model   (Adapted from: Garrison et al 2000) ", "acronyms": [[35, 38]], "long-forms": [[13, 33]], "ID": "2543"}, {"text": "sented by the S node) are extracted. ( 2) VPs (NPs) in a path on which all the nodes are VPs (NPs) are also recursively extracted and regarded as hav-", "acronyms": [[94, 97], [42, 45], [47, 50], [89, 92]], "long-forms": [], "ID": "2544"}, {"text": "during this period: Carpetbagger (CB),1 Daily Kos (DK),2 Matthew Yglesias (MY),3 Red State (RS),4 and Right Wing News (RWN).5 CB and MY ceased as independent bloggers in August 2008.6 Because", "acronyms": [[119, 122], [34, 36], [51, 53], [75, 77], [92, 94], [133, 135], [126, 128]], "long-forms": [[102, 117], [20, 32], [40, 49], [57, 73], [81, 90]], "ID": "2545"}, {"text": "Cognitive Science Department at Xiamen University (XMU) ? ?  Harbin Institute of Technology Shenzhen Graduate School (HITSZGS)    National Taipei University of Technology (NTUT)   ", "acronyms": [[118, 125], [51, 54], [172, 176]], "long-forms": [[61, 116], [32, 49], [130, 170]], "ID": "2546"}, {"text": "Turian et al. ( 2010) applied word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (", "acronyms": [[88, 91]], "long-forms": [[62, 86]], "ID": "2547"}, {"text": "J = Japanese ????? S = Spanish  JV = Joint Venture ?????????? ME = Microelectronics ", "acronyms": [[32, 34], [62, 64]], "long-forms": [[37, 50], [4, 12], [67, 83]], "ID": "2548"}, {"text": "0 ROOT Figure 2: Example of a CoNLL-style annotated sentence. Each word (FORM) is numbered (ID), lemmatized (LEMMA), annotated with two levels of part-of-speech tags (CPOSTAG and POSTAG), annotated with morpho-", "acronyms": [[92, 94], [30, 35], [73, 77], [109, 114], [167, 174], [179, 185]], "long-forms": [[79, 90], [97, 107], [146, 165]], "ID": "2549"}, {"text": "dimensional space, in which both texts and terms are represented by means of Domain Vectors (DVs), i.e. vectors representing the domain relevances among the linguistic object and", "acronyms": [[93, 96]], "long-forms": [[77, 91]], "ID": "2550"}, {"text": "SIZE = sizesensorreading85 SHAPE = shapesensorreading62 COLOUR = coloursensorreadning78 ?", "acronyms": [[56, 62], [0, 4], [27, 32]], "long-forms": [[65, 87], [7, 26], [35, 55]], "ID": "2551"}, {"text": "languages studied differ widely, there is a quasistandard for presenting the material, in the form of interlinearized glossed text (IGT). IGT typically", "acronyms": [[132, 135], [138, 141]], "long-forms": [[102, 130]], "ID": "2552"}, {"text": " This work addresses a major bottleneck in the development of Statistical MT (SMT) systems: the lack of sufficiently large parallel corpora for most", "acronyms": [[78, 81]], "long-forms": [[62, 76]], "ID": "2553"}, {"text": "In Proc. of the 4th Workshop  on Treebanks and Linguistic Theories (TLT), pages  149?160.", "acronyms": [[68, 71]], "long-forms": [[33, 66]], "ID": "2554"}, {"text": " 4. Template Relation(TR) recognition: Finding the relation between TEs and a question", "acronyms": [[22, 24]], "long-forms": [[4, 20]], "ID": "2555"}, {"text": " 3.2.6. Candidate word number (WNum)  Because there are candidates that are a multi-", "acronyms": [[31, 35]], "long-forms": [[18, 29]], "ID": "2556"}, {"text": "cognition(COG) competition(COMP)  contact(CeNT) motion(MOT)  emoeion(ENO) perception(PER)  possession(POSS) stat ive(STA) ", "acronyms": [[85, 88], [10, 13], [27, 31], [42, 46], [55, 58], [69, 72], [102, 106], [117, 120]], "long-forms": [[74, 84], [0, 9], [15, 26], [34, 41], [48, 54], [61, 68], [91, 101], [108, 116]], "ID": "2557"}, {"text": "colour histograms derived from images.  In the RGB (Red Green Blue) colour model, each pixel is represented as an integer in range of", "acronyms": [[47, 50]], "long-forms": [[52, 66]], "ID": "2558"}, {"text": "documents. Thus, HTMM can be seen both as a variation of Hidden Markov Model (HMM) and a variation of LDA.", "acronyms": [[78, 81], [17, 21], [102, 105]], "long-forms": [[57, 76]], "ID": "2559"}, {"text": "Route INJECTION ORAL SMOKING SNORTING Aspect CHEMISTRY (Pharmacology, TEK) CULTURE (Culture, Setting, Social, Spiritual) EFFECTS (Effects)", "acronyms": [[75, 82], [70, 73], [121, 128]], "long-forms": [[84, 91], [130, 137]], "ID": "2560"}, {"text": "1 Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tan-", "acronyms": [[98, 101]], "long-forms": [[65, 96]], "ID": "2561"}, {"text": "=   where,   ij  is the term frequency(TF) of the j-th word  in the vocabulary in the document , i.e. the ", "acronyms": [[39, 41]], "long-forms": [[24, 37]], "ID": "2562"}, {"text": "  1 Introduction  Sign Language (SL) is a visual-gestural language, using the whole upper body articulators ", "acronyms": [[33, 35]], "long-forms": [[18, 31]], "ID": "2563"}, {"text": "some plan P such that, if H executes P. then in the re-  sulting state, there exists a \\['F identifiable term P' such  that H knows that Denotation(Pl = Dem;tation(DI),  and 5\" intends that H execute P. ", "acronyms": [[164, 166]], "long-forms": [[153, 163]], "ID": "2564"}, {"text": "though this is never the case.) The second such  feature \"Theme as Chomeuf  (TAC) is the only  non-trinary-valued feature in our learner; it spec- ", "acronyms": [[77, 80]], "long-forms": [[58, 74]], "ID": "2565"}, {"text": "HLT/EMNLP, 2005  http://www.nist.gov/speech/tests/ace/ace07/doc, The  ACE 2007 (ACE07) Evaluation Plan, Evaluation of  the Detection and Recognition of ACE Entities, Val-", "acronyms": [[80, 85], [0, 9], [152, 155]], "long-forms": [[70, 78]], "ID": "2566"}, {"text": "]} (7) This optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).", "acronyms": [[78, 80]], "long-forms": [[52, 76]], "ID": "2567"}, {"text": " 1 Introduction Word Sense Disambiguation (WSD) is considered one of the most important prob-", "acronyms": [[43, 46]], "long-forms": [[16, 41]], "ID": "2568"}, {"text": "? TFN Bank(TFNB): 38,769 samples  CPN Bank(CPNB): 17,637 samples  The difficulty of identifying unknown words in ", "acronyms": [[43, 47], [11, 15]], "long-forms": [[34, 42], [2, 10]], "ID": "2569"}, {"text": "paper is the following:  Def. A generative system (GS) is a sequence TI,... ,Tn of TS,  whe~'~-TR(Tl,... ,Tn) is a relation between strings and D-trees and ", "acronyms": [[51, 53], [83, 85], [95, 97]], "long-forms": [[32, 49]], "ID": "2570"}, {"text": " First we used the C&C Combinatory Categorial Grammar (CCG) parser5 (C&C) by Clark and Curran (2004) using the biomedical model described in", "acronyms": [[69, 72], [19, 22], [55, 58]], "long-forms": [[23, 53], [77, 93]], "ID": "2571"}, {"text": "Figure 1: Na??ve Bayes Model The model described above is commonly known as a na??ve Bayes (NB) model. NB models have", "acronyms": [[92, 94], [103, 105]], "long-forms": [[78, 90], [10, 22]], "ID": "2572"}, {"text": "5 System Description The PEZ system consists of three components, viz (i) a Web Translation Memory (WebTM) crawler, (ii) the XLING reranker and (iii) a longest", "acronyms": [[100, 105], [25, 28], [125, 130]], "long-forms": [[76, 98]], "ID": "2573"}, {"text": "our proposed STRAIN approach. The results of using sentence training (STr) and sentence testing (STe) are shown in the STR/STE row of Table 5.", "acronyms": [[70, 73], [13, 19], [97, 100], [119, 126]], "long-forms": [[51, 68], [79, 95]], "ID": "2574"}, {"text": "? Toral (2013) explores the selection of data to train domain-specific language models (LM) from non-domain specific corpora by means", "acronyms": [[88, 90]], "long-forms": [[71, 86]], "ID": "2575"}, {"text": "P i jPMI P i P j=   Equation 2: Pointwise Mutual Information (PMI)  between two terms i and j. ", "acronyms": [[62, 65], [5, 8]], "long-forms": [[32, 60]], "ID": "2576"}, {"text": "2: boston sweep colorado to win world series 3: rookies respond in first crack at the big time C-LR=C-LexRank; WDS=Word Distributional Similarity Table 4: Top 3 ranked summaries of the redsox cluster", "acronyms": [[111, 114], [95, 99]], "long-forms": [[115, 145], [100, 109]], "ID": "2577"}, {"text": " X '1\"~,. \\[-1  Def in i t ion 2.2 A N:M sur face coerc ion  (SC)  ru le ix a quadruple (/,c/,c~,r) where l and r ", "acronyms": [[62, 64]], "long-forms": [[41, 59]], "ID": "2578"}, {"text": " 1 Introduction Spoken Dialogue Systems (SDSs) play a key role in achieving natural human-machine interaction.", "acronyms": [[41, 45]], "long-forms": [[16, 39]], "ID": "2579"}, {"text": "Table 10: A=acoustic, P=psycholinguistic, POS=part-of-speech, C=complexity, F=fluency, VR=vocabulary richness, CFG=CFG production rule features.", "acronyms": [[87, 89], [42, 45], [111, 114], [115, 118]], "long-forms": [[90, 109], [12, 20], [24, 40], [46, 60], [64, 74], [78, 85]], "ID": "2580"}, {"text": " 9 Here, we present the generation-oriented PG Workbench (PGW), which assists grammar developers, among other things, in testing whether the implemented syntactic and lexical knowledge allows all and only well-formed permutations. In Section 2, we describe PG?s topology-based linearizer implemented in the PGW gen-erator, whose software design is sketched in Section 3.", "acronyms": [[58, 61], [307, 310], [257, 261]], "long-forms": [[44, 56]], "ID": "2581"}, {"text": "Furthermore, to create a fully text-bound subset, family memberships relations (MEMBER) were resolved into single edges and suitable references to", "acronyms": [[80, 86]], "long-forms": [[57, 78]], "ID": "2582"}, {"text": "tation for the joint learning process. Specifically, we make use of the latent structural SVM (LS-SVM) (Yu and Joachims, 2009) formulation.", "acronyms": [[95, 101]], "long-forms": [[72, 93]], "ID": "2583"}, {"text": "2013 temporal summarization. In Proceedings of the 22nd Text Retrieval Conference (TREC), November.", "acronyms": [[83, 87]], "long-forms": [[56, 81]], "ID": "2584"}, {"text": "Fortunately, learning the reward function using IRL methods have already been proposed for the general (PO)MDP framework (Ng and Russell, 2000; Kim et al.,", "acronyms": [[104, 106], [48, 51], [107, 110]], "long-forms": [[78, 90]], "ID": "2585"}, {"text": " 156 The Basque Dependency Treebank (BDT) is a dependency treebank in its original design, due to", "acronyms": [[37, 40]], "long-forms": [[9, 35]], "ID": "2586"}, {"text": "? Negat ive Precis ion ( I~P)  :  * F -measure  (FM) ? ( ~2+I)?PP?PR /32 ?", "acronyms": [[49, 51], [25, 28]], "long-forms": [[36, 46]], "ID": "2587"}, {"text": "In Proc. IEEE Automatic Speech Recognition and Understanding (ASRU), Merano, Italy, December. ", "acronyms": [[62, 66], [9, 13]], "long-forms": [[14, 60]], "ID": "2588"}, {"text": "Table 4: Evaluation results. Abbreviations: TVN (Tone & Vowel Normalization); N-LM (N-order Language Modelling); DS (Dataset); PK (Prior Knowledge); WC (Weighting-based Corrector). ", "acronyms": [[113, 115], [127, 129], [44, 47], [78, 82], [149, 151]], "long-forms": [[117, 124], [131, 146], [84, 110], [153, 178]], "ID": "2589"}, {"text": " 3 Baseline SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sen-", "acronyms": [[68, 71], [12, 15]], "long-forms": [[35, 66]], "ID": "2590"}, {"text": "transduction and matching words approximately.  Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma.", "acronyms": [[57, 61]], "long-forms": [[48, 55]], "ID": "2591"}, {"text": "Technology (NAST), Lao PDR  ? Madan Puraskar Pustakalaya (MPP),  Nepal  ", "acronyms": [[58, 61]], "long-forms": [[30, 56]], "ID": "2592"}, {"text": "capture all the types of entities. Typical structures  of Chinese person name (CN), location name (LN)  and organization name (ON) are as follows: ", "acronyms": [[79, 81], [99, 101]], "long-forms": [[58, 77], [84, 97]], "ID": "2593"}, {"text": "2.1 Purver, Ginzburg and Healey (PGH) Purver, Ginzburg and Healey (2003) investigated CRs in the British National Corpus (BNC) (Burnard, 2000).", "acronyms": [[122, 125], [33, 36], [86, 89]], "long-forms": [[97, 120], [4, 31]], "ID": "2594"}, {"text": " 1 Introduction Natural Language Processing (NLP) systems often consist of a series of NLP components, each trained", "acronyms": [[45, 48], [87, 90]], "long-forms": [[16, 43]], "ID": "2595"}, {"text": " 2 (Durrett and Klein, 2013) call this error false new (FN). ", "acronyms": [[56, 58]], "long-forms": [[45, 54]], "ID": "2596"}, {"text": "ity, to validate Boosting NER hypotheses. We also use three Markov chain Monte Carlo (MCMC) algorithms for probabilistic inference in MLNs.", "acronyms": [[86, 90], [26, 29], [134, 138]], "long-forms": [[60, 84]], "ID": "2597"}, {"text": "by the connectives will yield better readability.  Entity Grid (EG) Along with the previous work (Pitler and", "acronyms": [[64, 66]], "long-forms": [[51, 62]], "ID": "2598"}, {"text": "For each bracketed phrase, if its FF label does not  fit into the corresponding default pattern, (like for  the noun phrase(NP), the default grammatical  structure is that the last noun in the phrase is the ", "acronyms": [[124, 126], [34, 36]], "long-forms": [[112, 123]], "ID": "2599"}, {"text": "i );3 MFS = Maximal Freq Sequences(d1 i", "acronyms": [[6, 9]], "long-forms": [[12, 37]], "ID": "2600"}, {"text": "attempted to cut down on certain items in the process.  Figure 2: Translation examples (SRC = source, BASE = baseline system, BACKOFF = backoff", "acronyms": [[88, 91]], "long-forms": [[94, 100]], "ID": "2601"}, {"text": "adjectives, and specifies the participants and properties of the situation it describes, the so called frame elements (FEs). ", "acronyms": [[119, 122]], "long-forms": [[103, 117]], "ID": "2602"}, {"text": "WI = wide; NA = narrow; CR = critical; CL = closed; ALV = alveolar; P-A = palato-alveolar; RET = retroflex. ", "acronyms": [[91, 94], [0, 2], [11, 13], [24, 26], [39, 41], [52, 55], [68, 71]], "long-forms": [[97, 106], [5, 9], [16, 22], [29, 37], [44, 50], [58, 66], [74, 89]], "ID": "2603"}, {"text": "ranking models on this data set, including Support Vector Machine (SVM) with a linear kernel, SVM with a radial basis function (RBF) kernel and Logistic Regression (LR).", "acronyms": [[128, 131], [67, 70], [94, 97], [165, 167]], "long-forms": [[105, 126], [43, 65], [144, 163]], "ID": "2604"}, {"text": "the specified length limit. This idea is realized using the integer linear programming-based (ILP) optimization framework, with objective function set to", "acronyms": [[94, 97]], "long-forms": [[60, 86]], "ID": "2605"}, {"text": "quency weighted recall evaluation. We used a  Japanese frequency dictionary (FD) generated  from the Japanese EDR corpus (Isahara, 2007) to ", "acronyms": [[77, 79]], "long-forms": [[55, 75]], "ID": "2606"}, {"text": "model. We tuned the parameters of these features with Minimum Error Rate Training (MERT) (Och, 2003) on the NIST MT03 Evaluation data set (919", "acronyms": [[83, 87], [108, 112], [113, 117]], "long-forms": [[54, 81]], "ID": "2607"}, {"text": "4 Experiments 4.1 Event Extraction We extract events by running the Semantic Role Labeling (SRL) tool in SENNA (Collobert et al., 2011).", "acronyms": [[92, 95], [105, 110]], "long-forms": [[68, 90]], "ID": "2608"}, {"text": "Figure 1: An entity and relation example (Roth and Yih, 2004). Person (PER) and location (LOC) entities are connected by Live in and Located in", "acronyms": [[71, 74], [90, 93]], "long-forms": [[63, 69], [80, 88]], "ID": "2609"}, {"text": "(3) The measure introduced by Resnik (Resnik, 1995) (RES) returns the information content (IC) of the LCS of two concepts:", "acronyms": [[91, 93], [53, 56], [102, 105]], "long-forms": [[70, 89]], "ID": "2610"}, {"text": "ley (2004). This framework for linguistic semantics is called Uni\u0002ed Eventity Representation (UER), because it is a true extension of the UML and not", "acronyms": [[94, 97], [138, 141]], "long-forms": [[62, 92]], "ID": "2611"}, {"text": " (d) The word?s position in the sentence (e) The word?s Part of speech (POS) tag, based on the Stanford POS tagger2", "acronyms": [[72, 75], [104, 107]], "long-forms": [[56, 70]], "ID": "2612"}, {"text": "Table 3: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, SDE=Software Development Engineer, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snowball=Snowball stemmer, WN-lemma=WordNet lemmatization, McCCJ=McClosky-Charniak-Johnson", "acronyms": [[138, 141], [73, 75], [94, 97], [173, 180], [199, 205], [222, 230], [249, 257], [281, 286]], "long-forms": [[142, 171], [76, 92], [98, 125], [181, 197], [206, 220], [231, 247], [258, 279], [287, 312]], "ID": "2613"}, {"text": "the Extraction of Potential Opinion Phrases. Notation: po=potential opinion, M=modifier, NP=noun phrase, S=subject, P=predicate, O=object.", "acronyms": [[89, 91], [55, 57]], "long-forms": [[92, 103], [58, 75], [79, 87], [107, 114], [118, 127], [131, 137]], "ID": "2614"}, {"text": "Here, we assume :  P(t i I G~ )  IP(tt I Pi-i PiWi) Pi-I piwi E dp  \\[ P(ti / Pi ) Pi-I Pi Wi ~ 1I) ", "acronyms": [[33, 35]], "long-forms": [[39, 43]], "ID": "2615"}, {"text": "and labeled by the people on Amazon Mechanical Turk, a web service. Amazon Mechanical Turk (MTurk) allows individuals to post jobs on MTurk with a set fee that are", "acronyms": [[92, 97]], "long-forms": [[75, 90]], "ID": "2616"}, {"text": "facts or splits the  goal into new subgo~ls uch as to show the facts in the premises of n. The derivation of a fact is  conveyed by so-called mathematics ommunicating acts (MCAs) and accompanied by storing the  fact as a chunk in the declarative memory.", "acronyms": [[173, 177]], "long-forms": [[142, 171]], "ID": "2617"}, {"text": "Brighton, BN1 9QN, England  Abstract  Generalised phrase structure grammars (GPSG's)  appear to offer a means by which the syntactic ", "acronyms": [[77, 83], [14, 17], [10, 13]], "long-forms": [[38, 75]], "ID": "2618"}, {"text": "and opportunities. In Proceedings of the 1st International Temporal Web Analytics Workshop (TWAW), pages 1?8.", "acronyms": [[92, 96]], "long-forms": [[59, 90]], "ID": "2619"}, {"text": "Model of Argumentation. Proceedings of American Association .for  Artificial Intelligence (AAAI) Conference: 313-315. ", "acronyms": [[91, 95]], "long-forms": [[39, 89]], "ID": "2620"}, {"text": "to ? constraints? in interactive topic models (ITM) (Hu et al, 2014).", "acronyms": [[47, 50]], "long-forms": [[21, 45]], "ID": "2621"}, {"text": " Support Vector Machine Support Vector Machines (SVMs) have been shown to be an effective classifier in text", "acronyms": [[49, 53]], "long-forms": [[24, 47]], "ID": "2622"}, {"text": "When using only the former type of feature function, our classifier is equivalent to a maximum entropy (MaxEnt) model. ", "acronyms": [[104, 110]], "long-forms": [[87, 102]], "ID": "2623"}, {"text": "that of Visweswariah et al(2011) ? hereby called Travelling Salesman Problem (TSP) model ? with", "acronyms": [[78, 81]], "long-forms": [[49, 76]], "ID": "2624"}, {"text": "computed from the rewrite rules by the examination of the interdependencies of the rules with the help of  KIT = Ktinsdiche lntelligenz und Textverstehen  (artificial intelligence and text understanding), FAST = ", "acronyms": [[107, 110], [205, 209]], "long-forms": [[113, 135]], "ID": "2625"}, {"text": "Table 2: Overall scores of whole task as well as separately for each annotation format in terms of labeled precision (LP), recall (LR) and F 1", "acronyms": [[118, 120], [131, 133]], "long-forms": [[99, 116], [123, 129]], "ID": "2626"}, {"text": "4 Supervised Named Entity Recognition In the first part of this work, we adopt a supervised named entity recognition (NER) framework for the attribute extraction problem from eBay listing titles.", "acronyms": [[118, 121]], "long-forms": [[92, 116]], "ID": "2627"}, {"text": "step of segmentation is presented in Section 3 with two variants: stochastic word alignment (GIZA) and integer linear programming (ILP). Then evaluations", "acronyms": [[131, 134], [93, 97]], "long-forms": [[103, 129]], "ID": "2628"}, {"text": "description where i t  is useful.  The posit ion of Linear Precedence (LP) state-  ments in th i s  formalism must now be c la r i f ied .", "acronyms": [[71, 73]], "long-forms": [[52, 69]], "ID": "2629"}, {"text": "(see, e.g., (Moschitti, 2006) for more details).  Syntactic Tree Kernel (STK), also known as a subset tree kernel (Collins and Duffy, 2002), maps", "acronyms": [[73, 76]], "long-forms": [[50, 71]], "ID": "2630"}, {"text": "Computational Linguistics, Volume 15, Number 1, March 1989 33  Michael C. McCord \\]Design of LMT: A Prolog-Based Machine Translation System  sions in a logical form language (LFL)  (McCord 1985a,  1987).", "acronyms": [[175, 178], [93, 96]], "long-forms": [[152, 173]], "ID": "2631"}, {"text": "2013. Overview of the pathway curation (PC) task of bioNLP shared task 2013.", "acronyms": [[40, 42]], "long-forms": [[22, 38]], "ID": "2632"}, {"text": "Bielefeld University  2 Center of Excellence ? Cognitive Interaction Technology?(CITEC), Bielefeld University     ", "acronyms": [[81, 86]], "long-forms": [[47, 79]], "ID": "2633"}, {"text": "c?2009 Association for Computational Linguistics Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE) Donna Byron", "acronyms": [[135, 139], [69, 72]], "long-forms": [[86, 133]], "ID": "2634"}, {"text": "Abstract  This paper provides a description of the Hong  Kong Polytechnic University (PolyU) System  that participated in the task #5 of SemEval-2, ", "acronyms": [[86, 91]], "long-forms": [[62, 84]], "ID": "2635"}, {"text": "sHDP 0.162 0.046 0.442 0.102 Table 2: Average topic coherence for various baselines (HDP, Gaussian LDA (G-LDA)) and sHDP. ", "acronyms": [[104, 109], [116, 120], [85, 88], [0, 4]], "long-forms": [[90, 102]], "ID": "2636"}, {"text": "In second method we compare CLIR performance of the two systems using Cross Language Evaluation Forum (CLEF) 2007 ad-hoc bilingual track (Hindi-English) docu-", "acronyms": [[103, 107], [28, 32]], "long-forms": [[70, 101]], "ID": "2637"}, {"text": " A baseline system was also implemented using  the principle of most frequent sense (MFS),  where each word sense distribution was retrieved ", "acronyms": [[85, 88]], "long-forms": [[64, 83]], "ID": "2638"}, {"text": "extraction. Using the alignments from HIER, we created phrase tables using model probabilities (MOD), and heuristic extraction on words (HEUR-W), blocks", "acronyms": [[96, 99], [38, 42], [137, 143]], "long-forms": [[75, 80], [106, 135]], "ID": "2639"}, {"text": " 2 Methodo logy   A User Centered (UC) approach was adopted for the  design of GEPPETTO.", "acronyms": [[35, 37], [79, 87]], "long-forms": [[20, 33]], "ID": "2640"}, {"text": " The joint model is trained in max-margin fashion using a latent structural SVM (LSSVM) where the answer-entailing structures are latent.", "acronyms": [[81, 86]], "long-forms": [[58, 79]], "ID": "2641"}, {"text": "translations. Although initially intended for  learners of English as Foreign Language (EFL)  in Taiwan, it is a gold mine of texts in English ", "acronyms": [[88, 91]], "long-forms": [[59, 86]], "ID": "2642"}, {"text": "tiguous correspondence. The Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR) mentioned by (Zhang et al, 2008a) can be regarded", "acronyms": [[93, 96], [56, 59]], "long-forms": [[65, 91], [28, 54]], "ID": "2643"}, {"text": "Lioma C. and Ounis I., A Syntactically-Based Query  Reformulation Technique for Information Retrieval,  Information Processing and Management (IPM), Elsevier Science, 2007 ", "acronyms": [[143, 146]], "long-forms": [[104, 141]], "ID": "2644"}, {"text": " 2 Story Segmentation using Modified Kmeans (MKM) Clustering The first step in multi-document summarization is", "acronyms": [[45, 48]], "long-forms": [[28, 43]], "ID": "2645"}, {"text": "mance.  We investigated chopping criteria based on a fixed number of words (FIXED), at  speaker changes (TURN), at pauses (PAUSE), and, for reference, at actual sentence ", "acronyms": [[76, 81], [105, 109], [123, 128]], "long-forms": [[53, 74], [115, 121]], "ID": "2646"}, {"text": "2.1 Conditional Random Fields  Conditional random field (CRF) was an extension  of both Maximum Entropy Model (MEMs) and  Hidden Markov Models (HMMs) that was firstly ", "acronyms": [[111, 115]], "long-forms": [[88, 109]], "ID": "2647"}, {"text": "Lima or Jessica Alba??. Therefore, we decided to employ a Conditional Random Fields (CRF) tagger (Lafferty et al, 2001) to the task, since CRF", "acronyms": [[85, 88], [139, 142]], "long-forms": [[58, 83]], "ID": "2648"}, {"text": "and (W-1,W0,W1) ? Gazetteers (GAZ): We use two sets of gazetteers.", "acronyms": [[30, 33], [5, 8], [9, 11], [12, 14]], "long-forms": [[18, 28]], "ID": "2649"}, {"text": "Best-Scoring-Choice Realization Pablo Gerva?s, Raquel Herva?s, Carlos Leo?n Natural Interaction based on Language (NIL) Universidad Complutense de Madrid", "acronyms": [[115, 118]], "long-forms": [[76, 113]], "ID": "2650"}, {"text": "word lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from", "acronyms": [[84, 87]], "long-forms": [[65, 82]], "ID": "2651"}, {"text": "CDD, which comprises a total of 173 gold annotated cues, we find that Classifier I mislabels 11 false positives (FPs) and seven false negatives (FNs). ", "acronyms": [[113, 116], [145, 148], [0, 3]], "long-forms": [[96, 111], [128, 143]], "ID": "2652"}, {"text": "A Combinatory Categorial Grammar parsing (CCG) (Steedman, 2000) tool and a Tree Kernel (TK) classifier constitute the core of the system.", "acronyms": [[88, 90], [42, 45]], "long-forms": [[75, 86], [2, 32]], "ID": "2653"}, {"text": "gorithm (PAS-PTK), which is highly more efficient and more accurate than the SSTK and (ii) a new kernel called Part of Speech sequence kernel (POSSK), which proves very accurate to represent shallow syn-", "acronyms": [[143, 148], [9, 16], [77, 81]], "long-forms": [[111, 141]], "ID": "2654"}, {"text": "The word nchi is dis-  ambiguated with a rule relying on the Ncl of the  following genitive connector (GEN-CON). ", "acronyms": [[103, 110], [61, 64]], "long-forms": [[83, 101]], "ID": "2655"}, {"text": "We begin by describing how for our typical model, the Viterbi EM objective can be formulated as a mixed integer quadratic programming (MIQP) problem with nonlinear constraints (Figure 2).", "acronyms": [[135, 139]], "long-forms": [[98, 133]], "ID": "2656"}, {"text": "Topic: Short, usually controversial statement that defines the subject of interest.   Context Dependent Claim (CDC): General, and concise statement, that directly supports or contests  the given Topic.", "acronyms": [[111, 114]], "long-forms": [[86, 109]], "ID": "2657"}, {"text": "100 Another measure of accuracy that is frequently used is the so called Out Of Vocabulary (OOV) measure, which represents the percentage of words that was not recog-", "acronyms": [[92, 95]], "long-forms": [[73, 90]], "ID": "2658"}, {"text": "  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477?487, October 25-29, 2014, Doha, Qatar.", "acronyms": [[90, 95]], "long-forms": [[40, 88]], "ID": "2659"}, {"text": " In order to overcome these limitations, some  techniques like word sense induction (WSI) have  been proposed for discovering words?", "acronyms": [[85, 88]], "long-forms": [[63, 83]], "ID": "2660"}, {"text": "is shown in PLATE 1. The second part is the  Prompt Piano Server (PPS), which is an IVR  (interactive voice response) server with a Dialogic ", "acronyms": [[66, 69], [84, 87]], "long-forms": [[45, 64]], "ID": "2661"}, {"text": "nority preference algorithm that models bridging recognition as a subtask of learning finegrained information status (IS). We substan-", "acronyms": [[118, 120]], "long-forms": [[98, 116]], "ID": "2662"}, {"text": "One means of achieving a fiat structure with extrinsic  ordering is by using the ID/LP formalism, a subformalism of  GPSG that allows immediate dominance (ID) information to be  specified separately from linear precedence (LP) notions. (", "acronyms": [[155, 157], [81, 86], [117, 121], [223, 225]], "long-forms": [[134, 153], [204, 221]], "ID": "2663"}, {"text": "ded systems. It is widely used by software certification authorities such as FAA (Federal Aviation Association), and it establishes some guidelines and", "acronyms": [[77, 80]], "long-forms": [[82, 110]], "ID": "2664"}, {"text": "knowledge resources (e.g., WordNet), and (ii) corpus-based that do not require any external knowledge source. Corpus-based metrics are formalized as Distributional Semantic Models (DSMs) (Baroni and Lenci, 2010) based on the distributional hypothesis of meaning (Harris, 1954).", "acronyms": [[181, 185]], "long-forms": [[149, 179]], "ID": "2665"}, {"text": "Several numerical algorithms, such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limitedmemory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright", "acronyms": [[131, 134]], "long-forms": [[112, 129]], "ID": "2666"}, {"text": "which allows POS tagged and chunked data to be represented (including recursion), and Shakti Standard Format (SSF)2. The editor allows", "acronyms": [[110, 113], [13, 16]], "long-forms": [[86, 108]], "ID": "2667"}, {"text": "This system tags, lemmatizes and parses corpus data using the current version of the RASP (Robust Accurate Statistical Parsing) toolkit (Briscoe et al, 2006), and on the basis of resulting", "acronyms": [[85, 89]], "long-forms": [[91, 126]], "ID": "2668"}, {"text": "It predicts four type of reordering patterns, namely MA (monotone adjacent), MG (monotone gap), RA (reverse adjacent), and RG (reverse gap).", "acronyms": [[96, 98], [53, 55], [77, 79], [123, 125]], "long-forms": [[100, 116], [57, 74], [81, 93], [127, 138]], "ID": "2669"}, {"text": "<AbstractText Label=?RESULTS? NlmCategory=?RESULTS?>Premature delivery rate was higher (p = 0.048) in the CKC group (14/36, 38.88%) than in control group (14/68, 20.5%) with a odds ratio (OR) of 2.455 (1.007 - 5.985); and premature delivery was related to cone depth, OR was significantly increased when the cone depth was more than", "acronyms": [[188, 190], [106, 109], [268, 270]], "long-forms": [[176, 186]], "ID": "2670"}, {"text": "sides identity (IDENT) we only marked up three associative relations (Hawkins, 1978): set membership (ELEMENT), subset (SUBSET), and ? gen-", "acronyms": [[120, 126], [16, 21], [102, 109]], "long-forms": [[112, 118], [6, 14]], "ID": "2671"}, {"text": " A manually  prepared seed list that is used to frame the  lexical patterns for conjunct verbs (ConjVs)  contains frequently used Light Verbs (LVs).", "acronyms": [[96, 102], [143, 146]], "long-forms": [[80, 94], [130, 141]], "ID": "2672"}, {"text": "pairments. Many have advocated the potential benefits of language sample analysis (LSA) (Johnston, 2006; Dunn et al.,", "acronyms": [[83, 86]], "long-forms": [[57, 81]], "ID": "2673"}, {"text": "email: allan.ramsay@manchester.ac.uk Debora Field University of Sheffield (UK) email: D.Field@sheffield.ac.uk", "acronyms": [[75, 77]], "long-forms": [[50, 73]], "ID": "2674"}, {"text": "6 BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire; UN = Usenet Newsgroups; and WL = Weblogs.", "acronyms": [[53, 56], [92, 94], [2, 4], [32, 34], [107, 109], [135, 137]], "long-forms": [[59, 90], [97, 105], [7, 30], [37, 51], [112, 129], [140, 147]], "ID": "2675"}, {"text": "In Proceedings of the 2005 International conference on Intelligent User Interfaces (IUI), pages 137?144. ACM Press.", "acronyms": [[84, 87], [105, 108]], "long-forms": [[55, 82]], "ID": "2676"}, {"text": "model.  Case Frame Editor (CFE): Maintains the lexical  Case Frame Factbase, a data file of how to infer ", "acronyms": [[27, 30]], "long-forms": [[8, 25]], "ID": "2677"}, {"text": "location(LOC) psych_feature(PSY)  cognition(COG) feeling(FEEL)  motivation(MOT) abstraction(ABS)  time(TIME) space(SPA) attribute(ATT) ", "acronyms": [[75, 78], [92, 95], [9, 12], [28, 31], [44, 47], [57, 61], [103, 107], [115, 118], [130, 133]], "long-forms": [[64, 73], [80, 90], [0, 8], [14, 27], [34, 43], [49, 56], [98, 102], [109, 114], [120, 129]], "ID": "2678"}, {"text": "shown in (1).2  2~Vc use lhe fo l low ing  abbrev ia t ions :  NOM : nominat ive ;   ACC = accusat ive ;  AI)N = adnomina l ;  CI. = c lass i l ier ;  ARGSTR ", "acronyms": [[85, 88], [106, 110]], "long-forms": [[91, 98], [113, 121]], "ID": "2679"}, {"text": "tity in the contrast set until no distractors are left.  Dale & Reiter speaker frequency (DR-sf) uses a different preferred attribute list for each speaker,", "acronyms": [[90, 95]], "long-forms": [[57, 88]], "ID": "2680"}, {"text": "3.1 Div is ion  and  L inear l i za t ion  o f   Cases   At first, we define a translation pattern (TPi) as fol-  lows.", "acronyms": [[100, 103]], "long-forms": [[79, 98]], "ID": "2681"}, {"text": "(UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance). ", "acronyms": [[90, 92], [1, 3], [46, 48]], "long-forms": [[95, 111], [6, 15], [51, 60]], "ID": "2682"}, {"text": "The model consists of two subtasks of boundary identification(BI) and semantic role classification(SRC). ", "acronyms": [[99, 102], [62, 64]], "long-forms": [[70, 97], [38, 61]], "ID": "2683"}, {"text": "vide a significant degree of control. Perhaps nowhere is this observation more keenly  felt than in weak lexical ontologies like Princeton WordNet (PWN). In PWN [1], ", "acronyms": [[148, 151], [157, 160]], "long-forms": [[129, 146]], "ID": "2684"}, {"text": "form (FFC); from this decision he/she formulates a natural language utterance with certain features including the sentence type (SeTp) the subject type (SuTp) and punctuation (Punct).", "acronyms": [[129, 133], [6, 9], [153, 157], [176, 181]], "long-forms": [[114, 127], [139, 151], [163, 174]], "ID": "2685"}, {"text": "Construct word representation model for  corpus in the base time, D(TB), and in the  target time, D(TT). ( Section 2.1) ", "acronyms": [[100, 102]], "long-forms": [[85, 96]], "ID": "2686"}, {"text": "84  Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 144?151, Ann Arbor, June 2005.", "acronyms": [[82, 87]], "long-forms": [[41, 80]], "ID": "2687"}, {"text": "(1992). Grammars  are defined over typed fea-  twre .structures (TFSs) which can be viewed as  generalizations of first-order terms (Carpenter, ", "acronyms": [[65, 69]], "long-forms": [[35, 63]], "ID": "2688"}, {"text": " FERGUS was originally trained on the Penn Tree Bank corpus consisting of Wall Street Journal text (WSJ). The results on", "acronyms": [[100, 103], [1, 7]], "long-forms": [[74, 93]], "ID": "2689"}, {"text": "for alignments.  Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art per-", "acronyms": [[43, 46]], "long-forms": [[17, 41]], "ID": "2690"}, {"text": "Therefore, we refined our reference performance level by combining the ME models (MEM) and handcrafted models (HCM). Suppose the score of a", "acronyms": [[111, 114], [82, 85]], "long-forms": [[91, 109], [71, 79]], "ID": "2691"}, {"text": "The entries in the  subjectivity word list have been labeled with  part of speech (POS) tags as well as either  strong or weak subjective tag depending on the ", "acronyms": [[83, 86]], "long-forms": [[67, 81]], "ID": "2692"}, {"text": "construct the desired model in a way that allows efficient inference, even for large datasets, using determinantal point processes (DPPs). We begin", "acronyms": [[132, 136]], "long-forms": [[101, 130]], "ID": "2693"}, {"text": "2.2 Conditional Random Fields Conditional random field (CRF) was an extension of both Maximum Entropy Model (MEMs) and Hidden Markov Models (HMMs) that was firstly", "acronyms": [[109, 113], [56, 59], [141, 145]], "long-forms": [[86, 107], [30, 54], [119, 139]], "ID": "2694"}, {"text": "patterns indicative of SLI. In this work, we use Language Models (LMs) for this task since they are a powerful statistical measure of language usage", "acronyms": [[66, 69], [23, 26]], "long-forms": [[49, 64]], "ID": "2695"}, {"text": "The project ? Reference Corpus Middle Low German/ Low Rhenish (1200?1650)?2 transliterates and grammatically annotates the Middle Low German (GML) texts from which we take our examples. Be-", "acronyms": [[142, 145]], "long-forms": [[95, 133]], "ID": "2696"}, {"text": " In addition, the user can supply relevance judgements on  any document by clicking Rel (relevant), NRel (not rel-  evant), or PRel (probably relevant).", "acronyms": [[100, 104], [84, 87], [127, 131]], "long-forms": [[106, 121], [89, 97], [133, 150]], "ID": "2697"}, {"text": "not explicit about this. ? P5E2N4S3, F W A Computer Processable Language (CPL) (Clark et al. 2005) is a controlled variant of", "acronyms": [[74, 77], [27, 35]], "long-forms": [[43, 72]], "ID": "2698"}, {"text": "RERANKED 92.7 42.9 92.0 32.6 ORACLE 97.6 81.2 96.7 72.5 Table 4: Word accuracies and error rate reductions (ERR) in percentages for CELEX G2P augmented by Combilex", "acronyms": [[108, 111]], "long-forms": [[85, 106]], "ID": "2699"}, {"text": " 1 Introduction Word Sense Disambiguation (WSD) is an important component in many information organization", "acronyms": [[43, 46]], "long-forms": [[16, 41]], "ID": "2700"}, {"text": " 7.1 Support vector machines  Support vector machines (SVMs) were introduced by (Vapnik, 1995) as an instantiation ", "acronyms": [[55, 59]], "long-forms": [[30, 53]], "ID": "2701"}, {"text": "ailehor ~  The following entry is associated with the class of  verbs taking an NP as indirect objects(IOBJ) which  may be possibly found within a prepositional phrase or ", "acronyms": [[103, 107], [80, 82]], "long-forms": [[86, 101]], "ID": "2702"}, {"text": "First, we investigate how laypeople intuitively recognize metaphor by conducting Amazon Mechanical Turk (MTurk) experiments.", "acronyms": [[105, 110]], "long-forms": [[88, 103]], "ID": "2703"}, {"text": "Table 9 (Hindi). Here, precision measures the number of correct Named Entities (NEs) in the machine tagged file over the total number of NEs in the ma-", "acronyms": [[80, 83], [137, 140]], "long-forms": [[64, 78]], "ID": "2704"}, {"text": " 53 Creative Information Retrieval (CIR) can be used as a platform for the design of many Web services that offer linguistic creativity on de-mand. By enabling the flexible retrieval of n-gram data for non-literal queries, CIR allows a wide variety of creative tasks to be reimagined as simple IR tasks (Veale 2013).", "acronyms": [[36, 39], [223, 226]], "long-forms": [[4, 34]], "ID": "2705"}, {"text": " 5.1 Calculation of Emotion Tag weights  Sense_Tag_Weight (STW): The tag weight has  been calculated using SentiWordNet.", "acronyms": [[59, 62]], "long-forms": [[41, 57]], "ID": "2706"}, {"text": "description model, the Dublin Core Metadata Set, together with an interchange method provided by the Open Archives Initiative (OAI), make it possible to construct a union catalog over", "acronyms": [[127, 130]], "long-forms": [[101, 125]], "ID": "2707"}, {"text": "appear although ModP and FocP are optional.  projections such as NegP (negation phrase) will  not be discussed although we assume there must ", "acronyms": [[65, 69], [16, 20], [25, 29]], "long-forms": [[71, 86]], "ID": "2708"}, {"text": "We show that hierarchies of this type can be  automatical!y constructed, by using the semantic ategory codes and the subject codes of the  Longman Dictionary of Contemporary English (LDOCE) to disambiguate the genus terms in  noun definitkms.", "acronyms": [[183, 188]], "long-forms": [[139, 181]], "ID": "2709"}, {"text": "Table 1 provides an overview of all entity classes and relations. The workflow consists of two steps: Firstly, rule- and ontology-based named entity recognition (NER) is performed (cf. Section", "acronyms": [[162, 165]], "long-forms": [[136, 160]], "ID": "2710"}, {"text": "and why they should be adhered to? involving a coordinated phrase in the object position consisting of an NP (najprostsze zasady ? the most basic principles?)", "acronyms": [[106, 108]], "long-forms": [[110, 121]], "ID": "2711"}, {"text": "The score measures the maximum overlap between a hypothesized cluster (HYP) and a corresponding gold standard cluster (GOLD), and computes a weighted average across all the HYP clus-", "acronyms": [[119, 123], [71, 74], [173, 176]], "long-forms": [[96, 109], [49, 61]], "ID": "2712"}, {"text": "3. Generation of Crisp Descriptions Arguably the most fundamental task in the generation of referring expressions (GRE), content determination (CD) requires finding a set of properties that jointly identify the in-", "acronyms": [[115, 118], [144, 146]], "long-forms": [[78, 113], [121, 142]], "ID": "2713"}, {"text": " 1 Introduction Biomedical Text Mining (TM) has become increasingly popular due to the pressing need to provide", "acronyms": [[40, 42]], "long-forms": [[27, 38]], "ID": "2714"}, {"text": "calizations.  Direct responses (DS) are essentially characterized by introductory markers like yes/no/this is pos-", "acronyms": [[32, 34]], "long-forms": [[14, 30]], "ID": "2715"}, {"text": "\u0003\u0003\t\f\u0006\u0011\u000b\u0006\b strategies(Lewis, 1992).  We use probability threshold(PT) strategy where each document is assigned to the categories above a thresh-", "acronyms": [[65, 67]], "long-forms": [[43, 64]], "ID": "2716"}, {"text": "CP OP LC RC where CP = Correspondence Part; OP = Operator; LC = Left Context; RC = Right Context There are four different kinds of rules that may be", "acronyms": [[59, 61], [78, 80], [0, 2], [3, 5], [6, 8], [9, 11], [18, 20], [44, 46]], "long-forms": [[64, 76], [83, 96], [23, 42], [49, 57]], "ID": "2717"}, {"text": "In this paper, I present a lexical representation  of the light  verb  ha  'do'  used in two types of  Korean light verb constructions (LVCs). These ", "acronyms": [[136, 140]], "long-forms": [[110, 134]], "ID": "2718"}, {"text": "an algorithm that combines the reference choice rules for  reason and the reference choice rules for methods, to pro-  duce preverbal messages (PMs) from PCAs. As such, the ", "acronyms": [[144, 147]], "long-forms": [[124, 142]], "ID": "2719"}, {"text": "No normalization 68.52 (70.45) 84.34 (82.8) 37.4 (45.2) 67.8 (67.1) Table 2: Binary logistic regression 10-fold cross validation with different feature normalization approaches: Scores within brackets are when the female speaker data is removed; S = Stressed, US = Unstressed, MCB = Majority Class Baseline. ", "acronyms": [[260, 262], [277, 280]], "long-forms": [[265, 275], [283, 306]], "ID": "2720"}, {"text": "j.nerbonne@rug.nl Abstract Pair Hidden Markov Models (PairHMMs) are trained to align the pronunciation tran-", "acronyms": [[54, 62]], "long-forms": [[27, 52]], "ID": "2721"}, {"text": "phrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E). For all baselines we", "acronyms": [[116, 119], [80, 83]], "long-forms": [[94, 114], [58, 78]], "ID": "2722"}, {"text": "If we put these two constraints together we obtain the constraint MINS = MAXS, which means that the area where quantifiers take scope (the MAXS-", "acronyms": [[73, 77], [66, 70], [139, 143]], "long-forms": [], "ID": "2723"}, {"text": "the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland. ", "acronyms": [[145, 149]], "long-forms": [[106, 143]], "ID": "2724"}, {"text": " ? REL = relation + property; ARG = NP/VP/ADJ", "acronyms": [[3, 6], [30, 33]], "long-forms": [[9, 17], [36, 45]], "ID": "2725"}, {"text": "Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN ? CREST, JST (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN", "acronyms": [[54, 57]], "long-forms": [[59, 87]], "ID": "2726"}, {"text": "categories include NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition, CONJ = conjunction, DET = determiner, PRON = 1http://www.wiktionary.org/", "acronyms": [[94, 98], [114, 117], [31, 34], [48, 51], [62, 65], [76, 79], [132, 136]], "long-forms": [[101, 112], [120, 130], [37, 46], [54, 60], [68, 74], [82, 92]], "ID": "2727"}, {"text": "fer to semantically similar words. We have applied the Markov Cluster algorithm (MCL) (van Dongen, 2000) to group semantically similar terms together.", "acronyms": [[81, 84]], "long-forms": [[55, 79]], "ID": "2728"}, {"text": "PRN = Pronoun A-FLEX = Adjectival Inflexion  CA = Case CL = Inflsxion Class GD = Gender MD = Mode  PF ~ Predicate Form PS = Person TN = Tense  NU = Number ", "acronyms": [[131, 133], [0, 3], [14, 20], [45, 47], [55, 57], [76, 78], [88, 90], [99, 101], [119, 121], [143, 145]], "long-forms": [[136, 141], [6, 13], [23, 43], [50, 54], [70, 75], [81, 87], [93, 97], [104, 118], [124, 130], [148, 154]], "ID": "2729"}, {"text": "In this paper, we address the problem of parsing transcribed spoken Levantine Arabic (LA).We do not assume the existence of any anno-", "acronyms": [[86, 88]], "long-forms": [[68, 84]], "ID": "2730"}, {"text": "See Table 3 for the complete list of non-predicate filters describing restrictions on the role text (RT), role span (RS), and predicate frame (PF) in terms of the semantic type", "acronyms": [[101, 103], [117, 119], [143, 145]], "long-forms": [[90, 99], [106, 115], [126, 141]], "ID": "2731"}, {"text": "Knowing the precise identity of Fisher vector ??(?), we propose a natural measure which we call  Weighted Gradient Uncertainty (WGU) based on the facts explained in the previous paragraph:  ????(???)", "acronyms": [[128, 131]], "long-forms": [[97, 126]], "ID": "2732"}, {"text": " 3. Transitional Phrases (TRP) We hypothesize that a more cohesive essay, being easier for a", "acronyms": [[26, 29]], "long-forms": [[4, 24]], "ID": "2733"}, {"text": "Pos i t i ve  Recal l  (PR)  :  ? Pos i t ive  Prec is ion  (PP)  :  d ?", "acronyms": [[61, 63], [24, 26]], "long-forms": [[34, 58], [0, 21]], "ID": "2734"}, {"text": "cal work is extensive. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking.", "acronyms": [[90, 93]], "long-forms": [[57, 88]], "ID": "2735"}, {"text": "We would expect this to be the case in general, but as always, cases exist where a conflict between a contrast (CoCo) and a change to a method (PModi) occur:", "acronyms": [[112, 116], [144, 149]], "long-forms": [[83, 110]], "ID": "2736"}, {"text": "End point SIP user agents: These are the SIP end points that exchange SIP signaling messages with the SIP Application server (AS) for call control.", "acronyms": [[126, 128], [41, 44], [70, 73], [102, 105]], "long-forms": [[106, 124], [10, 13]], "ID": "2737"}, {"text": "1 Introduction  Scalability in dialog systems is, of course, not only a  matter of the natural language understanding (NLU)  component, but also of the NLG part of the system.2 We ", "acronyms": [[119, 122], [152, 155]], "long-forms": [[87, 117]], "ID": "2738"}, {"text": "the divergence of their distributions in the targets and backgrounds. A support vector machine (SVM) was used to learn to classify between the targets and", "acronyms": [[96, 99]], "long-forms": [[72, 94]], "ID": "2739"}, {"text": "set of basically two algorithms. One algorithm is a  variant of Alignment Based Learning (ABL), as  described in Van Zaanen (2001).", "acronyms": [[90, 93]], "long-forms": [[64, 88]], "ID": "2740"}, {"text": "passenger vessel.  (5) Multiple mentions (MENTION): These alignments link one word to multiple occur-", "acronyms": [[42, 49]], "long-forms": [[32, 40]], "ID": "2741"}, {"text": " 3.1 Identifying verbal blocks (Vbs) Verbal blocks are composed of a head (Vb-H) and possibly accompanying dependents (Vb-D).", "acronyms": [[75, 79], [32, 35], [119, 123]], "long-forms": [[37, 73], [17, 30]], "ID": "2742"}, {"text": "In Proceedings of the 2nd Workshop on Treebanks and Linguistic Theories (TLT), pages 217?220, Va?xjo?. ", "acronyms": [[73, 76]], "long-forms": [[38, 71]], "ID": "2743"}, {"text": "Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer, McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary UTurku VIBGhent ConcordU HCMUS", "acronyms": [[187, 189], [15, 17], [36, 39], [80, 82], [124, 130], [147, 152], [222, 226]], "long-forms": [[190, 209], [18, 34], [40, 67], [83, 99], [114, 122], [131, 137], [153, 178], [227, 237]], "ID": "2744"}, {"text": " We compared the resulting ranked lists of bigrams with a list of target MWEs extracted from the British National Corpus (BNC)3. The target list was pro-", "acronyms": [[122, 125], [73, 77]], "long-forms": [[97, 120]], "ID": "2745"}, {"text": "ing measure of the loss in modeling accuracy:     Probability Loss (PL):   )()()(),( vuvuvu +?", "acronyms": [[68, 70]], "long-forms": [[50, 66]], "ID": "2746"}, {"text": " 1 Introduction The task of Semantic Role Labeling (SRL) is to identify predicate-argument relationships in natural", "acronyms": [[52, 55]], "long-forms": [[28, 50]], "ID": "2747"}, {"text": "rithms for learning neuropsychological and demographic data which are then used for the prediction of Clinical Dementia Rating (CDR) scores for different sub-types of Dementia and other cog-", "acronyms": [[128, 131]], "long-forms": [[102, 126]], "ID": "2748"}, {"text": "Conf.  Fifth Generation Computer Systems 1992 (FGCS'92),  pp.1133-1140, 1992.", "acronyms": [[47, 54]], "long-forms": [[7, 45]], "ID": "2749"}, {"text": "pairs. Transactions of the Association for Computational Linguistics (TACL), 2(10):377?392. ", "acronyms": [[70, 74]], "long-forms": [[7, 68]], "ID": "2750"}, {"text": "Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29 Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03 Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64 Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72", "acronyms": [[176, 180], [17, 21], [99, 103], [257, 261]], "long-forms": [[157, 174], [0, 15], [78, 97], [246, 255]], "ID": "2751"}, {"text": " Some implementation otes  The Carnegie Mellon Spoken Language Shell (CM-SLS)  was intentionally designed to have easily modifiable com- ", "acronyms": [[70, 76]], "long-forms": [[31, 68]], "ID": "2752"}, {"text": " The actual performance of a system is measured in terms of detection error tradeoff (DET) curves and the minimal normalized cost.", "acronyms": [[86, 89]], "long-forms": [[60, 84]], "ID": "2753"}, {"text": "for evaluating the ASR,  2. Concept F-measure (ConF) ? the F-measure of ", "acronyms": [[47, 51], [19, 22]], "long-forms": [[28, 37]], "ID": "2754"}, {"text": "Abstract  This paper proposes a novel reordering model  for statistical machine translation (SMT) by  means of modeling the translation orders of ", "acronyms": [[93, 96]], "long-forms": [[60, 91]], "ID": "2755"}, {"text": "is associated with the data sparseness problem. Most of the previously proposed methods to  extract compounds or to measure word association using mutual information (MI) either ignore  or penalize items with low co-occurrence ounts (Church and Hanks 1990, Su, Wu and Chang ", "acronyms": [[167, 169]], "long-forms": [[147, 165]], "ID": "2756"}, {"text": " From the results shown in Table 3, we could find the proposed semantic word embedding (SWE) model can consistently achieve 0.8% (or more) ab-", "acronyms": [[88, 91]], "long-forms": [[63, 86]], "ID": "2757"}, {"text": "cal machine translation. The 41th Annual meeting of the Association for Computational Linguistics (ACL), 311-318.", "acronyms": [[99, 102]], "long-forms": [[56, 97]], "ID": "2758"}, {"text": "tences in the other part of the corpus. Therefore, we performed a language identification (LID)based filtering afterwards (performed only on the", "acronyms": [[91, 94]], "long-forms": [[66, 89]], "ID": "2759"}, {"text": " Reference:  MedLine sample # 6  Autonym:  decoy receptor 3 (DcR3)  Information a soluble decoy receptor  ", "acronyms": [[61, 65]], "long-forms": [[43, 59]], "ID": "2760"}, {"text": " We perform our analyses on data from the 20082011 Text Analysis Conference (TAC)1 organized by the National Institute of Standards and Technol-", "acronyms": [[77, 80]], "long-forms": [[51, 75]], "ID": "2761"}, {"text": "1978. Longman Dictionary of  Contemporary lCnglish (LI)OCE). Long\\]nan, liar- ", "acronyms": [[51, 59]], "long-forms": [[6, 50]], "ID": "2762"}, {"text": "Harman D.K. 1983. Overview of the second Text Retrieval Conference (TREC-2). Information Processing", "acronyms": [[68, 74], [7, 10]], "long-forms": [[34, 66]], "ID": "2763"}, {"text": "parsing. In Tenth International Conference on Parsing Technologies (IWPT), pages 121?132, Prague, Czech Republic.", "acronyms": [[68, 72]], "long-forms": [[18, 66]], "ID": "2764"}, {"text": "In Processdings of Sixth International Conference on  Language Resources and Evaluation (LREC),  pages 2961-2968, Marrakech, Morocco.", "acronyms": [[89, 93]], "long-forms": [[54, 72]], "ID": "2765"}, {"text": " ? Because Dependency Grammar (DG) directly describes the functional relations between  words, and s dependency tree has not any non-terminal nodes, DG is suitable for our ", "acronyms": [[31, 33], [149, 151]], "long-forms": [[11, 29]], "ID": "2766"}, {"text": "other animate and inanimate), which cover ten categories of noun phrases, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other", "acronyms": [[115, 119], [95, 98], [133, 136]], "long-forms": [[121, 127], [100, 112], [150, 158]], "ID": "2767"}, {"text": "51 Table 1: Semantic restrictions on Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model of Anatomy, other ontologies identified in the text.", "acronyms": [[61, 64], [88, 91]], "long-forms": [[67, 86], [94, 123]], "ID": "2768"}, {"text": "ond accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal object, EP = expletive es 9AG = genitive adjunct", "acronyms": [[122, 124], [23, 25], [36, 38], [58, 60], [85, 87], [101, 103], [140, 143]], "long-forms": [[127, 136], [28, 34], [41, 56], [63, 83], [90, 99], [106, 120], [146, 162]], "ID": "2769"}, {"text": "Budanitsky and Hirst Lexical Semantic Relatedness Figure 5 Precision (PD), recall (RD), and F-measure (FD) for malapropism detection by measure and scope. ", "acronyms": [[70, 72], [83, 85], [103, 105]], "long-forms": [[59, 68], [75, 81], [92, 101]], "ID": "2770"}, {"text": "For words which failed to be guessed by  tile guessing rules we applied the standard method  of classifying them as common nouns (NN) if they  are not capitalised inside a sentence and proper ", "acronyms": [[130, 132]], "long-forms": [[123, 128]], "ID": "2771"}, {"text": "f ~ r l r ~ f  TRANSFORIATICNS *llrllrt*  SCAN C A L L E D  AT 1 I  ANTEST CALLED FOR 1 '*REDVOW \" (AACC) ,SD= 2. RES= 6.", "acronyms": [[82, 85], [100, 104], [107, 109], [114, 117]], "long-forms": [[68, 81]], "ID": "2772"}, {"text": "Currently a large proportion of languageindependent MT approaches are based on the  statistical machine translation (SMT) paradigm  (Koehn, 2010).", "acronyms": [[117, 120], [52, 54]], "long-forms": [[84, 115]], "ID": "2773"}, {"text": "By experiments, we show that the proposed model outperforms the bigram Hidden Markov model (HMM)based tagging model.", "acronyms": [[92, 95]], "long-forms": [[71, 90]], "ID": "2774"}, {"text": "PROBING QUESTION (QP) Do you think that looks correct? 4.99% 4.76% 0.731 QUESTION PROMPT (QQ) Any questions? 2.49% 2.24% 0.978", "acronyms": [[18, 20], [90, 92]], "long-forms": [[73, 88], [0, 16]], "ID": "2775"}, {"text": "\\[ Extra~:~nouns I IE~rac~'~ n?unsl i Calculating f requ~ vectors (FreqVa) I ICalculating frequency vectors (FreqVe)l  ._1 Calculating similarity I ", "acronyms": [[109, 115], [67, 73]], "long-forms": [[90, 107], [50, 65]], "ID": "2776"}, {"text": "In the next section we will explain the concep-  tual model of ILMs by means of the KADS Domain  Description Language (DDL) proposed in Schreiber  (Schreiber et al, 1993).", "acronyms": [[119, 122], [63, 67], [84, 88]], "long-forms": [[97, 117]], "ID": "2777"}, {"text": "In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD, October. ", "acronyms": [[117, 120], [134, 136]], "long-forms": [[104, 115]], "ID": "2778"}, {"text": "sentences. The third, following (Yates et al, 2006), is maximum recall (MR). MR simply predicts that all", "acronyms": [[72, 74], [77, 79]], "long-forms": [[56, 70]], "ID": "2779"}, {"text": "Our relation extraction system is hierarchical (Bunescu and Mooney, 2005b; Sun et al, 2011) and apply maximum entropy (MaxEnt) in the MALLET", "acronyms": [[119, 125], [134, 140]], "long-forms": [[102, 117]], "ID": "2780"}, {"text": "Processing, Hong Kong, Apr. HTK, 2004. Hidden Markov Model Toolkit (HTK) 3.2.", "acronyms": [[68, 71], [28, 31]], "long-forms": [[39, 66]], "ID": "2781"}, {"text": "Task (Pradhan et al 2011), one text from each of the five represented genres: Broadcast Conversations (BC), Broadcast News (BN), Magazine (MZ), News Wire (NW) and Web Blogs and News Groups", "acronyms": [[124, 126], [139, 141]], "long-forms": [[108, 122], [129, 137]], "ID": "2782"}, {"text": "i' SRI - text extraction  ~\" TRW - document detection output  i' University of Massachusetts (UMass) -  document detection ", "acronyms": [[94, 99], [29, 32], [3, 6]], "long-forms": [[65, 92]], "ID": "2783"}, {"text": "CTexT. 2011. Afrikaans WordNet. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.", "acronyms": [[60, 65], [0, 5]], "long-forms": [[32, 58]], "ID": "2784"}, {"text": "The task of location normalization is to identify  the correct sense of a possibly ambiguous  location Named Entity (NE). Ambiguity is very ", "acronyms": [[117, 119]], "long-forms": [[103, 115]], "ID": "2785"}, {"text": " In Proceedings ofthe 12th International Conference on  Computational Linguistics (COLING), Budapest. ", "acronyms": [[83, 89]], "long-forms": [[56, 81]], "ID": "2786"}, {"text": "Probabilistic topic models (PTM), such as probabilistic latent semantic indexing(PLSI) (Hofmann, 1999) and latent Dirichlet alocation(LDA) (Blei et al.,", "acronyms": [[134, 137], [28, 31], [81, 85]], "long-forms": [[107, 132], [0, 26], [42, 80]], "ID": "2787"}, {"text": "3.4 Algorithm The algorithm first splits the data into appropriate units (SL=source language, TL=target language): 1.", "acronyms": [[74, 76], [94, 96]], "long-forms": [[77, 92], [97, 112]], "ID": "2788"}, {"text": "4.3 Experiments with the QA data In the first set of experiments we focus on the Question Answering (QA) domain (CLEF corpus). ", "acronyms": [[101, 103], [25, 27], [113, 117]], "long-forms": [[81, 99]], "ID": "2789"}, {"text": "This work investigated four well-known specifications created by four different organizations: Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (Beijing)", "acronyms": [[112, 114], [147, 152]], "long-forms": [[95, 110], [117, 132]], "ID": "2790"}, {"text": "on the SMTnews dataset, with an increase in the Pearson correlation of over 0.10. MSRpar (MPar) is the only dataset in which TLsim (S?aric?", "acronyms": [[90, 94], [125, 130]], "long-forms": [[82, 88], [7, 10]], "ID": "2791"}, {"text": "processing. This paper explores grammatical issues in Scottish Gaelic by means of dependency tagging and combinatory categorial grammar (CCG), which we see as complementary approaches. As such it", "acronyms": [[137, 140]], "long-forms": [[105, 135]], "ID": "2792"}, {"text": "_ _eI$X=A\u0007I&HLH7K5HOG\u0007X5HOGPMLHLK ^ CWX=A$X=APH U\u0003I&K5X\u0010K5HOG\u0007X5H&GflMLHJa\u0007HLK5MOI5CEc\u0007CEG! ", "acronyms": [[44, 47]], "long-forms": [[48, 90]], "ID": "2793"}, {"text": "ILP.  An integer linear program(ILP) is basically the same as a linear program.", "acronyms": [[32, 35], [0, 3]], "long-forms": [[9, 30]], "ID": "2794"}, {"text": "than have them specified in a tag dictionary.  The Lexicon HMM (Lex-HMM) extends the Pitman-Yor HMM (PYP-HMM) described by", "acronyms": [[64, 71], [101, 108]], "long-forms": [[51, 62], [85, 99]], "ID": "2795"}, {"text": "Cleveland Family study dceweb1.case.edu/ serc/collab/project_family.shtml), CHS (the Cardiovascular Heart Study www. ", "acronyms": [[76, 79]], "long-forms": [[85, 111]], "ID": "2796"}, {"text": "editor, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http://www.lrec-", "acronyms": [[172, 176], [96, 103]], "long-forms": [[131, 170], [61, 94]], "ID": "2797"}, {"text": "of not requiring so much copying. On the con-  trary, constraint unification (CU) (Hasida 1986,  Tuda et al 1989), a disjunctive unification ", "acronyms": [[78, 80]], "long-forms": [[54, 76]], "ID": "2798"}, {"text": "such as Declarative Sentence(SDEC),  Noun Phrase(NP), Inf init ive  Phrase(INF), and Verb Phrase(VP),  are big structures with some.k~y ", "acronyms": [[97, 99], [29, 33], [49, 51], [75, 78]], "long-forms": [[85, 95], [8, 28], [37, 48], [54, 74]], "ID": "2799"}, {"text": "of Translation Studies SI - 1000 Ljubljana, A?ker?eva 2 aljosav@gmail.com     Abstract  We report on a series of experiments aimed at improving the machine translation of ambig-uous lexical items by using wordnet-based unsupervised Word Sense Disambiguation (WSD) and comparing its results to three MT systems. Our experiments are performed for the English-Slovene language pair using UKB, a freely available graph-based word sense disambiguation system.", "acronyms": [[259, 262], [299, 301], [385, 388]], "long-forms": [[232, 257]], "ID": "2800"}, {"text": " 4.3 MORPHOTACT1C MODEL  An associative Morphotactic Model (MTModel) is a pair  <{MRi},<*>, where {MRi} is a set of morphotactic rules ", "acronyms": [[60, 67], [82, 85], [99, 102]], "long-forms": [[40, 58]], "ID": "2801"}, {"text": " In Section 3, we report on two Amazon Mechanical Turk (MTurk) experiments, which demonstrate that crowdsourcing is a feasible way", "acronyms": [[56, 61]], "long-forms": [[39, 54]], "ID": "2802"}, {"text": "overall 412 5298 1519 750 Table 1: Corpus statistics: number of sentences (S), words (W), frame elements (FE) and alignments.", "acronyms": [[106, 108]], "long-forms": [[90, 104], [64, 73], [79, 84]], "ID": "2803"}, {"text": "workbench (Hall et al, 2009). For SVM, we employ the radial basis function kernel (RBF) and we use the wrapper provided by Weka for", "acronyms": [[83, 86], [34, 37]], "long-forms": [[53, 74]], "ID": "2804"}, {"text": "3 Architecture of SCQA As shown in Figure 2, SCQA consists of a pair of deep convolutional neural networks (CNN) with convolution, max pooling and rectified lin-", "acronyms": [[108, 111], [18, 22], [45, 49]], "long-forms": [[77, 106]], "ID": "2805"}, {"text": "  Abstract  Named entity recognition (NER) is nowadays an important task, which is responsi-", "acronyms": [[38, 41]], "long-forms": [[12, 36]], "ID": "2806"}, {"text": "plementation of SVR, with tuned parameters.  Ranking: An SVM model for ranking (SVMRank) is trained using as ranking pairs all pairs of stu-", "acronyms": [[80, 87], [16, 19]], "long-forms": [[57, 78]], "ID": "2807"}, {"text": " 110  ehange(CHA) communication(COMM)  cognition(COG) competition(COMP) ", "acronyms": [[32, 36], [13, 16], [49, 52], [66, 70]], "long-forms": [[18, 30], [6, 12], [39, 48], [54, 65]], "ID": "2808"}, {"text": "Kearns (2002) distinguishes between two usages of light verbs in LVCs: what she calls a true light verb (TLV), as in give a groan, and what she calls a vague action verb (VAV), as in", "acronyms": [[105, 108], [65, 69], [171, 174]], "long-forms": [[88, 103], [152, 169]], "ID": "2809"}, {"text": "Abstract Verbal comprehension questions appear very frequently in Intelligence Quotient (IQ) tests, which measure human?s verbal ability includ-", "acronyms": [[89, 91]], "long-forms": [[66, 87]], "ID": "2810"}, {"text": "Pr(f |e) Pr(e) (2) where Pr(f |e) is the translation model and Pr(e) is the target language model (LM). This ap-", "acronyms": [[99, 101]], "long-forms": [[83, 97]], "ID": "2811"}, {"text": "by (Punyakanok et al, 2004). The process is formulated as an integer linear programming (ILP) problem that takes as inputs the confidences over each", "acronyms": [[89, 92]], "long-forms": [[61, 87]], "ID": "2812"}, {"text": "tion. So we developed a method to optimize the CRFs towards the alignment error rate (AER) or the F-score with sure and possible links as introduced", "acronyms": [[86, 89]], "long-forms": [[64, 84]], "ID": "2813"}, {"text": "837 (a) (b) Figure 1: Deep recurrent neural network (DRNN) architectures: arrows represent connection matrices; white, black, and grey circles represent input frames, hidden states, and output frames, respectively; (a): L intermediate layer DRNN with recurrent connections", "acronyms": [[53, 57], [241, 245]], "long-forms": [[22, 51]], "ID": "2814"}, {"text": "2.2 Hidden Markov Models One simple family of models for part-of-speech induction are the Hidden Markov Models (HMMs), in which there is a sequence of hidden state vari-", "acronyms": [[112, 116]], "long-forms": [[90, 110]], "ID": "2815"}, {"text": "methods to identify such targets. The first method depends on identifying noun groups (NG). We con-", "acronyms": [[87, 89]], "long-forms": [[74, 85]], "ID": "2816"}, {"text": "We therefore chose to perform ASR using a statistical language model (LM) and employ CMU?s Sphinx to generate an n-best list of recogni-", "acronyms": [[70, 72], [30, 33], [85, 90]], "long-forms": [[54, 68]], "ID": "2817"}, {"text": "Table 2  Summary of error rates with the language model only (LM), the prosody model only (PM), the  combined ecision tree (CM-DT), and the combined HMM (CM-HMM). ( a) shows word-based ", "acronyms": [[124, 129], [154, 160], [62, 64], [91, 93]], "long-forms": [[101, 122], [140, 152], [41, 55], [71, 84]], "ID": "2818"}, {"text": "n of  PSP Positive?sentence percentage (PSP) statistics  ", "acronyms": [[40, 43], [6, 9]], "long-forms": [[10, 38]], "ID": "2819"}, {"text": "Ensemble NN + LR (w/o alternate grammar) 54.38 41.90 Ensemble NN + LR (w/o synthetic data) 53.98 42.41 Table 1: Accuracy of the Neural Network (NN) and Logistic Regression (LR) implementations of our system with various configurations.", "acronyms": [[144, 146], [173, 175], [9, 16], [62, 69]], "long-forms": [[128, 142], [152, 171]], "ID": "2820"}, {"text": "(Bjo?rne et al 2011).  The Turku Event Extraction System (TEES)1 is an open source program for extracting events and re-", "acronyms": [[58, 62]], "long-forms": [[27, 56]], "ID": "2821"}, {"text": "In  * This work has been sponsored by the Fonds zur  FSrderung der wissenschaftlichen Forschung (FWF),  Grant No.", "acronyms": [[97, 100]], "long-forms": [[53, 95]], "ID": "2822"}, {"text": "both in the form of documents and factual  databases. These knowledge sources (KSs) are  intrinsically heterogeneous and dynamic.", "acronyms": [[79, 82]], "long-forms": [[60, 77]], "ID": "2823"}, {"text": "LA     =   The average length (ALen) of chunks for each  type is the average number of tokens in each chunk ", "acronyms": [[31, 35], [0, 2]], "long-forms": [[15, 29]], "ID": "2824"}, {"text": "We show each sentence to three unique workers on Amazon Mechanical Turk (MTurk) and ask each to judge how well the paraphrase retains the mean-", "acronyms": [[73, 78]], "long-forms": [[56, 71]], "ID": "2825"}, {"text": " This  suspens ion takes place dur ing un i f icat ion  of  the Flat Concurrent  Pro log (FCP) pred icate   (see below), into which expert  rout ines are ", "acronyms": [[90, 93]], "long-forms": [[64, 88]], "ID": "2826"}, {"text": "The typical way to address these situations is to jointly model these relations, e.g., using Markov logic networks (MLN) (Poon and Vanderwende, 2010).", "acronyms": [[116, 119]], "long-forms": [[93, 114]], "ID": "2827"}, {"text": "In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA), Denver, Colorado. ", "acronyms": [[93, 97]], "long-forms": [[22, 91]], "ID": "2828"}, {"text": "are also kernel methods that directly take into account the multiclass nature of the problem such as the kernel partial least squares regression (KPLS). ", "acronyms": [[146, 150]], "long-forms": [[105, 144]], "ID": "2829"}, {"text": "A model-theoretic coreference scoring scheme. In Proceedings of Message Understanding Conference 6 (MUC-6), pages 45?52. ", "acronyms": [[100, 105]], "long-forms": [[64, 98]], "ID": "2830"}, {"text": "tially lexicalized) syntactic dependencies and patterns. The weight \u0000 is the Local Mutual Informa-tion (LMI) (Evert, 2005) computed on link type frequency (negative LMI values are raised to 0).3.1 Test set", "acronyms": [[104, 107], [165, 168]], "long-forms": [[77, 102]], "ID": "2831"}, {"text": "equivalent m Dutch For a sample of 59 Ital,an noun  s)nsets there is at least an overlap of 30% (20) with  Dutch Examples are Arbeltszeitverkurzung (DE)  = arbeidstijdverkortmg (NL) = (reduction of work- ", "acronyms": [[149, 151], [178, 180]], "long-forms": [[107, 121]], "ID": "2832"}, {"text": "The second approach is based on statistical modeling. We adopted one typical  implementation called the \"vector space model\" (VSM) (Frakes and Baeza-Yates 1992;  Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch/itze 1992), which has ", "acronyms": [[126, 129]], "long-forms": [[105, 123]], "ID": "2833"}, {"text": "2.1 The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and target sentences are used for training statistical", "acronyms": [[81, 83], [51, 55]], "long-forms": [[60, 79], [8, 49]], "ID": "2834"}, {"text": "versational participants. This type of HMM is called a speaker HMM (SHMM) and has been successfully utilized to model two-party conversa-", "acronyms": [[68, 72], [39, 42]], "long-forms": [[55, 66]], "ID": "2835"}, {"text": "there are two ways of feeding the context vector into the main recurrent language model (RLM); (1) early fusion (EF) and (2) late fusion (LF), from Sec.", "acronyms": [[113, 115], [138, 140], [89, 92]], "long-forms": [[99, 111], [125, 136], [63, 87]], "ID": "2836"}, {"text": "ADJ (adjectives), ADV (adverbs), CJ (conjunctions), CL (clitics), CN (common nouns), DA (definite articles), DEM (demonstratives), INF (infinitives), ITJ (interjections), NP (noun", "acronyms": [[85, 87], [109, 112], [0, 3], [18, 21], [33, 35], [52, 54], [66, 68], [131, 134], [150, 153], [171, 173]], "long-forms": [[89, 106], [114, 128], [5, 15], [23, 30], [37, 49], [56, 63], [70, 83], [136, 147], [155, 168], [175, 179]], "ID": "2837"}, {"text": "grammars is denoted CFGS.  In a linear indexed grammar (LIG),2 strings are derived from nonterminals with an associated", "acronyms": [[56, 59], [20, 24]], "long-forms": [[32, 54]], "ID": "2838"}, {"text": "1 Reinforcement Learning in Dialogue Machine Learning techniques, and particularly Reinforcement Learning (RL), have recently received great interest in research on dialogue man-", "acronyms": [[107, 109]], "long-forms": [[83, 105]], "ID": "2839"}, {"text": "168 Figure 1: Plots of concreteness vs. imageability scores for literal vs. nonliteral words in the VUAMC (Conc=concreteness, Imag=imageability, NL=nonliteral, L=literal) concrete than the dependent/s; H", "acronyms": [[145, 147], [100, 105], [107, 111], [126, 130]], "long-forms": [[148, 158], [112, 124], [131, 143], [162, 169]], "ID": "2840"}, {"text": " For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ). These", "acronyms": [[88, 93], [35, 40]], "long-forms": [[72, 86], [18, 33]], "ID": "2841"}, {"text": "= Majority Class, Acc. = Accuracy, SE = Standard Error) ing on 5 \u0000 with 20-fold cross-validation achieves an", "acronyms": [[35, 37], [18, 21]], "long-forms": [[40, 54], [25, 33]], "ID": "2842"}, {"text": "hypernym, hyponym, near-synonym, holonym, and mernoym are listed as below:  Hypernym(HYP) (a) IF x=ANT ", "acronyms": [[85, 88], [99, 102]], "long-forms": [[76, 83]], "ID": "2843"}, {"text": "chine learning models based on three different well known techniques, decision trees (C4.5), rule induction (RIPPER) and maximum entropy (MaxEnt), in order to find out which approach is the most suitable", "acronyms": [[138, 144], [109, 115]], "long-forms": [[121, 136]], "ID": "2844"}, {"text": "ABSTRACT  We present a progress report on our research  on nominal compounds (NC's). Recent approaches to ", "acronyms": [[78, 82]], "long-forms": [[59, 76]], "ID": "2845"}, {"text": "that to go from the head of the chunk to the target in the dependency graph (Figure 3), you traverse a SUB (subject) link upwards. ", "acronyms": [[103, 106]], "long-forms": [[108, 115]], "ID": "2846"}, {"text": "To our knowledge there exist two off the shelf English Arabic Machine Translation (MT) systems: Tarjim and Almisbar.3 We use both MT systems to translate", "acronyms": [[83, 85], [130, 132]], "long-forms": [[62, 81]], "ID": "2847"}, {"text": "NNS?, in this paper; other work makes a distinction between ESL (English as a Second Language) speakers (who live and speak in a primarily English-speaking environment) or EFL", "acronyms": [[60, 63], [0, 4], [172, 175]], "long-forms": [[65, 93]], "ID": "2848"}, {"text": "2 Preposition Semantic Role Disambiguation in Penn Treebank Significant numbers of prepositional phrases (PPs) in the Penn treebank [1] are tagged with their semantic role relative to the governing verb.", "acronyms": [[106, 109]], "long-forms": [[83, 104]], "ID": "2849"}, {"text": "competition submissions). Notice that most were using Support Vector Machine (SVM) with bagof-word features in a very small window, local col-", "acronyms": [[78, 81]], "long-forms": [[54, 76]], "ID": "2850"}, {"text": " 3.2 Swedish Constructicon The Swedish Constructicon (SweCcn) 4", "acronyms": [[54, 60]], "long-forms": [[31, 52]], "ID": "2851"}, {"text": "Labels Base NP modifier NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR (proper noun), NT (temporal noun), JJ (other noun-modifier), or PU (punctuation) Base NP head NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR", "acronyms": [[184, 186]], "long-forms": [[188, 199]], "ID": "2852"}, {"text": "The pair of speakers KI-KA is in the top quin-tile (>13.6%). Based on this evidence we can conclude the following:   ExpDisagreementDRX (KI,KA, 5, dialogue-1) which may be read as follows: speakers KI and KA have the highest level of expressive disagreement in dialogue-1. This measure is complemented by a Cumulative Disagreement Index (CDX), which is computed for each speaker as a percentage of all Disagree-Reject utterances in the discourse that are made by this speaker. Unlike DRX, which is computed for pairs of speakers, the CDX values are as-signed to each group participant and indicate the degree of disagreement that each person generates.", "acronyms": [[338, 341], [21, 26], [198, 200], [205, 207], [484, 487], [534, 537]], "long-forms": [[307, 336]], "ID": "2853"}, {"text": "This model is a multinomial DP model. Under the Chinese restaurant process (CRP) (Aldous, 1985) 394", "acronyms": [[76, 79], [28, 30]], "long-forms": [[48, 74]], "ID": "2854"}, {"text": "      Input source sentence (ISS)    ", "acronyms": [[29, 32]], "long-forms": [[6, 27]], "ID": "2855"}, {"text": "learning this decision is learned automatically.  Reinforcement Learning (RL) has been successfully used for learning dialogue management", "acronyms": [[74, 76]], "long-forms": [[50, 72]], "ID": "2856"}, {"text": "2.2 Graphical Representation Recently, Ding et al (2008) use skip-chain and 2D Conditional Random Fields (CRFs) (Lafferty et al, 2001) to perform the relational learning for", "acronyms": [[106, 110], [76, 78]], "long-forms": [[79, 104]], "ID": "2857"}, {"text": "10-fold open test 62.80-58.54 59.15 66.46-65.55 65.55 65.55-64.63 Table 5: Comparison of Optimizers (Opinions in KNB Corpus) Batch Training (BFGS) Online Training (SD) Simulated parameter initialization chunked data selection Annealing", "acronyms": [[141, 145], [113, 116], [164, 166]], "long-forms": [[125, 139]], "ID": "2858"}, {"text": "2005; Wieling et al, 2007) for string similarity  estimation, and is based on the notion of string  Edit Distance (ED). String ED is defined here as ", "acronyms": [[115, 117], [127, 129]], "long-forms": [[100, 113]], "ID": "2859"}, {"text": "tation for extracting entities from w. In our system, we let an extraction predicate be a simplified XML path (XPath) such as /html[1]/body[1]/table[2]/tr/td[1]", "acronyms": [[111, 116]], "long-forms": [[101, 109]], "ID": "2860"}, {"text": "maries that are too specific. In this paper, we propose a natural language generation (NLG) model for the automatic creation of indicative multidoc-", "acronyms": [[87, 90]], "long-forms": [[58, 85]], "ID": "2861"}, {"text": "*Event, *Mtrans-Action), and plans (i.e. *Pick-Up-  Gun). A hierarchy of Concept Class (CC) entities  stores knowledge both declaratively and procedurely ", "acronyms": [[88, 90]], "long-forms": [[73, 86]], "ID": "2862"}, {"text": "discussed in section 2, are represented.  Evaluation of machine translation (MT) systems has to consider the pre-processing of input and", "acronyms": [[77, 79]], "long-forms": [[56, 75]], "ID": "2863"}, {"text": "Two-level rules are generally of the form CP OP LC RC where CP = Correspondence Part; OP = Operator; LC = Left Context; RC = Right Context", "acronyms": [[60, 62], [86, 88], [42, 44], [45, 47], [48, 50], [51, 53], [101, 103], [120, 122]], "long-forms": [[65, 84], [91, 99], [106, 118], [125, 138]], "ID": "2864"}, {"text": "Normalization (WCCN) (Dehak et al., 2011) and Eigen Factor Radial (EFR) (Bousquet et al., 2011).", "acronyms": [[67, 70], [15, 19]], "long-forms": [[46, 65]], "ID": "2865"}, {"text": "corpora for our experiments. The first is a new corpus of 70 articles from New York Times (NYT) LDC corpus, each describing one or more terrorist events", "acronyms": [[91, 94], [96, 99]], "long-forms": [[75, 89]], "ID": "2866"}, {"text": "case where estimated user?s knowledge and preference are represented as discrete binary parameters instead of probability distributions (PDs). That is, the estimated", "acronyms": [[137, 140]], "long-forms": [[110, 135]], "ID": "2867"}, {"text": "full PTB, using 1st sense information. All results  are shown as labelled attachment score (LAS). ", "acronyms": [[92, 95], [5, 8]], "long-forms": [[65, 90]], "ID": "2868"}, {"text": "a. the 1000-headlines text (target domain) 1,181 40.2 32.1 35.7 b. the TEC (source domain) 32,954 29.9 26.1 27.9 c. the 1000-headlines text and the TEC (target and source) c.1.", "acronyms": [[148, 151], [71, 74]], "long-forms": [[153, 170]], "ID": "2869"}, {"text": " 1 In t roduct ion   For most natural language processing (NLP) systems,  thesauri comprise indispensable linguistic knowledge.", "acronyms": [[59, 62]], "long-forms": [[30, 57]], "ID": "2870"}, {"text": "words, from this the subscript b (bag-of-words).  Subtree Kernel (SbtK) is one of the simplest tree kernels as it only generates complete subtrees, i.e.,", "acronyms": [[66, 70]], "long-forms": [[50, 64]], "ID": "2871"}, {"text": " The structure of a Concept is completed by its set of  Structural Descriptions (SD's). These express how the ", "acronyms": [[81, 85]], "long-forms": [[56, 79]], "ID": "2872"}, {"text": "We then review some standard online learners (e.g. perceptron) before presenting the Bayes Point Machine (BPM) (Herbrich et al, 2001; Harrington et al, 2003).", "acronyms": [[106, 109]], "long-forms": [[85, 104]], "ID": "2873"}, {"text": "non-interactions   Y They are widely distributed and mediate all of the known biologic effects of  angiotensin II (AngII) through a variety of signal transduction systems, including activation of phospholipases C and A2, inhibition of adenylate cyc-", "acronyms": [[115, 120]], "long-forms": [[99, 113]], "ID": "2874"}, {"text": "3.1 Vector SpaceModei for Text Catego-  r izat ion  The bulk of the VSM for Information Retrieval (IR) is  representing naturallanguage xpressions as term ", "acronyms": [[99, 101], [68, 71]], "long-forms": [[76, 97]], "ID": "2875"}, {"text": "oracle? which determines the predominant sense, or most frequent sense (MFS), of each noun in our WSJ test data perfectly, and", "acronyms": [[72, 75], [98, 101]], "long-forms": [[51, 70]], "ID": "2876"}, {"text": "To further investigate the effectiveness of our  method, the third set of experiments evaluate the  negative transfer detection (NTD) compared to  co-training (CO) without negative transfer ", "acronyms": [[129, 132], [160, 162]], "long-forms": [[100, 127], [147, 158]], "ID": "2877"}, {"text": "Overall recall and precision were 0.80 and 0.87 for drugs, and 0.56 and 0.85 for adverse events. 1 Introduction  It is well-known that adverse drug reactions (ADRs) are an important health problem. Indeed, ADRs are the 4th cause of death in hospitalized patients (Wester et al.,", "acronyms": [[159, 163], [206, 210]], "long-forms": [[135, 157]], "ID": "2878"}, {"text": "More recently, (Areces et al, 2008) analysed GRE as a problem in Description Logic (DL), a formalism which, like Conceptual Graphs, is specifically designed for", "acronyms": [[84, 86], [45, 48]], "long-forms": [[65, 82]], "ID": "2879"}, {"text": "Features. In Proceedings of the 21st Conference on Computational Linguistics (COLING). ", "acronyms": [[78, 84]], "long-forms": [[51, 76]], "ID": "2880"}, {"text": " ? Template Element (TE) -- Extract  basic information related to organization and ", "acronyms": [[21, 23]], "long-forms": [[3, 19]], "ID": "2881"}, {"text": "Table 6: GAP scores for compared methods.  UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston", "acronyms": [[43, 45], [55, 57], [70, 72], [9, 12], [86, 88], [100, 102]], "long-forms": [[48, 53], [60, 68], [75, 84], [91, 98], [105, 121]], "ID": "2882"}, {"text": "He con~iciers them foolish.  2) ASTG (adicctive string), including  adjectival Vens and Vings (see VENDADJ I found it well-dcsigned.", "acronyms": [[32, 36], [100, 106]], "long-forms": [[38, 54]], "ID": "2883"}, {"text": "acoustic ambiguity. We measure performance in  terms of character error rate (CER), which is the  number of characters wrongly converted from the ", "acronyms": [[78, 81]], "long-forms": [[56, 76]], "ID": "2884"}, {"text": "Computational Linguistics Volume 40, Number 1 of the 22nd International Conference on Computational Linguistics (COLING?08), pages 71?84, Manchester.", "acronyms": [[113, 122]], "long-forms": [[86, 111]], "ID": "2885"}, {"text": "Given an occurrence of a word \u0002 in a natural language text, the task of word sense disambiguation (WSD) is to determine the correct sense of \u0002 in that context.", "acronyms": [[99, 102]], "long-forms": [[72, 97]], "ID": "2886"}, {"text": "Their study with three different learners ? na??ve Bayes, maximum entropy (MaxEnt) and the support vector machine (SVM) ?", "acronyms": [[75, 81], [115, 118]], "long-forms": [[58, 73], [91, 113]], "ID": "2887"}, {"text": "on the base. ( Code-a-phone, 1989)  (2d) In the STBY (standby) position, the phone  will ring whether the handset .is on the base or ", "acronyms": [[48, 52]], "long-forms": [[54, 61]], "ID": "2888"}, {"text": "Manning (2014) baseline. These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (?", "acronyms": [[71, 77], [90, 94], [117, 120]], "long-forms": [[45, 69]], "ID": "2889"}, {"text": "(CPs) increases the number of Complex Predicates (CPs) entries along with compound verbs  (CompVs) and conjunct verbs (ConjVs). The ", "acronyms": [[119, 125]], "long-forms": [[103, 117]], "ID": "2890"}, {"text": "actions and boolean b (> or ?) are used to ensure that unary reductions (RU) can only take place once after a SHIFT action.", "acronyms": [[73, 75], [110, 115]], "long-forms": [[61, 71]], "ID": "2891"}, {"text": "?  Figure 2: Hierarchical Dirichlet Process (HDP) for WSI. ", "acronyms": [[45, 48], [54, 57]], "long-forms": [[13, 43]], "ID": "2892"}, {"text": " Preclinical data have supported the use of  fludarabine and cyclophosphamide (FC) in  combination for the treatment of indolent ", "acronyms": [[79, 81]], "long-forms": [[45, 77]], "ID": "2893"}, {"text": "as feature vectors. The model of our choice is the maximum entropy model (MaxEnt), also known as logistic regression (?).", "acronyms": [[74, 80]], "long-forms": [[51, 66]], "ID": "2894"}, {"text": "and stochastic optimization. In Proceedings of the Conference on Learning Theory (COLT), pages 257?269.", "acronyms": [[82, 86]], "long-forms": [[51, 80]], "ID": "2895"}, {"text": "For example, NNS  (noun ? plural) became NN (noun). ", "acronyms": [[41, 43], [13, 16]], "long-forms": [[45, 49], [19, 25]], "ID": "2896"}, {"text": "s), correlation with error counts (re and ? e), average precision (AP) and pairwise accuracy.", "acronyms": [[67, 69]], "long-forms": [[48, 65]], "ID": "2897"}, {"text": "Dialogues were recorded and system and user behavior were logged automatically. The concept accuracy (CA) of each turn was manually labeled. If the", "acronyms": [[102, 104]], "long-forms": [[84, 100]], "ID": "2898"}, {"text": "We have proposed two independent evaluation measures: statistical analysis (SA) and classification accuracy (AC). ", "acronyms": [[109, 111], [76, 78]], "long-forms": [[99, 107], [54, 74]], "ID": "2899"}, {"text": " 1 Introduction Information Extraction (IE) refers to the problem of extracting structured information from unstructured", "acronyms": [[40, 42]], "long-forms": [[16, 38]], "ID": "2900"}, {"text": "In Proceedings of the 2012 ACM Interntional Conference on Intelligent User Interfaces (IUI), pages 189?198. ", "acronyms": [[87, 90], [27, 30]], "long-forms": [[58, 85]], "ID": "2901"}, {"text": " 3.2 Matching a review to an object Given the above review language model (RLM), we now state how to match a given review to an", "acronyms": [[75, 78]], "long-forms": [[52, 73]], "ID": "2902"}, {"text": "computation of distributional thesauri (Lin, 1998) has been around for decades, its full potential has yet to be utilized in Natural Language Processing (NLP) tasks and applications.", "acronyms": [[154, 157]], "long-forms": [[125, 152]], "ID": "2903"}, {"text": "expert users in spoken dialogue systems. The key component of a spoken language understanding (SLU) system is the semantic parser, which translates the users?", "acronyms": [[95, 98]], "long-forms": [[64, 93]], "ID": "2904"}, {"text": "interface.  The PRIDES User Interface Layer (PUI) is  responsible for creating and managing the screen ", "acronyms": [[45, 48]], "long-forms": [[16, 37]], "ID": "2905"}, {"text": "data. In International Conference on Machine Learning (ICML). ", "acronyms": [[55, 59]], "long-forms": [[9, 53]], "ID": "2906"}, {"text": "tions of words. In Proceedings of the International Conference on Computational Linguistics (COLING), Bombay, India, December.", "acronyms": [[93, 99]], "long-forms": [[66, 91]], "ID": "2907"}, {"text": "Maximum Entropy Markov Model (MEMM)-based word segmenter with Conditional Random Fields (CRF)based chunking; 3.", "acronyms": [[89, 92]], "long-forms": [[62, 87]], "ID": "2908"}, {"text": "z and a segmental grammar g, compute the sur-  face form y : g(z) of z.  The phonological recognition problem (PRP) is:  Given a (partially specified) surface form y, a dic- ", "acronyms": [[111, 114]], "long-forms": [[77, 109]], "ID": "2909"}, {"text": "proposed two novel features, Intra-sentence positional term weighting (IPTW) and the Patched language model (PLM), and showed their effectiveness by conducting automatic", "acronyms": [[109, 112], [71, 75]], "long-forms": [[85, 107], [29, 69]], "ID": "2910"}, {"text": "ically measuring the semantic similarity between two texts, which was the aim of the 2013 Semantic Textual Similarity (STS) task (Agirre et al 2013).", "acronyms": [[119, 122]], "long-forms": [[90, 117]], "ID": "2911"}, {"text": "  *COMPLEXITY: avoid semantic complexity  BC (BE CONCRETE): have a concrete meaning   ", "acronyms": [[42, 44]], "long-forms": [[46, 57]], "ID": "2912"}, {"text": "ported by Shared Annotated Resources (ShARe) project NIH 5R01GM090187 and Temporal Histories of Your Medical Events (THYME) project (NIH R01LM010090 and U54LM008748).", "acronyms": [[117, 122], [38, 43], [53, 56], [133, 136]], "long-forms": [[74, 115], [10, 36]], "ID": "2913"}, {"text": " In order to rate models M1, M2, M3 in comparison to the vector space model (VS) using MSTs, STs and CTs as alternative hierarchi-", "acronyms": [[77, 79], [87, 91], [93, 96], [101, 104]], "long-forms": [[57, 69]], "ID": "2914"}, {"text": "Spanish data set as Trec4S.  We used a Chinese-English lexicon from the  Linguistic Data Consortium (LDC). We pre- ", "acronyms": [[101, 104]], "long-forms": [[73, 99]], "ID": "2915"}, {"text": "opinions are about the 20 most popular Chicago hotels; deceptive opinions were generated using the Amazon Mechanical Turk (AMT)3, whereas ?", "acronyms": [[123, 126]], "long-forms": [[99, 121]], "ID": "2916"}, {"text": "We further validate our approach on a large publicly available manipulation action dataset (MANIAC) from (Aksoy et al, 2014), achieving promising ex-", "acronyms": [[92, 98]], "long-forms": [[63, 82]], "ID": "2917"}, {"text": "and Rozovskaya and Roth (2011). The article system is trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), imple-", "acronyms": [[93, 95]], "long-forms": [[72, 91]], "ID": "2918"}, {"text": "3.4 Innovative Features of SPTK The most similar kernel to SPTK is the Syntactic Semantic Tree Kernel (SSTK) proposed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Mos-", "acronyms": [[103, 107], [27, 31], [59, 63]], "long-forms": [[81, 101]], "ID": "2919"}, {"text": "ments. We experimented with several classifiers including: SVM, Logistic Regression (LR), and Naive Bayes.", "acronyms": [[85, 87], [59, 62]], "long-forms": [[64, 83]], "ID": "2920"}, {"text": "3.3 Evaluation Metric We use both Root Mean Square (RMS) error and Correlation Coefficient (CRCoef) to evaluate our model, since the two metrics evaluate different as-", "acronyms": [[92, 98]], "long-forms": [[67, 90]], "ID": "2921"}, {"text": "Centro de Sondi E Imagen S.L. (Spain)  - Lead Industrial User  University of Sunderland (UK)  - Academic Research ", "acronyms": [[89, 91], [25, 28]], "long-forms": [[63, 87], [31, 36]], "ID": "2922"}, {"text": "To create Bloom filter LMs we gathered n-gram counts from both the Europarl (EP) and the whole of the Gigaword Corpus (GW). Table 2 shows the", "acronyms": [[119, 121]], "long-forms": [[102, 110]], "ID": "2923"}, {"text": "edge mining.  Biomedical NER (Bio-NER) tasks are, in general, more difficult than ones in the news domain.", "acronyms": [[30, 37]], "long-forms": [[14, 28]], "ID": "2924"}, {"text": "Entity linking (EL) recognizes mentions in a text and associates them to their corresponding entries in a knowledge base (KB), for example, Wikipedia", "acronyms": [[122, 124], [16, 18]], "long-forms": [[106, 120], [0, 14]], "ID": "2925"}, {"text": "in the original English WordNet (Fellbaum, 1998) into lexicographer files. Arabic WordNet (AWN) (Elkateb et al 2006) allows us to recover super-", "acronyms": [[91, 94]], "long-forms": [[75, 89]], "ID": "2926"}, {"text": "RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. ", "acronyms": [[78, 81], [0, 2], [30, 33], [57, 59]], "long-forms": [[84, 95], [5, 17], [36, 55], [62, 76]], "ID": "2927"}, {"text": " Another important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors.", "acronyms": [[85, 88]], "long-forms": [[62, 83]], "ID": "2928"}, {"text": "The modifier*\"its\" of TOK188 was converted to a modifier of the form  (POSSBY SCAFFOLD184), which was semantically processed to make TOK188 a  LOCPART (LOCationlPAIiT) SFRAME whwe SEMOBJ (SEMantic ODJect) is  SCAFFOLD1 84; idelltif'ication of the location referents of TOK 188 yieldad the two ", "acronyms": [[143, 150], [22, 28], [71, 77], [78, 89], [133, 139], [168, 174], [180, 186], [188, 196], [197, 203], [209, 218], [269, 272]], "long-forms": [[152, 166]], "ID": "2929"}, {"text": " ? Research Question 1 (RQ1): How do we define suggestions in suggestion mining?", "acronyms": [[24, 27]], "long-forms": [[3, 22]], "ID": "2930"}, {"text": "precision, recall and f-measure. Precision measures the number of correct Named Entities(NEs) in the 107", "acronyms": [[89, 92]], "long-forms": [[74, 87]], "ID": "2931"}, {"text": "our method in this domain. The analysis of variance (ANOVA) and Tukey?s honestly significant differences (HSD) tests on the classification accuracies", "acronyms": [[53, 58], [106, 109]], "long-forms": [[31, 51], [72, 104]], "ID": "2932"}, {"text": "  The similarity of two words is the least common  ancestor information content(IC), and hence, the  higher the information content is, the more similar ", "acronyms": [[80, 82]], "long-forms": [[60, 78]], "ID": "2933"}, {"text": "Research in molecular-biology field is discovering enormous amount of new facts, and thus there is an increasing need for information extraction (IE) technology to support database building and to find", "acronyms": [[146, 148]], "long-forms": [[122, 144]], "ID": "2934"}, {"text": " i=1:n P (GR = gri|SCF = s) The three terms, given the hyper-parameters and", "acronyms": [[10, 12]], "long-forms": [[15, 22]], "ID": "2935"}, {"text": "ing to their different degree of specification. In the hierarchy of relations, Arguments (ARG) include Subject (SUBJ), Object (OBJ), Indirect", "acronyms": [[90, 93], [112, 116], [127, 130]], "long-forms": [[79, 88], [103, 110], [119, 125]], "ID": "2936"}, {"text": "PCEDT 0.7681 0.7072 0.7364 0.0712 Average 0.8402 0.8090 0.8241 0.1397 Table 3: Labeled precision (LP), recall (LR), F 1", "acronyms": [[98, 100], [0, 5], [111, 113]], "long-forms": [[79, 96]], "ID": "2937"}, {"text": "Three transliteration models have been used that  can generate the Hindi transliteration from an  English named entity (NE). An English NE is ", "acronyms": [[120, 122], [136, 138]], "long-forms": [[106, 118]], "ID": "2938"}, {"text": "the previous section. We contrast this metric with Normalized Pointwise Mutual Information (NPMI) which uses only the events A = X+a and B = X", "acronyms": [[92, 96]], "long-forms": [[51, 90]], "ID": "2939"}, {"text": "Figure 1 shows the example of the input format of ACABIT in XML makes use of which conforms to Document Type Definition (DTD) in Figure 2.", "acronyms": [[121, 124], [50, 56], [60, 63]], "long-forms": [[95, 119]], "ID": "2940"}, {"text": "Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pp.", "acronyms": [[101, 104]], "long-forms": [[58, 99]], "ID": "2941"}, {"text": "clude examples such as Facebook AI Research?s challenge problems for AI-complete QA (Weston et al, 2015) and the Allen Institute for AI?s (AI2) Aristo project (Clark, 2015) along with its recently", "acronyms": [[139, 142], [81, 83]], "long-forms": [[113, 137]], "ID": "2942"}, {"text": "analysis. To include more of the corpus, parameters are relaxed: the high group (HH) includes anyone whose score is above .5 SD", "acronyms": [[81, 83], [125, 127]], "long-forms": [[69, 73]], "ID": "2943"}, {"text": "linear chain, CRFs make a first-order Markov independence assumption, and thus can be understood as conditionally-trained finite state machines(FSMs). ", "acronyms": [[144, 148], [14, 18]], "long-forms": [[122, 142]], "ID": "2944"}, {"text": "In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI?09), pages 1,058?1,064, Pasadena, CA.", "acronyms": [[81, 89], [121, 123]], "long-forms": [[22, 79]], "ID": "2945"}, {"text": "NST = Noun Stem V-FLEX = Verbal Inflsxion  PRN = Pronoun A-FLEX = Adjectival Inflexion  CA = Case CL = Inflsxion Class GD = Gender MD = Mode  PF ~ Predicate Form PS = Person TN = Tense ", "acronyms": [[98, 100], [131, 133], [0, 3], [16, 22], [43, 46], [57, 63], [88, 90], [119, 121], [142, 144], [162, 164], [174, 176]], "long-forms": [[93, 97], [136, 140], [6, 15], [25, 41], [49, 56], [66, 86], [113, 118], [124, 130], [147, 161], [167, 173], [179, 184]], "ID": "2946"}, {"text": "transcripts of user utterances, and included lexical, syntactic, numeric, and features from the output of Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al.,", "acronyms": [[141, 145]], "long-forms": [[106, 139]], "ID": "2947"}, {"text": "Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P) pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score; NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.", "acronyms": [[167, 170], [80, 82], [241, 244], [201, 204], [135, 138]], "long-forms": [[173, 199], [141, 165], [257, 271]], "ID": "2948"}, {"text": "3 Model We introduce a topic-model based approach to declarative knowledge (DK) acquisition and describe how this knowledge can be applied to two unsuper-", "acronyms": [[76, 78]], "long-forms": [[53, 74]], "ID": "2949"}, {"text": "For ex-  ample, temporal PPs, such as \"in 1959\", where  the prepositional object is tagged CD (cardi-  nal), favor attachment to the VP, because tile ", "acronyms": [[91, 93], [133, 135], [25, 28]], "long-forms": [[95, 106]], "ID": "2950"}, {"text": "The following are  typlcal relation names: NT (narrower term); PT (part); FUN (function);  SYN (syntax); EG (example). ", "acronyms": [[91, 94], [43, 45], [47, 60], [63, 65], [74, 77], [105, 107]], "long-forms": [[96, 102], [67, 71], [79, 87], [109, 116]], "ID": "2951"}, {"text": "we combine different perspectives, the performance improves and we use the L+S with SVR for run 1 (LSSVR), L+P+S with SVR for run 2 (LPSSVR), and L+P+S with SVR using transductive learning", "acronyms": [[99, 104], [84, 87], [133, 139], [157, 160]], "long-forms": [[107, 129]], "ID": "2952"}, {"text": " A natural solution would be to take advantage  of machine readable dictionaries (MILD's), such  as Longman's Dictionary of Contemporary En- ", "acronyms": [[82, 88]], "long-forms": [[51, 80]], "ID": "2953"}, {"text": "3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al.,", "acronyms": [[99, 102], [46, 50]], "long-forms": [[67, 97]], "ID": "2954"}, {"text": "common view of the semantics of time. Since the target application domain is an historical database, we  present the essential features of the Historical Relational Database Model (HRDM), an extension to the  relational model motivated by the desire to incorporate more \"real world\" semantics into a database at ", "acronyms": [[181, 185]], "long-forms": [[143, 179]], "ID": "2955"}, {"text": ".  3.4 Stochastic Gradient Descent (SGD) Training With the likelihood gradients, we apply L2-norm regularized SGD training to iteratively learn the feature", "acronyms": [[36, 39], [110, 113]], "long-forms": [[7, 34]], "ID": "2956"}, {"text": "gramming. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050?1055. ", "acronyms": [[73, 77]], "long-forms": [[13, 71]], "ID": "2957"}, {"text": "Hyungjong Noh* Jeong-Won Cha** Gary Geunbae Lee*  *Department of Computer Science and Engineering  Pohang University of Science & Technology (POSTECH)  San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea ", "acronyms": [[142, 149]], "long-forms": [[99, 140]], "ID": "2958"}, {"text": "Second, we demonstrate correlation to a database of real-world international conflict events, the Militarized Interstate Dispute (MID) dataset (Jones et al, 1996).", "acronyms": [[130, 133]], "long-forms": [[98, 128]], "ID": "2959"}, {"text": "surveys of QA and DP data. The surveys are evaluated using nuggets drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).", "acronyms": [[97, 102], [11, 13], [18, 20], [119, 124], [150, 155]], "long-forms": [[78, 95], [105, 117], [131, 148]], "ID": "2960"}, {"text": "category. The following transfer knowledge involves  sets of three common ouns (CNs):  3A' is the transferred expression of A ", "acronyms": [[80, 83]], "long-forms": [[67, 78]], "ID": "2961"}, {"text": "In a spoken dialog system that can handle natural conversation between a human and a machine, spoken language understanding (SLU) is a crucial component aiming at capturing", "acronyms": [[125, 128]], "long-forms": [[94, 123]], "ID": "2962"}, {"text": "In addition, results from the machine learning based model are refined by a rule-based postprocessing, which is implemented using a finite state transducer (FST). The", "acronyms": [[157, 160]], "long-forms": [[132, 155]], "ID": "2963"}, {"text": "mark of language attainment at different stages of learning. The English Profile (EP)2 research programme aims to enhance the learning, teaching", "acronyms": [[82, 84]], "long-forms": [[65, 80]], "ID": "2964"}, {"text": " 1 Introduction Approaches to Machine Translation (MT) using Data-Oriented Parsing (DOP: (Bod, 1998; Bod et", "acronyms": [[51, 53], [84, 87]], "long-forms": [[30, 49], [61, 82]], "ID": "2965"}, {"text": "ALL X X X 0.614 0.186 0.706 0314 0.509 Table 2: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adap-", "acronyms": [[82, 86]], "long-forms": [[69, 80]], "ID": "2966"}, {"text": "The computation of associative responses to multiword stimuli. In  Proceedings of the  Workshop on Cognitive Aspects of the Lexicon (COGALEX) at COLING-2008, p.102?109. Manchester, UK", "acronyms": [[133, 140], [181, 183]], "long-forms": [[99, 131]], "ID": "2967"}, {"text": "smoothness. Before creating a POMDP structure, we used the dynamic Bayesian network (DBN) structure (Fig.", "acronyms": [[85, 88], [30, 35]], "long-forms": [[59, 83]], "ID": "2968"}, {"text": " minimizes the multiway normalized cut(MNCut): MNCut(I) = K ?", "acronyms": [[39, 44], [47, 52]], "long-forms": [[15, 37]], "ID": "2969"}, {"text": " In Proceedings of the Conference on Web Search and Web Data Mining (WSDM). ", "acronyms": [[69, 73]], "long-forms": [[37, 67]], "ID": "2970"}, {"text": "Abstract  We present a novel method for evaluating  the output of Machine Translation (MT),  based on comparing the dependency ", "acronyms": [[87, 89]], "long-forms": [[66, 85]], "ID": "2971"}, {"text": "Adding valency filtering to the setting in the preceding row. SUC = Subset of Stockholm-Umea? corpus of", "acronyms": [[62, 65]], "long-forms": [[68, 87]], "ID": "2972"}, {"text": "2008) and hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007) using the implementation in (Turian et al, 2010), Hellinger PCA (H-PCA) (Lebret and Collobert, 2014) and our connective-based representation (Bllip).", "acronyms": [[143, 148]], "long-forms": [[128, 141]], "ID": "2973"}, {"text": " 1 Introduction Word Sense Disambiguation (WSD) is a difficult Natural Language Processing task which requires", "acronyms": [[43, 46]], "long-forms": [[16, 41]], "ID": "2974"}, {"text": " 1 Introduction Word sense disambiguation (WSD) is the task of assigning sense tags to ambiguous lexical items", "acronyms": [[43, 46]], "long-forms": [[16, 41]], "ID": "2975"}, {"text": "These features capture the context of the adverb and help in deciding the presence of the manner (MNR) component. ", "acronyms": [[98, 101]], "long-forms": [[90, 96]], "ID": "2976"}, {"text": "end 1 <= V I ;   SPAN (SPANS, 'CONSTITUENTS ' ) = <TUP , CONSTITUENTS>;  TODO = TUP -t TODO;  r e t u r n ;  ", "acronyms": [[80, 83], [51, 54]], "long-forms": [[84, 86]], "ID": "2977"}, {"text": "make assertions that personal pronouns like \\she\" cannot co-refer with \\company\".  In MUC-7, we developed a word sense disambiguation (WSD) module, which removes some of the implausible senses from the list of potential senses.", "acronyms": [[135, 138], [86, 91]], "long-forms": [[108, 133]], "ID": "2978"}, {"text": "edges the support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract No.", "acronyms": [[132, 136], [64, 69]], "long-forms": [[101, 130], [21, 62]], "ID": "2979"}, {"text": "(Eisner, 1996). We define role value labeled precision (RLP) and role value labeled recall (RLR) on dependency links as follows:", "acronyms": [[92, 95], [56, 59]], "long-forms": [[65, 90], [26, 54]], "ID": "2980"}, {"text": "ratings of IP strategies. In the following we use a Reinforcement Learning (RL) as a statistical planning framework (Sutton and Barto,", "acronyms": [[76, 78], [11, 13]], "long-forms": [[52, 74]], "ID": "2981"}, {"text": "are defined:    (1)  VSM-based (Vector Space Model based)  trigger word similarity: the trigger words ", "acronyms": [[21, 30]], "long-forms": [[32, 50]], "ID": "2982"}, {"text": " 1 Introduction Statistical machine translation (SMT) relies on tokenization to split sentences into meaningful units", "acronyms": [[49, 52]], "long-forms": [[16, 47]], "ID": "2983"}, {"text": "insertion operator for combining subtrees as in tree adjoining grammars (TAG) (Joshi, 1985) or tree insertion grammars (TIG) (Schabes and Waters, 1995).", "acronyms": [[120, 123], [73, 76]], "long-forms": [[95, 118], [48, 70]], "ID": "2984"}, {"text": "predicate-only f-score, M = METEOR, WN = WordNet,  H_FL = human fluency score, H_AC = human accuracy  score, H_AVE = human average score.9   ", "acronyms": [[109, 114], [36, 38], [51, 55], [79, 83]], "long-forms": [[117, 130], [28, 34], [41, 48], [58, 71], [86, 100]], "ID": "2985"}, {"text": "2.2 Singular Value Decomposition Given any matrix S, its singular value decomposition (SVD) is S = U?V T . The matrix Sk =", "acronyms": [[87, 90], [99, 102]], "long-forms": [[57, 85]], "ID": "2986"}, {"text": "features: O-SEM ('ordinary semantics') and  I,'-SKEL (F-skeleton) of the type of a semantic ob-  ject, tile set-valued IS-CSTR (IS constraints) and  the binary MAX-F (for potential maximal focus).", "acronyms": [[119, 126], [10, 15], [44, 52], [160, 165]], "long-forms": [[128, 142], [18, 36], [54, 64]], "ID": "2987"}, {"text": " ? LEAD (lead-based): n% sentences are chosen from the beginning of the text.", "acronyms": [[3, 7]], "long-forms": [[9, 13]], "ID": "2988"}, {"text": "eigenvalues are not zero, then I? minimizes the multiway normalized cut(MNCut): MNCut(I) = K ??", "acronyms": [[72, 77], [80, 85]], "long-forms": [[48, 71]], "ID": "2989"}, {"text": " From the set of erroneous instances: True Positive (TP) ML class 6= student class False Negative (FN) ML class = student class", "acronyms": [[53, 55], [99, 101], [57, 59], [103, 105]], "long-forms": [[38, 51], [83, 97]], "ID": "2990"}, {"text": "Relative standard deviation of three intervals, left edge to anchor (LE-A), center to  anchor (CC-A), right edge to anchor (RE-A) calculated across productions of word sets by one ", "acronyms": [[124, 128], [69, 73], [95, 99]], "long-forms": [[102, 122], [48, 67], [76, 93]], "ID": "2991"}, {"text": "larger segments.  Speech Systems Incorporated (SSI) has been using a  version of this two-stage cascade of the MMI encoders in the ", "acronyms": [[47, 50], [111, 114]], "long-forms": [[18, 45]], "ID": "2992"}, {"text": "to cover terms frequently used by students, such as acronyms: E.L.C. (the English Language Center), R.O.C. (Republic of China), and so on. Other", "acronyms": [[100, 106], [62, 67]], "long-forms": [[108, 125], [74, 97]], "ID": "2993"}, {"text": "In order to  make the notion of focusing more precise I will use the  notion of Reference Time (RT), adopted from Reichen-  bach 1947 but reinterpreted pragmatically: RT is to be ", "acronyms": [[96, 98], [167, 169]], "long-forms": [[80, 94]], "ID": "2994"}, {"text": ". However, a method based on singular value decomposition (SVD) provides an efficient and exact solution to this prob-", "acronyms": [[59, 62]], "long-forms": [[29, 57]], "ID": "2995"}, {"text": "source yes better Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Transducer.", "acronyms": [[75, 78], [53, 56], [104, 108], [143, 145]], "long-forms": [[79, 102], [109, 141], [146, 161]], "ID": "2996"}, {"text": "based method is slightly better. The two systems share the same topic relevance score (REL) and sentiment score, but the sentence-ranking method", "acronyms": [[87, 90]], "long-forms": [[70, 79]], "ID": "2997"}, {"text": "Minimum Sub-Structure (MSS) 87.95 87.88 Context-Sensitive MSS (CMSS) 89.11 89.01 Chunking Tree (CT) 86.17 86.21 Linear Features (Kl) 90.79 90.46", "acronyms": [[96, 98], [23, 26], [58, 61], [63, 67], [129, 131]], "long-forms": [[81, 94], [0, 21]], "ID": "2998"}, {"text": "3.2 Feature Space An essential aspect of our approach below is the word sense disambiguation (WSD) of the noun. Us-", "acronyms": [[94, 97]], "long-forms": [[67, 92]], "ID": "2999"}, {"text": "1 will refer to pa~rs   primarily oriented towards the former goal as Practical Parsers (PP) and refer  to the others as Performance Model Parsers (PMP). With these distinctions ", "acronyms": [[148, 151], [89, 91]], "long-forms": [[121, 146], [70, 86]], "ID": "3000"}, {"text": "Head+Path 80.0 72.8 31.8 22.4 Path 80.0 72.7 31.6 22.0 Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled) Unlabeled Labeled", "acronyms": [[82, 84], [105, 107]], "long-forms": [[87, 103], [110, 121], [127, 136], [142, 149]], "ID": "3001"}, {"text": "2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR),41(2):10. ", "acronyms": [[66, 70], [43, 46]], "long-forms": [[47, 64]], "ID": "3002"}, {"text": "guages: German, French and Italian, with German  usually serving as the source language (SL),  French and Italian as the target language (TL). ", "acronyms": [[138, 140], [89, 91]], "long-forms": [[121, 136], [72, 87]], "ID": "3003"}, {"text": "1. Grider, T., Mosley, H., Snow, L, and Wilson, W., \"Users Manual for the  Dynamic Analytical Replanning Tool (DRAFT)\", prepared for BBN by  Systems Research and Applications Corporation, 9 November 1990.", "acronyms": [[111, 116]], "long-forms": [[75, 109]], "ID": "3004"}, {"text": "tion system; then the output is classified into a  small number of domain-specific classes called  Domain Acts (DAs) that can indicate directly to  the dispatcher the general intended meaning of ", "acronyms": [[112, 115]], "long-forms": [[99, 110]], "ID": "3005"}, {"text": "mentation in (Zhang et al, 2006) 2.2 OOV Recognition with Accessor Variety Accessor variety (AV) (Feng et al, 2004) is a simple and effective unsupervised method for extrac-", "acronyms": [[93, 95], [37, 40]], "long-forms": [[75, 91]], "ID": "3006"}, {"text": "word and sentence level QE. In this work we describe the Fondazione Bruno Kessler (FBK), Universitat Polit`ecnica de Val`encia (UPV) and Uni-", "acronyms": [[83, 86], [24, 26], [128, 131]], "long-forms": [[57, 81], [89, 126]], "ID": "3007"}, {"text": "ence.  Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale", "acronyms": [[36, 39]], "long-forms": [[7, 34]], "ID": "3008"}, {"text": "Specifically, we achieve a roughly 10% improvement in precision on text from the information technology (IT) business press via post hoc rule-based error reduction.", "acronyms": [[105, 107]], "long-forms": [[81, 103]], "ID": "3009"}, {"text": " 1 Introduction Referring expressions (REs) are expressions intended by speakers to identify entities to hearers.", "acronyms": [[39, 42]], "long-forms": [[16, 37]], "ID": "3010"}, {"text": "cause de la limite des outils informatiques li?s ? son traitement  automatique,  ce  qui  rend  difcile  son  adh?sion  ?  ses  cons?urs  dans  le domaine des nouvelles technologies de l'information et de la communication (NTIC). Par cons?quent, un ensemble de recherches scientifques et linguistiques sont lanc?es pour rem?dier ?", "acronyms": [[223, 227]], "long-forms": [[159, 221]], "ID": "3011"}, {"text": "Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al, 2005), and perform quite well on corresponding test sets.", "acronyms": [[117, 120]], "long-forms": [[99, 115]], "ID": "3012"}, {"text": "Advanced Research and Development Activity (ARDA)?s Advanced Question Answering for Intelligence (AQUAINT) Program, a DOI grant under the Reflex", "acronyms": [[98, 105], [44, 48], [118, 121]], "long-forms": [[0, 42], [52, 96]], "ID": "3013"}, {"text": "In Proc. 6th Canadian Conf on AI (CSCSI-86), pp. 78?83.", "acronyms": [[34, 42], [45, 47]], "long-forms": [[13, 32]], "ID": "3014"}, {"text": "Four training and testing corpora were used in the first bakeoff (Sproat and Emerson, 2003), including the Academia Sinica Corpus (AS), the Penn Chinese Treebank Corpus (CTB), the Hong Kong City Uni-", "acronyms": [[131, 133], [170, 173]], "long-forms": [[107, 129], [145, 161]], "ID": "3015"}, {"text": "context    Chinese Context(CC): ???????? ", "acronyms": [[27, 29]], "long-forms": [[11, 25]], "ID": "3016"}, {"text": "1 Introduction Creating the annotated corpus needed for training a NER (named entity recognition) model is costly. ", "acronyms": [[67, 70]], "long-forms": [[72, 96]], "ID": "3017"}, {"text": " We propose a new algorithm: collective iterative classification (CIC) to perform approximate inference to find the maximum a posteriori", "acronyms": [[66, 69]], "long-forms": [[29, 64]], "ID": "3018"}, {"text": "We suppose that the possessor of a noun phrase is  the subject or the noun phrase's nearest opic that  has a semantic mark,er HUM (human) or a seman-  tic marker AN I (animal).", "acronyms": [[126, 129], [162, 164]], "long-forms": [[131, 136]], "ID": "3019"}, {"text": "As a starting point, the classes for complements  and features developed by the New York Univer-  sity Linguistic String Project (LSP) (Fitzpatrick,  1981), were selected sin(x, the coverage is very ", "acronyms": [[130, 133]], "long-forms": [[103, 128]], "ID": "3020"}, {"text": "idea was an early version of branching entropy, one of the experts in VE, and they developed an algorithm called Phoneme to Morpheme (PtM) around it. ", "acronyms": [[134, 137], [70, 72]], "long-forms": [[113, 132]], "ID": "3021"}, {"text": "system components goes through GDM, thereby insu-  lating parts from each other and providing a uniform  API (applications programmer interface) for manip-  ulating the data produced by the system.", "acronyms": [[105, 108], [31, 34]], "long-forms": [[110, 143]], "ID": "3022"}, {"text": "SUBS = substantif  compl~ment  VT  = verbe conjugu6  PROC = pronom compl~ment  SUSU = substantif  sujet ", "acronyms": [[53, 57], [0, 4], [31, 33], [79, 83]], "long-forms": [[60, 72], [7, 17], [37, 42], [86, 103]], "ID": "3023"}, {"text": "This I will  claim to be in contrast with the ability of temporal  subordinate clauses and noun phrases (NPs) to direct the  listener to any position in the evolving structure.)", "acronyms": [[105, 108]], "long-forms": [[91, 103]], "ID": "3024"}, {"text": "formation of globally dispersed virtual communities, one of which is the very active and growing movement of Open Source Software (OSS) development.", "acronyms": [[131, 134]], "long-forms": [[109, 129]], "ID": "3025"}, {"text": "chose a method from the second group. We use the Hierarchical Agglomerative Clustering (HAC) algorithm (Jain et al, 1999) for all experiments reported", "acronyms": [[88, 91]], "long-forms": [[49, 86]], "ID": "3026"}, {"text": " We first examined three randomly selected portions of the listing  in the American Heritage Word Frequency Book (AHWFB).side by side  with the corresponding entry lists of the American Heritage School Dic- ", "acronyms": [[114, 119]], "long-forms": [[75, 112]], "ID": "3027"}, {"text": "framework. This algorithm uses grammars in the Chomsky Normal Form (CNF) so we employed the open source Natural Language Toolkit2", "acronyms": [[68, 71]], "long-forms": [[47, 66]], "ID": "3028"}, {"text": "5.1 Alternative Models To test LUX?s representations, we built a brute-force histogram model (HM) that discretizes HSV space and tracks frequency distributions of labels directly", "acronyms": [[94, 96], [31, 36], [115, 118]], "long-forms": [[77, 92]], "ID": "3029"}, {"text": "229  Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 19?26, Sofia, Bulgaria, August 9, 2013.", "acronyms": [[70, 77]], "long-forms": [[36, 68]], "ID": "3030"}, {"text": "This data may be presented in various forms, e.g. as  dictionaries, transition networks for lexical analysis,  augmented transition networks (ATN) for syntactic analysis,  semantic networks,  re la t ions ,  end so on.", "acronyms": [[142, 145]], "long-forms": [[111, 140]], "ID": "3031"}, {"text": "For each  model sentence MSij,, the model builder selects  the Required Lexicon (RLijk), a set of the most  essential lexical entities required to appear in a ", "acronyms": [[81, 86], [25, 29]], "long-forms": [[63, 79]], "ID": "3032"}, {"text": "ing to its current beliefs concerning the state of the dialogue. Reinforcement Learning (RL) has been more and more used for the optimisa-", "acronyms": [[89, 91]], "long-forms": [[65, 87]], "ID": "3033"}, {"text": "transitions, depending on whether the backward-looking center of Ui?1 is maintained or not in Ui and on whether CB(Ui) is also the most highly ranked entity (CP) of Ui: Center Continuation (CON): CB(Ui) = CB(Ui?1), and CB(Ui) is the most highly ranked CF (CP) of Ui (i.e., CP(Ui) = CB(Ui))", "acronyms": [[190, 193], [112, 114], [94, 96], [65, 67], [158, 160], [196, 198], [252, 254], [256, 258], [282, 284], [273, 275]], "long-forms": [[176, 188], [199, 201]], "ID": "3034"}, {"text": "After grammatical ambiguities are removed  by the stochastic parser, the phrase is divided  into noun phrases(NP) and verb phrases(VP),  giving, ", "acronyms": [[110, 112], [131, 133]], "long-forms": [[97, 108], [118, 129]], "ID": "3035"}, {"text": "that these heuristics have much effect not only in  the inductive inference (regular SVM) but also in  transductive inference (TSVM), especially when  the untagged data is large.", "acronyms": [[127, 131], [85, 88]], "long-forms": [[103, 125]], "ID": "3036"}, {"text": "\u0006? ( PM), tree matching without the auxiliary patterns (TM), and tree matching with the auxiliary patterns", "acronyms": [[56, 58]], "long-forms": [[49, 54]], "ID": "3037"}, {"text": " Figure 1. Reference answer representation revisions  Typical facets, as in (1a), are derived directly from a dependency parse, in this case retaining its dependency type label, NMod (noun modifier).  Other facets, such as (1b-e), are the result of com-bining multiple dependencies, VMod(stick, to) and PMod(to, nail) in the case of (1c).", "acronyms": [[178, 182], [283, 287], [303, 307]], "long-forms": [[184, 197]], "ID": "3038"}, {"text": "ellieioncy has been drastically improved for n)orpho-  logical analysis by representing large dictionaries with  Finite State Automata (FSA) and by representhig two-  level rnles and le?ical hlforination with finite-state ", "acronyms": [[136, 139]], "long-forms": [[113, 134]], "ID": "3039"}, {"text": "Kanayama et al99 Probabilistic model (ME + HPSG) EDR (192,778) 88.55 Haruno et al98 Probabilistic model (DT + Boosting) EDR (50,000) 85.03 Fujio et al98 Probabilistic model (ML) EDR (190,000) 86.67 Table 4: Comparison with the related work", "acronyms": [[174, 176], [38, 40], [43, 47], [49, 52], [105, 107], [120, 123], [178, 181]], "long-forms": [[167, 172]], "ID": "3040"}, {"text": "Table 3: Validation results for metaphor interpretation for English and Russian.  (ALL), or just two (TWO) validators. In most of", "acronyms": [[102, 105]], "long-forms": [[97, 100]], "ID": "3041"}, {"text": "The different resources used to build ArSenL.       The English WordNet (EWN) (Miller et al., ", "acronyms": [[73, 76]], "long-forms": [[56, 71]], "ID": "3042"}, {"text": "The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). ", "acronyms": [[100, 103], [58, 61]], "long-forms": [[73, 98]], "ID": "3043"}, {"text": "res. In Section 2 we present a structural reformulation of Support Vector Machines (SVMs) that can take similarities between different genres into ac-", "acronyms": [[84, 88]], "long-forms": [[59, 82]], "ID": "3044"}, {"text": "grammatically correct (readability). We engaged the services of Amazon Mechanical Turks (AMT) to judge the generated sentences based on a discrete", "acronyms": [[89, 92]], "long-forms": [[64, 87]], "ID": "3045"}, {"text": "Schauder 91). The grammar is divided into an LD  (linear dominance) and an LP (linear precedence) part  so that the piecewise construction of syntactic ", "acronyms": [[75, 77], [45, 47]], "long-forms": [[79, 96], [50, 67]], "ID": "3046"}, {"text": "Collaboratively constructed resources like Wiktionary (WKT) and OmegaWiki (OW) provide a viable option for such cases and seem especially suitable", "acronyms": [[75, 77]], "long-forms": [[64, 73]], "ID": "3047"}, {"text": "726  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657?669, October 25-29, 2014, Doha, Qatar.", "acronyms": [[93, 98]], "long-forms": [[43, 91]], "ID": "3048"}, {"text": "PP ??  ( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) ? ? ? ? ", "acronyms": [[9, 11], [0, 2], [31, 33], [50, 52]], "long-forms": [[14, 17]], "ID": "3049"}, {"text": "{tvu, aaiti, mzhang}@i2r.a-star.edu.sg  Abstract  Term Extraction (TE) is an important component of many NLP applications.", "acronyms": [[67, 69], [105, 108]], "long-forms": [[50, 65]], "ID": "3050"}, {"text": "siderable progress. The bakeoff series hosted by  the Chinese Information Processing Society (CIPS)  and ACL SIGHAN shows that an F measure of ", "acronyms": [[94, 98], [105, 108], [109, 115]], "long-forms": [[54, 92]], "ID": "3051"}, {"text": "To solve the ILP models we used lp solve, a highly efficient GNU-licence Mixed Integer Programming (MIP) solver11, that implements the Branch-and-Bound algorithm.", "acronyms": [[100, 103], [13, 16], [61, 72], [32, 34]], "long-forms": [[73, 98]], "ID": "3052"}, {"text": "slightly higher F-measures than the UMass dictionary.  The error rates (ERR) for all three dictionaries were  identical.", "acronyms": [[72, 75]], "long-forms": [[59, 70]], "ID": "3053"}, {"text": " 3.1 Evaluation methods We use the Relative Utility (RU) method (Radev et al, 2000) to compare our various summaries.", "acronyms": [[53, 55]], "long-forms": [[35, 51]], "ID": "3054"}, {"text": "belief updating model. In Proc American Association for Artificial Intelligence (AAAI) Workshop on Statistical and Empirical Approaches", "acronyms": [[81, 85]], "long-forms": [[40, 79]], "ID": "3055"}, {"text": "2007; Noh and Pad?o, 2013).  2.2 Entailment Core (EC) The Entailment Core performs the actual entail-", "acronyms": [[50, 52]], "long-forms": [[33, 48]], "ID": "3056"}, {"text": "mentation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English,", "acronyms": [[139, 142], [163, 166], [172, 175]], "long-forms": [[122, 130], [145, 154]], "ID": "3057"}, {"text": "lexical chaining.  4.1 LDA Mode Method (LDA-MM) The LDA-MM approach places all word tokens that", "acronyms": [[40, 46], [52, 58]], "long-forms": [[23, 38]], "ID": "3058"}, {"text": "  We extracted bag-of-word features and trained a  Support Vector Machine (SVM) classifier (Burges, 1998) using the above dataset.", "acronyms": [[75, 78]], "long-forms": [[51, 73]], "ID": "3059"}, {"text": "work first arose out of a broader family of approaches to pattern classifier design known as Generalized Probabilistic Descent (GPD) (Katagiri et al, 1991).", "acronyms": [[128, 131]], "long-forms": [[93, 126]], "ID": "3060"}, {"text": "ily' (lists truncated). Score = log-likelihood score; f = occurrence frequency of keyterm; NN = noun; VV = verb; AR =  article; AP = article+preposition; JJ = adjective; CC = con-", "acronyms": [[91, 93], [24, 29], [102, 104], [113, 115], [128, 130], [154, 156], [170, 172]], "long-forms": [[96, 100], [69, 78], [47, 52], [107, 111], [119, 126], [133, 152], [159, 168]], "ID": "3061"}, {"text": "Table 6 shows the number of true positive (TP), true negative (TN), false positive (FP) and false negative (FN) of models for the stocks. ", "acronyms": [[108, 110], [43, 45], [63, 65], [84, 86]], "long-forms": [[92, 106], [33, 41], [48, 61], [68, 82]], "ID": "3062"}, {"text": "adapt the model to character or word level, or limit  the conversion target to only noun or expand it to  other Part of Speech (POS) tags, a series of  experiments has been performed.", "acronyms": [[128, 131]], "long-forms": [[112, 126]], "ID": "3063"}, {"text": "Transcutaneous Oxygen (TcPO2 ) = 9.5. Transcutaneous CO2 (TcPCO2) = 6.7. ", "acronyms": [[58, 64], [23, 28]], "long-forms": [[38, 56], [0, 21]], "ID": "3064"}, {"text": "Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR).", "acronyms": [[106, 108], [85, 87], [142, 144]], "long-forms": [[90, 104], [59, 83], [115, 140]], "ID": "3065"}, {"text": "1 Introduction  Syntax parsing is one of the most fundamental  tasks in natural language processing (NLP) and  has attracted extensive attention during the past ", "acronyms": [[101, 104]], "long-forms": [[72, 99]], "ID": "3066"}, {"text": "While much of the focus in developing a statistical machine translation (SMT) system revolves around the translation model (TM), most systems do not emphasize the role of the language model (LM).", "acronyms": [[124, 126], [73, 76], [191, 193]], "long-forms": [[105, 122], [40, 71], [175, 189]], "ID": "3067"}, {"text": "ergistic working of several components: speech recognition (ASR), spoken language understanding (SLU), dialog management (DM), language generation (LG) and text-to-speech synthesis", "acronyms": [[122, 124], [60, 63], [97, 100], [148, 150]], "long-forms": [[103, 120], [40, 58], [66, 95], [127, 146]], "ID": "3068"}, {"text": " 1 Introduction  Feature term formalisms (FTF) have proven extremely  useful for the declarative representation f linguistic ", "acronyms": [[42, 45]], "long-forms": [[17, 40]], "ID": "3069"}, {"text": "true positive (TP) (i.e., a correct match), and an appropriate NNS triple not found in the gold standard set a false negative (FN) (i.e., an incorrect nonmatch), as shown in Table 4.", "acronyms": [[127, 129], [15, 17], [63, 66]], "long-forms": [[111, 125], [0, 13]], "ID": "3070"}, {"text": "the text. Our proposed metric is called Sentiment Annotation Complexity (SAC). ", "acronyms": [[73, 76]], "long-forms": [[40, 71]], "ID": "3071"}, {"text": " 2.5 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on", "acronyms": [[62, 65]], "long-forms": [[33, 60]], "ID": "3072"}, {"text": " We  also  produced  an  upper  bound  using  Naive  Bayes  multinomial  (NBm)  and Support Vector Machine (SVM)6 classifiers  with the NTU Sentiment  Dictionary (Ku  et al, ", "acronyms": [[108, 111], [74, 77], [136, 139]], "long-forms": [[84, 106], [46, 71]], "ID": "3073"}, {"text": "16 Production rule that expands n?s parent * * 17 Parse tree path from n to the nearest support verb * 18 Last part of speech (POS) subsumed by n * 19 Production rule that expands n?s left sibling *", "acronyms": [[127, 130]], "long-forms": [[111, 125]], "ID": "3074"}, {"text": "This work was partly supported by UK EPSRC project GR/N36462/93: ? Robust Accurate Statistical Parsing (RASP)?. ", "acronyms": [[104, 108], [34, 36], [37, 42]], "long-forms": [[67, 102]], "ID": "3075"}, {"text": "the expected output.  formally a weighted finite state automation (FSA), where V is the set of nodes andE is the set of edges.", "acronyms": [[67, 70]], "long-forms": [[42, 65], [120, 124]], "ID": "3076"}, {"text": "An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extraction) event types. We identify shortcomings of the prior bootstrapping methods, propose a more effective and stable ranking method, and consider the effect of different corpora and evaluation metrics.", "acronyms": [[336, 339], [396, 399]], "long-forms": [[341, 373], [401, 429]], "ID": "3077"}, {"text": "The problem of evaluating machine translation output without reference translations is called quality estimation (QE) and has recently been the centre of attention (Bojar et al.,", "acronyms": [[114, 116]], "long-forms": [[94, 112]], "ID": "3078"}, {"text": "Averaged Reca l l  (AR)  : 2  PP.~.NP ? Averaged Prec is ion  (AP)  : 2  (fl3-1-1)?PPxPR ?", "acronyms": [[63, 65], [20, 22], [30, 32], [35, 37], [83, 85], [86, 88]], "long-forms": [[40, 60], [0, 17]], "ID": "3079"}, {"text": "To extract the verb?noun combinations that have been used by non-native speakers in practice, we use the Cambridge Learner Corpus (CLC), which is a 52.5 million-word corpus of learner En-", "acronyms": [[131, 134]], "long-forms": [[105, 129]], "ID": "3080"}, {"text": "Query key words and concepts Token and concept Indexing Knowledge Source Adapters (KSAs)   integrate and deliver content from ", "acronyms": [[83, 87]], "long-forms": [[56, 81]], "ID": "3081"}, {"text": " Definition 1  A lexical conceptual structure (LCS) is a modified version of the representation proposed  by Jackendoff (1983, 1990) that conforms to the following structural form: ", "acronyms": [[47, 50]], "long-forms": [[17, 45]], "ID": "3082"}, {"text": "computational semantics. With the rise of massive and easily-accessible digital corpora, computation of co-occurrence statistics has enabled researchers in NLP to build distributional semantic models (DSMs) that have found relevance in many application areas.", "acronyms": [[201, 205], [156, 159]], "long-forms": [[169, 199]], "ID": "3083"}, {"text": "139  Computational Linguistics Volume 18, Number 2  (18a) sense of JUMP = jumping. ", "acronyms": [[67, 71]], "long-forms": [[74, 81]], "ID": "3084"}, {"text": "DC-10-30?s number 1 engine, a General Electric CF6-50C2, experienced a casing breach when the 2nd-stage low pressure turbine (LPT) anti-rotation nozzle locks failed.?", "acronyms": [[126, 129], [47, 55], [0, 8]], "long-forms": [[104, 124]], "ID": "3085"}, {"text": " The set of all homonyms built for a sentence is  called its morphological structure (MorphS). ", "acronyms": [[86, 92]], "long-forms": [[61, 84]], "ID": "3086"}, {"text": "C H A N G E .  H A V E  CSEXCH FOB BEPGFF IN 1 4   ANTEST CALLEC FOR 23\"NASREL: \" (AACC) S D =  24, RES= 3. TOP= 1:s ", "acronyms": [[65, 68], [83, 87], [89, 92], [100, 103], [108, 111], [24, 30], [31, 34], [35, 41]], "long-forms": [[51, 64]], "ID": "3087"}, {"text": "the problem as a multi-label classification task,  we trained a binary classification model for each  code using support vector machine (SVM) with  ten-fold cross-validation.", "acronyms": [[137, 140]], "long-forms": [[113, 135]], "ID": "3088"}, {"text": "of Petrov et al. ( 2012): NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP", "acronyms": [[54, 57], [72, 75]], "long-forms": [[59, 69], [77, 84]], "ID": "3089"}, {"text": "Soft cardinality has been shown to be a very strong text-overlapping baseline for the task of measuring semantic textual similarity (STS), obtaining 3rd place in SemEval-2012.", "acronyms": [[133, 136], [162, 174]], "long-forms": [[104, 131]], "ID": "3090"}, {"text": "The Meter Corpus chosen as the test data  is a collection of court reports from the British Press Association (PA) and some leading  British newspapers (Gaizauskas 2001; Clough ", "acronyms": [[111, 113]], "long-forms": [[92, 109]], "ID": "3091"}, {"text": " 1356 Batch Training (BFGS) Online Training (SD) Simulated parameter initialization chunked data selection Annealing", "acronyms": [[22, 26], [45, 47]], "long-forms": [[6, 20]], "ID": "3092"}, {"text": "mfly@sky.ru Abstract YARN (Yet Another RussNet) project started in 2013 aims at creating a large", "acronyms": [[21, 25]], "long-forms": [[27, 46]], "ID": "3093"}, {"text": "? airplane?. We will call this the GN (general noun) lexicon.", "acronyms": [[35, 37]], "long-forms": [[39, 51]], "ID": "3094"}, {"text": " Proceedings of 24th International Conference on Computational Linguistics (COLING): Posters. ", "acronyms": [[76, 82]], "long-forms": [[49, 74]], "ID": "3095"}, {"text": "46 2 Related Work and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic mem-", "acronyms": [[62, 64]], "long-forms": [[40, 60]], "ID": "3096"}, {"text": "(grammatical knowledge and lexical knowledge respectively). The insights gained from our work  with machine readable dictionaries (MRDs) the Longman Dictionary of Contemporary English,  henceforth LDOCE, and the Van Dale dictionary of contemporary Dutch \"Groot Woordenboek van ", "acronyms": [[131, 135], [197, 202]], "long-forms": [[100, 129]], "ID": "3097"}, {"text": "Another dictionary device deals wlth  unrecognized elements - the so-called  transducing dictionary (TD). TD re- ", "acronyms": [[101, 103]], "long-forms": [[77, 99]], "ID": "3098"}, {"text": "The model is presented below. 4 The Model This study employs feed-forward artificial neu-ral networks with a backpropagation algorithm as computational models for the analysis of un-accusative/unergative distinction in Turkish. 4.1 Artificial Neural Networks and Learn-ing Paradigms  An artificial neural network (ANN) is a compu-tational model that can be used as a non-linear statistical data modeling tool. ANNs are gener-ally used for deriving a function from observa-tions, in applications where the data are com-plex and it is difficult to devise a relationship ", "acronyms": [[314, 317]], "long-forms": [[287, 312]], "ID": "3099"}, {"text": "related NIST metric (Doddington 2002), along with WER and PER, have been widely used by many machine translation researchers. The Translation Edit Rate (TER) (Snover et al 2006) and the CDER measure (Leusch, Ueffing, and Ney 2006) are based on the edit", "acronyms": [[153, 156], [8, 12], [50, 53], [58, 61], [186, 190]], "long-forms": [[130, 151]], "ID": "3100"}, {"text": "experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et", "acronyms": [[70, 73]], "long-forms": [[43, 68]], "ID": "3101"}, {"text": "(equivalent to relation). Their relation instances are named entity(NE)-mention pairs conforming to a set of pre-specified rules.", "acronyms": [[68, 70]], "long-forms": [[55, 67]], "ID": "3102"}, {"text": " CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority (MDA)", "acronyms": [[81, 84], [1, 6], [135, 138]], "long-forms": [[51, 79], [106, 133]], "ID": "3103"}, {"text": "boring dependency structures. CC = coordinate concatenate, LA = left adjoining, and RA = right adjoining.", "acronyms": [[59, 61]], "long-forms": [[64, 78]], "ID": "3104"}, {"text": "3.2 Data Normalization The ACL shared task is very close in form and content to the Final Text Editions (FTE) task of the TCSTAR (TC-STAR, 2004) evaluation.", "acronyms": [[105, 108], [27, 30], [130, 137]], "long-forms": [[84, 103], [122, 128]], "ID": "3105"}, {"text": "Table 3: The effect of new features on the development set for English. UAS = unlabeled attachment score; UEM = unlabeled exact match.", "acronyms": [[72, 75], [106, 109]], "long-forms": [[78, 104], [112, 133]], "ID": "3106"}, {"text": "interested in formalisms which are being  used or have applications in the domain  of machine translation (MT). These can ", "acronyms": [[107, 109]], "long-forms": [[86, 105]], "ID": "3107"}, {"text": "as ht, ct = LSTM(xt, ht?1, ct?1).  Residual Networks (ResNet) are among the pioneering works (Szegedy et al, 2015; Srivastava et", "acronyms": [[54, 60], [12, 16]], "long-forms": [[35, 52]], "ID": "3108"}, {"text": "Semantic Specialization (ISS) (Girju, Badulescu,  and Moldovan, 2006), Na?ve Bayes (NB) 3  and  Maximum Entropy (ME)4.  ", "acronyms": [[113, 115], [25, 28], [84, 86]], "long-forms": [[96, 111], [0, 23], [71, 82]], "ID": "3109"}, {"text": "We focused on syntax, esp. noun phrase (NP) syntax from the beginning.", "acronyms": [[40, 42]], "long-forms": [[27, 38]], "ID": "3110"}, {"text": "blogs which accumulated a large number of posts during this period: Carpetbagger (CB),1 Daily Kos (DK),2 Matthew Yglesias (MY),3 Red State (RS),4 and Right Wing News (RWN).5 CB and MY ceased", "acronyms": [[123, 125], [140, 142], [82, 84], [167, 170], [181, 183], [174, 176]], "long-forms": [[105, 121], [129, 138], [68, 80], [150, 164]], "ID": "3111"}, {"text": "PIQ(Fy;R) or not.  z Predictive Information Redundancy(PIR) Based on the above two definitions, we can", "acronyms": [[55, 58], [0, 3], [4, 8]], "long-forms": [[21, 53]], "ID": "3112"}, {"text": " 3.2 Structure Mapping Theory French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner", "acronyms": [[76, 79], [152, 155]], "long-forms": [[50, 74], [126, 150]], "ID": "3113"}, {"text": "ing the most similar. This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED).", "acronyms": [[70, 74], [111, 116]], "long-forms": [[59, 63], [88, 93]], "ID": "3114"}, {"text": "Entailment [13] was organized by SemEval-2.  Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand ", "acronyms": [[76, 80], [96, 101]], "long-forms": [[45, 74]], "ID": "3115"}, {"text": "In this paper, we also implement type raising rules for compact representation of PP (prepositional phrase) and AP (adverbial phrase).", "acronyms": [[82, 84], [112, 114]], "long-forms": [[86, 106], [116, 132]], "ID": "3116"}, {"text": "Data Annotated. The data to be annotated in WSsim-1 were taken primarily from Semcor (Miller et al1993) and the Senseval-3 English lexical sample (SE-3) (Mihalcea, Chklovski, and Kilgarriff 2004).", "acronyms": [[147, 151]], "long-forms": [[112, 122]], "ID": "3117"}, {"text": "8  end if  Active characters are discussed in the section about identifying the SC (Section 8),  because the raison d'etre of the active-character component of an interpretation is that ", "acronyms": [[80, 82]], "long-forms": [[84, 91]], "ID": "3118"}, {"text": "Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al (2010), on this widely-used benchmark corpus are", "acronyms": [[108, 112], [55, 58]], "long-forms": [[96, 106]], "ID": "3119"}, {"text": "volution model (DTCNN). Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005).", "acronyms": [[80, 82], [16, 21]], "long-forms": [[65, 78]], "ID": "3120"}, {"text": "3.5 Classification   For our task of classifying ALI patients, we  picked the Maximum Entropy (MaxEnt) algorithm due to its good performance in text classi-", "acronyms": [[95, 101], [49, 52]], "long-forms": [[78, 93]], "ID": "3121"}, {"text": "patterns allows a 4-way classification of slopes of lines: fast rising, rising, level, falling.  These are the 4 Fundamental Pattern Features (FPF). A combination of 2 or  3 (of the 4) ", "acronyms": [[143, 146]], "long-forms": [[113, 141]], "ID": "3122"}, {"text": "Abstract We present a system that automatically induces Selectional Preferences (SPs) for Latin verbs from two treebanks by using", "acronyms": [[81, 84]], "long-forms": [[56, 79]], "ID": "3123"}, {"text": "scoring n-grams in the sentence.  Mean logprob (ML) This score is the logprob of the entire sentence divided by the length of the", "acronyms": [[48, 50]], "long-forms": [[34, 46]], "ID": "3124"}, {"text": " First, we employ a multiple output GP based on the Intrinsic Coregionalization Model (ICM) (", "acronyms": [[87, 90], [36, 38]], "long-forms": [[52, 85]], "ID": "3125"}, {"text": "and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS). ", "acronyms": [[94, 98]], "long-forms": [[55, 92]], "ID": "3126"}, {"text": "and intangible factors known to the engineer but unknown to the computer. For example, the engineer  may need to postpone the placement of equipment in a certain CSA (Carrier Serving Area) due to a fixed  cap on near-term expenditures, or she may decide to activate DLC (Digital Loop Carrier) equipment in ", "acronyms": [[162, 165]], "long-forms": [[167, 187]], "ID": "3127"}, {"text": "74.80 ?  Table 2: Parsing accuracy; AS = attachment score; ER = error reduction w.r.t. projective baseline (%)", "acronyms": [[36, 38], [59, 61]], "long-forms": [[41, 57], [64, 79]], "ID": "3128"}, {"text": " 3.2 Latent Structural SVM We employ the latent structural SVM (LS-SVM) model for learning the discriminative model of query", "acronyms": [[64, 70], [23, 26]], "long-forms": [[41, 62]], "ID": "3129"}, {"text": "I  zwemmen  (17) is ttms obtained by setting VPo = VPh Zo = Yo,  attd Zl = Vl.", "acronyms": [[51, 54], [45, 48], [70, 72]], "long-forms": [[55, 57]], "ID": "3130"}, {"text": "Features representing syntax (a) Phrase structure (PS) rules (b) Grammatical relation (GR) distance measures", "acronyms": [[87, 89], [51, 53]], "long-forms": [[65, 85], [33, 49]], "ID": "3131"}, {"text": "1 Introduction Semantic Parsing, the process of converting text into a formal meaning representation (MR), is one of the key challenges in natural language process-", "acronyms": [[102, 104]], "long-forms": [[78, 100]], "ID": "3132"}, {"text": "In Proceedings of the Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 376?387.", "acronyms": [[95, 102]], "long-forms": [[22, 93]], "ID": "3133"}, {"text": "AFIPS Washington Off ice   GAO REPORTS ON FEDERAL MODELING  The General Accounting 0fEic.e (GAO') has released a repor t -  on \"Ways to  Improve  &mug-nt of Federally kcnded ~o@ute r i z ed  ~ o d e Z s I ~  (#-  enc luse $1.00) .", "acronyms": [[92, 96], [0, 5], [27, 30]], "long-forms": [[64, 90]], "ID": "3134"}, {"text": "fier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring", "acronyms": [[95, 98]], "long-forms": [[65, 93]], "ID": "3135"}, {"text": "et al, 1993). The learning algorithm was inspired  by several Inductive Logic Programming (ILP) sys-  tems and primarily consists of a specific-to-general ", "acronyms": [[91, 94]], "long-forms": [[62, 89]], "ID": "3136"}, {"text": "dleware architecture (Scha?fer, 2006). It starts with sentence boundary detection (SBR) and regular expression-based tokenization using its built-in", "acronyms": [[83, 86]], "long-forms": [[54, 71]], "ID": "3137"}, {"text": "120 compared to approximate maximum likelihood estimation (MLE). However, this method has not been", "acronyms": [[59, 62]], "long-forms": [[28, 57]], "ID": "3138"}, {"text": "Such vectors can be used to perform all standard linear algebra operations applied in vector-based semantics: Measuring the cosine of the angle between vectors, applying singular value decomposition (SVD) to the whole matrix, and so on.", "acronyms": [[200, 203]], "long-forms": [[170, 198]], "ID": "3139"}, {"text": "It is encouraging that the results (after correcting the misaligned identifiers) for the patched system are approaching the Inter Tagger Agreement (ITA) level reported for OntoNotes sense tags by the task or-", "acronyms": [[148, 151]], "long-forms": [[124, 146]], "ID": "3140"}, {"text": "Winnows software package.  Maximum Entropy Model (MEM) is  especially suited for integrating evidences from ", "acronyms": [[50, 53]], "long-forms": [[27, 48]], "ID": "3141"}, {"text": " In this work, we explore n-gram models over Minimal Translation Units (MTUs) to explicitly capture contextual dependencies across", "acronyms": [[72, 76]], "long-forms": [[45, 70]], "ID": "3142"}, {"text": "The Cell or Tissue Type category was split into two fine grained classes, CELL and CLNE (cell line). ", "acronyms": [[83, 87], [74, 78]], "long-forms": [[89, 98]], "ID": "3143"}, {"text": "Additionally, we have acquired gazetteer lists for Hindi and used these gazetteers in the Maximum Entropy (MaxEnt) based Hindi NER system.", "acronyms": [[107, 113], [127, 130]], "long-forms": [[90, 105]], "ID": "3144"}, {"text": " The system determines the position of the main part of  the situation relative to the point of speech (PS). The ", "acronyms": [[104, 106]], "long-forms": [[87, 102]], "ID": "3145"}, {"text": "2 Conditional Random Fields 2.1 The model Conditional Random Fields(CRFs), a statistical sequence modeling framework, was first intro-", "acronyms": [[68, 72]], "long-forms": [[42, 66]], "ID": "3146"}, {"text": "lion tokens to 0.3832 for 237 million tokens. At such data sizes, Stupid Backoff (SB) with a constant backoff parameter ?", "acronyms": [[82, 84]], "long-forms": [[66, 80]], "ID": "3147"}, {"text": "640,000 non-empty abstracts were found.  Query Set (QSet): We download from PubMed the abstracts that mention a given gene name and its syn-", "acronyms": [[52, 56]], "long-forms": [[41, 50]], "ID": "3148"}, {"text": "(3) range, e.g., (age(value(0 100))).  The notion of IMPORTANCE VALUES (IVs) are  introduced here and are used to numerically describe ", "acronyms": [[72, 75]], "long-forms": [[53, 70]], "ID": "3149"}, {"text": "Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval (IR) system (Choueka, 1990; Ja?ppinen and Niemisto?,", "acronyms": [[121, 123]], "long-forms": [[98, 119]], "ID": "3150"}, {"text": "  Figure 1: Example of a BDT sentence in the CoNLL-X format  (V = main verb, AUXV = auxiliary verb, CONJ = conjunction, REL = subordinated clause, CMP = completive, ccomp_obj =  clausal complement object, ERG = ergative, SUBJ:3S: subject in 3rd person sing.,", "acronyms": [[77, 81], [100, 104], [147, 150]], "long-forms": [[84, 98], [107, 118], [153, 163]], "ID": "3151"}, {"text": "driver was measured in two ways. The driver performed a Tactile Detection Task (TDT) (van Winsum et al, 1999).", "acronyms": [[80, 83]], "long-forms": [[56, 78]], "ID": "3152"}, {"text": "terns for them. In the terminology of Frame Semantics, the roles are called frame elements (FEs), and the words which evoke the frame are referred", "acronyms": [[92, 95]], "long-forms": [[76, 90]], "ID": "3153"}, {"text": "Table 7 shows the effects of merging the sub-  ject accessibility bias with both recency biases and  the restricted memory bias (RM). The results in ", "acronyms": [[129, 131]], "long-forms": [[105, 122]], "ID": "3154"}, {"text": "Table 7: Feature template we use for our classification of event types. Feature examples are based on the sentence ta1 (he) bu4 (not) kan4 (read) zhen1tan4 (detective) xiao3shuo1 (novel) le0 (LE) ? he doesn?t", "acronyms": [[192, 194]], "long-forms": [[187, 190]], "ID": "3155"}, {"text": "ly corrected. An n-gram (n= 2 and 3) language model was then built from the Sinica corpus released  by the Association for Computational Linguistics and Chinese Language Processing (ACLCLP) using  the SRILM toolkit (Stolcke, 2002).", "acronyms": [[182, 188], [201, 206]], "long-forms": [[107, 180]], "ID": "3156"}, {"text": "ical analyzer for English (Sekine, 2001)are selected and translation candidates having POS tags other than NN (noun) are discarded. Selected translation", "acronyms": [[107, 109], [87, 90]], "long-forms": [[111, 115]], "ID": "3157"}, {"text": " The output is a prediction of whether the tweet is inside region (IR) or outside region (OR). We", "acronyms": [[67, 69], [90, 92]], "long-forms": [[52, 65], [74, 88]], "ID": "3158"}, {"text": "Building on the error analysis of the rule-based approach, we replace the rule-based component with support vector machine (SVM) classifiers trained on partial event annotation in the form of", "acronyms": [[124, 127]], "long-forms": [[100, 122]], "ID": "3159"}, {"text": " The third thread is the sponsorship of the international  Message Understanding Conferences (MUC's) and Text  Retrieval Conferences (TREC's).", "acronyms": [[94, 99], [134, 140]], "long-forms": [[59, 92], [105, 132]], "ID": "3160"}, {"text": "sists of given entities and their relation expression.  Here, we use a Salient Referent List (SRL) to obtain contextual structure.", "acronyms": [[94, 97]], "long-forms": [[71, 92]], "ID": "3161"}, {"text": "Frame representat ion ( for  mapping 4):  \\[agent/an\\] (i/PRP)  5PO$ abbreviations: PRP=personal pro-  noun, AUX=auxiliary verb, VB=main verb (non-inflected), ", "acronyms": [[84, 87], [109, 112], [129, 131]], "long-forms": [[88, 101], [113, 122], [137, 141]], "ID": "3162"}, {"text": "We have been concentrating on the Hong  Kong Hansard, which are the parliamentary pro-  ceedings of the Legislative Council (LegCo). Anal- ", "acronyms": [[125, 130]], "long-forms": [[104, 123]], "ID": "3163"}, {"text": "4. Crowdsourcing We used the Amazon Mechanical Turk (AMT) service to obtain annotations for different kinds of opposites.", "acronyms": [[53, 56]], "long-forms": [[29, 51]], "ID": "3164"}, {"text": "= argmaxjP (zi = j|sk).  4.2 Lexical Chain Segmenter (LCSeg) Our second model is the lexical chain based seg-", "acronyms": [[54, 59]], "long-forms": [[29, 52]], "ID": "3165"}, {"text": "modeled as distributed dense vectors of hidden layers. A recurrent neural network (RNN) is one such learner that can operate on sequential data of variable", "acronyms": [[83, 86]], "long-forms": [[57, 81]], "ID": "3166"}, {"text": "  Abstract  A Named Entity Recognizer (NER) generally  has worse performance on machine translated ", "acronyms": [[39, 42]], "long-forms": [[14, 37]], "ID": "3167"}, {"text": " We apply both support vector machine (SVM)  and Maximum Entropy (ME) algorithms with the  help of the SVM-light4 and Mallet5 tools.", "acronyms": [[66, 68], [39, 42], [103, 106]], "long-forms": [[49, 64], [15, 37]], "ID": "3168"}, {"text": "e.g. Microsoft 2. Candidate Definition Features (CDs) : These consist of the two following feature classes.", "acronyms": [[49, 52]], "long-forms": [[18, 47]], "ID": "3169"}, {"text": "  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787?798, October 25-29, 2014, Doha, Qatar.", "acronyms": [[90, 95]], "long-forms": [[40, 88]], "ID": "3170"}, {"text": "Fazly and Stevenson, 2007), our approach is to compare the context vector of a VNC with the composed vector of the verb and noun (V-N) component units of the VNC when they occur in iso-", "acronyms": [[130, 133]], "long-forms": [[115, 128]], "ID": "3171"}, {"text": "tions and keying in on student language to promote self-explanation of concepts, and its curriculum is based on the Full Option Science System (FOSS) 1 a proven system for inquiry based learning.", "acronyms": [[144, 148]], "long-forms": [[116, 142]], "ID": "3172"}, {"text": "document-level CLOA tasks, respectively. The evaluations on simplified Chinese (SC) opinion analysis by using small SC training data and large", "acronyms": [[80, 82], [15, 19], [116, 118]], "long-forms": [[60, 78]], "ID": "3173"}, {"text": "The variants of random walk topic models are compared against LDA and the relational topic model (RTM), each with 100 topics (Chang and Blei, 2010).", "acronyms": [[98, 101], [62, 65]], "long-forms": [[74, 96]], "ID": "3174"}, {"text": "ing is combined with dictionary-based (e.g.,  WordNet) reranking, which leads to a 25% increase in mean reciprocal rank (MRR). Xu et al ", "acronyms": [[121, 124]], "long-forms": [[99, 119]], "ID": "3175"}, {"text": "Specifically, the groups include children with ASD with language impairment (ALI); ASD with no language impairment (ALN); SLI alone; and typically developing (TD), which is", "acronyms": [[116, 119], [47, 50], [77, 80], [122, 125], [159, 161]], "long-forms": [[83, 114], [56, 75], [137, 157]], "ID": "3176"}, {"text": "(NL) into formal meaning representations (MR), and are applied for both parsing of textual input and in Spoken Language Understanding (SLU). In data-", "acronyms": [[135, 138], [1, 3], [42, 44]], "long-forms": [[104, 133], [17, 40]], "ID": "3177"}, {"text": "This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM.", "acronyms": [[92, 95], [116, 120]], "long-forms": [[71, 90]], "ID": "3178"}, {"text": "| lexica: The relative frequency of categories, based on the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al, 2007).", "acronyms": [[96, 100]], "long-forms": [[61, 94]], "ID": "3179"}, {"text": "latent topic space. Values within vectors are the TF-ITF (term frequency - inverse topic frequency) scores which are calculated in a completely ana-", "acronyms": [[50, 56]], "long-forms": [[58, 98]], "ID": "3180"}, {"text": "These two representations are associated in an abstract type map, called AAM (Abstract Associative Map). This", "acronyms": [[73, 76]], "long-forms": [[78, 102]], "ID": "3181"}, {"text": "second verb in Japanese compound verbs. We  investigate Japanese compound verbs (JCVs) and  extract semantic constraints for the purpose of ", "acronyms": [[81, 85]], "long-forms": [[56, 79]], "ID": "3182"}, {"text": "words, respectively. We include two versions of this general model; Continuous Bag of Words (CBOW) that predicts a word based on the context, and Skip-", "acronyms": [[93, 97]], "long-forms": [[68, 91]], "ID": "3183"}, {"text": "J~:u phom:,l:ical (graphemic) empressio~ in to  5 l \\ [ ,+vei~  tecLogt+ah~illa~ieg (levm+l o+ US+st abbrev,  TR) + ,+:mr+ar:e  synkam (SR) + (neP'pfie/r+ic5 (HR) '+ phnnemics ,~nd phnnetit:ts  (g raphmaics} , ,  Eact+ ef the  ieve l~ i s  in terpreted  a~ a set, ", "acronyms": [[136, 138], [110, 112], [159, 161]], "long-forms": [[128, 134]], "ID": "3184"}, {"text": " The first principle of a search engine is based  on shallow Natural Language Processing (NLP)  techniques, for instance, string matching, while ", "acronyms": [[90, 93]], "long-forms": [[61, 88]], "ID": "3185"}, {"text": "UAS = unlabeled attachment score; UEM = unlabeled exact match; LAS = labeled attachment score. ", "acronyms": [[63, 66], [0, 3], [34, 37]], "long-forms": [[69, 93], [6, 32], [40, 61]], "ID": "3186"}, {"text": "Social media in general exhibit a rich variety of  information sources. Question answering (QA) has  been particularly amenable to social media, as it ", "acronyms": [[92, 94]], "long-forms": [[72, 90]], "ID": "3187"}, {"text": "340   Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 37?45, Gothenburg, Sweden, April 26-30 2014.", "acronyms": [[91, 96], [25, 29]], "long-forms": [[47, 89]], "ID": "3188"}, {"text": " 2003. Social Communication Questionnaire (SCQ). ", "acronyms": [[43, 46]], "long-forms": [[7, 41]], "ID": "3189"}, {"text": "CHBWGE, H l V E  CSERCH FOR HERGEF IN 10  merge features  i n t o  node 10 (making it a schwa)  ANTEST CALLED FOB 211SCFIHDA (AACC) , SD= 3. RES= 7.", "acronyms": [[126, 130], [134, 136], [0, 6], [8, 15], [17, 23], [28, 34], [24, 27], [141, 144]], "long-forms": [[96, 124]], "ID": "3190"}, {"text": "EXPERIMENTS 3.1. Speaker Identification (SID) In order to investigate robust speaker identification under", "acronyms": [[41, 44]], "long-forms": [[17, 39]], "ID": "3191"}, {"text": "Figure 8: Alternative English lexical entry for *WORK-  FUNCIONAR  (GENDER and NUMBER) count/mass (COUNT) and  a trinary distinction of ANIMACY (human, animal, ", "acronyms": [[79, 85], [56, 65], [99, 104], [136, 143], [68, 74]], "long-forms": [[87, 92]], "ID": "3192"}, {"text": "Yoram Bachrach is a researcher in the Online Services and Advertising group at Microsoft Research Cambridge UK. His research area is artificial intelligence (AI), focusing on multi-agent systems and computational game theory.", "acronyms": [[158, 160], [108, 110]], "long-forms": [[133, 156]], "ID": "3193"}, {"text": "Each extractor receives a sentence as input and determines which noun phrases (NPs) in the sentence are fillers for the event role.", "acronyms": [[79, 82]], "long-forms": [[65, 77]], "ID": "3194"}, {"text": "knowledge in building the target domain classifier, we propose a novel optimization method based on the Na??ve Bayesian (NB) framework and stochastic gradient descent.", "acronyms": [[121, 123]], "long-forms": [[104, 119]], "ID": "3195"}, {"text": "problem so that it includes finding the most probable corrections tags.  WDCLOREI arg max Pr( WDCLOREIIA ) WDCLOREI ", "acronyms": [[94, 104], [107, 115]], "long-forms": [[73, 89]], "ID": "3196"}, {"text": "1 Introduction Recent research shows that it is possible, using current natural language processing (NLP) and machine learning technology, to automatically induce lex-", "acronyms": [[101, 104]], "long-forms": [[72, 99]], "ID": "3197"}, {"text": "crafted features, lexicons, and grammars.  Meanwhile, recurrent neural networks (RNNs) what are the major cities in utah ?", "acronyms": [[81, 85]], "long-forms": [[54, 79]], "ID": "3198"}, {"text": "gramming for probabilistic programming. In International Workshop on Statistical Relational AI (StarAI). ", "acronyms": [[96, 102]], "long-forms": [[69, 94]], "ID": "3199"}, {"text": "In order to take the transitivity of outscoping relations into account, we use the transitive closure (TC) of DAGs. Let G+ =", "acronyms": [[103, 105], [110, 114]], "long-forms": [[83, 101]], "ID": "3200"}, {"text": "the  locat ions  shown in F ig .4 .  The s lo t  SH~P represent  whether  the  par t   cor respond ing  to  th i s  frame is  a reg ion(~EG)  or a branch(BR A) The  SU~P s lo t  records  i t s  subpar ts  and the i r  locat ions  of or re ia t ions  to ", "acronyms": [[154, 158], [49, 53], [136, 139], [165, 169]], "long-forms": [[147, 152]], "ID": "3201"}, {"text": "We labeled the Reddit dataset using crowdworkers on Amazon Mechanical Turk (AMT), creating the first public corpus annotated with levels of dogmatism.", "acronyms": [[76, 79]], "long-forms": [[52, 74]], "ID": "3202"}, {"text": "ancient than transport 1.83E-102 old ancient 0.005 Table 8: Substitution Feature Table (SubFT) 3.3.2 Phrase Substitution", "acronyms": [[88, 93]], "long-forms": [[60, 86]], "ID": "3203"}, {"text": "words in similar context have similar meanings ?  distributed semantic models (DSM)s build vector representations based on corpus-extracted context.", "acronyms": [[79, 82]], "long-forms": [[50, 77]], "ID": "3204"}, {"text": "by using RBM to implement the middle layers,  since RBM can be learned very quickly by the  Contrastive Divergence (CD) approach. ", "acronyms": [[116, 118], [9, 12], [52, 55]], "long-forms": [[92, 114]], "ID": "3205"}, {"text": "topics vary over time, we aggregate the microblog posts published in a month as a document. Then, we use a Latent Dirichlet Allocation (LDA) to estimate their topics. Figure 1 illustrates an example, where", "acronyms": [[136, 139]], "long-forms": [[107, 134]], "ID": "3206"}, {"text": "We use three classes of features? Crowd Grades (CG), Force Alignment features (FA) and Natural Language Processing features (NLP).", "acronyms": [[79, 81], [48, 50], [125, 128]], "long-forms": [[53, 68], [34, 46], [87, 114]], "ID": "3207"}, {"text": "training set by cosine-similarity. Precision-recall (P-R) curves and mean average precision (MAP) are two metrics we used for evaluation.", "acronyms": [[93, 96], [53, 56]], "long-forms": [[69, 91], [35, 51]], "ID": "3208"}, {"text": "2005a; Jeon et al, 2005b) compared four different  retrieval methods, i.e. vector space model, Okapi,  language model (LM), and translation-based model,  for automatically fixing the lexical chasm between ", "acronyms": [[119, 121]], "long-forms": [[103, 117]], "ID": "3209"}, {"text": "Table 2: Three kinds of preprocessing of a sentence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP = copula, EXCL = exclamation mark.", "acronyms": [[95, 98], [121, 124], [75, 78], [138, 141], [152, 156]], "long-forms": [[101, 110], [127, 136], [69, 73], [81, 86], [144, 150], [159, 170]], "ID": "3210"}, {"text": "Berwick and Weinberg (1982). Gazdar noted that if  transformational grammars (TG's) were stripped of  all their transformations, they became CFL- ", "acronyms": [[78, 82], [141, 144]], "long-forms": [[51, 76]], "ID": "3211"}, {"text": "disasters or political crises in the media. There are two main approaches to audio hotspotting; one involves speech-to-text (STT), also known as large vocabulary continuous speech recognition (LVCSR),  and the other employs phonetic speech recognition.", "acronyms": [[193, 198], [125, 128]], "long-forms": [[145, 191], [109, 123]], "ID": "3212"}, {"text": "196  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1779?1785, October 25-29, 2014, Doha, Qatar.", "acronyms": [[93, 98]], "long-forms": [[43, 91]], "ID": "3213"}, {"text": "cognitive system (Wilkes, 1997). We refer to this  subset of knowledge store (KS) in operation for and in  a given discourse as discourse model (DM) and hold ", "acronyms": [[78, 80], [145, 147]], "long-forms": [[61, 76], [128, 143]], "ID": "3214"}, {"text": "tion algorithm. We use an existing unsupervised method, called Double Propagation (DP) (Qiu et al, 2011), for extraction.", "acronyms": [[83, 85]], "long-forms": [[63, 81]], "ID": "3215"}, {"text": "Some other related works should be mentioned. A notable method is Locality Sensitive Hashing (LSH) (Indyk et al, 1998).", "acronyms": [[94, 97]], "long-forms": [[66, 92]], "ID": "3216"}, {"text": " Take for example the word cliff which could be a  proper (NP) 1 or a common noun (NN) (ignoring ca-  pitalization of proper nouns for the moment).", "acronyms": [[83, 85], [59, 61]], "long-forms": [[77, 81]], "ID": "3217"}, {"text": " Issues, Tasks and Program Structures to Roadmap Research in Question & Answering (Q&A) http://www-nlpir.nist.gov/projrcts/", "acronyms": [[83, 86]], "long-forms": [[61, 81]], "ID": "3218"}, {"text": " The specific case focused on in this paper is that of AL with SVMs (AL-SVM) for imbalanced ?", "acronyms": [[69, 75]], "long-forms": [[55, 67]], "ID": "3219"}, {"text": "without any adaptation.  Laplacian SVM (L-SVM) This is a semisupervised learning method based on label", "acronyms": [[40, 45]], "long-forms": [[25, 38]], "ID": "3220"}, {"text": "The dependency inductions were evaluated on 3 metrics: directed accuracy, undirected accuracy and Neutral Edge Detection (NED) (Schwartz et al.,", "acronyms": [[122, 125]], "long-forms": [[98, 120]], "ID": "3221"}, {"text": "and the parameters ? can be estimated using maximum likelihood estimation(MLE) on a training corpus(Och and Ney, 2003).", "acronyms": [[74, 77]], "long-forms": [[44, 73]], "ID": "3222"}, {"text": "Xi, where Par(Xi) denotes the parents of Xi.  Conditional probability distributions (CPDs) can be defined in various ways, from look-up tables", "acronyms": [[85, 89], [10, 13]], "long-forms": [[46, 83], [30, 37]], "ID": "3223"}, {"text": "(FW), SeekBw (BW), ScrollFw (FS), ScrollBw (BS), Ratechange Increase (RCI), Ratechange Decrease (RCD). ", "acronyms": [[97, 100], [1, 3], [14, 16], [29, 31], [44, 46], [70, 73]], "long-forms": [[76, 95], [6, 12], [19, 27], [34, 42], [49, 68]], "ID": "3224"}, {"text": "3 Attribute Modeling based on LDA 3.1 Controled LDA This section introduces Controled LDA (C-LDA), a weakly supervised variant of LDA.", "acronyms": [[91, 96], [30, 33], [48, 51], [130, 133]], "long-forms": [[76, 89]], "ID": "3225"}, {"text": "the percentage of words that were placed in a segment perfectly identical to that in the reference. The dialogue act based metric (DER) was proposed in Zimmermann et al (2005). In this metric a word is", "acronyms": [[131, 134]], "long-forms": [[104, 129]], "ID": "3226"}, {"text": "connective label?. The results are reported for Same Sentence (SS) and Previous Sentence (PS) models, and the joined results for each of the argu-", "acronyms": [[63, 65], [90, 92]], "long-forms": [[48, 61], [71, 88]], "ID": "3227"}, {"text": "The TRIPS system here uses a wide range of statistically driven preprocessing, including part of speech tagging, constituent bracketing, inter-pretation of unknown words using WordNet, and named-entity recognition (Allen et al 2008). All these are generic off-the-shelf resources that ex-tend and help guide the deep parsing process.  The TRIPS LF (logical form) ontology1 is de-signed to be linguistically motivated and domain independent. The semantic types and selectional restrictions are driven by linguistic considera-tions rather than requirements from reasoning components in the system (Dzikovska et al 2003).", "acronyms": [[345, 347], [339, 344], [4, 9]], "long-forms": [[349, 361]], "ID": "3228"}, {"text": "Chunking Tree (CT) 86.17 86.21 Linear Features (Kl) 90.79 90.46 Kl w/o using LM feature (Kl-LM) 84.24 84.06 Composite Kernel (Kc: MST+Kl) 92.98 92.67", "acronyms": [[89, 94], [15, 17], [48, 50], [130, 133], [134, 136]], "long-forms": [[64, 79], [0, 13]], "ID": "3229"}, {"text": "the 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, may.  European Language Resources Association (ELRA). ", "acronyms": [[148, 152], [71, 75]], "long-forms": [[107, 146], [36, 53]], "ID": "3230"}, {"text": "New York, N.Y. 10027  Introduction  COMET (COordinated Multimedia Explanation  Testbed) is an experimental system that generates inter- ", "acronyms": [[36, 41], [10, 14]], "long-forms": [[43, 86], [0, 8]], "ID": "3231"}, {"text": "and Darooneh, 2011).  Recurrent neural networks(RNNs) (Elman, 1990) has been applied to many sequential prediction", "acronyms": [[48, 52]], "long-forms": [[22, 46]], "ID": "3232"}, {"text": "2.2 Natural Language Processing  2.2.1 Woods' Augmented Transition Networks  Thc Augmented Transition Network (ATN) of Woods [Woods 501 is a  powerful formalisnl for representing grammars.", "acronyms": [[111, 114]], "long-forms": [[81, 109]], "ID": "3233"}, {"text": "Phrase Extrac\"on GIZA++ Figure 3: Building a twin phrase table (PT). First, sep-", "acronyms": [[64, 66], [17, 23]], "long-forms": [[50, 62]], "ID": "3234"}, {"text": "  1 Introduction  Many Natural Language Processing (NLP)  applications need to recognize when the meaning ", "acronyms": [[52, 55]], "long-forms": [[23, 50]], "ID": "3235"}, {"text": "ability integral transform to generate empirical cumulative density functions (ECDF): now instead of the probability density function (PDF) space, we are working in the ECDF space where the value of each", "acronyms": [[135, 138], [79, 83], [169, 173]], "long-forms": [[105, 133], [39, 77]], "ID": "3236"}, {"text": "  Abbreviation: ACK=Acknowledgment; COMP=Completion;  SNU=Signal Non Understanding; Sources abbreviated as: F  = Friendship; V = Visibility; Rte = Route ", "acronyms": [[54, 57], [16, 19], [36, 40], [141, 144]], "long-forms": [[58, 82], [20, 33], [41, 51], [113, 123], [129, 139], [147, 152]], "ID": "3237"}, {"text": "terial with relevant events will be done along the 1MUMIS is an on-going EU-funded project within the Information Society Program (IST) of the European Union, section Human Language Technology (HLT).", "acronyms": [[131, 134]], "long-forms": [[102, 121]], "ID": "3238"}, {"text": "Initiative (ECI) plans to distribute similar material in a variety of languages. Even  greater esources are expected from the Linguistic Data Consortium (LDC). And the ", "acronyms": [[154, 157], [12, 15]], "long-forms": [[126, 152]], "ID": "3239"}, {"text": "These features have been obtained using the knowledge contained into the Multilingual Central Repository (MCR) of the MEANING project3 (Atserias et al, 2004).", "acronyms": [[106, 109]], "long-forms": [[73, 104]], "ID": "3240"}, {"text": "phrases (NPs) (representing 49.6% of the total number of phrases), verb phrases (VPs), prepositional phrases (PPs), adjectival phrases (ADJPs), and quantity phrases (QPs), representing 99.1% of", "acronyms": [[136, 141]], "long-forms": [[116, 134]], "ID": "3241"}, {"text": "cation), TMP (time), DIS (discourse connectives), PRP (purpose) or DIR (direction). Negations (NEG) and modals (MOD) are also marked.", "acronyms": [[95, 98], [9, 12], [21, 24], [50, 53], [67, 70], [112, 115]], "long-forms": [[84, 93], [26, 35], [55, 62], [72, 81], [104, 110]], "ID": "3242"}, {"text": "CIMA is an online information center maintained by the Spanish Agency for Medicines and Health Products (AEMPS). CIMA provides", "acronyms": [[105, 110], [0, 4], [113, 117]], "long-forms": [[84, 103]], "ID": "3243"}, {"text": "ture subset selection [2][4][8]. Yang and Pederson  found information gain (IG) and chi-square test (CHI)  most effective in aggressive term removal without los-", "acronyms": [[76, 78], [101, 104]], "long-forms": [[58, 74], [84, 99]], "ID": "3244"}, {"text": "information nuggets?, called Summary Content Units (SCUs), which are short, atomic statements of facts con-", "acronyms": [[52, 56]], "long-forms": [[29, 50]], "ID": "3245"}, {"text": "XC (compound),   NN (noun),   NNP (proper ", "acronyms": [[17, 19], [0, 2], [30, 33]], "long-forms": [[21, 25], [4, 12]], "ID": "3246"}, {"text": "them as \"LIG-equivalent formalisms\". LIG is a vari-  ant of index grammar (IG) (Aho, 1968). Like CFG, IG ", "acronyms": [[75, 77], [37, 40], [9, 12], [97, 100], [102, 104]], "long-forms": [[60, 73]], "ID": "3247"}, {"text": "semble learning. Ren et al (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised senti-", "acronyms": [[73, 75]], "long-forms": [[54, 71]], "ID": "3248"}, {"text": "Roget (RG) ablaze aglow alight argent auroral beaming blazing brilliant WordNet (WN) burnished sunny shiny lustrous undimmed sunshiny brilliant TransGraph (TG) nimble ringing fine aglow keen glad light picturesque Lin (LN) red yellow orange pink blue brilliant green white dark", "acronyms": [[156, 158], [219, 221], [81, 83], [7, 9]], "long-forms": [[144, 154], [214, 217], [72, 79], [0, 5]], "ID": "3249"}, {"text": "to five possible values (rules have been presented with the sentence pairs from which they have been acquired): entailment=yes (YES), i.e. correctness of the rule; entailment=more-phenomena (+PHEN), i.e.", "acronyms": [[128, 131], [192, 196]], "long-forms": [[123, 126], [180, 189]], "ID": "3250"}, {"text": "form is  employed to denote funct ion  i t se l f .   b) Conceptual  Phrase St ructure  (CPS) is  a data  s t ruc ture  in which syntact i c  and semant ic  in fo rma-  ", "acronyms": [[89, 92]], "long-forms": [[57, 86]], "ID": "3251"}, {"text": "2 D IA        1 In order to test this hypothesis,            1   a double-stranded oligonucleotide probe that corresponds to bp +10 to +60 of the CCR3 gene was prepared, 3 0 NN            referred to as E1-FL (exon 1- full length, Figure 2A). 3 D A    1 1   This is the exact sequence 3 D N          that was deleted in the CCR3(-exon1).pGL3 plasmid 3 D N          that demonstrated decreased activity 3 D N      1   compared to the full length 1.6 kb construct [27].", "acronyms": [[203, 208], [146, 150], [174, 176], [324, 328]], "long-forms": [[210, 229]], "ID": "3252"}, {"text": "VB (Verb) is a major source of confusion in automatic tagging.  It is collapsed with VB (Verb). ", "acronyms": [[85, 87], [0, 2]], "long-forms": [[89, 93], [4, 8]], "ID": "3253"}, {"text": "AB to letter l ji ? A where A is a regular letter alphabet and AB=A?{B} includes B as an abstract morpheme start symbol", "acronyms": [[63, 65]], "long-forms": [[66, 70]], "ID": "3254"}, {"text": "This paper describes a set of experiments on two sub-tasks of Quality Estimation of Machine Translation (MT) output.", "acronyms": [[105, 107]], "long-forms": [[84, 103]], "ID": "3255"}, {"text": " 1  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random FlySlip", "acronyms": [[58, 60], [81, 84], [105, 108], [116, 123]], "long-forms": [[37, 56], [61, 67]], "ID": "3256"}, {"text": "schematic representations of situations involving  various participants, props, and other conceptual  roles, each of which is a frame element (FE). ", "acronyms": [[143, 145]], "long-forms": [[128, 141]], "ID": "3257"}, {"text": "should not be allowed?.  Gay Marriage (GM) 5", "acronyms": [[39, 41]], "long-forms": [[25, 37]], "ID": "3258"}, {"text": "and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10", "acronyms": [[64, 67], [4, 9]], "long-forms": [[48, 55]], "ID": "3259"}, {"text": "(i) string kernels applied to sentences, or PTK applied to structural representations with and without embedded relational information (REL). This", "acronyms": [[136, 139], [44, 47]], "long-forms": [[112, 122]], "ID": "3260"}, {"text": "The scores are lowercased BLEU calculated on the held-out devtest set. NE = named entities. ", "acronyms": [[71, 73], [26, 30]], "long-forms": [[76, 90]], "ID": "3261"}, {"text": " Because LCS is a simple procedure, a second baseline based on Greedy String Tiling (GST) (Wise, 1996) was added after the evaluation pe-", "acronyms": [[85, 88], [9, 12]], "long-forms": [[63, 83]], "ID": "3262"}, {"text": " 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or", "acronyms": [[64, 68]], "long-forms": [[37, 62]], "ID": "3263"}, {"text": "explanations of these metrics are described below.  Symptom name recognition rate (RRdet),  recognition error rate (RERdet) and recognition ", "acronyms": [[83, 88], [116, 122]], "long-forms": [[65, 81], [92, 114]], "ID": "3264"}, {"text": "strategies for reducing ambiguity.  4.2.1 Longest ending filtering (LEF) The first approach to ambiguity reduction is based", "acronyms": [[68, 71]], "long-forms": [[42, 66]], "ID": "3265"}, {"text": "TV programs they watch.  Collaborative filtering (CF) (Resnick et al, 1994; Breese et al, 1998) and content-based (or", "acronyms": [[50, 52], [0, 2]], "long-forms": [[25, 48]], "ID": "3266"}, {"text": "(perhaps maller than traditional linguistic units) out of  component words: 1. noun group (NG), which consists  of a noun and its immediately preceding words (e.g., ", "acronyms": [[91, 93]], "long-forms": [[79, 89]], "ID": "3267"}, {"text": " The non-standard words in text are referred to as Out of Vocabulary (OOV) words. The nor-", "acronyms": [[70, 73]], "long-forms": [[51, 68]], "ID": "3268"}, {"text": "matic speech recognizers in Speech Normalized Orthographic Representation (SNOR) format, or from optical character recognition (OCR) output. For the", "acronyms": [[128, 131], [75, 79]], "long-forms": [[97, 126], [28, 73]], "ID": "3269"}, {"text": "web. We require parallel data to build a statistical machine translation (SMT) system that translates from German into Sim-", "acronyms": [[74, 77]], "long-forms": [[41, 72]], "ID": "3270"}, {"text": "creation (CR)  emotion (EM)  motion (MO)  perception (PC) ", "acronyms": [[37, 39], [10, 12], [24, 26], [54, 56]], "long-forms": [[29, 35], [0, 8], [15, 22], [42, 52]], "ID": "3271"}, {"text": "structure come in two different types: ? Grammatical Functions (GFs) indicate the relationship between the predicate and depen-", "acronyms": [[64, 67]], "long-forms": [[41, 62]], "ID": "3272"}, {"text": "Taalkommissie van die Suid-Afrikaanse Akademie vir Wetenskap en Kuns. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa. ", "acronyms": [[98, 103]], "long-forms": [[70, 96]], "ID": "3273"}, {"text": "run in parallel. This kind of parallelism is a good fit for the Same Instruction Multiple Thread (SIMT) hardware paradigm implemented by modern GPUs.", "acronyms": [[98, 102], [144, 148]], "long-forms": [[64, 96]], "ID": "3274"}, {"text": " Shirai, K., T. Tokunaga, and H. Tanaka: Automatic Extraction of Japanese Grammar f om  a Bracketed Corpus, in Natural Language Processing Pacific Rim Symposium(NLPRS'gs),  pp.", "acronyms": [[161, 169], [173, 175]], "long-forms": [[111, 160]], "ID": "3275"}, {"text": " 1 Introduction Multiword Expressions (MWEs) are commonly defined as ?", "acronyms": [[39, 43]], "long-forms": [[16, 37]], "ID": "3276"}, {"text": "8 Fully Supervised Model 53.7 70.8 61.1 53.0 70.3 60.4 52.0 69.6 59.6 53.1 70.5 60.6 Table 4: Results obtained using the MUC scoring program for the Broadcast News and Newswire data sets Broadcast News (BNEWS) Newswire (NWIRE) True Mentions System Mentions True Mentions System Mentions", "acronyms": [[203, 208], [220, 225], [121, 124]], "long-forms": [[187, 201], [210, 218]], "ID": "3277"}, {"text": "dominated by different Root Nodes. In this table, for each  possible Root Node category (RN), its corresponding Head  Node (HN), Dependent Nods/8 (DN) and Control Features (CF), ", "acronyms": [[89, 91], [124, 126], [147, 149], [173, 175]], "long-forms": [[69, 78], [112, 122], [129, 145], [155, 171]], "ID": "3278"}, {"text": "They used a statistical finite-state transducer (SFST) as a generative model and a support vector machine (SVM) and conditional random fields (CRF) as discrim-", "acronyms": [[107, 110], [49, 53], [143, 146]], "long-forms": [[83, 105], [12, 47], [116, 141]], "ID": "3279"}, {"text": " Acknowledgements  This work was supported by the Social Sciences and Humanities Research Council (SSHRC) of Canada. We ", "acronyms": [[99, 104]], "long-forms": [[50, 97]], "ID": "3280"}, {"text": "1992. 100 million words of  English: the British National Corpus (BNC). ", "acronyms": [[66, 69]], "long-forms": [[41, 64]], "ID": "3281"}, {"text": "1969)).  4 Ia the following \\] will use the term Unification Grammar (UG) aa hyperonym for  GPSG, LFG, FUG, IIPSG etc.,", "acronyms": [[70, 72], [92, 96], [98, 101], [103, 106], [108, 113]], "long-forms": [[49, 68]], "ID": "3282"}, {"text": "lated by the original Penn Treebank grammar to total PCFG surprisal calculated by the Nguyen et al (2012) Generalized Categorial Grammar (GCG). ", "acronyms": [[138, 141], [53, 57]], "long-forms": [[106, 136]], "ID": "3283"}, {"text": "1. Introduction Semantic role labeling (SRL) is the task of identifying arguments for a predicate and assigning semantically meaningful labels to them.", "acronyms": [[40, 43]], "long-forms": [[16, 38]], "ID": "3284"}, {"text": "special case describing whether/t /  is glottalized. A context other than preceding phoneme and following  phoneme is incorporated; the first split (nodes 0 and 10) in this tree is on syllable boundary (SYLL-BDRY),  indicating that when / t /  is glottalized it is generally in syllable-final position.", "acronyms": [[203, 212]], "long-forms": [[184, 201]], "ID": "3285"}, {"text": " 8SB = subject, OA = accusative object, OA2 = second accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal ob-", "acronyms": [[72, 74], [1, 4], [16, 18], [40, 43], [85, 87], [107, 109], [134, 136], [150, 152]], "long-forms": [[77, 83], [7, 14], [21, 38], [46, 70], [90, 105], [112, 132], [139, 148], [155, 165]], "ID": "3286"}, {"text": "notator. For brevity, we only considered PubMed as the source DB, and named entity recognition (NER)type annotations, which may be simply represented", "acronyms": [[96, 99]], "long-forms": [[70, 94]], "ID": "3287"}, {"text": "2 Textual Entailment for MT Evaluation 2.1 Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (", "acronyms": [[100, 102], [25, 27], [66, 68]], "long-forms": [[80, 98]], "ID": "3288"}, {"text": "? This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No.", "acronyms": [[96, 99], [62, 66]], "long-forms": [[67, 94]], "ID": "3289"}, {"text": "4.1 Preprocessing  HTML Page Parsing  The Document Object Model (DOM) is an application programming interface used for parsing ", "acronyms": [[65, 68], [19, 23]], "long-forms": [[42, 63]], "ID": "3290"}, {"text": "ments were annotated with word-level labels by professional translators using the core categories in MQM (Multidimensional Quality Metrics) 13", "acronyms": [[101, 104]], "long-forms": [[106, 138]], "ID": "3291"}, {"text": "distinct verbs, not occurrences. The seventh column shows the number of verbs that  were misclassified (MC)--the sum of false positives and false negatives. The eighth ", "acronyms": [[104, 106]], "long-forms": [[89, 102]], "ID": "3292"}, {"text": "ARG1, ARG2, MOD-BENEFICIARY, and MOD-TIME. To identify which slot has the most similarity  among its elements, we calculate the number of distinct elements (NDE) in each slot across the  propositions.", "acronyms": [[157, 160], [0, 4], [6, 10], [12, 27], [33, 41]], "long-forms": [[128, 155]], "ID": "3293"}, {"text": "5, we discuss our approach to increase the coverage of the model by using synset ID?s from the English WordNet (EWN). Section 6 describes our", "acronyms": [[112, 115], [81, 85]], "long-forms": [[95, 110]], "ID": "3294"}, {"text": "This paper discusses the automatic labeling of semantic relations in nominalized noun phrases (NPs) using a support vector machines learning algorithm.", "acronyms": [[95, 98]], "long-forms": [[81, 93]], "ID": "3295"}, {"text": "H A V E  CSEXCH F O R  MOVEF IN 11  C H A N G E ,  CALL EL3MOP P O R  E R A S E  0 I 2   ANTEST CALLED FOR 14'IGLOT \" (AACC) S D =  15. RES= '3.", "acronyms": [[103, 106], [119, 123], [125, 128], [9, 15], [16, 21], [23, 28], [136, 139]], "long-forms": [[89, 102], [107, 115]], "ID": "3296"}, {"text": "presented at the 16th International Conference on Computational Linguistics, Copenhagen. Gupta, D., Saul, M., & Gilbertson, J. (2004). Evaluation of a deidentification (DE-ID) software engine to share pathology reports and clinical documents for research. Am J Clin Pathol, 121(2), 176-186.", "acronyms": [[169, 174]], "long-forms": [[151, 167]], "ID": "3297"}, {"text": "come. 2008. Deciding strictly local (SL) languages.", "acronyms": [[37, 39]], "long-forms": [[21, 35]], "ID": "3298"}, {"text": " More recently, i2b2 organizers also reported a  Maximum Entropy (ME) based approach for the  2009 challenge (Halgrim, Xia, Solti, Cadag, & ", "acronyms": [[66, 68], [16, 20]], "long-forms": [[49, 64]], "ID": "3299"}, {"text": "Abstract In this work we present results from using Amazon?s Mechanical Turk (MTurk) to annotate translation lexicons between English", "acronyms": [[78, 83]], "long-forms": [[61, 76]], "ID": "3300"}, {"text": "inference procedures?Markov Chain Monte Carlo (MCMC) (Johnson et al 2012), Expectation Maximization (EM), and Variational Bayes (VB)10?as well as the discourse-level model described above.", "acronyms": [[129, 131], [47, 51], [101, 103]], "long-forms": [[110, 127], [21, 45], [75, 99]], "ID": "3301"}, {"text": "of binary relations that vary in length and since a  question representation (SRq) can be answered by a  sentence candidate (SRc) that includes more information  than the question specified, the Arity constraint i~ revised ", "acronyms": [[125, 128], [78, 81]], "long-forms": [[105, 123], [53, 76]], "ID": "3302"}, {"text": "the percentage of questions with a correct answer at rank 1, Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP). The reported", "acronyms": [[117, 120], [83, 86]], "long-forms": [[93, 115], [61, 81]], "ID": "3303"}, {"text": "The major categories of components included in these capabilities include: Sentence Splitter, Phrase Chunker, Tokenizer, Part-of-Speech (POS) Tagger, Shallow Parser, Name Entity Recognizer (NER), Coreference Solution, etc.", "acronyms": [[190, 193], [137, 140]], "long-forms": [[166, 188], [121, 135]], "ID": "3304"}, {"text": "PCFG score (SP ) 49.5 50.0 TSG score (ST ) 49.5 49.7 Charniak score (SC) 50.0 50.0 l + S3 61.0 64.3", "acronyms": [[69, 71], [0, 4], [12, 14], [27, 30], [38, 40]], "long-forms": [[62, 67]], "ID": "3305"}, {"text": "In order to solve idiomatic expressions as well as  collocations and frozen compound nouns, we  have developed the compound unit(CU)  recognizer (Jung et.", "acronyms": [[129, 131]], "long-forms": [[115, 127]], "ID": "3306"}, {"text": "Results on the dev set using two metrics: instance classification accuracy (CA), and soundbite name  recognition accuracy (RA). The oracle RA is 79.1%.", "acronyms": [[123, 125], [76, 78], [139, 141]], "long-forms": [[101, 121], [51, 74]], "ID": "3307"}, {"text": "The representative vectors for each phoneme category consist of the mean vectors of the Gaussian Mixture Model (GMM). ", "acronyms": [[112, 115]], "long-forms": [[88, 110]], "ID": "3308"}, {"text": "tion of them is also given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the relative clause pronoun (RclSubjA).", "acronyms": [[105, 110], [40, 43], [57, 60], [80, 85], [161, 169]], "long-forms": [[88, 103], [28, 38], [46, 55], [63, 78], [121, 159]], "ID": "3309"}, {"text": "representing natural language in a traversable graph, composed of propositions and their semantic interrelations ? A Propositional Knowledge Graph (PKG). The resulting structure provides a representation", "acronyms": [[148, 151]], "long-forms": [[117, 146]], "ID": "3310"}, {"text": " 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilis-", "acronyms": [[49, 52]], "long-forms": [[26, 47]], "ID": "3311"}, {"text": "of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999;", "acronyms": [[217, 220]], "long-forms": [[188, 215]], "ID": "3312"}, {"text": "used as the seed to build a reliable hedge cue set. Maximum Entropy (MaxEnt) model is used as the learning technique to", "acronyms": [[69, 75]], "long-forms": [[52, 67]], "ID": "3313"}, {"text": "Tokens (T) .72 / .77 .83 / .84 .72 / .76 .85 / .84 .82 / .84 .88 / .86 Named Entities (NE) .75 / .80 .84 / .79 .75 / .77 .85 / .78 .89 / .78 .89 / .73 NE targeted (NE-T) .54 / .55 .49 / .47 .66 / .64 .60 / .57 .64 / .64 .57 / .58 Host (H) .72 / .57 .64 / .48 .67 / .51 .58 / .41 .67 / .63 .59 / .55", "acronyms": [[164, 168]], "long-forms": [[151, 162]], "ID": "3314"}, {"text": "Abstract This paper suggests two ways of improving semantic role labeling (SRL). First, we intro-", "acronyms": [[75, 78]], "long-forms": [[51, 73]], "ID": "3315"}, {"text": "studied three activation functions: 1. Rectified linear units (ReLUs) (Nair and Hinton, 2010):", "acronyms": [[63, 68]], "long-forms": [[39, 61]], "ID": "3316"}, {"text": " One of the most competitive summarization methods is based on Integer Linear Programming (ILP). ", "acronyms": [[91, 94]], "long-forms": [[63, 89]], "ID": "3317"}, {"text": "2 Classifiers 2.1 NB Naive Bayes(NB) probabilistic classifiers are commonly studied in machine learning(Mitchell, 1996).", "acronyms": [[33, 35]], "long-forms": [[21, 31]], "ID": "3318"}, {"text": " To adapt for Chinese phonetic rule, we divide the  continuous CLs into independent CLs(IC) and  divide structure of CL+VL+CL into CL+VL and ", "acronyms": [[88, 90], [63, 66], [117, 119], [120, 122], [123, 125], [131, 133], [134, 136]], "long-forms": [[72, 86]], "ID": "3319"}, {"text": "against the human annotation.  2.3 Distributed Tree Kernel (DTK) Distributed Tree Kernel (DTK) (Zanzotto and", "acronyms": [[60, 63], [90, 93]], "long-forms": [[35, 58], [65, 88]], "ID": "3320"}, {"text": "and reranking for the statistical parsing of spanish. In Proceedings of Human Language Technology (HLT) and the Conference on Empirical Methods in Natural", "acronyms": [[99, 102]], "long-forms": [[72, 97]], "ID": "3321"}, {"text": "3.1 Data We use the TiGer treebank release 2.2 (TIGER), and the NeGra treebank (NEGRA). For TIGER,", "acronyms": [[80, 85], [48, 53], [92, 97]], "long-forms": [[64, 78], [20, 34]], "ID": "3322"}, {"text": " As an example, consider a user who is looking for information on digital video recorders (DVR), in particular, on how she can use a DVR with a", "acronyms": [[91, 94], [133, 136]], "long-forms": [[66, 89]], "ID": "3323"}, {"text": "Section 2 discusses related work. Section 3 introduces the Condition Random Fields(CRFs)  and the defined Long-Dependency CRFs ", "acronyms": [[83, 87], [122, 126]], "long-forms": [[59, 82]], "ID": "3324"}, {"text": " 2 American National Corpus The American National Corpus (ANC) project (Ide and Macleod, 2001; Ide and Suderman, 2004) has", "acronyms": [[58, 61]], "long-forms": [[32, 56]], "ID": "3325"}, {"text": "dhi/NEP road/NEL. The structure of the tagged  element using the Shakti Standard Format (SSF)5  will be as follows: ", "acronyms": [[89, 92], [0, 7], [8, 16]], "long-forms": [[65, 87]], "ID": "3326"}, {"text": " We observe the following: First, pre-enrollment reviews have noun phrases(NP) that contain fewer leaf nodes than in the post-enrollment reviews.", "acronyms": [[75, 77]], "long-forms": [[62, 73]], "ID": "3327"}, {"text": "sity is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).", "acronyms": [[123, 128], [52, 56]], "long-forms": [[95, 121]], "ID": "3328"}, {"text": "as director of Pentagon telecom~~nicntions and commc~nd :~nd control systcms.  Requests for coments on a proposed Federal information processing -t.lndard (PII'S)  for the National Communications System have been requested by .Innuor).", "acronyms": [[156, 161]], "long-forms": [[105, 144]], "ID": "3329"}, {"text": "CN Bank(CNB): 200,000 samples  ? TFN Bank(TFNB): 38,769 samples  CPN Bank(CPNB): 17,637 samples ", "acronyms": [[42, 46], [8, 11], [74, 78]], "long-forms": [[33, 40], [0, 7], [65, 73]], "ID": "3330"}, {"text": "Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of longest span entering nodes.", "acronyms": [[123, 127]], "long-forms": [[98, 121]], "ID": "3331"}, {"text": " ? Target types per Source type (TpS), i.e. the number of target types a specific source type", "acronyms": [[33, 36]], "long-forms": [[10, 26]], "ID": "3332"}, {"text": "init ial ly be described. I wil l  c laim that  such an initial descr ipt ion (ID) is  cr it ical  to both model synthesis and ", "acronyms": [[79, 81]], "long-forms": [[56, 69]], "ID": "3333"}, {"text": " The implemented system has three main modules: the Feature Extractor (FE), the Generalized Iterative Scaling (GIS), and the Classifica-", "acronyms": [[71, 73], [111, 114]], "long-forms": [[52, 69], [80, 109]], "ID": "3334"}, {"text": " Figure 2 shows the learning curve for pseudoprojective parsing (P-Proj), compared to using only projectivized training data (Proj), measured as error", "acronyms": [[65, 71], [126, 130]], "long-forms": [[45, 63]], "ID": "3335"}, {"text": "1(c)) for each k. 3.3.2 PLEB PLEB (Point Location in Equal Balls) was first proposed by Indyk and Motwani (1998) and further", "acronyms": [[29, 33], [24, 28]], "long-forms": [[35, 64]], "ID": "3336"}, {"text": "CoTrain vs. LEX(CN) 9.53E-10 7.15E-11 1.17E-09 CoTrain vs. LEX(EN) 1.87E-05 1.64E-05 8.92E-07 CoTrain vs. SVM(CN) 5.7E-08 2.91E-09 2.27E-11 CoTrain vs. SVM(EN) 3.74E-15 5.77E-17 1.18E-20", "acronyms": [[110, 112], [12, 15], [16, 18], [59, 62], [63, 65], [152, 155], [156, 158]], "long-forms": [[94, 101]], "ID": "3337"}, {"text": " 66 adopt a minimum Bayes risk (MBR) approach, with the goal of finding the graph with the lowest", "acronyms": [[32, 35]], "long-forms": [[12, 30]], "ID": "3338"}, {"text": "Abstract Data sparsity is one of the main factors that make word sense disambiguation (WSD) difficult.", "acronyms": [[87, 90]], "long-forms": [[60, 85]], "ID": "3339"}, {"text": "aKemp|es  de  t. raL~uct lon  du  russe  eD f ranca ls  rea l  i s le  par   le  G~TA a Grenob le ,   MUTS-CLES:  TAO ( t raduct ion  ass i s t ,  co  par  o rd lnateur )e   ana lyse  morpno l  og ique~ t rans fer t  lex lca l~ 0enerat  1o~ ", "acronyms": [[114, 117], [81, 85], [102, 111]], "long-forms": [[120, 168]], "ID": "3340"}, {"text": "Adjoining Grammars as the compilation of a  more abstract and modular layer of linguistic  description : the  metagrammar (MG). MG ", "acronyms": [[123, 125], [128, 130]], "long-forms": [[110, 121]], "ID": "3341"}, {"text": "This paper presents a web application and a web service for the diagnostic evaluation of Machine Translation (MT). These web-based", "acronyms": [[110, 112]], "long-forms": [[89, 108]], "ID": "3342"}, {"text": "require. We solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a PAS.", "acronyms": [[78, 82]], "long-forms": [[48, 76]], "ID": "3343"}, {"text": "In (Frasconi et al., 2004) we introduced declarative kernels (DK) as a set of kernels working on mereotopological", "acronyms": [[62, 64]], "long-forms": [[41, 60]], "ID": "3344"}, {"text": "ing default parameters. Error analysis was done by means of Mean Squared Error estimate (MSE). ", "acronyms": [[89, 92]], "long-forms": [[60, 78]], "ID": "3345"}, {"text": " 5 Conclusions and Future Work Distributed Smoothed Trees (DST) are a novel class of Compositional Distributional Semantics Models (CDSM) that effectively encode structural information and distributional semantics in tractable 2-", "acronyms": [[59, 62], [132, 136]], "long-forms": [[31, 57], [85, 129]], "ID": "3346"}, {"text": " MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numerical and time information.", "acronyms": [[68, 71], [1, 4], [22, 25], [43, 46]], "long-forms": [[52, 60], [11, 20], [28, 41]], "ID": "3347"}, {"text": "Keyword 0.168 0.168 0.157 6.3 Table 3: Title quality as compared to the reference for the hierarchical discriminative (HD), flat discriminative (FD), hierarchical generative (HG), flat", "acronyms": [[119, 121], [145, 147], [175, 177]], "long-forms": [[90, 117], [124, 143], [150, 173]], "ID": "3348"}, {"text": " 5.4 Nonshared Concept Activation with  No Identif ication Intention (NSNI). ", "acronyms": [[70, 74]], "long-forms": [[40, 68]], "ID": "3349"}, {"text": "The first step is the identification of the presence of a graphical image in the web page by a  Browser Helper Object (BHO) (Elzer et al., 2007).", "acronyms": [[119, 122]], "long-forms": [[96, 117]], "ID": "3350"}, {"text": "to efficiently implement their computation.  In Natural Language Processing (NLP), the typical dimensionality of databases, which are made", "acronyms": [[77, 80]], "long-forms": [[48, 75]], "ID": "3351"}, {"text": "? iyi = 0 This is a quadratic programming (QP) problem and we can always find the global maximum of", "acronyms": [[43, 45]], "long-forms": [[20, 41]], "ID": "3352"}, {"text": "result. We have used the symbol Comp in that case (e.g., if  ANTI (A)=B and CONV(B)=C, then the relation result-  ing from the composition is simply ANTI(CONV(A))----C).", "acronyms": [[61, 65], [149, 153], [76, 80]], "long-forms": [], "ID": "3353"}, {"text": "Sofa. The viewer makes use the open-source library Java Speech Toolkit (JSTK)5. ", "acronyms": [[72, 76]], "long-forms": [[51, 70]], "ID": "3354"}, {"text": "scoring the candidate set of which class of anaphoric expression (DNOM = definite  NP, PER{I,2,3} = first/second/third person pronouns, POS{1,2,3} = first/second/third  person possessives, RELA = relative pronouns, REFL = reflexive/reciprocal pronouns). ", "acronyms": [[189, 193], [215, 219], [66, 70], [83, 85], [87, 90], [136, 139]], "long-forms": [[196, 204], [222, 231], [73, 81]], "ID": "3355"}, {"text": "provided for the slots over the course of the dialog.  These are our String Consistency (SC) features. ", "acronyms": [[89, 91]], "long-forms": [[69, 87]], "ID": "3356"}, {"text": "even for very high-dimensional data.  4.2 Open Directory Project (ODP) Open Directory Project (ODP)7 is a multilingual", "acronyms": [[66, 69], [95, 98]], "long-forms": [[42, 64], [71, 93]], "ID": "3357"}, {"text": "2007. CRFsuite: a fast implementation of conditional random fields (CRFs). ", "acronyms": [[68, 72], [6, 14]], "long-forms": [[41, 66]], "ID": "3358"}, {"text": "4.1 English training sets To train an SVO metaphor classifier, we employ the TroFi (Trope Finder) dataset. ", "acronyms": [[77, 82], [38, 41]], "long-forms": [[84, 96]], "ID": "3359"}, {"text": "Step 1  Tagging using Global Distribution (NEIG) Trained Model Statistical System (MEMM) Step 2 MEMM Based Statistical System(S-MEMM)Final Tagged DataSet Added    as a feature", "acronyms": [[126, 132], [43, 47], [83, 87], [146, 153]], "long-forms": [[89, 100]], "ID": "3360"}, {"text": "eal papers. Ill PTvcccdings of thc Pac'lific Sym-  posium on Biocomp'uting'98 (PSB'98), .Jan-  1uAYy.", "acronyms": [[79, 85], [16, 27]], "long-forms": [[51, 77]], "ID": "3361"}, {"text": "Preliminary experimental data indicates that the approach is likely to be successful. The major benefit of the approach is that it makes extending pronunciation lexi-cons accessible to average users. 1 Introduction Speech recognition (SR) systems use pronuncia-tion lexicons to map words into the phoneme-like units used for acoustic modeling. Text-to-speech (TTS) systems also make use of pronunciation lexicons, both internally and as ?", "acronyms": [[235, 237], [360, 363]], "long-forms": [[215, 233], [344, 358]], "ID": "3362"}, {"text": "? All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).", "acronyms": [[60, 64], [92, 95]], "long-forms": [[50, 58], [82, 90]], "ID": "3363"}, {"text": "Head and Obj3 and the counts f(gf, fe) of occurrences of the grammatical functions together with the roles DEGREE (DEG), THEME (THM), DEPICTIVE (DEP) and LOCATION (LOC).", "acronyms": [[128, 131], [9, 13], [115, 118], [145, 148], [164, 167]], "long-forms": [[121, 126], [107, 113], [134, 143], [154, 162]], "ID": "3364"}, {"text": "1.1 A System Exhibiting Reinforcement Learning The central motivation for building this dialogue system is as a platform for Reinforcement Learning (RL) experiments.", "acronyms": [[149, 151]], "long-forms": [[125, 147]], "ID": "3365"}, {"text": " In Proceedings of the 17th International Conference on Computational Linguistics (COLING), Montreal, Canada.", "acronyms": [[83, 89]], "long-forms": [[56, 81]], "ID": "3366"}, {"text": "4.4.2 Optimization To maximize the objective in (6), we employ a stochastic gradient descent (SGD) algorithm (Rendle et al, 2009).", "acronyms": [[94, 97]], "long-forms": [[65, 92]], "ID": "3367"}, {"text": "226  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304?313, Seoul, South Korea, 5-6 July 2012.", "acronyms": [[101, 108]], "long-forms": [[51, 99]], "ID": "3368"}, {"text": " ? True Positive (TP), the predicted e was correctly referred to by s.   ", "acronyms": [[18, 20]], "long-forms": [[3, 16]], "ID": "3369"}, {"text": "Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti,  Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on  Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.  ", "acronyms": [[210, 214], [147, 151]], "long-forms": [[166, 208]], "ID": "3370"}, {"text": "Probable PR+ (probable) PR? ( not probable) [NA] Possible PS+ (possible) PS? ( not certain) [NA]", "acronyms": [[58, 61]], "long-forms": [[63, 71]], "ID": "3371"}, {"text": "numeroinen se on (UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance).", "acronyms": [[63, 65], [18, 20], [107, 109]], "long-forms": [[68, 77], [23, 32], [112, 128]], "ID": "3372"}, {"text": "review  ? Product feature level (PFL)  ?", "acronyms": [[33, 36]], "long-forms": [[10, 31]], "ID": "3373"}, {"text": "mathematical models from experimental data. The  algorithm is similar to Genetic Programming (GP),  but uses fixed-length character strings (called chro-", "acronyms": [[94, 96]], "long-forms": [[73, 92]], "ID": "3374"}, {"text": "similar to how Iida et al (2011) computed features to present to their classifier: namely Ling (linguistic features), TaskSp (task specific features), and Gaze (from SV only).", "acronyms": [[118, 124], [166, 168]], "long-forms": [[126, 139]], "ID": "3375"}, {"text": "compared is a familiar problem from the fields of information retrieval (IR), text mining (TM), textual data analysis (TDA) and natural language processing (NLP) (Lebart and Rajman, 2000).", "acronyms": [[119, 122], [73, 75], [91, 93], [157, 160]], "long-forms": [[96, 117], [50, 71], [78, 89], [128, 155]], "ID": "3376"}, {"text": " Examples of lexical and contextual rules learned by  the Brill tagger.  NNP = proper noun, CD = cardinal number,  CC = coordinating conjunction, JJ = adjective, VBG = verb, ", "acronyms": [[92, 94], [73, 76], [115, 117], [146, 148], [162, 165]], "long-forms": [[97, 105], [79, 90], [120, 144], [151, 160], [168, 172]], "ID": "3377"}, {"text": "tice. They found that a domain-specific corpus performs better than a Wall Street Journal (WSJ) corpus for the trigram LM.", "acronyms": [[91, 94]], "long-forms": [[70, 89]], "ID": "3378"}, {"text": "the parameters. We trained our network with stochastic gradient descent (SGD), mini-batches and adagrad updates (Duchi et al, 2011), using", "acronyms": [[73, 76]], "long-forms": [[44, 71]], "ID": "3379"}, {"text": " 4.4 SLU Features The SLU (Spoken Language Understanding) features are used to resolve implicit and explicit REs.", "acronyms": [[22, 25], [5, 8], [109, 112]], "long-forms": [[27, 56]], "ID": "3380"}, {"text": "ment. In Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci), Sapporo. ", "acronyms": [[82, 88]], "long-forms": [[55, 80]], "ID": "3381"}, {"text": "carded in LSI-based approaches. We dub our model ONETA (OrthoNormal Explicit Topic Analysis) and empirically show that on a cross-lingual retrieval", "acronyms": [[49, 54], [10, 13]], "long-forms": [[56, 91]], "ID": "3382"}, {"text": " ? Lexical Overlap and Length (LO): This set of features represents the lexical overlap between", "acronyms": [[31, 33]], "long-forms": [[3, 18]], "ID": "3383"}, {"text": " 1997) has led us to employ, among other param-  eters, mutual information (MI) bits of individ-  ual characters derived from large hierarchically ", "acronyms": [[76, 78]], "long-forms": [[56, 74]], "ID": "3384"}, {"text": " ? Shift(SH): Push NEXT onto the stack. ", "acronyms": [[9, 11], [19, 23]], "long-forms": [[3, 8]], "ID": "3385"}, {"text": "categories of both test texts  PoS tags  DET (Determiner)  NM (Pronoun) ", "acronyms": [[41, 44], [59, 61]], "long-forms": [[46, 56]], "ID": "3386"}, {"text": "Pro = percent of the words as pronominals. WPS = Words per sentence. 6LTR = percent of words that are longer than 6 letters.", "acronyms": [[43, 46], [70, 73]], "long-forms": [[49, 67]], "ID": "3387"}, {"text": "BLEU-4 (Papineni et al, 2002) used in the two  experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation.", "acronyms": [[118, 122], [0, 6]], "long-forms": [[97, 116]], "ID": "3388"}, {"text": "71 tweets, called T-NER, is presented which employs Conditional Random Fields (CRF) for named entity segmentation and labelled topic modelling for", "acronyms": [[79, 82], [18, 23]], "long-forms": [[52, 77]], "ID": "3389"}, {"text": "cal search. We examine this by using three different parsers in phase 0: (i) MS (Marecek and Straka, 657", "acronyms": [[77, 79]], "long-forms": [[81, 99]], "ID": "3390"}, {"text": "that are of interest o specific users. An example of IE is the  Named Ent i ty  (NE) task, which has become established  as the important first step in many other IE tasks, provid- ", "acronyms": [[81, 83], [53, 55], [163, 165]], "long-forms": [[64, 78]], "ID": "3391"}, {"text": "State (STT) 42.85 76.93 Gender (GEN) 67.42 84.17 Determiner (DET) 59.71 85.41 Number (NUM) 70.61 87.31", "acronyms": [[61, 64]], "long-forms": [[49, 59]], "ID": "3392"}, {"text": "To model the relations between objects and verbs, we follow the data preparation in (Le et al 2013), using the British National Corpus (BNC) which has been preprocessed and parsed using TreeTagger and", "acronyms": [[136, 139]], "long-forms": [[111, 134]], "ID": "3393"}, {"text": "larities of MSA. ARET has two subparts tools : the  Arabic Reading Facilitation Tool (ARFT) and the  Arabic Reading Assessment Tool (ARAT).", "acronyms": [[86, 90], [17, 21], [12, 15], [133, 137]], "long-forms": [[52, 84], [101, 131]], "ID": "3394"}, {"text": "The rules, which may be different for each  dictionary, are written using a formalism in the spirit of  the \"definite clause grammars\" (DCG's) of Pereira and  Warren (1980) and \"modular logic grammars\" (MLG's) ", "acronyms": [[136, 141], [203, 208]], "long-forms": [[109, 133], [178, 200]], "ID": "3395"}, {"text": "al., 2005) to quickly find possible answers, given the relational conjunction (RC) of the question. ", "acronyms": [[79, 81]], "long-forms": [[55, 77]], "ID": "3396"}, {"text": "11 Figure 4: Variation of Average Information Processing Indices(IPI) for Video 4-6 Figure 5: Variation of Average Information Processing Indices(IPI) for the full course the last week, i.e, students who do not finish the", "acronyms": [[146, 149], [65, 68]], "long-forms": [[115, 144], [34, 64]], "ID": "3397"}, {"text": "also included as features.  Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al (2007) to", "acronyms": [[56, 60]], "long-forms": [[28, 54]], "ID": "3398"}, {"text": "****I TRANSFORPlATICNS **SIW:  S C A N  C-ALLED AT 1 I  ANTFST CALLED FOR SrqSYLLA B (AACC) ,SD= 6 .  RES= 1 1 .", "acronyms": [[70, 73], [93, 95], [86, 90]], "long-forms": [[56, 69]], "ID": "3399"}, {"text": "Thus, it is necessary to define a proper ECS for each  language: Japanese ECS (J-ECS) \\[6\\] for Japanese language and  English ECS (E-ECS) \\[7\\] for English language. In the translation ", "acronyms": [[132, 137], [41, 44], [79, 84]], "long-forms": [[119, 130], [65, 77]], "ID": "3400"}, {"text": "line debates. In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM)@ EACL, pages 35?43.", "acronyms": [[88, 92], [95, 99]], "long-forms": [[52, 86]], "ID": "3401"}, {"text": "The prefer-  ence score (PS) for a pair is determined by the ratio  of its local dominance count (LDC)--the total num-  ber of cases in which the pair is locally dominant--to ", "acronyms": [[98, 101], [25, 27]], "long-forms": [[75, 96], [4, 23]], "ID": "3402"}, {"text": "the parameters ? = (s,W,b,x) via backpropagation with stochastic gradient descent (SGD). ", "acronyms": [[83, 86]], "long-forms": [[54, 81]], "ID": "3403"}, {"text": "mapping all non-core argument labels in the guessed and correct labelings to NONE.  Coarse Modifier Argument Measures (COARSEARGM). Sometimes it is sufficient to", "acronyms": [[119, 129], [77, 81]], "long-forms": [[84, 117]], "ID": "3404"}, {"text": "137 pare our system with a non-sequential classifier,  a support vector machine (SVM), with the same  settings as those described above.", "acronyms": [[81, 84]], "long-forms": [[57, 79]], "ID": "3405"}, {"text": "grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) Figure 3: Plots of imageability scores for literal vs. nonliteral/metaphorical words in the VUAMC, grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) clearest distinction between literal vs. nonliteral items.", "acronyms": [[223, 225], [39, 41], [177, 182]], "long-forms": [[226, 236], [30, 37], [42, 52], [56, 60], [64, 68], [72, 83], [214, 221], [240, 244], [248, 252], [256, 267]], "ID": "3406"}, {"text": "email: elenimi@linc.cis.upenn.edu Jerry R. Hobbs University of Southern California (USA) email: hobbs@isi.edu", "acronyms": [[84, 87]], "long-forms": [[49, 82]], "ID": "3407"}, {"text": "Re-moving PHI from these free text portions requires application of techniques from natural language processing that are capable of identifying phrases of specific types based on the lexical content (the words that make up the phrases) and the surround-ing words.  3 Current Methods and Metrics Fortunately, the problem of identifying types of information in free text is a well-studied problem in the natural language processing community. We can leverage several decades of research on infor-mation extraction and the named entity identifica-tion problem in particular, including multiple community evaluations such as the Message Un-derstanding Conferences (MUC) (Grishman & Sundheim, 1996) and the subsequent Automated Content Extraction (ACE) evaluations1 ? both fo-cused on extraction from newswire -- as well as evaluations of biomedical entity extraction from the published literature e.g., in the BioCreative evaluations (Krallinger, et al, 2008).", "acronyms": [[743, 746]], "long-forms": [[713, 741]], "ID": "3408"}, {"text": "Electronic Text Encoding and Interchange were first published in April 1994 and were initially based on Standard Generalized Markup Language (SGML).  The", "acronyms": [[142, 146]], "long-forms": [[104, 140]], "ID": "3409"}, {"text": "Semantic Computing Group Cognitive Interaction Technology ? Center of Excellence (CITEC), Bielefeld University, Germany", "acronyms": [[82, 87]], "long-forms": [[60, 80]], "ID": "3410"}, {"text": "C STORE SPEECH WAVE POINT  C  NBUF (NPT) =IFIX(YN)  750 CONTINUE ", "acronyms": [[36, 39], [47, 49]], "long-forms": [[30, 34]], "ID": "3411"}, {"text": "exist solely to guide processing. These words, known  as function words (FW's), are quite common, and  include articles, prepositions, and auxiliary verbs.", "acronyms": [[73, 77]], "long-forms": [[57, 71]], "ID": "3412"}, {"text": "Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein,  1998), latent semantic analysis (LSA) (Gong and  Liu, 2001).", "acronyms": [[115, 118], [50, 53]], "long-forms": [[89, 113], [22, 48]], "ID": "3413"}, {"text": " - 19  -  Both L I and L 2 are CSL's (Context sensitive languages). They ", "acronyms": [[31, 36]], "long-forms": [[38, 65]], "ID": "3414"}, {"text": "In order to make it man-  ageable and intuitive, it employs yntactic constructs  called Typed Feature Structures (TFSs). The ,~vocab- ", "acronyms": [[114, 118]], "long-forms": [[88, 112]], "ID": "3415"}, {"text": " 4.1 Representing Transformations as FSTs Finite State Transducers (FSTs) provide a natural formalism for representing output transformations.", "acronyms": [[68, 72], [37, 41]], "long-forms": [[42, 66]], "ID": "3416"}, {"text": "following formulas.  PPe = Pe(Se)? ", "acronyms": [[21, 24]], "long-forms": [[27, 32]], "ID": "3417"}, {"text": "For each disjunetiort in indef.\"  Let compatible-disjuncts = CHECK-DISJ (disjunction, cond). ", "acronyms": [[61, 71]], "long-forms": [[73, 84]], "ID": "3418"}, {"text": "al., 2005) filters and summarizes the OmniPage output into Intermediate XML (IXML), as well as correcting certain characteristic errors from that stage.", "acronyms": [[77, 81]], "long-forms": [[59, 75]], "ID": "3419"}, {"text": "and only these. The possible binary tree structures of ABCD are covered by  ABCD = A(BCD) U (AB)(CD) U (ABC)D.  Since we are to handle binary concatenations only, we consider two concatenations ", "acronyms": [[76, 80], [55, 59]], "long-forms": [[83, 88]], "ID": "3420"}, {"text": "selection (FS). For word-level prediction, generalised linear models (GLM) (Collins, 2002) and GLM with dynamic learning", "acronyms": [[70, 73], [11, 13], [95, 98]], "long-forms": [[43, 68]], "ID": "3421"}, {"text": "knowledge powered model to several baselines.  Random Guess Model (RG). Random guess is", "acronyms": [[67, 69]], "long-forms": [[47, 59]], "ID": "3422"}, {"text": "the natural (CC natural) and strong (CC strong)  levels; and (b) advanced level texts from a popular  science magazine called Ci?ncia Hoje (CH). Table ", "acronyms": [[140, 142], [13, 15], [37, 39]], "long-forms": [[126, 138]], "ID": "3423"}, {"text": "puts to a system and the outputs it is intended to produce. In Machine Translation (MT), such resources take the form of sentence-aligned parallel corpora of", "acronyms": [[84, 86]], "long-forms": [[63, 82]], "ID": "3424"}, {"text": "tleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing.", "acronyms": [[131, 136]], "long-forms": [[114, 129]], "ID": "3425"}, {"text": "Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen {v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl Center for Language and Cognition Groningen (CLCG) University of Groningen, The Netherlands", "acronyms": [[163, 167]], "long-forms": [[118, 161]], "ID": "3426"}, {"text": "BP = log (Prob(N2 | N1)) (2) ? Pointwise Mutual Information (PMI): Defined as a measure of how collocated two words are,", "acronyms": [[61, 64], [0, 2]], "long-forms": [[31, 59], [5, 28]], "ID": "3427"}, {"text": " 1 Introduction The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), based on the Rhetorical Struc-", "acronyms": [[44, 50]], "long-forms": [[20, 42]], "ID": "3428"}, {"text": "This paper is an at tempt  to provide part  of the basis for a general theory of robust  process ing in Machine Trans lat ion  (MT) wi th  relevance to other areas of Natural  Language ", "acronyms": [[128, 130]], "long-forms": [[104, 125]], "ID": "3429"}, {"text": "Diacritization evaluation of our experiments is  reported in terms of word error rate (WER), and  diacritization error rate (DER)5. ", "acronyms": [[125, 128], [87, 90]], "long-forms": [[98, 123], [70, 85]], "ID": "3430"}, {"text": "Other functions such as textual element (ET), sentence adjunct (AO), negation (NEG), vocative (VOC) and verb modifiers (MOD) were tagged, but did not receive", "acronyms": [[79, 82], [95, 98], [41, 43], [64, 66], [120, 123]], "long-forms": [[69, 77], [85, 93], [32, 39], [109, 117], [46, 62]], "ID": "3431"}, {"text": "Parse tree of tagged sentence in Box 1  3 Geographic Information Retrieval  3.1 Propositional logic of context (PLC)  As previously discussed, candidate named entities ", "acronyms": [[112, 115]], "long-forms": [[80, 110]], "ID": "3432"}, {"text": "tiveness of rule-based (Zilka et al., 2013), maximum entropy (MaxEnt) (Lee and Eskenazi, 2013) and deep neural network (DNN) (Henderson et al.,", "acronyms": [[62, 68]], "long-forms": [[49, 60]], "ID": "3433"}, {"text": " This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past", "acronyms": [[92, 95], [127, 130]], "long-forms": [[57, 90], [101, 125]], "ID": "3434"}, {"text": "2 Background  2.1 Gene Expression Programming  Gene Expression Programming (GEP), first introduced by (Ferreira 2001), is an evolutionary algo-", "acronyms": [[76, 79]], "long-forms": [[47, 74]], "ID": "3435"}, {"text": "of Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). The data", "acronyms": [[132, 136], [71, 74], [87, 90], [107, 110]], "long-forms": [[117, 130], [63, 69], [77, 85], [93, 105]], "ID": "3436"}, {"text": "types through various features based on e.g., partof-speech tagging (POS) and named entity recognition (NER). ", "acronyms": [[104, 107], [69, 72]], "long-forms": [[78, 102], [46, 59]], "ID": "3437"}, {"text": "cies and percentages of which are given in Table 1 (where the letters in example (3)  correspond to the letters in the table). Example (3a) uses a to infinitive form (TNF). ", "acronyms": [[167, 170]], "long-forms": [[147, 165]], "ID": "3438"}, {"text": "five tasks) representing fine-grained bio-IE.  2.1 Genia task (GE) The GE task (Kim et al, 2011) preserves the task", "acronyms": [[63, 65], [71, 73], [38, 45]], "long-forms": [[51, 56]], "ID": "3439"}, {"text": "evaluation metrics. Legend: d = dependency f-score, _pr =  predicate-only f-score, M = METEOR, WN = WordNet,  H_FL = human fluency score, H_AC = human accuracy ", "acronyms": [[95, 97], [53, 55], [110, 114], [138, 142]], "long-forms": [[100, 107], [32, 42], [59, 73], [87, 93], [117, 136], [145, 159]], "ID": "3440"}, {"text": "neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring", "acronyms": [[73, 79]], "long-forms": [[56, 71]], "ID": "3441"}, {"text": "guese to Swedish via different pivot language.  RW=Random Walk. * indicates the results are signifi-", "acronyms": [[48, 50]], "long-forms": [[51, 62]], "ID": "3442"}, {"text": "Data. We evaluate our model on predicting paraphrases from the Lexical Substitution (LexSub) dataset (McCarthy and Navigli, 2009).", "acronyms": [[85, 91]], "long-forms": [[63, 83]], "ID": "3443"}, {"text": "Abbreviations: CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, BLem=BioLemmatizer, Snowball=Snowball stemmer, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion annotation process introduced by Pyysalo et al", "acronyms": [[177, 179], [15, 22], [41, 47], [64, 68], [84, 92], [111, 116], [151, 159]], "long-forms": [[180, 199], [32, 39], [48, 54], [69, 82], [93, 101], [117, 142], [160, 168]], "ID": "3444"}, {"text": "nextpos part of speech of next word in the sentence determiner if the word has a determiner prepgoverning if the word is governed by a prepositional phrase (PP), we extract the preposition insidequotes if the word is inside quotes", "acronyms": [[157, 159]], "long-forms": [[135, 155]], "ID": "3445"}, {"text": "in a Swedish Clinical Corpus Hercules Dalianis, Maria Skeppstedt Department of Computer and Systems Sciences (DSV) Stockholm University", "acronyms": [[110, 113]], "long-forms": [[65, 108]], "ID": "3446"}, {"text": "  In the late 1970s a research team at USCInformation Sciences Institute (ISI) studied natural  dialogues with particular interest in applying the ", "acronyms": [[74, 77], [39, 42]], "long-forms": [[42, 72]], "ID": "3447"}, {"text": " 2 Hidden Markov Models Hidden Markov models (HMMs) are commonly used to represent a wide range of linguistic phe-", "acronyms": [[46, 50]], "long-forms": [[24, 44]], "ID": "3448"}, {"text": "domains: ? Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings", "acronyms": [[36, 38], [80, 85]], "long-forms": [[11, 34]], "ID": "3449"}, {"text": "Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)English(EN)-Hindi(HI) code mixing. ", "acronyms": [[97, 99], [107, 109], [86, 88]], "long-forms": [[89, 95], [101, 106], [78, 85]], "ID": "3450"}, {"text": "1 Introduction A fairly novel area of retrieval called topic detection and tracking (TDT) attempts to design methods to automatically (1) spot new, previously unreported events, and (2)", "acronyms": [[85, 88]], "long-forms": [[55, 83]], "ID": "3451"}, {"text": "Program. It is intended that this interface be coupled to the battle management system being  developed under DARPA's Fleet Command Center Battle Management Program(FCCBMP). In the ", "acronyms": [[165, 171], [110, 115]], "long-forms": [[118, 163]], "ID": "3452"}, {"text": "a first-order statistical language model can reduce perplex-  ity by at least a factor of 10 with little computation, while  applying complete natural language (NL) models of syn-  tax and semantics to all partial hypotheses typically requires ", "acronyms": [[161, 163]], "long-forms": [[143, 159]], "ID": "3453"}, {"text": "translation architecture, was developed under sponsorship from  Swedish Telecom by a collaboration between SRI International,  the Swedish Institute of Computer Science (SICS), and Telia  Research.", "acronyms": [[170, 174], [107, 110]], "long-forms": [[131, 168]], "ID": "3454"}, {"text": " Undoubtedly, Web 2.0 and the constant increase of User Generated Content (UGC) lead to a  higher demand for translation.", "acronyms": [[75, 78]], "long-forms": [[51, 73]], "ID": "3455"}, {"text": " 1 In t roduct ion   Language Understanding (LU) has been the focus of much research work in the last twenty ears. ", "acronyms": [[45, 47]], "long-forms": [[21, 43]], "ID": "3456"}, {"text": "1 In t roduct ion   For some NLP applications, it is important o  identify, \"named entities\" (NE), such as person  names, organization ames, time, date, or money ", "acronyms": [[94, 96]], "long-forms": [[77, 91]], "ID": "3457"}, {"text": "Figure 1: Overall architecture of Sentiment Classifier when a word is used with highly positive (HP), positive (P), highly negative (HN), negative (N) or objective (O) meaning based on a sentiment sense inven-", "acronyms": [[133, 135], [97, 99]], "long-forms": [[116, 131], [80, 95], [102, 110], [138, 146], [154, 163]], "ID": "3458"}, {"text": "systems. This shared task was partially supported by Shared Annotated Resources (ShARe) project NIH 5R01GM090187 and Temporal His-", "acronyms": [[81, 86], [96, 99]], "long-forms": [[53, 79]], "ID": "3459"}, {"text": "124   Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 202?203, Vancouver, October 2005.", "acronyms": [[79, 83]], "long-forms": [[31, 77]], "ID": "3460"}, {"text": "represents the editing cycle. Given a  Semantic Network (SN) in a knowledge base (KB),  the system generates a description of the SN in the ", "acronyms": [[57, 59], [82, 84], [130, 132]], "long-forms": [[39, 55], [66, 80]], "ID": "3461"}, {"text": "Measuring speech quality for text-to-speech systems: Development and assessment of a modified mean opinion score (MOS) scale. Computer Speech", "acronyms": [[114, 117]], "long-forms": [[94, 112]], "ID": "3462"}, {"text": "has been discussed extensively in various formulations in the NLP literature, notably in PP attachment, semantic role labeling (SRL) and subcategorization acquisition.", "acronyms": [[128, 131], [62, 65], [89, 91]], "long-forms": [[104, 126]], "ID": "3463"}, {"text": "The focus of the robust CSR techniques  on SLS applications is being facilitated by development and implementation of a well-structured  interface between a CSR and a natural language processor (NLP), allowing collaboration with other  groups developing NLPs for SLS applications.", "acronyms": [[195, 198], [24, 27], [43, 46], [157, 160], [254, 258], [263, 266]], "long-forms": [[167, 193]], "ID": "3464"}, {"text": " 3.3 Prosodic Model Training We choose to use a support vector machine (SVM) classifier1 for the prosodic model based on previous", "acronyms": [[72, 75]], "long-forms": [[48, 70]], "ID": "3465"}, {"text": "compiled two datasets consisting of research papers from two top-tier machine learning conferences: World Wide Web (WWW) and Knowledge Discovery and Data Mining (KDD).", "acronyms": [[116, 119], [162, 165]], "long-forms": [[100, 114], [125, 153]], "ID": "3466"}, {"text": " In Proceedings of Natural Language Processing in Biomedicine (BioNLP) NAACL 2009 Workshop, pages 1?9.", "acronyms": [[63, 69], [71, 76]], "long-forms": [[50, 61]], "ID": "3467"}, {"text": "boundaries. Additionally, a local character-based joint segmentation and tagging solver (SegTagL) is used to provide word boundaries as well as inaccu-", "acronyms": [[89, 96]], "long-forms": [[56, 87]], "ID": "3468"}, {"text": "2 Background 2.1 Surface Realization with Combinatory Categorial Grammar (CCG) CCG (Steedman, 2000) is a unification-based cat-", "acronyms": [[74, 77], [79, 82]], "long-forms": [[42, 72]], "ID": "3469"}, {"text": "For aviation incidents, the advantage of the proposed prior is reflected in the location (LO) and country (CO) slots, which may confuse the various models as they both belong to the entity type loca-", "acronyms": [[107, 109], [90, 92]], "long-forms": [[98, 105], [80, 88]], "ID": "3470"}, {"text": "to say\" and the \"what to say\" is still an open  question. RAGS (rags, 1999) proposes astandard  architecture for the data, but leaves the ", "acronyms": [[58, 62]], "long-forms": [[64, 68]], "ID": "3471"}, {"text": "Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al, 2000; Moore and Paris,", "acronyms": [[139, 142]], "long-forms": [[110, 137]], "ID": "3472"}, {"text": "280  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203?1209, October 25-29, 2014, Doha, Qatar.", "acronyms": [[93, 98]], "long-forms": [[43, 91]], "ID": "3473"}, {"text": "irrespective of word order.  Longest Common Substring (LCS): This measures the longest sequence of words shared between", "acronyms": [[55, 58]], "long-forms": [[29, 53]], "ID": "3474"}, {"text": "In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. ", "acronyms": [[131, 134], [144, 146]], "long-forms": [[107, 129]], "ID": "3475"}, {"text": "alignment. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 215?222.", "acronyms": [[103, 106]], "long-forms": [[60, 101]], "ID": "3476"}, {"text": "Texts The performance of punctuation prediction on both Chinese (CN) and English (EN) texts in the correctly recognized output of the BTEC and CT datasets are", "acronyms": [[65, 67], [82, 84], [134, 138], [143, 145]], "long-forms": [[56, 63], [73, 80]], "ID": "3477"}, {"text": "A Machine Learning based Approach to Evaluating Retrieval Systems Huyen-Trang Vu and Patrick Gallinari Laboratory of Computer Science (LIP6) University of Pierre and Marie Curie", "acronyms": [[135, 139]], "long-forms": [[103, 133]], "ID": "3478"}, {"text": "employing them simultaneously. We also include the oracle word error rate (WER) of the WCNs and lattices for each ASR configuration.", "acronyms": [[75, 78], [87, 91], [114, 117]], "long-forms": [[58, 73]], "ID": "3479"}, {"text": "tribution from most features but C1 seems to benefit more from GENIA named entity tagging, Human Phenotype Ontology (HPO), Foundation Model of Anatomy (FMA) and", "acronyms": [[117, 120], [33, 35], [63, 68], [152, 155]], "long-forms": [[91, 115], [123, 150]], "ID": "3480"}, {"text": " 1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful ap-", "acronyms": [[37, 39]], "long-forms": [[16, 35]], "ID": "3481"}, {"text": "face form via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), HeadDriven Phrase Structure Grammar (HPSG), Com-", "acronyms": [[130, 133], [173, 177]], "long-forms": [[102, 128], [136, 171]], "ID": "3482"}, {"text": " We trained a dialog independent (DI) class based LM and dialog dependent (DD) grammar based LM. In all LMs n is set to 3.", "acronyms": [[75, 77], [34, 36], [93, 95], [104, 107]], "long-forms": [[64, 73], [14, 32]], "ID": "3483"}, {"text": "(ST), i.e. any node of a tree along with all its descendants. A subset tree (SST) exploited by the SubSetTreeKernel is a more", "acronyms": [[77, 80], [1, 3]], "long-forms": [[64, 75], [99, 109]], "ID": "3484"}, {"text": "gree (TTCD) 1, tongue body constriction location (TBCL) and degree (TBCD), lower tooth height (LTH), and glottal vibration (GLO). For example,", "acronyms": [[124, 127], [6, 10], [50, 54], [68, 72], [95, 98]], "long-forms": [[105, 122], [15, 48], [75, 93]], "ID": "3485"}, {"text": "telling. In Computers in Entertainment (CIE), Association for Computing Machinery (ACM), volume 5.", "acronyms": [[83, 86], [40, 43]], "long-forms": [[46, 81], [12, 38]], "ID": "3486"}, {"text": "object (J), which is bad for the Jews (the  event is marked by minus), and after that (>) a  Jewish bearer of Sacred Power (J*SP) miracu-  lously (MIR) protects the (above) Jewish object, ", "acronyms": [[124, 128], [147, 150]], "long-forms": [[93, 122], [130, 145]], "ID": "3487"}, {"text": " ? Reduce Left - X (RL) : Pops the top two nodes from the stack, combines them into a new node", "acronyms": [[20, 22]], "long-forms": [[3, 14]], "ID": "3488"}, {"text": "In Proc. of IEEE/ACL workshop on Spoken Language Technology (SLT). ", "acronyms": [[61, 64], [12, 20]], "long-forms": [[33, 59]], "ID": "3489"}, {"text": "NN = Noun, NN-PL = Plural Noun  DET = Determiner, PREP = Preposition  POS = Possessive, J J = Adjective  Table h Patterns for partOf(basement,building) ", "acronyms": [[70, 73], [0, 2], [11, 16], [32, 35], [50, 54], [88, 91]], "long-forms": [[76, 86], [5, 9], [19, 30], [38, 48], [57, 68], [94, 103]], "ID": "3490"}, {"text": "........................................ LTH  ........... Link between STH and LTHs  TLink (Translation Link) between language LTHs  Figure 2: Example of an STH linked to a Fragment ", "acronyms": [[85, 90], [71, 74], [79, 83], [127, 131], [157, 160], [41, 44]], "long-forms": [[92, 108]], "ID": "3491"}, {"text": "Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the the number of lexical categories per word used (from first to last parsing attempt).", "acronyms": [[186, 188], [46, 49], [128, 135], [202, 204], [218, 220]], "long-forms": [[168, 178], [23, 44]], "ID": "3492"}, {"text": "We review the hierarchical Dirichlet document (HDD) model in section 2, and present our proposed hierarchical Dirichlet tree (HDT) document model in section 3.", "acronyms": [[126, 129], [47, 50]], "long-forms": [[97, 124], [14, 45]], "ID": "3493"}, {"text": "ditto Shi-fen three-hours ten-minute, i.e. 'three-  ten'), post-position phrases (GPs), preposition  phrases (PPs), br adverbs (ADVs). They all share ", "acronyms": [[128, 132], [82, 85], [110, 113]], "long-forms": [[119, 126], [88, 108], [59, 80]], "ID": "3494"}, {"text": "emes are represented, and not function words.  Conceptual representations (ConcSs) used by  PRESENTOR are inspired by the characteristics ", "acronyms": [[75, 81], [92, 101]], "long-forms": [[47, 73]], "ID": "3495"}, {"text": "1.10% 0.20% 18.86% 0    We define Simple MNP (SMNP) whose  length is less than 5 words and Complete MNP ", "acronyms": [[46, 50], [100, 103]], "long-forms": [[34, 44]], "ID": "3496"}, {"text": "also recorded in the miscellaneous information field of the lexeme. Similarly, Gene Ontology (GO) (Consortium.,", "acronyms": [[94, 96]], "long-forms": [[79, 92]], "ID": "3497"}, {"text": " 3 Corpus description The British National Corpus (BNC) (Leech, 1992) is annotated with POS tags, using the CLAWS-4", "acronyms": [[51, 54], [88, 91], [108, 115]], "long-forms": [[26, 49]], "ID": "3498"}, {"text": "(Marcus et al., 1993) whereas IN = preposition or conjunction, subordinating; CC = Coordinating Conjunction; VBN = Verb, past participle; VBG = verb, gerund or present partici-", "acronyms": [[78, 80], [30, 32], [109, 112], [138, 141]], "long-forms": [[83, 107], [115, 136], [144, 175]], "ID": "3499"}, {"text": "Abstract The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate.", "acronyms": [[72, 74]], "long-forms": [[51, 70]], "ID": "3500"}, {"text": "to be relevant: 1. Subject (SS) 2.", "acronyms": [[28, 30]], "long-forms": [[19, 26]], "ID": "3501"}, {"text": "PARSEVAL scores for our parse results. We focus on the Labeled Precision (LP) and Labeled Recall (LR) scores only in this paper, as these are", "acronyms": [[74, 76], [0, 8], [98, 100]], "long-forms": [[55, 72], [82, 96]], "ID": "3502"}, {"text": "In this paper we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II treebank, with about 1 mil-", "acronyms": [[100, 103]], "long-forms": [[79, 98]], "ID": "3503"}, {"text": "The knowledge about actions and plans is stored in  a plan library structured on the basis of two main hier-  archies: the Decomposition Hierarchy (DH) and the  Generalization Hierarchy (GH) \\[Kautz and Allen, 86\\].", "acronyms": [[148, 150], [187, 189]], "long-forms": [[123, 146], [161, 185]], "ID": "3504"}, {"text": "autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR), pages 131?140, Gothenburg, Sweden, April. Association for Computational Linguistics.", "acronyms": [[119, 123]], "long-forms": [[46, 117]], "ID": "3505"}, {"text": "CoTrain vs. LEX(CN) 6.09E-29 3.72E-21 1.61E-24 CoTrain vs. LEX(EN) 0.0018 0.0276 0.00329 CoTrain vs. SVM(CN) 1.26E-13 6.45E-10 2.7E-14 CoTrain vs. SVM(EN) 2.15E-18 1.1E-17 3.46E-13", "acronyms": [[105, 107]], "long-forms": [[89, 96]], "ID": "3506"}, {"text": "Another important note is that, although the audio sets consist of both broadcast news (BN) and broadcast conversations (BC), we did not perform BN or BC-specific tuning.", "acronyms": [[121, 123], [88, 90], [145, 147], [151, 153]], "long-forms": [[96, 119], [72, 86]], "ID": "3507"}, {"text": "4} is not a most frequent token  and will reach at bi-gram queue manager only  after passing  through all forms generator (AFG).  ", "acronyms": [[123, 126]], "long-forms": [[102, 121]], "ID": "3508"}, {"text": "placed in a transcript. Here, we focus on the syndrome of primary progressive aphasia (PPA). PPA", "acronyms": [[87, 90], [93, 96]], "long-forms": [[58, 85]], "ID": "3509"}, {"text": "of-the-art dependency parser, the 2014 online version. 4) PPAD Naive Bayes(NB) is the same as PPAD but uses a generative model, as opposed to", "acronyms": [[75, 77], [58, 62], [94, 98]], "long-forms": [[63, 73]], "ID": "3510"}, {"text": "BP as a computational expression graph ? Automatic differentiation (AD) 7.", "acronyms": [[68, 70], [0, 2]], "long-forms": [[41, 66]], "ID": "3511"}, {"text": "marn et al 2007). The training procedure usually employs an expectation maximization (EM) procedure and the resulting transducer can be used to", "acronyms": [[86, 88]], "long-forms": [[60, 84]], "ID": "3512"}, {"text": "where C is the concept that subsumes both C1 and C2 and has the highest information content (i.e., it is the least common subsumer (LCS)). ", "acronyms": [[132, 135]], "long-forms": [[109, 130]], "ID": "3513"}, {"text": "grammatical person and number (1PS, 1PP, 2P, 3PS, 3PP), the quantified pronouns (QUANT), and a group including all other expressions (OTHER). ", "acronyms": [[134, 139], [81, 86]], "long-forms": [[115, 132], [60, 79]], "ID": "3514"}, {"text": "in table 4. As we can see a small improvement is obtained for the interpretation error rate (IER) with the integrated strategy (strat2).", "acronyms": [[93, 96]], "long-forms": [[66, 91]], "ID": "3515"}, {"text": "event coreference resolver has to rely on the noisy outputs produced by its upstream components in the standard information extraction (IE) pipeline. ", "acronyms": [[136, 138]], "long-forms": [[112, 134]], "ID": "3516"}, {"text": " The practical application of flame-based knowledge-based systems, such as in expert systems, requires the maintenance of  potentially very large amounts of declarative knowledge stored in their knowledge bases (KBs). As a KB grows in size and ", "acronyms": [[212, 215], [223, 225]], "long-forms": [[195, 210]], "ID": "3517"}, {"text": " 1 In t roduct ion   Word Sense Disambiguation (WSD) is an open prob-  lem in Natural Language Processing.", "acronyms": [[48, 51]], "long-forms": [[21, 46]], "ID": "3518"}, {"text": "difficult to disambiguate.  Preposition sense disambiguation (PSD) has many potential uses.", "acronyms": [[62, 65]], "long-forms": [[28, 60]], "ID": "3519"}, {"text": "the source and target sides are lexicons (terminals) 2) Unlexicalized (ULex): all leaf nodes in both the 46", "acronyms": [[71, 75]], "long-forms": [[56, 69]], "ID": "3520"}, {"text": "end returnmodels [] Algorithm 1: Positive Diversity Tuning (PDT) lectively produce very diverse translations.3", "acronyms": [[60, 63]], "long-forms": [[33, 58]], "ID": "3521"}, {"text": "2 GT  and  Move-c~  The central operations of the Minimalist Program  are Generalized Transformation (GT) and Move-  ~. GT is a structure-building operation that builds ", "acronyms": [[102, 104], [2, 4], [120, 122]], "long-forms": [[74, 100]], "ID": "3522"}, {"text": "ANTEST R E T U R N S  ** 1**  CHANGT, HAVE CSEXCE1 FOR HESGEF IN  ARTEST CALLEC FOR 18\"REGVO I C E u  (AACC)  ANTEST E T U E N S  ** l** ", "acronyms": [[103, 107], [43, 50], [51, 54], [55, 61], [0, 6]], "long-forms": [[66, 96]], "ID": "3523"}, {"text": " 1 Introduction In the semantic dependency parsing (SDP) task of SemEval 2014, the meaning of a sentence is repre-", "acronyms": [[52, 55]], "long-forms": [[23, 50]], "ID": "3524"}, {"text": "questions evaluated in TREC1. For example questions 1The Text REtrieval Conferences (TREC) are evaluation workshops in which Information Retrieval tasks are annually", "acronyms": [[85, 89], [23, 27]], "long-forms": [[57, 83]], "ID": "3525"}, {"text": "tic attachment. Eight categories of syntactic onstituent were used: sentence (S), noun  phrase (NP), verb phrase (VP), prepositional phrase (PP), wh-noun phrase (WHNP),  adjective or adverbial phrase (AP), any other constituent (O), and both words in the ", "acronyms": [[114, 116], [141, 143], [96, 98], [162, 166], [201, 203]], "long-forms": [[101, 112], [119, 139], [68, 76], [82, 94], [146, 160], [170, 199], [206, 227]], "ID": "3526"}, {"text": "The basic aim of Acquilex is the  development of techniques and methods in order to use  Machine Readable Dictionaries (MRD) * for building lexical  components for Natural Language Processing Systems.", "acronyms": [[120, 123]], "long-forms": [[89, 118]], "ID": "3527"}, {"text": "While initial-state ECR provides a measure of the likelihood of a favorable outcome, it does not address how well a particular state representation captures key decision points. That is, it does not directly represent the extent to which each deci-sion along the path to a successful outcome con-tributed to that outcome, or whether the second-best decision in a particular state would have been equally useful. In order to measure this dif-ference, we introduce the Separation Ratio (SR), which represents how much better a particular policy is compared to its alternatives. SR for a state is calculated by taking the absolute differ-ence between the estimated values of two actions in that state and dividing by the mean of the two values.", "acronyms": [[485, 487], [576, 578], [20, 23]], "long-forms": [[467, 483]], "ID": "3528"}, {"text": "document is different from the question. Also, in  Information Extraction (IE), in which the system  tries to extract elements of some events (e.g. ", "acronyms": [[75, 77]], "long-forms": [[51, 73]], "ID": "3529"}, {"text": " LTP. LTP (Language Technology Platform  developed by HIT) is a package of tools to ", "acronyms": [[6, 9], [1, 4], [54, 57]], "long-forms": [[11, 39]], "ID": "3530"}, {"text": " 1 Introduction The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sen-", "acronyms": [[61, 64]], "long-forms": [[28, 59]], "ID": "3531"}, {"text": "SK(d1,d1)?SK(d2,d2) 4 Experiments The use of WordNet (WN) in the term similarity function introduces a prior knowledge whose impact", "acronyms": [[54, 56], [0, 2], [10, 12]], "long-forms": [[45, 52]], "ID": "3532"}, {"text": "In my semantic role labeling research, I used the Tilburg Memory Learner (TiMBL) for this purpose.", "acronyms": [[74, 79]], "long-forms": [[50, 72]], "ID": "3533"}, {"text": "An entity has  three types of mention: NAM (proper name), NOM  (nominal) or PRO (pronoun). A relation was ", "acronyms": [[76, 79], [39, 42], [58, 61]], "long-forms": [[81, 88], [51, 55], [64, 71]], "ID": "3534"}, {"text": "In this part, we introduce how to make use of the  original and opposite training/test data together  for dual training and dual prediction (DTDP). ", "acronyms": [[141, 145]], "long-forms": [[106, 139]], "ID": "3535"}, {"text": "?? ( sE) one or more times are considered to be locative MWEs (LOC). In contrast, bigrams", "acronyms": [[63, 66], [57, 61]], "long-forms": [[48, 56]], "ID": "3536"}, {"text": "2.3.1 KiF (Knowledge in Frame) The whole system, training and prediction, has been implemented in KiF (Knowledge in Frame), a script language that has been implemented into", "acronyms": [[98, 101], [6, 9]], "long-forms": [[103, 121], [11, 29]], "ID": "3537"}, {"text": "A simple solution to this problem is  to compute the probability of words in the target  language as maximum likelihood estimates (MLE)  over a large corpus and reformulate the general ", "acronyms": [[131, 134]], "long-forms": [[101, 129]], "ID": "3538"}, {"text": "+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system", "acronyms": [[91, 93], [104, 106], [125, 127]], "long-forms": [[94, 102], [107, 123], [128, 143]], "ID": "3539"}, {"text": "GaChalign (Crosslingual): Gale-Church Sentence-level Aligner with variable parameters ? Indotag (Indonesian): Conditional Random Field (CRF) POS tagger. ", "acronyms": [[136, 139], [141, 144]], "long-forms": [[110, 134]], "ID": "3540"}, {"text": "these 153,014 verb-noun collocations.  We used 'Bunrui Goi Hyou'(BGH) (NLRI, 1993)  as the Japanese thesaurus.", "acronyms": [[65, 68], [71, 75]], "long-forms": [[48, 63]], "ID": "3541"}, {"text": "symbols as well as full names. Groups such as the  Human Genome Organisation (HUGO), Mouse Genome  Institute (MGI), UniProt, and the National Center for ", "acronyms": [[78, 82], [110, 113]], "long-forms": [[51, 76], [85, 108]], "ID": "3542"}, {"text": " The algorithm was first proposed by the Institute for Computer Science  and Technology (ICST) of the National Bureau of Standards (NBS') in 1973. ", "acronyms": [[132, 136], [89, 93]], "long-forms": [[102, 130], [41, 87]], "ID": "3543"}, {"text": "answers accordingly. Specially, we develop a supervised Maximum Entropy (MaxEnt) based model to rescore the answers from the pipelines,", "acronyms": [[73, 79]], "long-forms": [[56, 71]], "ID": "3544"}, {"text": "We show how these strategies are captured in a grammar developed in the Grammatical Framework (GF).1 We evaluated our method by experimenting", "acronyms": [[95, 97]], "long-forms": [[72, 93]], "ID": "3545"}, {"text": "slightly simplified version of Webber's original rule  schema, says that for any formula that meets the  structural description (SD), a discourse ntity identified  by the ID formula is to be constructed.", "acronyms": [[129, 131], [171, 173]], "long-forms": [[105, 127]], "ID": "3546"}, {"text": "event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. ", "acronyms": [[110, 113]], "long-forms": [[81, 108]], "ID": "3547"}, {"text": "For comparison, the graphs in Figures 1 and 2 also show the curves corresponding to the evaluation of Pointwise Mutual Information (PMI).8 The cooccurrence statistics of the expressions in Disco-En-", "acronyms": [[132, 135]], "long-forms": [[102, 130]], "ID": "3548"}, {"text": "ate the surface form in the opposite direction.  Amazon?s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources", "acronyms": [[75, 80]], "long-forms": [[58, 73]], "ID": "3549"}, {"text": "niscent of balanced-tree structures using left and right  rotations. A left rotation changes a (A(BC)) structure to  a ((AB)C) structure, and vice versa for a right rotation.", "acronyms": [[96, 100], [120, 126]], "long-forms": [[69, 92]], "ID": "3550"}, {"text": "ing decisions in multi-party discussions.  Several types of dialogue act (DA) are distinguished on the basis of their roles in", "acronyms": [[74, 76]], "long-forms": [[60, 72]], "ID": "3551"}, {"text": "written japanese. In Proceedings of the 6th  Workshop on Asian Language Resources (ALR),  pages 101?102.", "acronyms": [[83, 86]], "long-forms": [[57, 81]], "ID": "3552"}, {"text": " Type Short time members Long time members Abbreviations Husband My DD (Dear Daughter), Your PS (Plastic Surgeon) Social networks Facebook, fb", "acronyms": [[68, 70], [93, 95]], "long-forms": [[72, 85], [97, 112]], "ID": "3553"}, {"text": " In addition, the tense,                                                             1 S=Subject; IO=Indirect Object; DO=Direct Object;  V=Verb; ERG=Ergative; DAT=Dative ", "acronyms": [[98, 100], [118, 120], [145, 148], [159, 162]], "long-forms": [[101, 116], [121, 134], [89, 96], [139, 143], [149, 157], [163, 169]], "ID": "3554"}, {"text": "for the different settings. GM = Google Maps, CI = Campus Indoor, CO = Campus Outdoor. ", "acronyms": [[66, 68], [28, 30], [46, 48]], "long-forms": [[71, 85], [33, 44], [51, 64]], "ID": "3555"}, {"text": "of General Linguistics  MAINE, JUNE '94  SEMANTIC SYNTAX (SeSyn) is a direct continuation of work done in the '60s and '70s under the name of GENERATIVE  SEMANTICS.", "acronyms": [[58, 63]], "long-forms": [[41, 56]], "ID": "3556"}, {"text": "model organism databases (e.g., for mouse3 and  yeast4) as well as various protein databases (e.g.,  Protein Information Resource5 (PIR) or SWISS-                                                                                           tor), a model organism for genetics research: ", "acronyms": [[132, 135]], "long-forms": [[101, 130]], "ID": "3557"}, {"text": "Table 4: Entailment judgment in closed test  of mutual information (T=True, F=False,  MI=mutual information). ", "acronyms": [[86, 88]], "long-forms": [[89, 107], [70, 74], [78, 83]], "ID": "3558"}, {"text": " ? Unaligned word penalty feature (UWP): hUWP (Q,S,A), which is defined as the ratio", "acronyms": [[35, 38], [41, 45]], "long-forms": [[3, 25]], "ID": "3559"}, {"text": "weiwei@cs.columbia.edu Abstract In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence,", "acronyms": [[64, 67]], "long-forms": [[35, 62]], "ID": "3560"}, {"text": "dependency tree.  3.1 Naive Approach (NA)  In this approach we first run a parser on the input ", "acronyms": [[38, 40]], "long-forms": [[22, 36]], "ID": "3561"}, {"text": "ity. We presented an evaluation of these parameters for preposition sense disambiguation (PSD). ", "acronyms": [[90, 93]], "long-forms": [[56, 88]], "ID": "3562"}, {"text": " B. MTE features We use the following MTE metrics (MTFEATS), which compare the similarity between the question and a candidate answer:", "acronyms": [[51, 58], [4, 7], [38, 41]], "long-forms": [[42, 49]], "ID": "3563"}, {"text": "that have to be defined in a theory-specific way.  Thus a document representation (DocRep) has two components, a DocAttr and a DocRepSeq.", "acronyms": [[83, 89], [113, 119], [127, 136]], "long-forms": [[58, 81]], "ID": "3564"}, {"text": " 2 Swedish FrameNet Swedish FrameNet (SweFN), which began in 2011, is part of Swedish FrameNet++ (Borin et al.,", "acronyms": [[38, 43]], "long-forms": [[20, 36]], "ID": "3565"}, {"text": "i. The Eng!\\[sh t_qMal_a~franslationSsSSSSSSSS_2~trm  Baak~reusd  Computer Aided T~anslation (CAT) research at Universiti  Sa~m MalsysL~ (USM) began in 1976 as an individual research ", "acronyms": [[94, 97], [138, 141]], "long-forms": [[66, 82], [111, 136]], "ID": "3566"}, {"text": " NOM (nominative), ACC (accusative), DAT (dative), ABL (ablative), CMI (comitative), GEN (genitive) and TOP (topic marker).", "acronyms": [[51, 54], [67, 70], [85, 88], [1, 4], [19, 22], [37, 40], [104, 107]], "long-forms": [[56, 64], [72, 82], [90, 98], [6, 16], [24, 34], [42, 48], [109, 121]], "ID": "3567"}, {"text": "ings. They evaluate their work against the SENSEVAL 2 AW test data (SV2AW). They tune the", "acronyms": [[68, 73]], "long-forms": [[43, 56]], "ID": "3568"}, {"text": "In t roduct ion   This paper discusses the relationstfip between Tree Adjoin-  ing Grammars (TAG's) and :Head Grammars (HG's). TAG's ", "acronyms": [[120, 124], [93, 98], [127, 132]], "long-forms": [[105, 118], [65, 91]], "ID": "3569"}, {"text": "it is the in f in i t ive  form of a verb. If SO, it is to be attached to the  parsing tree, and given the additionql feature MVB (main verb). The current ", "acronyms": [[126, 129], [46, 48]], "long-forms": [[131, 140]], "ID": "3570"}, {"text": "ponent Analysis (PCA). We then compare our embeddings with the CW (Collobert and Weston, 2008), Turian (Turian et al.,", "acronyms": [[63, 65]], "long-forms": [[67, 87]], "ID": "3571"}, {"text": "     elements; ? |? is used for alternating elements; TOP = topic marker. ", "acronyms": [[54, 57]], "long-forms": [[60, 65]], "ID": "3572"}, {"text": " 2.3 Parsing and DSyntS conversion We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can", "acronyms": [[71, 78], [17, 23]], "long-forms": [[44, 69]], "ID": "3573"}, {"text": " 1 Introduction Information extraction (IE) systems recover structured information from text.", "acronyms": [[40, 42]], "long-forms": [[16, 38]], "ID": "3574"}, {"text": "cation and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks.", "acronyms": [[116, 119]], "long-forms": [[87, 114]], "ID": "3575"}, {"text": "ical functions. Apart from commonly accepted grammatical functions, such as SB (subject) or OA (accusative object), Negra grammatical func-", "acronyms": [[76, 78], [92, 94]], "long-forms": [[80, 87], [96, 113]], "ID": "3576"}, {"text": "SEPA parameters are S = 13, 000, N = 20. In both rows, SEPA results for the in-domain (left) and adaptation (middle) scenarios are compared to the confidence (CB) and minimum length (ML) baselines. The", "acronyms": [[183, 185], [0, 4], [55, 59], [159, 161]], "long-forms": [[167, 181]], "ID": "3577"}, {"text": "4 Discussion We provide a brief introduction to the framework of Linear Categorial Grammar (LinCG). One of", "acronyms": [[92, 97]], "long-forms": [[65, 90]], "ID": "3578"}, {"text": "pathway representation formats: Systems Biology Markup Language (SBML)3 (Hucka et al, 2003) and Biological Pathway Exchange (BioPAX)4 (Demir et al, 2010).", "acronyms": [[125, 131], [65, 69]], "long-forms": [[96, 123], [32, 63]], "ID": "3579"}, {"text": "ing mobile devices. Such an application typically uses a speech recognizer (ASR) for transforming the user?s speech input to text and a search component", "acronyms": [[76, 79]], "long-forms": [[55, 74]], "ID": "3580"}, {"text": "ple sentences. Such knowledge is also important for Textual Entailment (TE), a generic framework for modeling semantic inference.", "acronyms": [[72, 74]], "long-forms": [[52, 70]], "ID": "3581"}, {"text": " As an example consider the multiple alignments in Figure 4, with the gold standard alignment (GS) on the left and the generated alignment (GA) on", "acronyms": [[95, 97]], "long-forms": [[70, 83]], "ID": "3582"}, {"text": "pendencies is only context-free (section 2.1). Our argument is based on our desire to use a discourse grammar in natural language generation (NLG). It is well-known that", "acronyms": [[142, 145]], "long-forms": [[113, 140]], "ID": "3583"}, {"text": "But still we can say that even in languages with  that kind of structural properties like Slavic languages have, named entities (NE) form a subset of  natural language expressions that demonstrates ", "acronyms": [[129, 131]], "long-forms": [[113, 127]], "ID": "3584"}, {"text": "semantic processing component. A detailed descrip-  tion of the lexical conccplual structure (LCS) which  serves as the interlingua is not given here, but see ", "acronyms": [[94, 97]], "long-forms": [[64, 92]], "ID": "3585"}, {"text": "five COMPARE3, and five RECOMMEND CPs.  Each of the 112 textplans (TPs) that produced 4", "acronyms": [[67, 70], [34, 37]], "long-forms": [[56, 65]], "ID": "3586"}, {"text": "monly used tasks: small vocabulary recognition (TI-digits), read and spontaneous text dictation (WSJ), and goal-oriented spoken dialog (ATIS). The broadcast news task is quite general, covering a", "acronyms": [[136, 140], [48, 57], [97, 100]], "long-forms": [], "ID": "3587"}, {"text": "(Joshi, Vijay?Shanker, and Weir, 1989; Weir and Joshi,  1988) have shown that LIGs, Combinatory Categorial  Grammars (CCG), Tree Adjoinig Grammars (TAGs),  and Head Grammars (HGs) are weakly equivalent.", "acronyms": [[148, 152], [78, 82], [118, 121], [175, 178]], "long-forms": [[124, 146], [84, 116], [160, 173]], "ID": "3588"}, {"text": "The training set consisted of 12,927 texts: News reports (News) about Education (Edu), Editorials (Edit.) about Defense (Def), and Letters to the Editor (LttE) about Medicine (Med). ", "acronyms": [[154, 158], [81, 84], [99, 103], [121, 124], [176, 179]], "long-forms": [[131, 152], [70, 79], [87, 97], [112, 119], [166, 174]], "ID": "3589"}, {"text": "3.3 Sentiment Model The design of the sentiment model used in our system was based on the assumption that the opinions expressed would be highly subjective and contextualized.  Therefore, for generating data for model training and testing, we used a crowd-sourcing approach to do sentiment annotation on in-domain political data. To create a baseline sentiment model, we used Amazon Mechanical Turk (AMT) to get as varied a population of annotators as possible. We designed an interface that allowed annotators to perform the annotations outside of AMT so that they could participate anonymously.", "acronyms": [[400, 403]], "long-forms": [[376, 398]], "ID": "3590"}, {"text": "The types of links  traversed in the search (in the first case) or the checked slots (in the second case) are a  function of the semantic lass (SEM-C) of the first constituent. This function assigns to each ", "acronyms": [[144, 149]], "long-forms": [[129, 137]], "ID": "3591"}, {"text": "a PP (Mirroshandel and Ghassem-Sani, 2011), the prepositional lexeme of the PP if e2 is governed by a PP, the POS of the head of the verb phrase (VP) if e1 is governed by a VP, the POS of the head of the", "acronyms": [[146, 148], [2, 4], [76, 78], [102, 104], [110, 113], [173, 175], [181, 184]], "long-forms": [[133, 144]], "ID": "3592"}, {"text": "segmentable candidates, and picks a correct segmentation candidate from the list by using a value of LEF (Likelihood Evaluation Function, Section 2.1) and so on.", "acronyms": [[101, 104]], "long-forms": [[106, 136]], "ID": "3593"}, {"text": "152 Kuhn A Survey and Classification of Controlled Natural Languages Controlled Language for Crisis Management (CLCM) (Temnikova 2010, 2011, 2012) is a language for writing instructions about how to deal with crisis situations.", "acronyms": [[112, 116]], "long-forms": [[69, 110]], "ID": "3594"}, {"text": "1 Introduction ? Language Model (LM) Growing? refers to adding", "acronyms": [[33, 35]], "long-forms": [[17, 31]], "ID": "3595"}, {"text": "   Figure 2. Tutor's Priming Ratio aggregated by task set (TS = Task Set)     Figure 3.", "acronyms": [[59, 61]], "long-forms": [[64, 72]], "ID": "3596"}, {"text": "pense of introducing noise.  We propose Embedded base phrase(EBP) detection as algorithm.2.", "acronyms": [[61, 64]], "long-forms": [[40, 60]], "ID": "3597"}, {"text": "induction algorithm (Bisk and Hockenmaier, 2012), which induces a language-specific Combinatory Categorial grammar (CCG) and lexicon based on a small number of linguistic", "acronyms": [[116, 119]], "long-forms": [[84, 114]], "ID": "3598"}, {"text": "labeled as negative; otherwise, the review is labeled as positive.  3.2 Lexicon-Based Method in Chinese Language: LEX(CN) This method first translates English sentiment lexica into Chinese lexica, and then", "acronyms": [[118, 120], [114, 117]], "long-forms": [[96, 112]], "ID": "3599"}, {"text": "Two types of initial parameter configurations were tried for BFGS; initial parameters have the same fixed values, or were chosen randomly. Steepest Descent (SD) was used as online training where some portion (i.e. chunk) of the training data were used during an iteration.", "acronyms": [[157, 159], [61, 65]], "long-forms": [[139, 155]], "ID": "3600"}, {"text": "is different from the annotation scheme (Abbas, 2012; Abbas, 2014) of phrase structure (PS) and the hyper dependency structure (HDS) of the URDU.KON-TB treebank along with the different", "acronyms": [[128, 131], [88, 90], [140, 151]], "long-forms": [[100, 126], [70, 86]], "ID": "3601"}, {"text": " Table 5: EPPS task: translation quality and time for different input conditions (CN=confusion network, time in seconds per sentence).", "acronyms": [[82, 84], [10, 14]], "long-forms": [[85, 102]], "ID": "3602"}, {"text": "learning community: ? Rectified Linear Unit (ReLU) (Nair and Hinton, 2010): max(0, x);", "acronyms": [[45, 49]], "long-forms": [[22, 43]], "ID": "3603"}, {"text": "Japanese  (Jap). Germanic (Get), and Southern Romance (SRom). Only the ", "acronyms": [[55, 59], [11, 14], [27, 30]], "long-forms": [[37, 53], [0, 8], [17, 25]], "ID": "3604"}, {"text": "prim. = primary source; C06?C09 = CoNLL 2006?2009; I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip", "acronyms": [[68, 70], [90, 92], [112, 114], [128, 131], [156, 159], [189, 191], [194, 196], [197, 200]], "long-forms": [[73, 88], [95, 103]], "ID": "3605"}, {"text": " Acknowledgments This work has been funded in part by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289 (INSIGHT) and by the EU FP7 program in the context of the project LIDER", "acronyms": [[104, 107], [209, 214], [164, 166], [167, 170]], "long-forms": [[76, 102]], "ID": "3606"}, {"text": "ran, and the historical ancestor of the other varieties. Modern Standard Arabic (MSA) is the modern  version of CA and is, broadly speaking, the univer-", "acronyms": [[81, 84]], "long-forms": [[57, 79]], "ID": "3607"}, {"text": "5http://wordnet.princeton.edu/. 58 that, as in Informtion Retrieval (IR), multiple occurrences in the same document count as just one", "acronyms": [[69, 71]], "long-forms": [[47, 67]], "ID": "3608"}, {"text": "EVG smoothed-skip-head 65.0 (5.7) L-EVG smoothed 68.8 (4.5) Table 1: Directed accuracy (DA) for WSJ10, section 23. ", "acronyms": [[88, 90], [0, 3], [34, 39]], "long-forms": [[69, 86]], "ID": "3609"}, {"text": "been semi-automatically detected in human reference and machine translations from English (EN) to French (FR) and German (DE) (Section 3). ", "acronyms": [[106, 108], [91, 93], [122, 124]], "long-forms": [[98, 104], [82, 89], [114, 120]], "ID": "3610"}, {"text": "occurs from different production sources, we  propose an extension to this genre of technique  in the form of a Group Sparse Model (GSM)  which enforces sparsity with a L2,1 norm instead ", "acronyms": [[132, 135]], "long-forms": [[112, 130]], "ID": "3611"}, {"text": " (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.", "acronyms": [[81, 85], [38, 44], [2, 5]], "long-forms": [[50, 79], [21, 36]], "ID": "3612"}, {"text": "query, some form of translation is required. One might conjecture that a combination of two existing fields, IR and machine translation (MT), would be satisfactory for accomplishing the combined translation and retrieval task.", "acronyms": [[137, 139]], "long-forms": [[116, 135]], "ID": "3613"}, {"text": "are usually followed by a number n ? 0. DU = discourse unit, ce = conversational event, K = DRS, u = utterance, sem", "acronyms": [[40, 42], [92, 95]], "long-forms": [[45, 54], [101, 110]], "ID": "3614"}, {"text": "approach to structuring knowledge is based  on:  z automatic term recognition (ATR)  z automatic term clustering (ATC) as an ", "acronyms": [[79, 82], [114, 117]], "long-forms": [[51, 77], [87, 112]], "ID": "3615"}, {"text": "paring average precision values obtained 1) against the original, manual chronologies (APC), and 2) against the expert assessment (APE). These values", "acronyms": [[131, 134], [87, 90]], "long-forms": [[100, 129]], "ID": "3616"}, {"text": " Finally there is a set of measures relating to the receiver operating characteristic (ROC) curves, which measure the discrimination of the scores for", "acronyms": [[87, 90]], "long-forms": [[52, 85]], "ID": "3617"}, {"text": "performance\" (MOPs) presented here with  results we still need from system-external  \"measures of effectiveness\" (MOEs)25 MOE-  based methods evaluate (i) baseline unaided ", "acronyms": [[114, 118], [14, 18], [122, 125]], "long-forms": [[86, 111]], "ID": "3618"}, {"text": "Table 1: Summary of distance values between the 380 observed A-N pairs and the predictions from each model (ADD=additive, MUL=multiplicative, PLSR=Partial Least Squares Regression).", "acronyms": [[108, 111], [122, 125], [61, 64], [142, 146]], "long-forms": [[112, 120], [126, 140], [147, 179]], "ID": "3619"}, {"text": "Abstract  There has been a long-standing methodology for  evaluating work in speech recognition (SR), but until  recently no community-wide methodology existed for either ", "acronyms": [[97, 99]], "long-forms": [[77, 95]], "ID": "3620"}, {"text": "on a Wikipedia corpus instead of the ukWaC corpus. The University of Sussex (UoS) team submitted two WSI systems that use dependency-parsed", "acronyms": [[77, 80], [37, 42], [101, 104]], "long-forms": [[55, 75]], "ID": "3621"}, {"text": " The third is the confusion between adjective  (JJ) and noun (NN), when the word in question  modifies a noun that immediately follows.", "acronyms": [[62, 64], [48, 50]], "long-forms": [[56, 60], [36, 45]], "ID": "3622"}, {"text": "restrictive relative clauses, and epithets ? trigger conventional implicatures (CI) whose truth is necessarily presupposed, even if the truth conditions", "acronyms": [[80, 82]], "long-forms": [[53, 78]], "ID": "3623"}, {"text": "nique was used in The MAYO Clinic Vocabulary  Server (MCVS)5, which encodes clinical expressions into medical ontology (SNOMED-CT) and  identifies whether the event is positive or negative.", "acronyms": [[120, 129], [22, 26], [54, 58]], "long-forms": [], "ID": "3624"}, {"text": " 2007. The Syntax Augmented MT (SAMT) system at the Shared Task for the 2007 ACL Workshop on", "acronyms": [[32, 36], [77, 80]], "long-forms": [[11, 30]], "ID": "3625"}, {"text": "tion, Generation, Question Answering (QA), etc.  STS is related to both Textual Entailment (TE) and Paraphrasing, but differs in a number of ways", "acronyms": [[92, 94], [38, 40], [49, 52]], "long-forms": [[72, 90], [18, 36]], "ID": "3626"}, {"text": " The LRS representation of (1) is shown in Figure 1, where INCONT (INTERNAL CONTENT) encodes the core semantic contribution of the head, EXCONT", "acronyms": [[59, 65], [5, 8], [137, 143]], "long-forms": [[67, 83]], "ID": "3627"}, {"text": "It is a comprehensive study but not directly related to ours, as we model our problem with Markov Decision Processes (MDP) and evaluate model-based and model-free algorithms on a", "acronyms": [[118, 121]], "long-forms": [[91, 116]], "ID": "3628"}, {"text": "2 Dialogue data The dialogue corpus used to perform the experiments is the Switchboard database (SWBD). It", "acronyms": [[97, 101]], "long-forms": [[75, 95]], "ID": "3629"}, {"text": "Instead of using words directly, it is possible to employ a (much smaller) controlled vocabulary: Medical Subject Headings (MeSH), consisting of 22,500 codes, are (mostly) manually assigned to", "acronyms": [[124, 128]], "long-forms": [[98, 122]], "ID": "3630"}, {"text": " 4.3 Spanish?English (ES?EN), French?English (FR?EN) In Table 3, we see that on the ES?EN and", "acronyms": [[46, 51]], "long-forms": [[30, 44]], "ID": "3631"}, {"text": " 4 DOM Tree Alignment Model  The Document Object Model (DOM) is an application programming interface for valid HTML ", "acronyms": [[56, 59], [3, 6], [111, 115]], "long-forms": [[33, 54]], "ID": "3632"}, {"text": "egorization. ACM Transactions on Information Systems (TOIS), 12(3):233?251. ", "acronyms": [[54, 58], [13, 16]], "long-forms": [[17, 52]], "ID": "3633"}, {"text": "Answer Pinpointing. In Proceedings of the DARPA Human Language Technology Conference (HLT). ", "acronyms": [[86, 89], [42, 47]], "long-forms": [[48, 73]], "ID": "3634"}, {"text": "ysis. The Chinese comma is viewed as a delimiter of elementary discourse units (EDUs), in the sense of the Rhetorical Structure Theory (Carlson et al,", "acronyms": [[80, 84]], "long-forms": [[52, 78]], "ID": "3635"}, {"text": "dictionary (viz. /hu:pana-taNa/ and /Fi:tiki-RaNa/), I also searched the Ma?ori Broadcast Corpus (MBC) for words ending as if they had gerundial suffixes", "acronyms": [[98, 101]], "long-forms": [[73, 96]], "ID": "3636"}, {"text": "future work.1 1 Introduction and Motivation Typically, Information Extraction (IE) systems learn an extractor for each target relation from la-", "acronyms": [[79, 81]], "long-forms": [[55, 77]], "ID": "3637"}, {"text": "3.1 Graph-based parsing model The graph-based parsing model aims to search for the maximum spanning tree (MST) in a graph (McDonald et al, 2005).", "acronyms": [[106, 109]], "long-forms": [[83, 104]], "ID": "3638"}, {"text": " School of Information Science Japan Advanced Institute of Science and Technology (JAIST), Japan nthnhung@jaist.ac.jp", "acronyms": [[83, 88]], "long-forms": [[31, 81]], "ID": "3639"}, {"text": "sLDA is a model that is an extension of Latent Dirichlet Allocation (LDA) (Blei et al, 2003) that models each document as having an output variable in addition to", "acronyms": [[69, 72], [0, 4]], "long-forms": [[40, 67]], "ID": "3640"}, {"text": "In Proceedings of the First International Conference on Language Resources and Evaluation (LREC), pages 581?588, Granada.", "acronyms": [[91, 95]], "long-forms": [[56, 74]], "ID": "3641"}, {"text": "(Para), POS, syntactic dependency tree (DT), syntactic constituent tree (CT), named entities (NE), WordNet Relations (WNR), WordNet supersenses (WNSS), semantic role labeling (SRL), causal relations (CR), query classes (QC), query-log co-ocurrences (QLCoOcc).Models: bag-of-words scoring (BOW), tree matching (TreeMatch), linear (LM), log-linear (LLM), statistical learning", "acronyms": [[220, 222], [8, 11], [40, 42], [73, 75], [94, 96], [118, 121], [145, 149], [176, 179], [200, 202], [250, 257], [289, 292], [310, 319], [330, 332], [347, 350]], "long-forms": [[205, 218], [23, 38], [55, 71], [78, 92], [99, 116], [124, 143], [152, 174], [182, 198], [225, 248], [267, 279], [295, 308], [322, 328], [335, 345]], "ID": "3642"}, {"text": "According to the fifth rule, the Arabic letter Z may match an empty string on the English side, if there is an English consonant (EC) in the right context of the English side.", "acronyms": [[130, 132]], "long-forms": [[111, 128]], "ID": "3643"}, {"text": "KEY: Number of discussions and posts on the topic (Discs, Posts).  Number of authors (NumA). Posts per author (P/A).", "acronyms": [[86, 90], [111, 114]], "long-forms": [[67, 84], [93, 109]], "ID": "3644"}, {"text": "To address this problem, Xiong et al (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual", "acronyms": [[84, 90], [56, 59]], "long-forms": [[67, 82]], "ID": "3645"}, {"text": "Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), page 89, Beijing, August 2010 Multi-Word Expressions as Discourse Relation Markers (DRMs) Aravind K. Joshi", "acronyms": [[166, 170], [71, 74]], "long-forms": [[138, 164], [19, 39], [112, 133]], "ID": "3646"}, {"text": "ters instead of Y or N as labels. The character-level machine translation (MT) approach (Pennell and Liu, 2011) was modified in (Li and Liu, 2012a)", "acronyms": [[75, 77]], "long-forms": [[54, 73]], "ID": "3647"}, {"text": "of FIS model used to resolve each expression.  4.1 Similarity Features (SIM) Similarity features represent the lexical overlap", "acronyms": [[72, 75], [3, 6]], "long-forms": [[51, 61]], "ID": "3648"}, {"text": "phases that are performed sequentially without feedback: Question Processing (QP), Passage Retrieval (PR) and Answer Extraction (AE). More", "acronyms": [[129, 131], [78, 80], [102, 104]], "long-forms": [[110, 127], [57, 76], [83, 100]], "ID": "3649"}, {"text": "nodes  where  the re lat ion cor responds  to a verb .   Verb  nodes  (VERBSTR)  conta in  a po in ter  to the  RE-  LATION represented  by  the  verb ?", "acronyms": [[71, 78]], "long-forms": [[57, 61]], "ID": "3650"}, {"text": "represents the UTB statistics. For Telugu, the Telugu Treebank (TTB) released for ICON 2010 Shared Task (Husain et al. ( 2010)) was used for evaluation.", "acronyms": [[64, 67], [15, 18], [82, 86]], "long-forms": [[54, 62]], "ID": "3651"}, {"text": "No 0 12 23 292 Table 4: Confusion matrix (SVM) for argument component classification (MC = Major Claim; Cl = Claim; Pr = Premise; No = None)", "acronyms": [[86, 88], [42, 46], [104, 106], [116, 118], [130, 132]], "long-forms": [[91, 102], [109, 114], [121, 128], [135, 139]], "ID": "3652"}, {"text": "For each dataset, we report Pearson?s correlation r with human judgments on pairs that are found in both resources (BOTH). Otherwise, the re-", "acronyms": [[116, 120]], "long-forms": [[100, 104]], "ID": "3653"}, {"text": "understood as follows: ? if the POS-tag of current word  is VB (Verb) and  its word-form  is ? can?", "acronyms": [[60, 62], [32, 39]], "long-forms": [[64, 68]], "ID": "3654"}, {"text": "4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments. STS is related to both Recognizing Textual En-tailment (RTE) and Paraphrase Recognition, but  54", "acronyms": [[371, 374], [12, 14], [36, 38], [132, 135], [153, 156], [315, 318]], "long-forms": [[338, 369], [98, 126]], "ID": "3655"}, {"text": "all levels of syntactic represent-  ation, name\\].y, D-structure,  S-structure, and LF (logical form). ", "acronyms": [[84, 86]], "long-forms": [[88, 100], [55, 64], [69, 78]], "ID": "3656"}, {"text": "demanding.  Latent Semantic Analysis (LSA) (Deerwester et al.,", "acronyms": [[38, 41]], "long-forms": [[12, 36]], "ID": "3657"}, {"text": "Abstract We describe a machine learning approach, a Random Forest (RF) classifier, that is used to automatically compile bilingual", "acronyms": [[67, 69]], "long-forms": [[52, 65]], "ID": "3658"}, {"text": "of MT and HT in terms of the following two ratios, LC = lexical cohesion devices / content words, RC = repetition / content words. ", "acronyms": [[98, 100], [3, 5], [10, 12], [51, 53]], "long-forms": [[103, 123], [56, 72]], "ID": "3659"}, {"text": "tation from a user?s speech. That is, it consists of automatic speech recognition (ASR) and language understanding (LU).", "acronyms": [[83, 86], [116, 118]], "long-forms": [[53, 81], [92, 114]], "ID": "3660"}, {"text": "   Additionally, Mean Reciprocal Rank (MRR) is  also reported.", "acronyms": [[39, 42]], "long-forms": [[17, 37]], "ID": "3661"}, {"text": "transmission envelope; the German and Portuguese services  are sent in the transmission enveloped esigned by the  International Press Telecommunications Council (IPTC). ", "acronyms": [[162, 166]], "long-forms": [[114, 160]], "ID": "3662"}, {"text": "navigation3  SCAN was developed initially for the TREC-6  Spoken Document Retrieval (SDR) task, which em-  ploys the NIST/DARPA HUB4 Broadcast News cor- ", "acronyms": [[85, 88], [117, 127], [128, 132]], "long-forms": [[58, 83], [50, 54]], "ID": "3663"}, {"text": "Finite State Automata (FSA) have traditionally been used in speech processing, but they are clearly  inappropriate for spoken language systems. In this section, we contrast unification grammars (UGs) with  Context-Free grammars (CFGs) and discuss extensions needed for spoken language systems.", "acronyms": [[195, 198], [23, 26], [229, 233]], "long-forms": [[173, 193], [0, 21], [206, 227]], "ID": "3664"}, {"text": "Then, a supervised machine learning algorithm (e.g., Support Vector Machines  (SVM), na?ve Bayesian classifier (NB)) is applied  to the training examples to build a classifier that is ", "acronyms": [[79, 82], [112, 114]], "long-forms": [[85, 99], [53, 76]], "ID": "3665"}, {"text": "~968)). Le SN d~fini  es tmarqu~ par la presence du d~ter-  minati f  d~fini  (DEF), lequel peut ~tre anaphorique (ANAPH),  ou d~monstrat i f  (DEM), selon qu'i l  se r~f~re ~ l 'envi- ", "acronyms": [[115, 120], [11, 13], [79, 82], [144, 147]], "long-forms": [[102, 113], [49, 76], [127, 141]], "ID": "3666"}, {"text": "The  suggestions made here for the organization of space are only a working  set for which l i t t le  justification can be offered: location (LOC)--a  neutral statement of position, contact--in physical contact, and -* near ", "acronyms": [[143, 146]], "long-forms": [[133, 141]], "ID": "3667"}, {"text": "In the cross-validation process, Multinomial Naive Bayes (MNB) has shown better results than Support Vector Machines (SVM) as a component for AdaBoost.", "acronyms": [[118, 121], [58, 61]], "long-forms": [[93, 116], [33, 56]], "ID": "3668"}, {"text": "? ( S (NP (DT the) (NN man)) (VP (VBZ plays) (NP (DT the) (NN piano)))", "acronyms": [[34, 37], [7, 9], [11, 13], [20, 22], [30, 32], [46, 48], [50, 52], [59, 61]], "long-forms": [[38, 43]], "ID": "3669"}, {"text": "la AR  TD  par PREP (prEposition)  main SUBS (substamif)  REC8 (rEcursif simple) ", "acronyms": [[40, 44], [3, 5], [7, 9], [15, 18], [58, 62]], "long-forms": [[46, 55], [21, 32], [64, 72]], "ID": "3670"}, {"text": " 1 Introduction Most Open Information Extraction (Open-IE) systems (Banko et al, 2007) extract textual relational", "acronyms": [[50, 57]], "long-forms": [[21, 48]], "ID": "3671"}, {"text": "1 In t roduct ion   The main goal of the proposed project is to develop  a language model(LM) that uses syntactic structure. ", "acronyms": [[90, 92]], "long-forms": [[75, 88]], "ID": "3672"}, {"text": " The growing need to  manage and search large video collections presents  a challenge to traditional information retrieval (IR)  technologies.", "acronyms": [[124, 126]], "long-forms": [[101, 122]], "ID": "3673"}, {"text": " The second set of experiments was run on the Chinese Treebank (CTB) data sets from Bakeoff-3 (Levow, 2006), which contains a training and a test", "acronyms": [[64, 67]], "long-forms": [[46, 62]], "ID": "3674"}, {"text": "Translation. In Proceedings of the 13th International  Conference on Computational Linguistics (COLING-90),  Vol.", "acronyms": [[96, 105]], "long-forms": [[55, 94]], "ID": "3675"}, {"text": "3.4 Example  of  in tegrat ion   Figure 3 shows the starting point of an integra-  tion process with the trigger word (TW) lelter, its  definition, its temporary graph (TG), the concept ", "acronyms": [[119, 121], [169, 171]], "long-forms": [[105, 117], [152, 167]], "ID": "3676"}, {"text": "~ ~ . ~  5  Object S t r i n g  (OBJL1,ST) Refercn'ce Guide. .............. 9 ", "acronyms": [[33, 41]], "long-forms": [[12, 30]], "ID": "3677"}, {"text": "Nevertheless, LSI has known a resurging interest. Supervised Semantic Indexing (SSI) (Bai et al.,", "acronyms": [[80, 83], [14, 17]], "long-forms": [[50, 78]], "ID": "3678"}, {"text": "we used two other error-annotated learner corpora.  The NUS Corpus of Learner English (NUCLE) contains one million words of academic writing", "acronyms": [[87, 92]], "long-forms": [[56, 85]], "ID": "3679"}, {"text": "We evaluated our parsers using standard labeled accuracy scores (LAS) and unlabeled accuracy scores (UAS) excluding punctuation.", "acronyms": [[101, 104], [64, 68]], "long-forms": [[74, 99], [40, 63]], "ID": "3680"}, {"text": "MOR = morphological features (set) ? LEM = lemma ?", "acronyms": [[37, 40], [0, 3]], "long-forms": [[43, 48], [6, 19]], "ID": "3681"}, {"text": "proaches, namely Bisecting K-Means (Steinbach et al.,  2000), and Latent Semantic Analysis (LSA)-based document clustering (for short, LSA).", "acronyms": [[92, 95], [135, 138]], "long-forms": [[66, 90]], "ID": "3682"}, {"text": "produced by a transliteration system 1. Word Accuracy in Top-1 (ACC) Also known as Word Error Rate, it measures correct-", "acronyms": [[64, 67]], "long-forms": [[45, 53]], "ID": "3683"}, {"text": "sisted of 2000 features, representing the most frequent (head, subject) and (head, object) pairs in the British National Corpus (BNC). The feature-", "acronyms": [[129, 132]], "long-forms": [[104, 127]], "ID": "3684"}, {"text": "tics.  Linguistic Data Consortium (LDC). 2013.", "acronyms": [[35, 38]], "long-forms": [[7, 33]], "ID": "3685"}, {"text": "NP NP\\NP (S\\NP )/NP NP< >NP S\\NP <S Fruit flies like bananas NP S\\NP (S\\S)/NP NP< >S S\\S <S (b) The search space in this work, with one node for each", "acronyms": [[64, 68], [0, 2], [3, 8], [61, 63], [25, 27], [28, 32]], "long-forms": [[70, 77]], "ID": "3686"}, {"text": "For the bilingual corpus, we  used the BTEC and PIVOT data of IWSLT 2008,  HIT corpus 5  and other Chinese LDC (CLDC)                                                            ", "acronyms": [[112, 116], [39, 43], [48, 53], [62, 67], [75, 78]], "long-forms": [[99, 110]], "ID": "3687"}, {"text": "entire sentence length.  SyntaxBased (SyntB): contextual features have been computed according to the ?", "acronyms": [[38, 43]], "long-forms": [[25, 36]], "ID": "3688"}, {"text": "In l~,ooth's approach, the FSV is detined by re-  (:ursion on the truth conditional structure which  is itself derived from LF (i.e. Logical Form, the  Government and Binding level of semantic rep- ", "acronyms": [[124, 126], [27, 30]], "long-forms": [[133, 145]], "ID": "3689"}, {"text": "SemEval 2012 competition for evaluating Natural  Language Processing (NLP) systems presents a  new task called Semantic Textual Similarity (STS)  (Agirre et al, 2012).", "acronyms": [[140, 143], [70, 73]], "long-forms": [[111, 138], [40, 68]], "ID": "3690"}, {"text": "2. Definition of Stochastic Context-Free Grammars  We will now define stochastic ontext free grammars (SCFGs) and establish some  notation.", "acronyms": [[103, 108]], "long-forms": [[70, 101]], "ID": "3691"}, {"text": " 1 Introduction Verb Phrase Ellipsis (VPE) is an anaphoric construction in which a verbal constituent has been omitted.", "acronyms": [[38, 41]], "long-forms": [[16, 36]], "ID": "3692"}, {"text": " In Proceedings of the International Conference on Language Resources and Evaluation (LREC). ", "acronyms": [[86, 90]], "long-forms": [[51, 69]], "ID": "3693"}, {"text": "and a segment relation are identified.  Topic break index (TBI) takes the value of 1 or 2: the boundary with TBI=2 is less con-", "acronyms": [[59, 62], [109, 112]], "long-forms": [[40, 57]], "ID": "3694"}, {"text": "rate words (PERFECT). Second, we let the parser introduce the EEs itself (INSERT). ", "acronyms": [[74, 80], [12, 19]], "long-forms": [[48, 72]], "ID": "3695"}, {"text": "Final step in treebuilding.  The English Destressin8 Rule (EDR) is used to  determ/ne which vowels should be reduced.", "acronyms": [[59, 62]], "long-forms": [[33, 57]], "ID": "3696"}, {"text": " University of Texas at Austin Word sense disambiguation (WSD) is an old and important task in computational linguistics that still remains challenging, to machines as well as to human annotators.", "acronyms": [[58, 61]], "long-forms": [[31, 56]], "ID": "3697"}, {"text": " 2.3 Linear Programming A Linear Program (LP) is an optimization problem where a linear objective function is minimized (or maximized) under linear constraints.", "acronyms": [[42, 44]], "long-forms": [[26, 40]], "ID": "3698"}, {"text": "229 2 Participation in STS-SEM2013  The Semantic Textual Similarity (STS) task consists of estimated the value of semantic similarity ", "acronyms": [[69, 72], [23, 34]], "long-forms": [[40, 67]], "ID": "3699"}, {"text": "report false positives. This can be quantified by the positive predictive value (PPV), or probability that a research finding is true.", "acronyms": [[81, 84]], "long-forms": [[54, 79]], "ID": "3700"}, {"text": "man::n a::d Figure 2: Lexical Only Centered Tree (LOCT) be::v", "acronyms": [[50, 54]], "long-forms": [[22, 48]], "ID": "3701"}, {"text": "since it appears in the corpus with the six differ-  ent tags: CD (cardinal), DT (determiner), JJ (ad-  jective), NN (noun). NNP (proper noun) and VBP ", "acronyms": [[114, 116], [63, 65], [78, 80], [95, 97], [125, 128], [147, 150]], "long-forms": [[118, 122], [67, 75], [82, 92], [104, 111], [130, 141]], "ID": "3702"}, {"text": "three things:  (1) a new formalism for logic grammars, which we  call modifier structure grammars (MSGs),  (2) an interpreter (or parser) for MSGs that takes all ", "acronyms": [[99, 103], [142, 146]], "long-forms": [[70, 97]], "ID": "3703"}, {"text": " A stack based extraction Algorithm 1 was designed to extract a context free grammar (CFG) from the URDU.KON-TB treebank.", "acronyms": [[86, 89], [100, 111]], "long-forms": [[64, 84]], "ID": "3704"}, {"text": "Lack of Orientation (LO). If there is at least one obstacle of the former, more serious kind, we will speak of Dead End (DE). For example, in the case of the DE Exam-", "acronyms": [[121, 123], [20, 23], [158, 160]], "long-forms": [[111, 119], [0, 19]], "ID": "3705"}, {"text": "1 Introduction We extend a popular model, latent Dirichlet al location (LDA), to unbounded streams of documents.", "acronyms": [[72, 75]], "long-forms": [[42, 70]], "ID": "3706"}, {"text": "rejected by the space-saving algorithm.  Least Recently Used (LRU) When the volume of flow in a text stream rapidly increases, it is likely to relate to a burst of a certain topic.", "acronyms": [[62, 65]], "long-forms": [[41, 60]], "ID": "3707"}, {"text": "RERANKED 56.2 13.5 57.3 12.7 ORACLE 85.0 70.3 80.4 60.0 Table 2: Word accuracies and error rate reductions (ERR) in percentages for English-to-Japanese MTL augmented", "acronyms": [[108, 111], [152, 155]], "long-forms": [[85, 106]], "ID": "3708"}, {"text": "4 Hierarchic Autoepistemic  Logic  Autoepistemic (AE) logic was developed by Moore  \\[I0\\] as a reconstruction of McDermott's nonmono- ", "acronyms": [[50, 52]], "long-forms": [[35, 48]], "ID": "3709"}, {"text": "minimal human SO annotation Table 2: Key details of semantic orientation (SO) lexicons. ASL = affix seeds lexicon, GI = General Inquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN =", "acronyms": [[88, 91], [74, 76], [115, 117], [138, 142], [185, 188], [218, 221]], "long-forms": [[94, 113], [52, 72], [120, 136], [145, 183], [191, 216]], "ID": "3710"}, {"text": "This representation uses the logical formulation of feature structures  as given by Kasper and Rounds (1986) and Johnson (1988) and is similar in approach  to the logical formulation of Functional Unification Grammar (FUG) given by Rounds  and Manaster-Ramer (1987).", "acronyms": [[218, 221]], "long-forms": [[186, 216]], "ID": "3711"}, {"text": " 2 Forced Derivation Tree for SMT A forced derivation tree (FDT) of a sentence pair {f, e} can be defined as a pair G =< D,A >:", "acronyms": [[60, 63], [30, 33]], "long-forms": [[36, 58]], "ID": "3712"}, {"text": "4 Variational vs. Min-Risk Decoding In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR): y?", "acronyms": [[125, 128], [18, 26], [52, 55]], "long-forms": [[105, 123]], "ID": "3713"}, {"text": "ed AIC   a) Dialogue ActsOnly (DAONLY)    N (number of hidden states) ", "acronyms": [[31, 37], [3, 6]], "long-forms": [[12, 29]], "ID": "3714"}, {"text": "To v e r i f y  th'e r e l a t i o n s h i p s  between the s t a t i s t i c a l  models of word  importance and t h s  vector space model, dcsument c o l l e c t i o n s  are used i n  three  d i f f e r e n t  subjec t  a reas ,  including aerodynamics (cRAN),  medicine (MED) and  world a f f a i r s  (TIME).", "acronyms": [[275, 278], [257, 261]], "long-forms": [[265, 273]], "ID": "3715"}, {"text": " An alternative paradigm is to view error correction as a statistical machine translation (SMT) problem from ?", "acronyms": [[91, 94]], "long-forms": [[58, 89]], "ID": "3716"}, {"text": "DO: parent:number := node:number; parent:gender := node:gender; 3.1.5 Preposition without children (PrepNoCh) In our dependency trees, the preposition is the", "acronyms": [[100, 108], [0, 2]], "long-forms": [[70, 98]], "ID": "3717"}, {"text": " 1 Introduction Tree substitution grammar (TSG) is a promising formalism for modeling language data.", "acronyms": [[43, 46]], "long-forms": [[16, 41]], "ID": "3718"}, {"text": "X:?  TypeChanging (TCR) X:? ? Y:?(?)", "acronyms": [[19, 22]], "long-forms": [[5, 17]], "ID": "3719"}, {"text": " 2 HMM transitions can be modeled using Weighted Finite State Automata (WFSAs), corresponding to regular expressions.", "acronyms": [[72, 77], [3, 6]], "long-forms": [[40, 70]], "ID": "3720"}, {"text": "S i 3.3.3 Method 3: TrueSkill (TS) TrueSkill is an adaptive, online system that em-", "acronyms": [[31, 33]], "long-forms": [[20, 29]], "ID": "3721"}, {"text": "\u0000 OBST(obstacle): The boy stumbled over a stumb.  \u0000 INTT (intent): He came there to look for Jane. ", "acronyms": [[52, 56], [2, 6]], "long-forms": [[58, 64], [7, 15]], "ID": "3722"}, {"text": "Classification, Word Sense Disambiguation, etc. In  Natural Language Processing (NLP), one of the  most used resources for WSD and other tasks is ", "acronyms": [[81, 84], [123, 126]], "long-forms": [[52, 79]], "ID": "3723"}, {"text": "II: irrelevant input due to ASR errors or noise.  We adopt logistic regression (LR)-based dialogue act tagging approach (Tur et al.,", "acronyms": [[80, 82], [28, 31]], "long-forms": [[59, 78]], "ID": "3724"}, {"text": "ysis incorporating social networks. In Proceedings of Knowledge Discovery and Data Mining (KDD). ", "acronyms": [[91, 94]], "long-forms": [[54, 82]], "ID": "3725"}, {"text": "means, correct  sentences could then be  computat iona l ly  generated from the lo-  g ical  patters  ment ioned  in (II). ", "acronyms": [], "long-forms": [], "ID": "3726"}, {"text": "with their scope and corresponding negated events is an important task that could benefit other natural language processing (NLP) tasks such as extraction of factual information", "acronyms": [[125, 128]], "long-forms": [[96, 123]], "ID": "3727"}, {"text": "jective U-shaped is an example of gesture enriching an adjectival meaning through the interface default Adjective meaning extended (AdjMExt) AdjMExt: Adjective(u), sem(u) is ?", "acronyms": [[132, 139], [141, 148]], "long-forms": [[104, 130]], "ID": "3728"}, {"text": "2. We propose a new topic model, Topic Sentiment Latent Dirichlet Allocation (TSLDA), which can capture the topic and sentiment si-", "acronyms": [[78, 83]], "long-forms": [[33, 76]], "ID": "3729"}, {"text": "minimal set of defaults.  A Preferential Default Description Logic (PDDL)  based on weigthed defaults has been developed in ", "acronyms": [[68, 72]], "long-forms": [[28, 66]], "ID": "3730"}, {"text": "attempts to use synchronous parsing to produce the  tree structure of both source language (SL) and  target language (TL) simultaneously, most SSMT  approaches make use of monolingual parser to ", "acronyms": [[118, 120], [92, 94], [143, 147]], "long-forms": [[101, 116], [75, 90]], "ID": "3731"}, {"text": "In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 1262?1267, Genoa, Italy, May.", "acronyms": [[89, 93]], "long-forms": [[54, 87]], "ID": "3732"}, {"text": "vector built from training set. However, this  could lead to many out of vocabulary (OOV)  cases, in addition to long vector.", "acronyms": [[85, 88]], "long-forms": [[66, 83]], "ID": "3733"}, {"text": "to such an extent that?)  Multiword interjections (MWI) are a small category with expressions such as mille sabords (?", "acronyms": [[51, 54]], "long-forms": [[26, 49]], "ID": "3734"}, {"text": "1 Int roduct ion   In a recent paper Boguraev and Levin (1990) point out inadequacies in common concep-  tions of what a Lexical Knowledge Base (LKB) should be, inadequacies which stem from  the assumption that a machine-readable dictionary (MRD) is not only the right source for ", "acronyms": [[145, 148]], "long-forms": [[121, 143]], "ID": "3735"}, {"text": " It is a Neo-Reichenbachian representation (Reichenbach,  1966) in that its s imple tense s t ructures  (STSs) re-  late the following three entities: the time of the event ", "acronyms": [[105, 109]], "long-forms": [[76, 102]], "ID": "3736"}, {"text": " c?2015 Association for Computational Linguistics Building a Scientific Concept Hierarchy Database (SCHBASE) Eytan Adar", "acronyms": [[100, 107]], "long-forms": [[61, 98]], "ID": "3737"}, {"text": "for more general audience of users.  Helping our own (HOO) is an initiative that could in future spark a new interest in the re-", "acronyms": [[54, 57]], "long-forms": [[37, 52]], "ID": "3738"}, {"text": " In a second experiment, we used the original TGRD corpus but added the language variety (LV) (i.e., MSA and DA) features.", "acronyms": [[90, 92], [46, 50], [101, 104], [109, 111]], "long-forms": [[72, 88]], "ID": "3739"}, {"text": "Abstract   In recent years, statistical approaches on  ATR (Automatic Term Recognition) have  achieved good results.", "acronyms": [[55, 58]], "long-forms": [[60, 86]], "ID": "3740"}, {"text": "wk} be its vocabulary. In the monolingual settings, the Vector Space Model (VSM) is a k-dimensional space Rk, in which the text tj ?", "acronyms": [[76, 79], [106, 108]], "long-forms": [[56, 74]], "ID": "3741"}, {"text": "The starting point for the approach followed here was a dissatisfaction with certain  aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and  implemented in SRI's Core Language Engine (CLE). In the CLE-QLF approach, as ra- ", "acronyms": [[216, 219], [188, 192], [229, 236]], "long-forms": [[194, 214]], "ID": "3742"}, {"text": ", VP ...... Others (OTHER): The remaining cases of comma receive the OTHER label, indicating they do", "acronyms": [[20, 25]], "long-forms": [[12, 18]], "ID": "3743"}, {"text": "2014).  The RST Discourse Treebank (RST-DT) (Carlson et al.,", "acronyms": [[36, 42]], "long-forms": [[12, 34]], "ID": "3744"}, {"text": "resent horizontal movement of the eyebrows.  2.2 Continuous Profile Models (CPM) Continuous Profile Model (CPM) aligns a set", "acronyms": [[76, 79], [107, 110]], "long-forms": [[49, 74], [81, 105]], "ID": "3745"}, {"text": "annotations of sentiment values for individual syntactic phrases in a binarized tree, and an approach based on recursive neural tensor networks (RNTN) which yields significant improvements over the ear-", "acronyms": [[145, 149]], "long-forms": [[111, 143]], "ID": "3746"}, {"text": "used throughout the paper. The notation is that  used in the Core Language Engine (CLE) devel-  oped by SKI's Cambridge Computer Science Re- ", "acronyms": [[83, 86], [104, 107]], "long-forms": [[61, 81]], "ID": "3747"}, {"text": "We show that the best  prediction of translation complexity is given by the  average number of syllables per word (ASW). The ", "acronyms": [[115, 118]], "long-forms": [[77, 113]], "ID": "3748"}, {"text": "5 Conclusion In this pilot experiment, we explore the possibility of using Amazon Mechanical Turk (MTurk) to collect bilingual word alignment data to assist automatic word align-", "acronyms": [[99, 104]], "long-forms": [[82, 97]], "ID": "3749"}, {"text": "representation. Thus, it is necessary to define a proper ECS for each  language: Japanese ECS (J-ECS) \\[6\\] for Japanese language and  English ECS (E-ECS) \\[7\\] for English language.", "acronyms": [[95, 100], [57, 60], [148, 153]], "long-forms": [[81, 93], [135, 146]], "ID": "3750"}, {"text": "   In the SUM (Summarization) setting, the  entailment pairs were generated using two proce-", "acronyms": [[10, 13]], "long-forms": [[15, 28]], "ID": "3751"}, {"text": "ability and to give analytical insights into the  features. Classification Accuracy (CAcc), the  percentage of the correctly labeled instances over ", "acronyms": [[85, 89]], "long-forms": [[60, 83]], "ID": "3752"}, {"text": " 3.1 Underspecified domains An underspecified domain (UD) represents a partially specified reference domain corresponding to the", "acronyms": [[54, 56]], "long-forms": [[31, 52]], "ID": "3753"}, {"text": "ers. In Proceedings of the 19th International Conference on Compuatational Linguistics (COLING),  pages 556?562.", "acronyms": [[88, 94]], "long-forms": [[60, 86]], "ID": "3754"}, {"text": "Due to the existence of CTB-I, we were able to train new automatic Chinese language processing (CLP) tools, which crucially use annotated corpora as training", "acronyms": [[96, 99], [24, 29]], "long-forms": [[67, 94]], "ID": "3755"}, {"text": "explicitly addresses the language ambiguity issue. Key to our approach is the use of Word Sense Induction (WSI), that is, techniques aimed at automatically discovering the different meanings of a given term (i.e., query).", "acronyms": [[107, 110]], "long-forms": [[85, 105]], "ID": "3756"}, {"text": "NE = Named Entity CE = Correlated Entity EP = Entity Profile SVO = Subject-Verb-Object", "acronyms": [[41, 43], [0, 2], [18, 20], [61, 64]], "long-forms": [[46, 60], [5, 17], [23, 40], [67, 86]], "ID": "3757"}, {"text": "2-21. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conver-", "acronyms": [[69, 73]], "long-forms": [[47, 67]], "ID": "3758"}, {"text": "verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering.", "acronyms": [[74, 76], [42, 44]], "long-forms": [[50, 72], [24, 40]], "ID": "3759"}, {"text": "though domain sublanguages are characterized by specific vocabularies, a well-defined border between specific sublanguages (SLs) and general language (GL) vocabularies is difficult to establish", "acronyms": [[124, 127], [151, 153]], "long-forms": [[110, 122], [133, 149]], "ID": "3760"}, {"text": " 1 Introduction The Semantic Textual Similarity (STS) shared task consists of several data sets of paired passages of", "acronyms": [[49, 52]], "long-forms": [[20, 47]], "ID": "3761"}, {"text": "1 Introduction Kernel methods are considered the most effective techniques for various relation extraction (RE) tasks on both general (e.g. newspaper text) and", "acronyms": [[108, 110]], "long-forms": [[87, 106]], "ID": "3762"}, {"text": "Haile) and (?????, Selassie). TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al, 2009a).", "acronyms": [[98, 100], [135, 138], [185, 188], [189, 191], [30, 32]], "long-forms": [[75, 96], [106, 133]], "ID": "3763"}, {"text": "referred to as Maximum Mutual Information Estimation (MMIE) and the second component Maximum Likelihood Estimation (MLE), therefore in this paper we use a brief notation for (1) just for conve-", "acronyms": [[116, 119]], "long-forms": [[89, 114]], "ID": "3764"}, {"text": "include: 1) candidate frequency and its distribution in different Web pages, 2) length ratio between source terms and target candidates (S-T), 3)  distance between S-T, and 4) keywords, key ", "acronyms": [[137, 140], [164, 167]], "long-forms": [[101, 135]], "ID": "3765"}, {"text": "process (PR)  quantity (QU)  relation (RE)  shape (SH) ", "acronyms": [[39, 41], [9, 11], [24, 26], [51, 53]], "long-forms": [[29, 37], [0, 7], [14, 22], [44, 49]], "ID": "3766"}, {"text": "As a result, a text to text similarity measure is built based on word to word similarity. It is as-sumed that combining word to word similarity metrics to text level would be a good indicator of text to text similarity. 4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments.", "acronyms": [[352, 355], [232, 234], [256, 258], [373, 376]], "long-forms": [[318, 346]], "ID": "3767"}, {"text": "There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase", "acronyms": [[116, 118]], "long-forms": [[93, 114]], "ID": "3768"}, {"text": " 2 Tutorial Dialogue Setting and Data My Science Tutor (MyST) (Ward et al, 2010) is a conversational virtual tutor designed to improve science learning and understanding for students in grades 3-5.", "acronyms": [[56, 60]], "long-forms": [[38, 54]], "ID": "3769"}, {"text": "tion). All markables have named entity types such as FACILITY, GPE (geopolitical entity), PERSON, LOCATION, ORGANIZATION, PERSON, VEHI-", "acronyms": [[63, 66]], "long-forms": [[68, 87]], "ID": "3770"}, {"text": "of money to perform tasks that are simple for humans but difficult for computers. Examples of these Human Intelligence Tasks (HITs) range from labeling images to moderating blog comments to providing feedback on the relevance of results for a search query.", "acronyms": [[126, 130]], "long-forms": [[100, 124]], "ID": "3771"}, {"text": " 1 In t roduct ion :  a problem  Grammar development environments (GDE's) for  analysis and for generation have not yet come to- ", "acronyms": [[67, 72]], "long-forms": [[33, 65]], "ID": "3772"}, {"text": "natural language utterances. It accesses a database typi-  cal for the information retrieval task (CORDIS). ", "acronyms": [[99, 105]], "long-forms": [[59, 97]], "ID": "3773"}, {"text": "as automatic speech recognition (ASR), natural language understanding (NLU), dialogue management (DM), natural language generation (NLG), and speech synthesis (TTS).", "acronyms": [[132, 135], [33, 36], [71, 74], [98, 100], [160, 163]], "long-forms": [[103, 130], [3, 31], [39, 69], [77, 96]], "ID": "3774"}, {"text": "posterior distribution.  We utilize maximum entropy (MaxEnt) model  (Berger et al, 1996) to design the basic classifier ", "acronyms": [[53, 59]], "long-forms": [[36, 51]], "ID": "3775"}, {"text": " Below we focus on a special case of the latter problem: noun compound (NC) coordination. Con-", "acronyms": [[72, 74]], "long-forms": [[57, 70]], "ID": "3776"}, {"text": " [and, therefore, so]  Contrastive Connectives (CC)  men den ?", "acronyms": [[48, 50]], "long-forms": [[23, 46]], "ID": "3777"}, {"text": "Note that the proponents of the BootCaT method seem to acknowledge this evolution, see for example Marco Baroni?s talk at this year?s BootCaTters of the world unite (BOTWU) workshop: ?", "acronyms": [[166, 171], [32, 39]], "long-forms": [[134, 164]], "ID": "3778"}, {"text": "Method (METH) the methods used Result (RES) the results achieved Conclusion (CON) the authors? conclusions", "acronyms": [[77, 80], [8, 12], [39, 42]], "long-forms": [[65, 75], [0, 6], [31, 37]], "ID": "3779"}, {"text": "294  The surface phonetic tones are:  LC = low constant (in Baule, only initial)  HC = high constant (only initial) ", "acronyms": [[38, 40], [82, 84]], "long-forms": [[43, 55], [87, 100]], "ID": "3780"}, {"text": "many competing approaches to tagging problems including Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs).", "acronyms": [[116, 121], [78, 82], [154, 158]], "long-forms": [[89, 114], [56, 76], [127, 152]], "ID": "3781"}, {"text": "cation which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).", "acronyms": [[92, 94], [124, 128]], "long-forms": [[80, 90], [100, 123]], "ID": "3782"}, {"text": "The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al.,", "acronyms": [[104, 106], [63, 66]], "long-forms": [[78, 102], [42, 61]], "ID": "3783"}, {"text": "NIST-06. The bilingual training corpus comes from Linguistic Data Consortium (LDC)6, which consists of 3.4M sentence pairs with 64M/70M Chi-", "acronyms": [[78, 81], [0, 7]], "long-forms": [[50, 76]], "ID": "3784"}, {"text": " 1 Introduction Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output of an MT system given an input and no reference translation (Blatz et al.,", "acronyms": [[36, 38], [65, 67], [122, 124]], "long-forms": [[16, 34], [44, 63]], "ID": "3785"}, {"text": "pondence between proofs and dependency structures.  Dependency grammar (DG) takes as fundamental  ~This approach of 'normal form parsing' has been ", "acronyms": [[72, 74]], "long-forms": [[52, 70]], "ID": "3786"}, {"text": "@\"' itself is counted. Another way is to count how  many parts of words (PWs) are eontMnmd in the  SA.", "acronyms": [[73, 76], [99, 101]], "long-forms": [[57, 71]], "ID": "3787"}, {"text": " CTexT. 2005. CKarma (C5 KompositumAnaliseerder vir Robuuste Morfologiese Analise). [ C5 Compound", "acronyms": [[14, 20], [1, 6]], "long-forms": [[22, 47]], "ID": "3788"}, {"text": "Source Material (SMaterial) e.g., As, InGaAs   ? Source Material Characteristic  (SMChar) e.g. ,(111)B  ", "acronyms": [[82, 88], [17, 26], [38, 44]], "long-forms": [[49, 79], [0, 15]], "ID": "3789"}, {"text": "namely person name, location name, organization name and miscellaneous name to apply  Support Vector Machine (SVM) based machine  learning technique.", "acronyms": [[110, 113]], "long-forms": [[86, 108]], "ID": "3790"}, {"text": "standing that shares tasks with OIE. AMR parsing (Banarescu et al, ), semantic role labeling (SRL) (Toutanova et al, 2008; Punyakanok et al, 2008)", "acronyms": [[94, 97], [37, 40], [32, 35]], "long-forms": [[70, 92]], "ID": "3791"}, {"text": "It  is at this critical point, when care is being trans-  ferred from the Operating Room (OR) to the ICU  and monitoring is at a minimum, that the pa- ", "acronyms": [[90, 92], [101, 104]], "long-forms": [[74, 88]], "ID": "3792"}, {"text": "As the entire sentence is informative to determine the proper conjunction of all roles, a Smoothed Partial Tree Kernel (SPTK) within the classifier that enhances both syntactic and lexical in-", "acronyms": [[120, 124]], "long-forms": [[90, 118]], "ID": "3793"}, {"text": "The input to the first block are the words of the TEXTCHUNK, represented by CW (Collobert and Weston, 2008) embeddings.", "acronyms": [[76, 78], [50, 59]], "long-forms": [[80, 100]], "ID": "3794"}, {"text": "conjuncts depend on it. Nilsson et al (2007)  advocate the Mel?cuk style (MS) for parsing  Czech, taking the first conjunct as the head, ", "acronyms": [[74, 76]], "long-forms": [[59, 72]], "ID": "3795"}, {"text": "The data are packaged as the payload of incremental units (IU) which are passed between modules. The IUs themselves are also interconnected via so-called same level links (SLL) and groundedin links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that", "acronyms": [[172, 175], [59, 61], [101, 104], [199, 203], [241, 244]], "long-forms": [[154, 170], [40, 57], [181, 197]], "ID": "3796"}, {"text": "142 ? National University of Mongolia (NUM),  Mongolia ", "acronyms": [[39, 42]], "long-forms": [[6, 37]], "ID": "3797"}, {"text": "LPM output using application knowledge  ? Function Generator Module (FGM) converting  SAM output into executable function calls ", "acronyms": [[69, 72], [86, 89], [0, 3]], "long-forms": [[42, 67]], "ID": "3798"}, {"text": "respect to phonology, morphology, syntax and the lexicon. Linguistic resources (lexica, corpora) and natural language processing (NLP) tools for such dialects (parsers) are very rare. ", "acronyms": [[130, 133]], "long-forms": [[101, 128]], "ID": "3799"}, {"text": "Lexical features show a much more mixed result. Type?Token Ratio (TTR) is only important for document classification, whereas most of the", "acronyms": [[66, 69]], "long-forms": [[48, 64]], "ID": "3800"}, {"text": "AVERAGE 70.8% 70.1% Table 2: Ten-fold cross-validation classification results, using a Na??ve Bayes (NB) or Support Vector Machines (SVM) classifier", "acronyms": [[101, 103], [133, 136]], "long-forms": [[87, 99], [108, 131]], "ID": "3801"}, {"text": "Ravi and Knight (2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while Corlett and Penn (2010) solve the problem using", "acronyms": [[134, 137]], "long-forms": [[110, 132]], "ID": "3802"}, {"text": "is re-written as NN, and NNPS as NNP.  Parent (PP) The category of the parent node of the NP. ", "acronyms": [[47, 49], [17, 19], [25, 29], [33, 36], [90, 92]], "long-forms": [[39, 45]], "ID": "3803"}, {"text": "subset by eliminating the redundant features.  In this paper, Rough Set Theory (RST) based  feature selection method is applied for sen-", "acronyms": [[80, 83]], "long-forms": [[62, 78]], "ID": "3804"}, {"text": "Model level, there is an intermediate stage of analysis , characterized by a  formal language , resp r  - The Engliaboriented Formal Language ( EFL) , which containa  constant^ that  correspond to the terms of English, This language is wed to represent the ", "acronyms": [[144, 147]], "long-forms": [[110, 141]], "ID": "3805"}, {"text": "This  was a motivating factor for the establishment of the  Common Pattern Specification Language (CPSL)  Working Group devoted to formulating a CPSL in ", "acronyms": [[99, 103], [145, 149]], "long-forms": [[60, 97]], "ID": "3806"}, {"text": "CFG. The proof is based on a lexicalization procedure related to the lexicalization  procedure used to create Greibach normal form (GNF) as presented in Harrison 1978. ", "acronyms": [[132, 135], [0, 3]], "long-forms": [[110, 130]], "ID": "3807"}, {"text": "kitchen cabinet and will hardly be able to win the elections]. The parse tree contains phrase labels NP (Noun Phrase), PP (Prepositional Phrase), VP (Verb Phrase), S (Sentence), and CS (Coordinated Sentence).", "acronyms": [[119, 121], [146, 148]], "long-forms": [[123, 143], [150, 161]], "ID": "3808"}, {"text": "inference of lexical semantic roles After the training phase, a testing procedure using the Markov Chain Monte Carlo (MCMC) inference engine can be used to infer role labels.", "acronyms": [[118, 122]], "long-forms": [[92, 116]], "ID": "3809"}, {"text": "segmentation While the model structure is reminiscent of a factorial hidden Markov model (HMM), there are important differences that prevent the direct application of", "acronyms": [[90, 93]], "long-forms": [[69, 88]], "ID": "3810"}, {"text": "KEA: Practical automatic keyphrase  extraction. Proceedings of Digital Libraries 99 (DL'99), pp. ", "acronyms": [[85, 90], [0, 3], [93, 95]], "long-forms": [[63, 83]], "ID": "3811"}, {"text": "and Chinese (ZH). The second section gives the result for the English (EN) test set, PTB Section 23. ", "acronyms": [[71, 73], [13, 15], [85, 88]], "long-forms": [[62, 69], [4, 11]], "ID": "3812"}, {"text": "figure 5).  Base clauses (BC)  are subclauses of type sub-  junctive and subordinate.", "acronyms": [[26, 28]], "long-forms": [[12, 24]], "ID": "3813"}, {"text": " enard Centre de Recherche Informatique de Montr?eal (CRIM) Montr?eal, QC, Canada", "acronyms": [[54, 58], [71, 73]], "long-forms": [[7, 52]], "ID": "3814"}, {"text": " Table 10: Average value of the mutual infor-  mation (MI) of compound noun seeds  .Number of elements \\[ 2 I 3 ", "acronyms": [[55, 57]], "long-forms": [[47, 53]], "ID": "3815"}, {"text": "(iv) Embedded appositional phrases.  (2) Very long PE's (Phrasal Elements) appear occasionally. ( eg.", "acronyms": [[51, 55]], "long-forms": [[57, 73]], "ID": "3816"}, {"text": "The next four columns show the number of true positives (TP)--verbs judged +S both  by machine and by hand; false positives (FP)--verbs judged +S by machine, -S  by  hand; true negatives (TN)--verbs judged -S  both by machine and by hand; and false  negatives (FN)--verbs judged -S  by machine, +S by hand.", "acronyms": [[188, 190], [57, 59], [125, 127], [261, 263]], "long-forms": [[172, 186], [41, 54], [108, 122], [243, 258]], "ID": "3817"}, {"text": "prior polarity of verb, verb score (V_score).  Verb-PP (prepositional phrase) rules:  1.", "acronyms": [[52, 54]], "long-forms": [[56, 76]], "ID": "3818"}, {"text": "In: Ernst Buch-berger (ed.): Tagungsband der 7. Konferenz zur Verarbeitung nat?rlicher Sprache (KONVENS), Universit?t Wien, 161?168. Strube, Gerhard (1984).", "acronyms": [[96, 103]], "long-forms": [[48, 94]], "ID": "3819"}, {"text": "  2 Dimensionality Reduction   VSM (Vector Space Model) is a basic technique  to transform text documents to numeric vectors.", "acronyms": [[31, 34]], "long-forms": [[36, 54]], "ID": "3820"}, {"text": "words. In Proceedings of the International Conference on Computational Linguistics (COLING). ", "acronyms": [[84, 90]], "long-forms": [[57, 82]], "ID": "3821"}, {"text": "CC = coordinating conjunction; CD = cardinal number; JJ = adjective; MD = modal; NN = singular noun; NNP = proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB = base form verb; VBD = past tense verb; VBZ = third-person singular present verb.", "acronyms": [[179, 181], [0, 2], [31, 33], [53, 55], [69, 71], [81, 83], [101, 104], [120, 124], [147, 150], [166, 168], [188, 190], [209, 212], [232, 235]], "long-forms": [[184, 186], [5, 29], [36, 44], [58, 67], [74, 79], [95, 99], [107, 118], [127, 145], [153, 164], [171, 177], [203, 207], [215, 230], [238, 273]], "ID": "3822"}, {"text": "2003). Recently, Huang et al (2015) showed that building a conditional random field (CRF) layer on top of bidirectional LSTM-RNNs performs com-", "acronyms": [[85, 88], [120, 129]], "long-forms": [[59, 83]], "ID": "3823"}, {"text": " 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly", "acronyms": [[47, 49]], "long-forms": [[27, 45]], "ID": "3824"}, {"text": "is a very laborious and costly process. Silver standard corpus (SSC) annotation is a very recent direction of corpus development which", "acronyms": [[64, 67]], "long-forms": [[40, 62]], "ID": "3825"}, {"text": "the boys) or part of a complex coordinating conjunction (both boys and girls), the Penn  Treebank tags both differently in each of these syntactic ontexts--as PDT (predeter-  miner), RB (adverb), NNS (plural common noun) and coordinating conjunction (CC),  respectively.", "acronyms": [[251, 253], [159, 162], [183, 185], [196, 199]], "long-forms": [[225, 249], [164, 173]], "ID": "3826"}, {"text": "noun modification? which generally is shown in the form of a Noun phrase (NP) [A DE B]. A in-", "acronyms": [[74, 76]], "long-forms": [[61, 72]], "ID": "3827"}, {"text": "tion), and thus mitigating the overfitting problem.  A Dirichlet process (DP) prior is typically used to achieve this interplay.", "acronyms": [[74, 76]], "long-forms": [[55, 72]], "ID": "3828"}, {"text": "We performed a 10-fold cross-validation on each dataset and experimented with three feature sets by using a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995).", "acronyms": [[132, 135]], "long-forms": [[108, 130]], "ID": "3829"}, {"text": "potential can be fully exploited by our convolution partial tree kernel: - Dependency Words (DW) tree is similar to PET adapted for dependency tree constituted", "acronyms": [[93, 95], [116, 119]], "long-forms": [[75, 91]], "ID": "3830"}, {"text": " 1 Introduction Currently the Machine Translation (MT) research community attempts to seamlessly integrate both", "acronyms": [[51, 53]], "long-forms": [[30, 49]], "ID": "3831"}, {"text": "+ b) (4) where f is a non-linear activation function such as rectified linear unit (ReLu) or sigmoid function. ", "acronyms": [[84, 88]], "long-forms": [[61, 82]], "ID": "3832"}, {"text": "PCFG = Probabilistic  Context-Free Grammar, LM = Bigram Model with Witten-Bell smoothing,  PM = Priority Model. ", "acronyms": [[91, 93], [0, 4], [44, 46]], "long-forms": [[96, 110], [7, 42], [49, 61]], "ID": "3833"}, {"text": "4.1 The NIST evaluation scheme The National Institute of Science and Technology (NIST) proposed an evaluation scheme that looks at the following properties when", "acronyms": [[81, 85], [8, 12]], "long-forms": [[35, 79]], "ID": "3834"}, {"text": " 6. Decision Tree (DT) - with 12,782 MWEs of D5.", "acronyms": [[19, 21]], "long-forms": [[4, 17]], "ID": "3835"}, {"text": " 4.4 Tokenizing Multiword Expressions      Multiword Expressions (MWEs) are two or  more words that behave like a single word syntac-", "acronyms": [[66, 70]], "long-forms": [[43, 64]], "ID": "3836"}, {"text": "VBL (Light Verb) is used in complex predicates (Butt 1995), but its syntactic similarity with  VB (Verb) is a major source of confusion in automatic tagging.", "acronyms": [[95, 97], [0, 3]], "long-forms": [[99, 103], [5, 15]], "ID": "3837"}, {"text": " Since we planned to eventually test our algorithms in  word recognition on the Resource Management (RM)  database, our phone classification experiments were also ", "acronyms": [[101, 103]], "long-forms": [[80, 99]], "ID": "3838"}, {"text": "We present an open source, freely available Java implementation of Align, Disambiguate, and Walk (ADW), a state-of-the-art approach for measuring semantic similarity based on", "acronyms": [[98, 101]], "long-forms": [[67, 96]], "ID": "3839"}, {"text": "suffix and prefix information, as well as information about the sorrounding words and their tags are used to develop a Maximum Entropy (MaxEnt) based Hindi NER system.", "acronyms": [[136, 142], [156, 159]], "long-forms": [[119, 134]], "ID": "3840"}, {"text": "which fixes its results after a given time ? and report the corresponding word error rate (WER). This", "acronyms": [[91, 94]], "long-forms": [[74, 89]], "ID": "3841"}, {"text": "set of pivots from a given pair of domains such as the minimum frequency of occurrence of a feature in the two domains, mutual information (MI), and the entropy of the feature distribution over the", "acronyms": [[140, 142]], "long-forms": [[120, 138]], "ID": "3842"}, {"text": "Abstract  Since statistical machine translation (SMT)  and translation memory (TM) complement  each other in matched and unmatched regions, ", "acronyms": [[79, 81], [49, 52]], "long-forms": [[59, 77], [16, 47]], "ID": "3843"}, {"text": " In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain.", "acronyms": [[74, 77], [85, 87]], "long-forms": [[53, 72]], "ID": "3844"}, {"text": "Senior Researcher and Lecturer Knowledge Management Group Applied Computer Science Institute (AIFB) University of Karlsruhe, Germany", "acronyms": [[94, 98]], "long-forms": [[58, 92]], "ID": "3845"}, {"text": "We gratefully acknowledge the support of Turkish  Scientific and  Technological Research Council of  Turkey  (TUBITAK)  and  METU  Scientific  Research  Fund  (no.", "acronyms": [[110, 117], [125, 129]], "long-forms": [], "ID": "3846"}, {"text": "ing at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a syn-", "acronyms": [[100, 103]], "long-forms": [[69, 98]], "ID": "3847"}, {"text": "sided brevity penalty (C = 0.01). Table 2 shows the average compression rates (CompR) for McDonald (2006) and our model (STSG) as well as their perfor-", "acronyms": [[79, 84], [121, 125]], "long-forms": [[60, 77]], "ID": "3848"}, {"text": "sentence are not participate in evaluation. Exact  match rate(EMR), violate match rate(VMR), and  inside match rate(IMR) denote the ratio of three ", "acronyms": [[87, 90], [62, 65], [116, 119]], "long-forms": [[68, 85], [44, 61], [98, 115]], "ID": "3849"}, {"text": "extract phrasal translations or transliterations of  phrase based on machine learning, or more  specifically the conditional random fields (CRF)  model.", "acronyms": [[140, 143]], "long-forms": [[113, 138]], "ID": "3850"}, {"text": "unigram model (BoW)+HI, where in addition to representing what words occur in a text, we also represent what Harvard Inquirer (HI)3 word classes occur in it.", "acronyms": [[127, 129], [15, 18], [20, 23]], "long-forms": [[109, 125]], "ID": "3851"}, {"text": "10-best(+AG) 4.35 90.0 Table 1: Task completion rate according to using the AG (Agenda Graph) and n-best hypotheses for n=1 and n=10.", "acronyms": [[76, 78], [9, 11]], "long-forms": [[80, 92]], "ID": "3852"}, {"text": "Total 2,910 1,086 3,996 Table 1: Number of annotated elements per category in our gold standard (CR=controlled requirements, UR=uncontrolled requirements)", "acronyms": [[97, 99], [125, 127]], "long-forms": [[100, 123], [128, 140]], "ID": "3853"}, {"text": "lem of selecting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that crosslingual word embeddings will allow for accurate", "acronyms": [[128, 130]], "long-forms": [[102, 126]], "ID": "3854"}, {"text": "~ '1  procedural component  Q data structure  SSP = SemanticJSyntacticJPhonological  Figl~e 1: The SYNPHONICS Formulator ", "acronyms": [[46, 49], [99, 109]], "long-forms": [[52, 83]], "ID": "3855"}, {"text": "proper textual evidences. We formulate this task as an Integer Linear Programming (ILP). Instead of", "acronyms": [[83, 86]], "long-forms": [[55, 81]], "ID": "3856"}, {"text": "8. Strong forms of pronouns not preceded by preposition (unless they carry IC) t Table 1: Annotation guidelines; IC = Intonation Center 4.2 Evaluation framework", "acronyms": [[113, 115], [75, 77]], "long-forms": [[118, 135]], "ID": "3857"}, {"text": " scores cw(ei) are combined: MEAN CM (cM (eI1)) is computed as the geometric mean of the confidence scores of the", "acronyms": [[34, 36]], "long-forms": [[38, 40]], "ID": "3858"}, {"text": "probabilistic Earley?s, and minimum edit distance algorithms). Dynamic programming (DP) involves solving certain kinds of recursive equations", "acronyms": [[84, 86]], "long-forms": [[63, 82]], "ID": "3859"}, {"text": "Our model can now be represented like this:  241  Database (DB)  Facts about hotels ", "acronyms": [[60, 62]], "long-forms": [[50, 58]], "ID": "3860"}, {"text": "sible transliteration candidates. We measured performance using the Mean Reciprocal Rank (MRR) measure.", "acronyms": [[90, 93]], "long-forms": [[68, 88]], "ID": "3861"}, {"text": "Traditional readability measures for L1 Swedish at the text level include LIX (L?asbarthetsindex, ? Readability index?)", "acronyms": [[74, 77], [37, 39]], "long-forms": [[79, 96]], "ID": "3862"}, {"text": " 1 Introduction  Statistical Machine Translation(SMT) is currently the state of the art solution to the machine ", "acronyms": [[49, 52]], "long-forms": [[17, 47]], "ID": "3863"}, {"text": "email: mal@aber.ac.uk Stephen Pulman University of Oxford (UK) email: sgp@clg.ox.ac.uk", "acronyms": [[59, 61]], "long-forms": [[37, 57]], "ID": "3864"}, {"text": "The parser uses a semantic grammar with approx-  imately 1000 rules which maps the input sentence  onto an interlingua representation (ILT) which rep-  resents the meaning of the sentence in a language- ", "acronyms": [[135, 138]], "long-forms": [[107, 133]], "ID": "3865"}, {"text": "? SRI has developed the DECIPHER speaker-independent speech recognition system, a  hidden Markov model (HMM)-based system that achieves tate-of-the-art recognition  performance through accurate modeling of phonetic and phonological detail.", "acronyms": [[104, 107], [2, 5], [24, 32]], "long-forms": [[83, 102]], "ID": "3866"}, {"text": "8. THE SAIL INTERFACING SYSTEM  The SAIL Interfacing System (S.I.S.) is the f ramework   where a user  can interact with SAIL in developing NL ", "acronyms": [[61, 67], [140, 142], [121, 125]], "long-forms": [[36, 59]], "ID": "3867"}, {"text": "? ? ? ?  Figure 2: Laten Event Model (LEM). ", "acronyms": [[38, 41]], "long-forms": [[19, 36]], "ID": "3868"}, {"text": "ing different methods: The methods respectively without prediction(NP), with prediction(P),  with prediction and feedback(PF) only using term  frequency (TM), and with prediction and feed-", "acronyms": [[122, 124], [67, 69], [154, 156]], "long-forms": [[98, 120], [77, 87], [137, 152]], "ID": "3869"}, {"text": "Language Weaver, Inc. This article shows that the structure of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. ", "acronyms": [[198, 201]], "long-forms": [[165, 196]], "ID": "3870"}, {"text": "construction relies on existing natural language processing tools, e.g., syntactic parsers (Wiebe, 2000), information extraction (IE) tools (Riloff and Wiebe, 2003) or rich lexical resources such", "acronyms": [[130, 132]], "long-forms": [[106, 128]], "ID": "3871"}, {"text": " To combat this inefficiency, after every state transition we estimate the effective sample size (ESS) of the particle weights as ??", "acronyms": [[98, 101]], "long-forms": [[75, 96]], "ID": "3872"}, {"text": " (UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance).", "acronyms": [[47, 49], [2, 4], [91, 93]], "long-forms": [[52, 61], [7, 16], [96, 112]], "ID": "3873"}, {"text": "(Vehicle). Each mention of an entity has a mention  type: NAM (proper name), NOM (nominal) or                                                            ", "acronyms": [[77, 80]], "long-forms": [[82, 89]], "ID": "3874"}, {"text": "4.1 Selection of PPs in the Lexicon Our parser makes use of the computational lexicon HaGenLex (Hagen German Lexicon, see (Hartrumpf et al, 2003)), which is a general do-", "acronyms": [[86, 94], [17, 20]], "long-forms": [[96, 116]], "ID": "3875"}, {"text": "Abstract This work looks at a temporal aspect of multiword expressions (MWEs), namely that the behaviour of a given n-gram and", "acronyms": [[72, 76]], "long-forms": [[49, 70]], "ID": "3876"}, {"text": " Association for Computational Linguistics.                       ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,                   Unsupervised Lexical Acquisition: Proceedings of the Workshop of the", "acronyms": [[109, 115], [66, 69]], "long-forms": [[70, 107]], "ID": "3877"}, {"text": "also accessible through a phrase internal reordering. A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li", "acronyms": [[94, 96]], "long-forms": [[80, 92]], "ID": "3878"}, {"text": "collocations in each sentence.  Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram language", "acronyms": [[64, 67]], "long-forms": [[32, 62]], "ID": "3879"}, {"text": "Scores for PK    Because Academia Sinica corpora (AS) are  provided by us, we are not allowed to participate ", "acronyms": [[50, 52], [11, 13]], "long-forms": [[25, 40]], "ID": "3880"}, {"text": "tive standard deviation of three intervals, left  edge to anchor (LE-A), center to anchor (CC-A),  right edge to anchor (RE-A), calculated across  productions of pot, sot, spot, lot, plot, and splot ", "acronyms": [[121, 125], [91, 95], [66, 70]], "long-forms": [[99, 119], [73, 89], [44, 64]], "ID": "3881"}, {"text": "School of Computer Science, University of Manchester, UK ? National Centre for Text Mining (NaCTeM), UK ?", "acronyms": [[92, 98], [54, 56], [101, 103]], "long-forms": [[59, 90]], "ID": "3882"}, {"text": "the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-)divergence: K\u0002L^M_NQP\u0012R`NQS S54%6\u001a7", "acronyms": [[118, 121], [146, 149], [134, 143]], "long-forms": [[100, 116]], "ID": "3883"}, {"text": "Table 4. WSD precision recall and F-measure for  the algorithm based on aligned wordnets (AWN),  for AWN with clustering (AWN+C) and for ", "acronyms": [[90, 93], [9, 12], [122, 127], [101, 104]], "long-forms": [[72, 88]], "ID": "3884"}, {"text": "Visweswariah et al (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words.", "acronyms": [[92, 95], [109, 112]], "long-forms": [[64, 90]], "ID": "3885"}, {"text": "In practice we can find approximate solution using such algorithms as: Loopy Belief Propagation (BP), Mean Field (MF), Gibbs Sampling (Gibbs). ", "acronyms": [[114, 116], [97, 99], [135, 140]], "long-forms": [[102, 112], [77, 95], [119, 124]], "ID": "3886"}, {"text": "I.e., we consider three increasingly constrained conditions: (1) substitution according only to the form constraints (FORM), (2) substitution according to both form and taboo", "acronyms": [[118, 122]], "long-forms": [[100, 104]], "ID": "3887"}, {"text": "value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment task and 64.1 (AUTO), 79.3 (TABLETS) for", "acronyms": [[70, 74], [125, 129], [138, 145], [83, 90]], "long-forms": [], "ID": "3888"}, {"text": "Verb classification performance (precision, recall, and F for MS are macro-averaged). Global accuracy supplemented by 95% binomial confidence intervals (CI). ", "acronyms": [[153, 155], [62, 64]], "long-forms": [[131, 151]], "ID": "3889"}, {"text": "cerd,jurafsky,manning@stanford.edu Abstract Minimum error rate training (MERT) is a widely used learning procedure for statistical", "acronyms": [[73, 77]], "long-forms": [[44, 71]], "ID": "3890"}, {"text": "We present experiments using counts of three types of ngrams: lemma ngrams (LN), POS ngrams (PN) and mixed ngrams (MN).2 Mixed ngram is a restricted formulation of lemma ngram where open-", "acronyms": [[115, 117], [76, 78], [93, 95]], "long-forms": [[101, 113], [62, 74], [81, 91]], "ID": "3891"}, {"text": "Direction, Manner, and Purpose are Prop-Bank adjunctive argument labels (Palmer et al, 2005). Quantifier, Means, Cause-to-Know and copulas were added to the preceding roles. Finally, anything that did not fit into the above categories retained its dependency parse type: VMod (Verb Modifier), NMod (Noun Modifier), AMod (Adjec-tive or Adverb Modifier), and Root (Root was used when a single word in the answer, typically yes, no, agree, disagree, A-D, etc., stood alone without a significant relation to the remainder of the refer-ence answer; this occurred only 21 times, account-ing for fewer than 1% of the reference answer facets).", "acronyms": [[271, 275], [293, 297], [315, 319]], "long-forms": [[277, 290], [299, 312], [321, 350]], "ID": "3892"}, {"text": "In  Proceedings of the 16th International Conference on  Computational Linguistics (COLING-96), Copenhagen,  Denmark, pp 459-465.", "acronyms": [[84, 93]], "long-forms": [[57, 82]], "ID": "3893"}, {"text": " 3.1 Ng and Cardie (2002a) Ng and Cardie (N&C) do not attempt to improve PA, simply using the anaphoricity information it pro-", "acronyms": [[42, 45], [73, 75]], "long-forms": [[27, 40]], "ID": "3894"}, {"text": "on the labels from DSlabels+MinCut: (4) MaxEnt with named entities (NE); (5) MaxEnt with NE and semantic (SEM) features; (6) CRF with NE; (7) MaxEnt with NE and sequential (SQ) features;", "acronyms": [[106, 109], [68, 70], [89, 91], [125, 128], [134, 136], [142, 148], [154, 156], [173, 175], [40, 46]], "long-forms": [[96, 104], [52, 66]], "ID": "3895"}, {"text": "also republish the baseline results of Schnabel and Sch?utze (2014) using the Stanford POS Tagger, a maximum entropy Markov model (MEMM) tagger. ", "acronyms": [[131, 135], [87, 90]], "long-forms": [[101, 129]], "ID": "3896"}, {"text": "ferent sources. The first feature source comes  from our DSSMs (DSSM and DSSM_BOW) using the output layers as feature generators as de-", "acronyms": [[57, 62]], "long-forms": [[64, 81]], "ID": "3897"}, {"text": "every node be covered by some lexeme.  Partial SemSpec (PSemSpec): The contribution that the lexeme can  make to a sentence SemSpec.", "acronyms": [[56, 64]], "long-forms": [[39, 54]], "ID": "3898"}, {"text": "not be effective due to the brevity of contributions.  Barzilay and Lee?s algorithm (B&L) did not  generalize well to either dialogue corpus.", "acronyms": [[85, 88]], "long-forms": [[55, 83]], "ID": "3899"}, {"text": " 2.3 Perceptron Sequential Tagger This system uses a Global Linear Model (GLM), a sequential tagger using the perceptron algorithm", "acronyms": [[74, 77]], "long-forms": [[53, 72]], "ID": "3900"}, {"text": "In Proceedings of ACM 12th International Conference on Intelligent User interfaces (IUI). ", "acronyms": [[84, 87], [18, 21]], "long-forms": [[55, 82]], "ID": "3901"}, {"text": "Since all covariance matrices are positive semi-definite, the quadratic program (QP) remains convex in w?, ", "acronyms": [[81, 83]], "long-forms": [[62, 79]], "ID": "3902"}, {"text": "Figure 2: Translation examples (SRC = source, BASE = baseline system, BACKOFF = backoff system, REF = reference). OOVs and their trans-", "acronyms": [[96, 99], [32, 35], [46, 50], [70, 77], [114, 118]], "long-forms": [[102, 111], [38, 44], [53, 61], [80, 87]], "ID": "3903"}, {"text": " Results (in percentages) are for per-logical-predication (PR) and per-whole-graph (GRPH) tagging accurcies. ", "acronyms": [[84, 88], [59, 61]], "long-forms": [[77, 82], [46, 57]], "ID": "3904"}, {"text": "In our development of 60 Japanese predicates (verb and verbal noun) frequently appearing in Kyoto University Text Corpus (KTC) (Kurohashi and Nagao, 1997) , 37.6% of the frames included", "acronyms": [[122, 125]], "long-forms": [[92, 120]], "ID": "3905"}, {"text": "(disharmonic) combinators to increase the expressive power of the model.  \u0001 KZGS10 (Kwiatkowski et al2010) uses a restricted higher-order unification procedure, which iteratively breaks up a logical form into", "acronyms": [[76, 82]], "long-forms": [[84, 105]], "ID": "3906"}, {"text": "\t\u000f\u000e \u0010\u0011\t\u0013\u0012\u0014\u0010\u0016\u0015\u0016\u0015\u0016\u0015\u0017\u0010\u0011\t\u0019\u0018\u001a\u0007 consists of a directed acyclic graph (DAG) that encodes a set of conditional independence assertions about vari-", "acronyms": [[64, 67]], "long-forms": [[40, 62]], "ID": "3907"}, {"text": "one of German, English or Japanese. The system has been designed  around the task of conference r gistration (CR). It has initially been ", "acronyms": [[110, 112]], "long-forms": [[85, 108]], "ID": "3908"}, {"text": "For Task 2-2, we design two kinds of evaluation metrics:  1) POS accuracy (POS-A)  This index is used to evaluate the performance ", "acronyms": [[75, 80]], "long-forms": [[61, 73]], "ID": "3909"}, {"text": "On the standard view in transformational theory (Chomsky, 1981) both subject raising and object raising, or Exceptional Case Marking (ECM), cases are explained by the same principles.", "acronyms": [[134, 137]], "long-forms": [[108, 132]], "ID": "3910"}, {"text": "Mean values (with standard deviations) of each of the first eight features on each sub-corpus are displayed in Table 4. The number of unusual punctuation marks (UnPunc) is the only feature whose value does not differ significantly between the original and simplified versions of the texts in any of the four", "acronyms": [[161, 167]], "long-forms": [[134, 153]], "ID": "3911"}, {"text": "Here the construction of the Chinese VP involves joining a prepositional phrase (PP) and a smaller verbal phrase (VP-A), with the preposition at the beginning as a PP marker.", "acronyms": [[114, 118], [37, 39], [81, 83], [164, 166]], "long-forms": [[99, 112], [59, 79]], "ID": "3912"}, {"text": "Universitat Polit`ecnica de Catalunya (UPC), Barcelona 2 Centro de Investigaci?on en Computaci?on (CIC), Instituto Polit?ecnico Nacional (IPN), Mexico 1", "acronyms": [[99, 102], [138, 141], [39, 42]], "long-forms": [[57, 97], [105, 136], [0, 37]], "ID": "3913"}, {"text": "proposed by (Jia and Zhao, 2013). We will mainly consider MIU accuracy (MIU-Acc) which is the ratio of the number of completely corrected gen-", "acronyms": [[72, 79]], "long-forms": [[58, 70]], "ID": "3914"}, {"text": "(41a) PP = ~ Cat i (PN)  i i>0  (41b) NP = N ~ Cati(P N) i  1 ", "acronyms": [[38, 40], [6, 8], [20, 22]], "long-forms": [[43, 53]], "ID": "3915"}, {"text": "In  Proc. of Intelligent Tutoring Systems (ITS). ", "acronyms": [[43, 46]], "long-forms": [[13, 41]], "ID": "3916"}, {"text": "model, it then shows how meaning specificity affects the linguistic behavior and semantic content of Chinese resultative verb compounds (RVCs). ", "acronyms": [[137, 141]], "long-forms": [[109, 135]], "ID": "3917"}, {"text": " ? National Drug File7 (NDF): this ontology  contains information about a comprehensive ", "acronyms": [[24, 27]], "long-forms": [[3, 22]], "ID": "3918"}, {"text": "The/AT table\\]NN is/BEZ ready/J/./.  (PPS = subject pronoun; MD = modal; V'B =  verb (no inflection); AT = article; NN = noun;  BEZ ffi present 3rd sg form of \"to be\"; Jl = ", "acronyms": [[102, 104], [116, 118], [38, 41], [61, 63], [73, 76]], "long-forms": [[107, 114], [121, 125], [66, 71], [80, 84], [44, 59]], "ID": "3919"}, {"text": " 4.2 Data  We used the Wall Street Journal (WSJ) of the years  88-89.", "acronyms": [[44, 47]], "long-forms": [[23, 42]], "ID": "3920"}, {"text": "transcripts. The print news consisted of 22 New York Times (NYT) articles from January 1998.", "acronyms": [[60, 63]], "long-forms": [[44, 58]], "ID": "3921"}, {"text": "5 5 of such properties is acyclicity, as in Hidden Markov Models (HMMs). For", "acronyms": [[66, 70]], "long-forms": [[44, 64]], "ID": "3922"}, {"text": " 3.3 Cascaded ATN Grammars A Cascaded ATN Grammars (CATN) (Woods 1980) is a cooperating sequence of ATN transducers, each feeding its output to the next stage.", "acronyms": [[52, 56], [14, 17], [100, 103]], "long-forms": [[29, 41]], "ID": "3923"}, {"text": "2.1 Description of the procedure Two specialized topics In this study MA student interpreters were invited to prepare for simultaneous interpreting tasks on two specialised topics: fast reactors (FR) and Seabed minerals (SM). They", "acronyms": [[196, 198], [221, 223], [70, 72]], "long-forms": [[181, 194], [204, 219]], "ID": "3924"}, {"text": "5) is blocked by (7), because of the  passinginto the subject'a story about es'; i.e., the specifier of  INFL (in the transformational ccount) or of VP (in theories  like GPSG, etc.).", "acronyms": [[105, 109], [149, 151], [171, 175]], "long-forms": [[111, 134]], "ID": "3925"}, {"text": "In order to normalize Thai input character  sequences to a canonical Unicode form, we developed a finite state transducer (FST) which  detects and repairs a number of sequencing er-", "acronyms": [[123, 126]], "long-forms": [[98, 121]], "ID": "3926"}, {"text": "ically used as the target. For example, the NIST Open Machine Translation Evaluation (OpenMT) 2009 (Garofolo, 2009) constrained Arabic-English", "acronyms": [[86, 92], [44, 48]], "long-forms": [[49, 84]], "ID": "3927"}, {"text": "1. Construct word representation model for  corpus in the base time, D(TB), and in the  target time, D(TT). (", "acronyms": [[71, 73], [103, 105]], "long-forms": [[54, 62], [88, 99]], "ID": "3928"}, {"text": "5 Learning Algorithms We used two non-parametric learning approaches, Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs)", "acronyms": [[95, 99], [162, 165]], "long-forms": [[70, 93], [142, 160]], "ID": "3929"}, {"text": "score word pairs for relatedness (on a scale of 0 to 10), which is in contrast to the similarity judgments requested of the Miller and Charles (MC) and Rubenstein and Goodenough (RG) participants.", "acronyms": [[144, 146], [179, 181]], "long-forms": [[124, 142], [152, 177]], "ID": "3930"}, {"text": " Recently researchers have been investigating Amazon Mechanical Turk (MTurk) as a source of non-expert natural language annotation, which is a", "acronyms": [[70, 75]], "long-forms": [[53, 68]], "ID": "3931"}, {"text": " 1 Introduction Weighted Context Free Grammars (WCFG) define an important class of languages.", "acronyms": [[48, 52]], "long-forms": [[16, 46]], "ID": "3932"}, {"text": "BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire; UN = Usenet Newsgroups; and WL = Weblogs. ", "acronyms": [[105, 107], [133, 135], [0, 2], [30, 32], [51, 54], [90, 92]], "long-forms": [[110, 127], [138, 145], [5, 28], [35, 49], [57, 88], [95, 103]], "ID": "3933"}, {"text": "based Translation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING?90), pages 247?252.", "acronyms": [[101, 110]], "long-forms": [[60, 99]], "ID": "3934"}, {"text": "5 23   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69, Sofia, Bulgaria, August 9, 2013.", "acronyms": [[72, 79]], "long-forms": [[38, 70]], "ID": "3935"}, {"text": " In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 865? ", "acronyms": [[83, 89]], "long-forms": [[56, 81]], "ID": "3936"}, {"text": "(V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG =  ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP,  &MAINV = main verb, &<AUXMOD = verbal auxiliary modifier). ", "acronyms": [[235, 240], [26, 30], [49, 54], [108, 111], [69, 78], [138, 144], [158, 164], [188, 192], [212, 216], [256, 262]], "long-forms": [[243, 252], [5, 14], [20, 24], [33, 47], [57, 67], [115, 123], [81, 107], [128, 136], [147, 156], [167, 186], [195, 210], [219, 231], [265, 292]], "ID": "3937"}, {"text": "tell verb base (VB), tell VB tell told verb past tense (VBD), tell VBD,VBN tell verb past participle (VBN), tell tells verb present 3rd person sing (VBZ), tell VBZ tell", "acronyms": [[102, 105], [16, 18], [26, 28], [56, 59], [67, 70], [71, 74], [149, 152], [160, 163]], "long-forms": [[80, 100], [5, 14], [39, 54], [119, 142]], "ID": "3938"}, {"text": "semaatic grounds, new referent objects must be created. The number of objects to be  created is set equal to the QTY (quantity) attribute of the noun phrase if specified (as in  \"two boys\" (P20)), to two if the noun phrase is plural and not compound, to the number ", "acronyms": [[113, 116]], "long-forms": [[118, 126]], "ID": "3939"}, {"text": " We ran three parsing experiments: (i) replacing the value of the surface form (FORM) of pronominal prepositions with their lemma form (LEMMA), for", "acronyms": [[80, 84], [136, 141]], "long-forms": [[74, 78], [124, 129]], "ID": "3940"}, {"text": "data. The QA surveys are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from", "acronyms": [[76, 78], [95, 97], [10, 12], [104, 106]], "long-forms": [[60, 74], [84, 93]], "ID": "3941"}, {"text": "topic distributions for all phrase pairs in the phrase table in an unsupervised fashion, using a variant of Latent Dirichlet Allocation (LDA). The underly-", "acronyms": [[137, 140]], "long-forms": [[108, 135]], "ID": "3942"}, {"text": "translations: ovc~iflow in the Data Processing  category (DPR,) and out-o\\[-./lushnc.ss in the Air-  m'M't Structure category (STR). As shown in the ", "acronyms": [[127, 130], [58, 61]], "long-forms": [[107, 125], [31, 56]], "ID": "3943"}, {"text": "Following the ideas of (Collobert et al, 2011), Zeng et al (2014) first solve relation classification using convolutional neural network (CNN). The", "acronyms": [[138, 141]], "long-forms": [[108, 136]], "ID": "3944"}, {"text": "1. Introduction  A verb phrase ellipsis (VPE) exists when a  sentence has an auxiliary verb but no verb phrase ", "acronyms": [[41, 44]], "long-forms": [[19, 39]], "ID": "3945"}, {"text": "have a tea and read a good criminal book) and we cannot forget that entertainment is also one of this Embodied Conversational Agent (ECA)?s goal. ", "acronyms": [[133, 136]], "long-forms": [[102, 131]], "ID": "3946"}, {"text": "uation contained in the RST Discourse Treebank (RST-DTB)(Carlson et al 2001) distributed by the Linguistic Data Consortium (LDC)3. The RST-DTB", "acronyms": [[124, 127], [48, 55], [135, 142]], "long-forms": [[96, 122], [24, 46]], "ID": "3947"}, {"text": "3 The  S imulat ion  Mode l   The computational simulation supports the evolu-  tion of a population of Language Agents (LAgts),  similar to Holland's (1993) Echo agents.", "acronyms": [[121, 126]], "long-forms": [[104, 119]], "ID": "3948"}, {"text": "data are classified manually (Human) into three  stability classes. Decision Tree (DT) automatic  algorithm C4.5 (Quinlan, 1993; Weiss & Kulikowski, ", "acronyms": [[83, 85]], "long-forms": [[68, 81]], "ID": "3949"}, {"text": "the conjunction but. Turney and Littman (2003) use pointwise mutual information (PMI) (Church and Hanks, 1990) and latent semantic analysis", "acronyms": [[81, 84]], "long-forms": [[51, 79]], "ID": "3950"}, {"text": "It is particularly interesting to see that when hypotheses selection is applied, oracle error rate (OER) drops of 2% points from an already accurate OER", "acronyms": [[100, 103], [149, 152]], "long-forms": [[81, 98]], "ID": "3951"}, {"text": " 1 Introduction Named entity recognition (NER) is the task of identifying and classifying phrases that denote certain types of named", "acronyms": [[42, 45]], "long-forms": [[16, 40]], "ID": "3952"}, {"text": "meaning of the other. Examples are:  * Senior research assistant at the Belgian National Fund for Scientific Research (F.N.R.S.). ", "acronyms": [[119, 127]], "long-forms": [[89, 117]], "ID": "3953"}, {"text": "ACC mPUR + ACC The random baseline(BL) is calculated as follows: BL = 1/number of classes", "acronyms": [[35, 37], [0, 3], [4, 8], [11, 14], [65, 67]], "long-forms": [[26, 33]], "ID": "3954"}, {"text": "Several different learning algorithms have been explored for text classification (Dumais et al 1998) and support vector machines (SVMs) (Vapnik, 1995) were found to be the most computationally ef-", "acronyms": [[130, 134]], "long-forms": [[105, 128]], "ID": "3955"}, {"text": "extrinsic and language independent features.  The student response analysis (SRA) task (Dzikovska et al 2013) addresses the fol-", "acronyms": [[77, 80]], "long-forms": [[50, 75]], "ID": "3956"}, {"text": "knowledge. The use of Proposition Stores as  Background Knowledge Bases (BKB) have been  argued to be useful for improving parsing, co-", "acronyms": [[73, 76]], "long-forms": [[45, 71]], "ID": "3957"}, {"text": "give an indication as to what the vibhakti/TAM are.  Words with PSP (postposition) and NST (noun with spatial and temporal properties) tags are generally", "acronyms": [[64, 67], [34, 46], [87, 90]], "long-forms": [[69, 81]], "ID": "3958"}, {"text": "The GALE manual WA corpus and the Chinese to English corpus from the shared task of the NIST open machine translation (OpenMT) 2006 evaluation 6 were employed as the experimental corpus", "acronyms": [[119, 125], [4, 8], [16, 18], [88, 92]], "long-forms": [[93, 117]], "ID": "3959"}, {"text": "It is well known that for English, the automatic conversion of a constituency parser?s output to dependency format can achieve competitive unlabeled attachment scores (ULA) to a dependency parser?s output trained on automatically converted trees", "acronyms": [[168, 171]], "long-forms": [[139, 159]], "ID": "3960"}, {"text": "` and survival variables S`. Languages shown are Latin (LA), Vulgar Latin (VL), Proto-Iberian (PI), Italian (IT), Portuguese (PT), and Spanish (ES). Note that only modern", "acronyms": [[75, 77], [126, 128], [56, 58], [95, 97], [109, 111], [144, 146]], "long-forms": [[61, 73], [114, 124], [49, 54], [80, 93], [100, 107], [135, 142], [6, 24]], "ID": "3961"}, {"text": "The work described in this paper is based on the output of Inputlog3, but it can also be applied to the output of other keystroke-logging programs.  To promote more linguistically-oriented writing process research, Inputlog aggregates the logged process data from the character level (keystroke) to the word level.  In a subsequent step, we use various Natural Language Processing (NLP) tools to further annotate the logged process data with different kinds of linguistic information: part-of-speech tags, lemmata, chunk boundaries, syllable boundaries, and word frequency.  The remainder of this paper is structured as follows.", "acronyms": [[382, 385]], "long-forms": [[353, 380]], "ID": "3962"}, {"text": "hierarchical phrase-based model, but constrained so that the English part of the right-hand side is restricted to a Greibach Normal Form (GNF)like structure: A contiguous sequence of termi-", "acronyms": [[138, 141]], "long-forms": [[116, 136]], "ID": "3963"}, {"text": "We introduce a  exible history reference mechanism called an ACT (arboreal context tree; an extension of the context tree to tree-shaped his-", "acronyms": [[61, 64]], "long-forms": [[66, 87]], "ID": "3964"}, {"text": "the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING) 1998, pages 41?47, Montre?al.", "acronyms": [[116, 122], [47, 50]], "long-forms": [[89, 114], [4, 45]], "ID": "3965"}, {"text": "research relies is that the failures of current automatic metrics are not algorithmic: BLEU, Meteor, TER (Translation Edit Rate), and other metrics efficiently and correctly compute informative distance", "acronyms": [[101, 104], [87, 91]], "long-forms": [[106, 127]], "ID": "3966"}, {"text": "for each word in the DAL. ( e.g., the verb definition for LOL (laugh out loud) in Wiktionary is ? To laugh", "acronyms": [[58, 61], [21, 24]], "long-forms": [[63, 77]], "ID": "3967"}, {"text": "The primary purpose of the toolkit is to allow students to concentrate on building natural language processing (NLP) systems.", "acronyms": [[112, 115]], "long-forms": [[83, 110]], "ID": "3968"}, {"text": "hdaume,marcu \u0001 @isi.edu Abstract Entity detection and tracking (EDT) is the task of identifying textual mentions", "acronyms": [[64, 67]], "long-forms": [[33, 62]], "ID": "3969"}, {"text": "trol agreement principle.  Consider first the foot feature principle (FFP). ", "acronyms": [[70, 73]], "long-forms": [[46, 68]], "ID": "3970"}, {"text": "Here we perform a set of experiments where we investigate the potential of multi-source transfer for NER, in German (DE), English (EN), Spanish (ES) and Dutch (NL), using cross-lingual", "acronyms": [[131, 133], [101, 104], [117, 119], [145, 147], [160, 162]], "long-forms": [[122, 129], [109, 115], [136, 143], [153, 158]], "ID": "3971"}, {"text": "tures. Since the Parallel Alignment TreeBank is a subset of the Chinese TreeBank (CTB) 8.0, we automatically parsed the CTB 8.0 by doing a 10-", "acronyms": [[82, 85], [120, 123]], "long-forms": [[64, 80]], "ID": "3972"}, {"text": " ? Bigram Predictability (BP): Defined as the predictability of a word given a previous word, it", "acronyms": [[26, 28]], "long-forms": [[3, 24]], "ID": "3973"}, {"text": "We use two different windows to define a triggering environment: one for morpheme and another for its part of speech (POS) tag. Figure 2 shows ", "acronyms": [[118, 121]], "long-forms": [[102, 116]], "ID": "3974"}, {"text": "linguistic resources  ? Semantic Analysis Module (SAM) interpreting  LPM output using application knowledge ", "acronyms": [[50, 53], [69, 72]], "long-forms": [[24, 48]], "ID": "3975"}, {"text": "take et al (2005)) for the representation of meaning.  LXGram is developed in the Linguistic Knowledge Builder (LKB) system (Copestake, 2002), a development environment for constraint-based grammars.", "acronyms": [[112, 115], [55, 61]], "long-forms": [[82, 110]], "ID": "3976"}, {"text": "clickthrough data. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 133?142.", "acronyms": [[84, 87], [22, 25], [26, 32]], "long-forms": [[47, 75]], "ID": "3977"}, {"text": "S2LS. The best parameters were then used on the Senseval-3 English Lexical Sample task (S3LS), where a similar semi-supervised method was used", "acronyms": [[88, 92], [0, 4]], "long-forms": [[48, 86]], "ID": "3978"}, {"text": "C-NOUN: Nouns with POS NN that are not marked O-NOUN or PER-NOUN.  Verbs Only ING-VERBs (VBG) and ED-VERBs (VBN and VBD) are needed for this task (other verbs trigger state O).", "acronyms": [[89, 92], [0, 6], [19, 22], [23, 25], [46, 52], [56, 64], [98, 106], [108, 111], [116, 119]], "long-forms": [[67, 81]], "ID": "3979"}, {"text": "text corpus, to be made available without royalties for scientific research. The text will  be formatted using SGML (the Standard Generalized Markup Language). To date ", "acronyms": [[111, 115]], "long-forms": [[121, 157]], "ID": "3980"}]