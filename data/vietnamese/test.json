[{"text": "Trong hai mục trên chúng ta đã xây dựng hai mô hình cụ thể của NCF , là GMF và MLP . GMF với nhân là hàm tuyến tính để mô hình hóa tương tác giữa", "ID": "1"}, {"text": "của nhiều loại bệnh ung thư ( ung thư biểu mô tế bào vảy phổi ( LUSC ) , ung thư biểu 9 mô tế bào nhú thận ( KIRP ) , … ) và tìm ra được một số nhóm gene có tác động tới các", "ID": "2"}, {"text": "nhau đáng kể . Precision = TP / ( TP + FP ) , Recall = TP / ( TP + FN ) . Precision càng", "ID": "3"}, {"text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 42", "ID": "4"}, {"text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 12 2 KIẾN THỨC CƠ SỞ", "ID": "5"}, {"text": "Trong mô hình này , vector V được đưa qua hai mạng feed forward khác nhau sử dụng cho hai bài toán ta được :", "ID": "6"}, {"text": "Còn về mặt thực nghiệm , các mạng AE thường bị overfitting và các mẫu sinh ra bất hợp lý . Ví dụ đơn giản ,", "ID": "7"}, {"text": "... ... 15 Hình 8 So sánh hiệu năng của mô hình HAN trên các learning rate schedule s khác nhau . 22", "ID": "8"}, {"text": "dữ liệu đầu vào là các từ [ ’ tôi ’ , ’ yêu ’ , ’ Bách ’ , ’ , ‘ Khoa ’ ] được gọi là chuỗi dữ liệu liên tục . Trong bài toán xử lý ngôn ngữ tự nhiên ( NLP ) thì không thể xử lý", "ID": "9"}, {"text": "theo bất kỳ công trình nào khác . Tất cả những tham khảo trong ĐATN – bao gồm hình ảnh , bảng biểu , số liệu , và các câu từ trích dẫn – đều được ghi rõ ràng và", "ID": "10"}, {"text": "dữ liệu mạng xương sống . Compacting , Picking and Growing ( CPG ) ( Steven C.Y. Hung , 2019 ) đề xuất vòng lặp gồm ba quá trình được thực hiện trong mỗi task gồm có : đóng gói", "ID": "11"}, {"text": "Chương 3 : Trình bày ý tưởng và mô hình học biểu diễn đề xuất và mô hình hệ gợi ý dựa trên ý tưởng của mô hình BERT Chương 4 : Các thực nghiệm , đánh giá và phân tích , so sánh giữa các mô", "ID": "12"}, {"text": "37 Câu 1 : Up to now , 900 numbers have mainly been used on local TV stations and cable channels .", "ID": "13"}, {"text": "PHÂN LOẠI CẢM XÚC VĂN BẢN TIẾNG VIỆT Sinh viên thực hiện : Trần Thị Hồng", "ID": "14"}, {"text": "án này do em chưa tìm ra được thuật ngữ Tiếng Việt nào hay hơn nên sẽ dùng tạm từ gốc chính là CL . Hãy bắt đầu từ những pretext task mà các nghiên cứu", "ID": "15"}, {"text": "Tương tự cho mỗi vector i ta được : i  (  T ) 1  ri Phân rã ma trận Gaussian với ràng buộc biến Giả sử rằng , các giá trị thuộc tính người dùng u và thuộc tính sản phẩm i biến động", "ID": "16"}, {"text": "Sau phần trình bày ở chương trước , chúng ta đã hiểu rõ về hướng nghiên cứu SSL , mục tiêu , lợi ích của SSL , các phương pháp trong hướng nghiên cứu này , các ưu nhược điểm của các phương pháp khác và cách mà các phương pháp ra đời sau", "ID": "17"}, {"text": "Mạng nơ-ron Bayes ( BNN ) : Cũng tương tự như mạng nơ-ron nhân tạo ( ANN ) hay mạng nơ-ron tích chập ( CNN ) , thông tin đầu vào qua BNN sẽ được đi qua các tầng ẩn và được biến đối", "ID": "18"}, {"text": "Như đã đề cập , ba phương pháp nền tảng sẽ được sử dụng để áp dụng ALV cho quá trình thử nghiệm đánh giá là EWC , VCL và UCL , cùng với các bộ dữ liệu đã trình bày trong mục 4 . 1 . Các phân phối xác suất trên đại lượng tham số cục bộ 𝑠", "ID": "19"}, {"text": "Chương 5 : Tổng kết lại nội dung , kết quả đồ án và định hướng trong tương lai . GIỚI THIỆU ĐỀ TÀI", "ID": "20"}, {"text": "22 2 KIẾN THỨC CƠ SỞ", "ID": "21"}, {"text": "Ở đây , chúng ta chọn mô hình \" MF với bias \" cho ma trận ratings , và chọn mô hình MF thông thường cho ma trận \" views \" và \" wants \" . Cụ thể , dự đoán rating của người dùng u cho bộ phim i sẽ được thực hiện như", "ID": "22"}, {"text": "a . Kết quả thử nghiệm trên Split MNIST : Bảng 4 . 4 Kết quả thử nghiệm trên Split MNIST ( % ) EWC", "ID": "23"}, {"text": "Gaussian Mixture Model ( GMM ) là một mô hình xác suất với giả định rằng các quan sát tuân theo phân phối Gauss ( phân phối chuẩn ) hỗn hợp gồm K thành phần ( hay K cụm ) 𝐶1 , 𝐶2 , … , 𝐶𝐾 :", "ID": "24"}, {"text": "việc sử dụng phân phối Dirichlet cũng tồn tại hạn chế là rất khó để áp dụng trực tiếp cho AEVB [ 3 ] , vì AEVB sử dụng reparameterization trick ( RT ) để xây dựng công cụ ước tính Monte Carlo khác biệt cho giới hạn dưới ( lower bound ) mà rất", "ID": "25"}, {"text": "của các biến ẩn tương ứng với các giả sử giới hạn không gian tìm kiếm ban đầu . Để các công thức cập nhật ( 10 ) → ( 12 ) đảm bảo sự hội tụ đến điểm tối ưu ( optimal point ) của hàm lower bound , chúng ta sử dụng thuật toán ExpectationMaximization ( EM ) , trong đó chia quá trình cập nhật thành hai bước : bước \" E \"", "ID": "26"}, {"text": "Kết hợp với các đẳng thức ( 1 ) và ( 4 ) chúng ta được : D X", "ID": "27"}, {"text": "Có thể thấy rằng các mô hình ITE đạt hiệu năng rất tốt trên bộ Retailrocket . Trên hình 20 , ta thấy kết quả ở bộ Recobell cũng giống như những gì ta", "ID": "28"}, {"text": "lượng khả năng xảy ra trong công thức PT 2 . 3 , có khả năng giải quyết vấn đề phương sai lớn cũng như tính toán hiệu quả hơn kĩ thuật đổi biến thông thường . LRT được đề xuất bởi Kingma và các cộng sự [ 11 ] nhằm cải thiện hiệu quả", "ID": "29"}, {"text": "Danh sách các từ viết tắt và thuật ngữ Từ viết tắt và thuật ngữ Ý nghĩa LDA", "ID": "30"}, {"text": "Đây là giá trị k mà hầu hết các Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 44", "ID": "31"}, {"text": "cách sử dụng forgetting factor để quên đi thông tin liên quan tới dữ liệu cũ với tốc độ hàm mũ , do đó nó có thể thích ứng tốt hơn với thông tin từ dữ liệu mới . PVB cũng có khả năng đương đầu với concept drift vì phương sai của phân phối", "ID": "32"}, {"text": "trong nhánh CL , với các kết quả thuộc hàng SOTA hiện nay . Trong khuôn khổ đồ án này cũng đã trình bày về hai phương pháp SimCLR và MOCO rất đặc trưng", "ID": "33"}, {"text": "22 quả của hai thành phần đó có thể thu được một cách chính xác khi áp dụng phương pháp tối ưu PGD .", "ID": "34"}, {"text": "Tương tự là biểu diễn pcat của zone trong mô hình ITE - 2 và ITE - 3 . • Trong các kết quả thử nghiệm , công bằng mà nói thì nhìn chung mô hình", "ID": "35"}, {"text": "thường ( đề cập trong phương pháp SVB ) . • Với dạng được chọn , phân phối biến phân q ( ρt | wt ) có cùng họ như p ( ρt | γ ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT", "ID": "36"}, {"text": "Training epoch s Topics coherence ( NPMI ) Hình 4 . 23 Đồ thị biểu diễn sự thay đổi giá trị NPMI khi thay đổi số lần học mô", "ID": "37"}, {"text": "Mục tiêu của chúng ta là tìm một giá trị của tham số biến phân toàn cục λ để cực đại hoá hàm F - ELB0 . Biểu diễn tham số biến phân cục bộ φ theo λ , tức", "ID": "38"}, {"text": "PN k α trong đó λ̂kj = η + |B |", "ID": "39"}, {"text": "tham số tối ưu của hai tác vụ liên tiếp để đưa ra độ quan trọng của tham số . Ngoài ra , một số phương pháp như HAT [ 16 ] thay vì sửa lại hàm mất mát , sử dụng độ", "ID": "40"}, {"text": "thập ở các cấp độ khác nhau trong tế bào ( các dữ liệu về DNA như đột biến , CNV , dữ liệu về RNA như gene expression , dữ liệu về protein như PPIs , … ) . Việc kết hợp nhiều", "ID": "41"}, {"text": ". DKL ( P ||Q ) còn được gọi là KL forward , DKL ( Q||P ) được gọi là KL reverse . Tương ứng với 2 loại", "ID": "42"}, {"text": "số khi nó được kết nối với nút quan trọng bằng cách thay đổi thành phần ( 𝑎 ) trong đại lượng KL bởi biểu thức : 𝐿", "ID": "43"}, {"text": "trí tuệ nhân tạo ( AI ) đóng góp một phần không hề nhỏ cho các giải pháp thông minh như quản lý ra vào ( hệ thống điểm danh bằng khuôn mặt , hệ thống quản lý bãi đỗ xe , ... ) , kiểm soát an ninh khu vực ( đưa ra các cảnh báo cho các hành vi bất thường : bạo", "ID": "44"}, {"text": "2 . 3 Đồ thị và Graph Convolutional Networks ( GCN ) 8", "ID": "45"}, {"text": "Hàm mất mát mới này được gọi là regularized loss function và được định nghĩa như sau : Lreg ( θ ) = L ( θ ) + λR ( θ )", "ID": "46"}, {"text": "cao thì đấy là một detector tốt . g ) Average Precision ( AP )", "ID": "47"}, {"text": "tính toán , nên EWC sẽ xấp xỉ bằng một ma trận Fisher đường chéo với 𝐹 = ∗ ∗", "ID": "48"}, {"text": "cập trong chương 3 phần 3 . 1 ) Phiên bản BDDX _ v2 : Sử dụng mô hình học biểu diễn đề xuất thay thế mô - đun", "ID": "49"}, {"text": "𝑫𝟐 𝒌 Sau mỗi convolution MobileNet sẽ sử dụng Batch Normalization ( BN ) và ReLU", "ID": "50"}, {"text": "Bảng 15 : Xóa các bài đã đăng tuyển Xóa các bài đã đăng tuyển NTD", "ID": "51"}, {"text": "Xác nhận của giáo viên hướng dẫn về mức độ hoàn thành ĐATN và cho phép bảo vệ : Giảng viên hướng dẫn", "ID": "52"}, {"text": "biểu diễn phân bố hậu nghiệm trên dữ liệu quá khứ dưới dạng sau : Z t", "ID": "53"}, {"text": "1 trái mô tả độ chính xác trung bình của tất cả các tác vụ từ đầu cho đến tác vụ hiện tại . Có thể nhận thấy VBD - CL đạt kết quả tương đương", "ID": "54"}, {"text": "không có cơ chế học liên tục . Như đã phân tích , HAT và VBD - CL là hai phương pháp sử dụng ít bộ nhớ nhất nhờ việc ràng buộc gradient của trọng số thay vì phải lưu lại bộ trọng số tối ưu của", "ID": "55"}, {"text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT Mc M", "ID": "56"}, {"text": "Số lượng nơ-ron ở các tầng của kiến trúc mạng lần lượt là 784 , 400 , 400 , 10 . Kiến trúc chi tiết với phương pháp đề xuất được mô tả trong Hình 4 . 4 . Hình 4 . 4 Kiến trúc mạng cho thử nghiệm trên PMNIST", "ID": "57"}, {"text": "phần hỗ trợ giúp cho mô hình thích nghi với dữ liệu , đóng vai trò giống như giá trị nhiễu trong Dropout nhưng linh động hơn . Tuy nhiên nhược điểm của VD là chỉ", "ID": "58"}, {"text": "² ← Lấy ngẫu nhiên từ phân phối nhiễu Gaussian N ( 0, I ) Lấy mẫu Z theo reparameterizaiton trick : Z = µ + ² σ", "ID": "59"}, {"text": "Bảng 17 : Lưu hồ sơ ứng viên Lưu hồ sơ ứng viên NTD", "ID": "60"}, {"text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 29", "ID": "61"}, {"text": ") 2 . F- ELBO là hàm cận dưới của hàm negative - KL đối với phân phối hậu", "ID": "62"}, {"text": "sự khổng lồ của mạng xã hội nên rất khó để xác định , đồng thời hiện nay các nhà cung cấp giải pháp xác định người ảnh hưởng ( KOL ) dựa vào lượng bạn bè và lượng người theo dõi không phản ánh hết được tính chất động của tiếng nói hay", "ID": "63"}, {"text": "Bổ đề Xét biến ngẫu nhiên liên tục X có hàm mật độ f ( x ) . Biến ngẫu nhiên liên tục Z = g ( X ) ,", "ID": "64"}, {"text": "mạng học được cần phải dự đoán xem hai phần đó có vị trí tương đối với nhau như thế nào ( nằm trên , nằm dưới , nằm bên trái hay bên phải ) ( trong bài [ DGE15 ] ) . Do khuôn khổ có hạn của đồ án , em xin được chọn một bài báo đại diện cho những", "ID": "65"}, {"text": "trước đó . Để được bảo vệ quyền riêng tư của người dùng trên trang báo điện tử Microsoft News , mỗi ID của người dùng đã được băm ( hash ) thành một ID ẩn", "ID": "66"}, {"text": "Công nghệ thông tin và Truyền thông HÀ NỘI , 6/2021 1", "ID": "67"}, {"text": "nữa , hay nói cách khác , loại bỏ được hoàn toàn vấn đề cold start . 4 . 5 Mô hình ITE - 3", "ID": "68"}, {"text": "này . Tuy nhiên về sau các mô hình CNN đã được chứng minh là có hiệu quả đối với xử lý ngôn ngữ tự nhiên ( NLP ) và đạt được kết quả tuyệt vời trong phân tích", "ID": "69"}, {"text": "( 3FC ) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 . 5 Ảnh hưởng của hệ số phạt đối với hiệu quả của các phương pháp phạt 4.6", "ID": "70"}, {"text": "Tuy nhiên , ta sẽ sử dụng các vec-tơ đầu vào là one-hot để xem xét khả Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 17", "ID": "71"}, {"text": "này cho phép mỗi βkj có một không gian tìm kiếm riêng , phù hợp với đặc tính hình học của từng chiều dữ liệu . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT", "ID": "72"}, {"text": "từ quá khứ có thể không mô tả một cách đúng đắn tính chất của dữ liệu hiện tại . Cụ thể hơn , SVB có thể gặp phải vấn đề vanishing variance sẽ được trình", "ID": "73"}, {"text": "Ma trận Fisher Information là ma trận hiệp phương sai của gradient của log likeli - 18 hood đối với dữ liệu được sinh từ phân phối của mô hình [ REF , 99 al ] :", "ID": "74"}, {"text": ", độ chính xác trung bình ( ACC ) có công thức : ACC =", "ID": "75"}, {"text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 41", "ID": "76"}, {"text": "• get _ operation ( V ) : trả về phép toán tính ra V , được biểu diễn bởi các cạnh tới V trên Sinh viên thực hiện : Nguyễn Văn Minh , 20142950 , K59 , Lớp CNTT - TT 2.04 39", "ID": "77"}, {"text": "Như vậy , ta có thể hiểu hàm mất mát của AE truyền thống được thiết kế để học cách mã hóa và giải mã đầu ra càng giống đầu vào càng tốt , mặc dù tình cờ nó Đồ án tốt nghiệp", "ID": "78"}, {"text": "mới với số chiều ít hơn , Y là ma trận các điểm yi . Thuật toán sẽ tìm các điểm yi P", "ID": "79"}, {"text": "Để giải quyết vấn đề này , các tác giả định nghĩa một hàm lower bound mới như sau : L̂HP P ( λt , φt , wt | D1:t , η) = Eq [ log p( Dt , zt | Θt , η)]", "ID": "80"}, {"text": "là kiểu dữ liệu ảnh nhị phân đen trắng . Kích thước của mỗi ảnh là 28 ×28 điểm ảnh . MNIST đã và đang được sử dụng rộng rãi trong việc huấn luyện và kiểm thử các", "ID": "81"}, {"text": "với hàm mục tiêu ELBO 2.14 , và kỳ vọng của W̃ sau khi học được chính là trọng số của mạng DNN ban đầu . Dropout biến phân ( VD ) [ 30 ] mở rộng Gaussian Dropout", "ID": "82"}, {"text": "hai mô hình LDA và NB , cuối cùng là phân tích các tính chất lý thuyết nổi bật của iDropout để làm rõ cơ chế của nó khi đối mặt với các vấn đề đã đề cập trên . 4 . 1 Kỹ thuật Dropout", "ID": "83"}, {"text": "quả đáng chú ý để mã hóa yếu tố ràng buộc trọng số . Ta kí hiệu ℒ 𝑡 là hàm mục tiêu ∗", "ID": "84"}, {"text": "GRU ( Gated Recurrent Unit ) là một phiên bản cải tiến của mạng RNN tiêu chuẩn nhằm giải quyết vấn đề vanishing gradient . Hình ảnh phía dưới mô tả những tính toán trong một đơn vị RNN .", "ID": "85"}, {"text": "Nearest - neighbor Contrastive Learning của Google Deepmine ( ở [ Dwi +21 ] ) lại định nghĩa cặp positive dựa trên lân cận trong tập biểu diễn ẩn , khác với việc chỉ sử dụng DA để định nghĩa các cặp view positive như trước đây vẫn thường làm .", "ID": "86"}, {"text": "( tính chất factorized distribution ) . Cụ thể với các biến ẩn Z = { Zi , i = 1 ... M } ,", "ID": "87"}, {"text": "biến ngẫu nhiên liên tục có dạng tổng quát sau : Z ∞", "ID": "88"}, {"text": "tiến được định nghĩa như sau : N 1 X", "ID": "89"}, {"text": "VCL và UCL đã được tiến hành để chứng minh hiệu quả của ALV và cho ra các kết quả như mong đợi . Đồ án cũng đã chỉ ra những tính chất lý thuyết chứng tỏ cho khả năng của ALV .", "ID": "90"}, {"text": "Multi Layer Perceptron ( MLP ) là mạng nơ-ron nhân tạo đơn giản và là nền tảng của các mô hình mạng nơ-ron phức tạp được biết đến rộng rãi hiện nay như các kiến trúc mạng nơ- ron tích chập ( Convolution Neural Networks - CNN ) ứng dụng", "ID": "91"}, {"text": "2 . Với từ thứ n = 1 , 2 , ... , Nd lần lượt : a ) Chọn một chủ đề zdn ∼ M ultinomial ( θd ) ( zdn ∈ { 1 , 2 , ... , K } , còn", "ID": "92"}, {"text": "Tuy nhiên , với tập W ∗ , nó không chỉ chứa các cặp ( u , i ) quan sát được trong W , mà còn có thêm ngẫu nhiên các cặp ( u , i ) mà giá trị tương ứng của chúng trong W bằng 0 .", "ID": "93"}, {"text": "Hơn nữa natural gradient cho phép tính toán đạo hàm của hàm L - ELBO trên một cách đơn giản hơn . Cụ thể dưới giả sử mô hình conjugate trên phân phối", "ID": "94"}, {"text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 38 Câu 1 : The stock has fallen $ 87.25 , or 31 % , in the three trading days since", "ID": "95"}, {"text": "Page 19 Sinh viên thực hiện : Nguyễn Văn Chức Kullback – Leibler ( KL ) giữa hai phân phối q ( z ) và p ( z|x ) .", "ID": "96"}, {"text": "MFVI Mean - field variational inference Suy diễn biến phân trên trường trung gian", "ID": "97"}, {"text": "ẩn phía trên nhỏ hơn ( thường bằng một nửa ) của tầng ẩn phía dưới . Mô hình NMTR dùng mô hình NeuMF cho các tầng NCF , theo báo cáo là tốt nhất trong", "ID": "98"}, {"text": "Sau khi học xong T1 , mô hình tìm được tham số tối ưu T1 thuộc vùng tham số hiệu quả của T1 . Nếu học", "ID": "99"}, {"text": "4.3 Hiệu năng của các mô hình trước đó với bài toán PKD . Soft-LR ( EM ) tỏ ra vượt trội", "ID": "100"}, {"text": "Hay Học liên tục dựa trên sự không chắc chắn ( Uncertainty-based Continual Learning , UCL ) [ 15 ] dưới", "ID": "101"}, {"text": "3.1. Cơ bản về graph convolution neural network ( GCN ) Trong thực tế có rất nhiều dữ liệu dưới dạng các đồ thị hay dạng mạng ( hoặc", "ID": "102"}, {"text": "Trong đó , BLEU là thang đo phổ biến hơn cả . Điểm BLEU được tính như sau :", "ID": "103"}, {"text": "gốc dưới góc độ của VI . Hơn nữa thông tin nhiễu thêm vào sẽ là một biến ngẫu nhiên liên tục tuân theo một phân phối với tham số có thể học trong quá trình tối", "ID": "104"}, {"text": "Xây dựng Graph Convolutional Networks ( GCN ) Point wise mutual information ( PMI ) Pointwise mutual information ( PMI ) hay point mutual information là thước đo", "ID": "105"}, {"text": "giữa các tác vụ , ta gọi ngắn gọn là “ kịch bản gia tăng tác vụ khác số nhãn ” : bộ dữ liệu Split Omniglot . Ngoài việc thử nghiệm áp dụng ALV cho các phương pháp gốc , đồ án sẽ thực", "ID": "106"}, {"text": "Trong quá trình học tập , em đã từng học và nghiên cứu về mô hình Phân tích ma trận phân tử ( MF ) nằm trong phương pháp lọc cộng tác của các hệ thống gợi ý thông qua", "ID": "107"}, {"text": "Sau khi đã có phân phối xấp xỉ 𝑞1 ( θ ) , một cách đệ quy theo công thức PT 2 . 13 ta tổng", "ID": "108"}, {"text": ", với w là cỡ của bộ lọc ( thường là 3 hoặc 5 ) và nf là tổng số bộ lọc . Có một chú ý là chiều thứ ba d3 của bộ lọc luôn bằng với chiều thứ ba của đầu vào X do đó cỡ của bộ lọc thường", "ID": "109"}, {"text": "Thứ hai , tỉ lệ drop tự học được của VBD có thể tận dụng để đánh giá độ quan trọng của nơ-ron cho mỗi tác vụ . Phạm vi đánh giá trên nơ-ron có lợi thế về mặt bộ nhớ", "ID": "110"}, {"text": "lập của mỗi biến ứng với các hệ số ẩn . Thật vậy , tập các quan sát X sẽ được chuyển thành ̃ :", "ID": "111"}, {"text": "Mô hình máy dịch dựa trên mạng nơ-ron sử dụng RNN trong thực nghiệm này sử dụng LSTM . Encoder và decoder tạo thành từ 2 lớp LSTM , lớp thấp nhất ở phía encoder là LSTM", "ID": "112"}, {"text": "𝐶𝐷 ( 𝑣 ) = deg ( 𝑣 ) Tính toán độ trung tâm bậc cho tất cả các nút trong đồ thị sẽ mất một thời gian tính toán 𝛩 ( 𝑉 2 ) đối với đồ thị ma trận kề dày đặc và 𝛩 ( 𝐸 ) đối với đồ thị có ma", "ID": "113"}, {"text": "quan hệ ( cause , contrast ) từ các cặp câu được trích rút ra từ các văn bản mà không có các từ nối ( but , so ) . IDRR được áp dụng trong các ứng dụng về hỏi đáp ( Liakata et al. ,", "ID": "114"}, {"text": "luôn tận tình hướng dẫn cũng như động viên , khích lệ tinh thần những lúc em gặp khó khăn . Em cũng xin cảm ơn TS. Thân Quang Khoát - Trưởng phòng", "ID": "115"}, {"text": "VBD - CL không cao bằng UCL và EWC có thể do ảnh hưởng hai ràng buộc KL và L1 . Độ hiệu quả và ổn định của VBD - CL được thể hiện rõ hơn ở hình 5 . 5 . Độ", "ID": "116"}, {"text": "Kết quả là , chúng ta nhận được bốn ma trận biểu diễn thuộc tính ẩn . Ma trận P ∈ Rk×M trong đó k là số thuộc tính ẩn là ma trận biểu diễn thuộc tính ẩn", "ID": "117"}, {"text": "Công thức 13 thực chất là đang cố gắng kéo các xác suất thuộc vào các cụm của biểu diễn ẩn tạo từ 2 view của cùng một dữ liệu lại gần nhau bằng KL divergence .", "ID": "118"}, {"text": "như trạng thái khởi tạo đầu vào của mạng GRU . Cách tiếp cận này được gọi là LSTUR - ini , đầu ra của trạng thái cuối cùng trong chuỗi mạng GRU sẽ được sử", "ID": "119"}, {"text": "E vào đầu vào của mỗi tầng của mạng tại mỗi vòng lặp trong quá trình huấn luyện : B = ( A E ) W , với Emk ∼ p ( Emk ) và là phép toán nhân từng phần tử ( elementwise product ) . Phiên bản đầu tiên của Dropout , Bernoulli Dropout [ 28 ] , lấy mẫu Emk theo phân", "ID": "120"}, {"text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 29 Chương 3 Các phương pháp học dòng cơ sở cho", "ID": "121"}, {"text": "• input _ length : Độ dài của chuỗi dữ liệu đầu vào MAX _ SENT _ LENGTH . Dữ liệu sau khi đi qua tầng Embedding đã được học và biểu diễn dưới dạng vec -tơ", "ID": "122"}, {"text": "diễn đầu vào cho người dùng và item được thay thế bằng vec- tơ biểu diễn LDA ( xem mục 2 . 2 . 1 ) . Vec - tơ pcat của người dùng được xây dựng từ những banner", "ID": "123"}, {"text": "tiếp theo phân biệt 2 − 3 . Permuted MNIST : Tập dữ liệu này cũng được xây dựng từ tập gốc MNIST . Mỗi", "ID": "124"}, {"text": "của dữ liệu hành vi tiềm ẩn và rõ ràng được thể hiện bằng cách : tầng cuối cùng trong mô hình tương tác tiềm ẩn được đẩy lên làm đầu vào cho một mạng MLP", "ID": "125"}, {"text": "Discounted Cumulative Gain ( DCG ) : CG chỉ tính tổng độ liên quan nên chưa đánh giá được yếu tố vị trí , vì thế DCG có thể giải quyết vấn đề này bằng cách", "ID": "126"}, {"text": "Trong công việc này , bộ PMNIST sẽ gồm 10 tác vụ đến liên tục , tương ứng với 10 cách hoán vị khác nhau lên MNIST . 25", "ID": "127"}, {"text": "3 . 4 . 7 . Xem các công việc đã lưu hình 28 : ACD ca sử dụng xem các công việc đã lưu", "ID": "128"}, {"text": "Các biến thể Group Lasso , Sparse Group Lasso được kết hợp với K-means , K-medoids , phân cụm phân cấp ( HAC ) là", "ID": "129"}, {"text": "Đối với tập Split MNIST , tầng đầu ra có 10 nhánh - mỗi nhánh 10 nơ - ron ứng với một tác vụ , tức tuân theo kịch bản nhiều đầu ra . Còn với tập Permuted MNIST ,", "ID": "130"}, {"text": "biểu diễn ẩn và quan sát mức độ hội tụ của mô hình sẽ sử dụng phương pháp KNN để phân loại trên tập test . Đồ án tốt nghiệp", "ID": "131"}, {"text": "Ước lượng β t bằng cách cực đại hoá hàm mục tiêu ( 54 ) end for 4 . 2 . 3 Áp dụng iDropout vào mô hình NB", "ID": "132"}, {"text": "quá trình học tại mỗi minibatch cho hai mô hình này có thể viết tổng quát dưới bài toán cực đại hoá hàm mục tiêu có dạng sau đây : V", "ID": "133"}, {"text": "Thuật toán 3 : Thuật toán EM cho mô hình Gaussian Mixture Model Đầu vào : Tập dữ liệu { 𝑥1 , 𝑥2 , … , 𝑥𝑁 } ; 𝑥𝑛 ∈ ℝ𝑀 ; số cụm K ( K < N ) . Đầu ra :", "ID": "134"}, {"text": "HAN , ta có thể sử dụng mô hình học này thể hiện tính diễn giải cho văn bản . Đây là một sự khác biệt lớn thể hiện sự theo sát thực tế của những sản phẩm trí tuệ nhân tạo .", "ID": "135"}, {"text": "THỬ NGHIỆM Chương này trình bày thử nghiệm của phương pháp đề xuất , bao gồm tập dữ liệu , kịch bản đánh giá , các phương pháp so sánh và nhận xét , đánh giá .", "ID": "136"}, {"text": "t • Khi có biến ẩn toàn cục Θt ( đồng thời cũng là Θ̃ ) , thực hiện suy diễn các biến cục bộ z kế thừa từ mô hình gốc B", "ID": "137"}, {"text": "VCL – Không cần Tham số cho ALV KL - weight", "ID": "138"}, {"text": "2011) Đối với PDTB - Ji em sử dụng 4 tầng convolutional encoder block còn PDTB - Lin dùng", "ID": "139"}, {"text": "Hàm mục tiêu của quá trình huấn luyện được mô tả như sau X 1", "ID": "140"}, {"text": "Nếu như mạng nơ-ron chỉ là tầng vào 𝑥 đi qua tầng ẩn 𝑠 và cho ra tầng ra 𝑜 và các tầng kết nối đầy đủ với nhau thì trong RNN , các đầu vào 𝑥𝑡 tại thời điểm 𝑡 sẽ được kết hợp với tầng ẩn 𝑠𝑡 −1 bằng hàm 𝑓𝑤 để tính toán ra tầng ẩn 𝑠𝑡 hiện tại và đầu ra 𝑜𝑡", "ID": "141"}, {"text": "Vì vậy , trong quá trình nghiên cứu đề tài này , em sử dụng 3 độ đo AUC , MRR và nDCG@k để đánh giá và so sánh kết quả giữa các mô hình .", "ID": "142"}, {"text": "Xử lý ngôn ngữ tự nhiên GRU Gated Recurrent Unit", "ID": "143"}, {"text": "SVB - PP có thể trở nên quá cứng nhắc để thích ứng với những sự thay đổi trong dữ liệu , iDropout luôn sẵn sàng với những thay đổi này . Dropout cũng có vai trò quan trọng trong việc giúp iDropout đương đầu với", "ID": "144"}, {"text": "43 5 THỬ NGHIỆM VÀ ĐÁNH GIÁ", "ID": "145"}, {"text": "năm 2021 Tác giả ĐATN", "ID": "146"}, {"text": "( b ) Hình 2 . 5 : Tác động của tham số phạt α lên điểm BLEU của ( a ) ConvS2S- base và ( b ) Transformerbase trên tập tst 2012 chiều Anh - Việt . Mô hình ConvS2S- base sử dụng beam size 10 , mô hình", "ID": "147"}, {"text": "xây dựng dựa trên các mô-đun chính : User-encoder , News-encoder và Click Predictor . Bài báo mô hình NAML được giới thiệu tại hội nghị IJCAI năm 2019 và được", "ID": "148"}, {"text": "Thêm vào đó UCL cũng sử dụng một 𝜇 thêm một đại lượng cho thành phần ( 𝑎 ) lấy ý tưởng từ chỉ số tín hiệu nhiễu , để", "ID": "149"}, {"text": "( 2.16 ) = ⇒ maxw , µ , σ , γ Tương tự như VD , VBD cần đảm bảo đại lượng KL trong ELBO không phụ thuộc", "ID": "150"}, {"text": "NST số 22 gây ung thư máu ác tính , mất đoạn vai ngắn trên nhiễm sắc thể số 5 gây hội chứng tiếng mèo kêu , … 2 . 3 . Nghiên cứu bệnh – chứng", "ID": "151"}, {"text": "hàm mất mát sử dụng trong những bài như SimCLR và MOCO là muốn ép sao cho các biểu diễn ẩn của các cặp positive về gần nhau , nên khi mà các mạng nơ-ron được huấn luyện đến hội tụ , một hiện tượng hay xảy ra đó là các biểu diễn ẩn này", "ID": "152"}, {"text": "Xác định ma trận input X , ma trận Ã gcn_ out1 ← ReLU ( gcn_ layer1 ( X, Ã )", "ID": "153"}, {"text": "Hỗ trợ xây dựng CNN , RNN và có thể kết hợp cả hai . Em chọn keras để xây dựng mô hình học được đề xuất HAN và triển khai huấn luyện trên một số mạng nơ-ron khác để tiến hành so sánh và đánh giá với mô", "ID": "154"}, {"text": "Hình 5 . 4 : Sự thay đổi của độ chính xác của các tác vụ trong quá trình học Split CIFAR100 . 5.4.4", "ID": "155"}, {"text": "MF với công thức dự đoán đơn giản là tích vô hướng của hai vec - tơ k chiều ( k thường rất bé so với M và N ) , hoàn toàn đáp ứng được điều này . 2 . 3 Lớp các mô hình mạng nơ-ron lọc cộng tác", "ID": "156"}, {"text": "Do đó , LSTM ra đời dựa trên RNN , giúp giải quyết vấn đề “ quên ” trong việc học của mạng RNN . Mạng bộ nhớ dài - ngắn ( LSTM )", "ID": "157"}, {"text": "HAN lấy ý tưởng từ cấu trúc của văn bản , đó là cấu trúc phân cấp . Bên cạnh đó , mô hình còn thể hiện sự phụ thuộc ngữ nghĩa của một từ đối với ngữ cảnh mà từ đó xuất", "ID": "158"}, {"text": "KIẾN THỨC NỀN TẢNG 2 . 1 Mô hình Biterm Topic Model ( BTM ) 2 . 1 . 1 Giới thiệu về BTM", "ID": "159"}, {"text": ". ... 43 DANH MỤC BẢNG BIỂU", "ID": "160"}]