[
  {
    "text": "• Đổi chỗ 𝜇𝑘 và 𝑥𝑛 , gán lại cụm cho các quan sát . Tính giá trị hàm lỗi L • Nếu hàm lỗi không giảm thì hủy bỏ thao tác đổi chỗ .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1"
  },
  {
    "text": "Tổng quan về SSL Khái niêm Trước tiên , đồ án sẽ trình bày một số khái niệm về SSL ở trên mạng rồi mới phân",
    "acronyms": [
      [
        13,
        16
      ],
      [
        79,
        82
      ]
    ],
    "long-forms": [],
    "ID": "2"
  },
  {
    "text": "Đầu vào : Dòng dữ liệu với các minibatch { D1 , D2 , ... , Dt , ... } , các tham số drop rate dr , phương sai σ 2 Đầu ra : Biến toàn cục β",
    "acronyms": [],
    "long-forms": [],
    "ID": "3"
  },
  {
    "text": "Bayes , trong đó mỗi node trong mạng đại diện cho từng cấu trúc ngữ nghĩa ẩn của tập văn bản được sinh ra đó . Cụ thể LDA giả sử rằng mỗi văn bản d là",
    "acronyms": [
      [
        118,
        121
      ]
    ],
    "long-forms": [],
    "ID": "4"
  },
  {
    "text": "LDA có thể không hoạt động tốt đối với các văn bản ngắn do sự thưa thớt dữ liệu . Mô hình chủ đề Biterm ( BTM ) học các chủ đề bằng cách mô hình hóa các cặp từ",
    "acronyms": [
      [
        0,
        3
      ],
      [
        106,
        109
      ]
    ],
    "long-forms": [],
    "ID": "5"
  },
  {
    "text": "cho 𝜎𝑖 . Hard Attention to the Task ( HAT ) ( Joan Serra , 2018 ) xác định độ quan trọng của nơron thông qua cơ chế hard attention trên nơ-ron sau mỗi tầng . Tại mỗi tầng của mạng sẽ thêm vào một tầng task - embedding , được huấn luyện cùng với mạng nhờ sự hỗ trợ của một kỹ",
    "acronyms": [
      [
        38,
        41
      ]
    ],
    "long-forms": [],
    "ID": "6"
  },
  {
    "text": "Chú ý rằng khi suy diễn , X l được nhân với µl . X̃i = Xi ∗ µi ∗ I [ pi < thresh ] ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "7"
  },
  {
    "text": "( MLM ) đều được huấn luyện không giám sát , chỉ yêu cầu dữ liệu đơn ngữ và không tận dụng được dữ liệu song ngữ có sẵn . Mô hình ngôn ngữ dịch ( TLM ) giúp cải",
    "acronyms": [
      [
        2,
        5
      ],
      [
        146,
        149
      ]
    ],
    "long-forms": [
      [
        122,
        143
      ]
    ],
    "ID": "8"
  },
  {
    "text": "Công thức tường minh của đại lượng KL trong 3.14 khi xét các wij độc lập nhau :  L",
    "acronyms": [
      [
        35,
        37
      ]
    ],
    "long-forms": [],
    "ID": "9"
  },
  {
    "text": "bag-of-words : wd = { wd1 , wd2 , ... , wdNd } trong đó wdn là từ thứ n trong dãy các từ của văn bản d . • Tập văn bản bao gồm M văn bản được ký hiệu bởi D = { w1 , w2 , ... , wM }",
    "acronyms": [],
    "long-forms": [],
    "ID": "10"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 49 in short",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "11"
  },
  {
    "text": "là việc biểu diễn ẩn đấy còn giữ được một số thông tin nhất định từ đầu vào , hay nói cách khác , nếu ta xem X là dữ liệu , Y là một biểu diễn ẩn nào đó mà ta tìm được , thì việc tìm ra một ánh xạ q tốt từ X đến Y được đưa về nghiệm của bài",
    "acronyms": [],
    "long-forms": [],
    "ID": "12"
  },
  {
    "text": "Mô hình đề xuất ITE - onehot . . . . .",
    "acronyms": [
      [
        16,
        19
      ]
    ],
    "long-forms": [],
    "ID": "13"
  },
  {
    "text": "Điểm BLEU của các mô hình học đối ngẫu sử dụng nhiễu . . . . . . . . . . 44",
    "acronyms": [
      [
        5,
        9
      ]
    ],
    "long-forms": [],
    "ID": "14"
  },
  {
    "text": "• So sánh các mô hình qua các vòng lặp với các k khác nhau ( a ) HR @ 10",
    "acronyms": [
      [
        65,
        67
      ]
    ],
    "long-forms": [],
    "ID": "15"
  },
  {
    "text": "Tối ưu hàm mất mát : Khi cố định P , việc tối ưu Q được đưa về bài toán tối ưu hàm",
    "acronyms": [],
    "long-forms": [],
    "ID": "16"
  },
  {
    "text": "Tóm lại , bài toán học mô hình LDA chính là việc ước lượng các biến ẩn khi đã biết các từ của các văn bản . Dưới lý thuyết Bayesian , công việc này chính là",
    "acronyms": [
      [
        31,
        34
      ]
    ],
    "long-forms": [],
    "ID": "17"
  },
  {
    "text": "Tài liệu tham khảo Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 66",
    "acronyms": [
      [
        79,
        90
      ]
    ],
    "long-forms": [],
    "ID": "18"
  },
  {
    "text": "i=1 ( 37 ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        71,
        82
      ]
    ],
    "long-forms": [],
    "ID": "19"
  },
  {
    "text": "với phân phối tiên nghiệm sẽ được lựa chọn . Trong phương pháp đề xuất 𝑠𝑡 , 𝑚𝑑 có thể tùy biến lựa chọn từ các phân phối dạng khác chứ không hạn chế như trong VD",
    "acronyms": [
      [
        159,
        161
      ]
    ],
    "long-forms": [],
    "ID": "20"
  },
  {
    "text": "M Sỗ lượng người dùng N",
    "acronyms": [],
    "long-forms": [],
    "ID": "21"
  },
  {
    "text": "False Negative ( FN ) : Không phát hiện được ground truth True Negative ( TN ) : Không được sử dụng trong bài toán này . Trong nhiệm vụ",
    "acronyms": [
      [
        17,
        19
      ],
      [
        74,
        76
      ]
    ],
    "long-forms": [],
    "ID": "22"
  },
  {
    "text": "sao cho : θM LE = argmaxθ p ( D | θ ) . ( 2.9 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "23"
  },
  {
    "text": "Long short-term memory Long short-term memory ( LSTM ) được giới thiệu lần đầu tiên bởi Hochreiter và Schmidhuber ( 1997 ) [ 12 ] , và được tinh chỉnh cũng như phổ biến rộng rãi bởi cộng đồng nghiên cứu . LSTM được thiết kế theo cách đặc biệt với memory cell ( cell state ) để giải quyết vấn đề phụ",
    "acronyms": [
      [
        48,
        52
      ],
      [
        205,
        209
      ]
    ],
    "long-forms": [],
    "ID": "24"
  },
  {
    "text": "Layout LM sử dụng 2 chiến lược huấn luyện : • Mask Visual-language Model ( MVLM ) • Multi-label Document Classification ( MDC )",
    "acronyms": [
      [
        75,
        79
      ],
      [
        122,
        125
      ],
      [
        7,
        9
      ]
    ],
    "long-forms": [],
    "ID": "25"
  },
  {
    "text": "Để quyết định nơ-ron nào được giữ lại , VBD học ra một tỉ lệ drop cho mỗi nơ-ron . Tỉ lệ này mang thông tin về độ quan trọng của nơ-ron đó đối với tác vụ",
    "acronyms": [
      [
        40,
        43
      ]
    ],
    "long-forms": [],
    "ID": "26"
  },
  {
    "text": "Như vậy , có vẻ nhóm tác giả không quan tâm tới độ tin cậy của dữ liệu trong Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 27",
    "acronyms": [
      [
        140,
        151
      ]
    ],
    "long-forms": [],
    "ID": "27"
  },
  {
    "text": "Hình 21 : So sánh các mô hình trên bộ Retailrocket qua từng vòng lặp với k = 8 ( a ) HR @ 10",
    "acronyms": [
      [
        85,
        87
      ]
    ],
    "long-forms": [],
    "ID": "28"
  },
  {
    "text": "Mỗi tác 31 vụ chỉ cần tốn O ( M + N ) đơn vị bộ nhớ để lưu lại độ quan trọng cho nơ-ron , thay",
    "acronyms": [],
    "long-forms": [],
    "ID": "29"
  },
  {
    "text": "PD cập PNnhật d",
    "acronyms": [
      [
        0,
        2
      ]
    ],
    "long-forms": [],
    "ID": "30"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 40 Câu 1 : viewers will be given a 900 number to call .",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "31"
  },
  {
    "text": "• Các phép DA liên quan đến màu sắc : độ sáng , độ tương phản , giảm màu , độ bão hòa Các phép DA được sử dụng kết hợp với nhau tạo thành một tập hợp và mỗi",
    "acronyms": [
      [
        11,
        13
      ],
      [
        95,
        97
      ]
    ],
    "long-forms": [],
    "ID": "32"
  },
  {
    "text": "thời gian ( item trong hệ thống này là các banner quảng cáo ) . Ở đây , view là một hành vi tiềm ẩn , nó xảy ra khi người dùng lướt các trang web có hiển thị quảng",
    "acronyms": [],
    "long-forms": [],
    "ID": "33"
  },
  {
    "text": "cứu đã khai thác thông tin từ nối trên chính bộ dữ liệu PDTB này ( Liu et al. , 2016 ; Wu et al. , 2016 ; Lan et al. , 2017 ; Bai and Zhao , 2018 ) . Đó cũng chính là hướng tiếp cận thứ",
    "acronyms": [
      [
        56,
        60
      ]
    ],
    "long-forms": [],
    "ID": "34"
  },
  {
    "text": "Chúng tôi sử dụng mô hình LDA để phân tích chủ đề của 7 bộ dữ liệu văn bản phổ biến , trong đó có : • 6 bộ không có nhãn thời gian bao gồm 3 bộ gồm các văn bản dài là { Grolier",
    "acronyms": [
      [
        26,
        29
      ]
    ],
    "long-forms": [],
    "ID": "35"
  },
  {
    "text": "này khác với MLE hay MAP bởi hai ước lượng này chỉ tìm được một giá trị của tham số , hay còn gọi là ước lượng điểm ( Point estimate ) . Trong rất nhiều trường",
    "acronyms": [
      [
        13,
        16
      ],
      [
        21,
        24
      ]
    ],
    "long-forms": [],
    "ID": "36"
  },
  {
    "text": "Như đã đề cập lúc đầu , mỗi từ nối có thể kết hợp với một hoặc nhiều mối quan hệ . Ví dụ , trong bộ dữ liệu PDTB , em phát hiện ra khoảng 53 % từ nối chỉ kết hợp với một",
    "acronyms": [
      [
        108,
        112
      ]
    ],
    "long-forms": [],
    "ID": "37"
  },
  {
    "text": "3.4 . 2 Mutil - Layer Perceptron ( MLP ) Hình 19 . Mô hình Mutil - Layer Perceptron [ 3 ]",
    "acronyms": [
      [
        35,
        38
      ]
    ],
    "long-forms": [],
    "ID": "38"
  },
  {
    "text": "mạng RNN cơ bản được mô tả như trên Hình 1 . 5 . Mạng nơ-ron hồi quy cho phép ta mô hình hóa các sự phụ thuộc dài hạn về thông tin vì nó có khả năng cho phép thông tin được lan",
    "acronyms": [
      [
        5,
        8
      ]
    ],
    "long-forms": [],
    "ID": "39"
  },
  {
    "text": "National Institute of Information and Communications Technology , Japan ( NICT ) khởi xướng vào năm 2014 . Quá trình xây",
    "acronyms": [
      [
        74,
        78
      ]
    ],
    "long-forms": [],
    "ID": "40"
  },
  {
    "text": "mỗi tầng nhiễu l này bao gồm hai biến ngẫu nhiên { µl , σ l } . X 2 = ( X 1 E 1 ) W 2 .",
    "acronyms": [],
    "long-forms": [],
    "ID": "41"
  },
  {
    "text": "CSS CSDL URL",
    "acronyms": [],
    "long-forms": [],
    "ID": "42"
  },
  {
    "text": "truyền qua hoặc bị chặn lại ở đầu vào hoặc đầu ra của LSTM . Giá trị các phần tử của it và ut luôn nằm trong khoảng từ 0 đến 1 .",
    "acronyms": [
      [
        54,
        58
      ]
    ],
    "long-forms": [],
    "ID": "43"
  },
  {
    "text": "và chuyển nó xuống mạng . Ở đây sử dụng cổng update gate . Nó quyết định thông tin",
    "acronyms": [],
    "long-forms": [],
    "ID": "44"
  },
  {
    "text": "tiếp liên quan đến sự kiện chính Anecdotal Event ( D2 ) : là các sự kiện khó xác minh , các tình huống hư cấu hoặc",
    "acronyms": [],
    "long-forms": [],
    "ID": "45"
  },
  {
    "text": "Đầu ra : • Y : ma trận dữ liệu đã được giảm chiều , với chiều m × d . Mỗi hàng là một điểm",
    "acronyms": [],
    "long-forms": [],
    "ID": "46"
  },
  {
    "text": "( 10 ) ( u , i ) ∈Y∪Y − Hàm mất mát là độ lệch bình phương sẽ phù hợp hơn khi các tập các giá trị",
    "acronyms": [],
    "long-forms": [],
    "ID": "47"
  },
  {
    "text": "việc sử dụng và không sử dụng Dropout . Từ đó , ta có thể thấy rằng ALV có khả năng giúp mô hình cân bằng tốt giữa hai yếu tố là tính mềm dẻo và ổn định trong",
    "acronyms": [
      [
        68,
        71
      ]
    ],
    "long-forms": [],
    "ID": "48"
  },
  {
    "text": "pháp học dựa trên ràng buộc trọng số đã có . 3 . Các nhiệm vụ cụ thể của ĐATN - Tìm hiểu các vấn đề và giải pháp trong lĩnh vực Học liên tục .",
    "acronyms": [
      [
        73,
        77
      ]
    ],
    "long-forms": [],
    "ID": "49"
  },
  {
    "text": "sử dụng Dropout và ALV đem lại hiệu quả thực sự đáng kể cho EWC , sự cải thiện này rõ rệt hơn rất nhiều so thử nghiệm trong hai phương pháp UCL và VCL . Điều này có thể giải thích là vì UCL và VCL sử dụng kiến trúc mạng BNN để biểu diễn",
    "acronyms": [
      [
        19,
        22
      ],
      [
        60,
        63
      ],
      [
        140,
        143
      ],
      [
        147,
        150
      ],
      [
        186,
        189
      ],
      [
        193,
        196
      ],
      [
        220,
        223
      ]
    ],
    "long-forms": [],
    "ID": "50"
  },
  {
    "text": "giá biểu diễn ẩn , thì thu được kết quả không có một phép DA nào cho biểu diễn ẩn quá tốt dù cho mô hình có thể giải quyết pretext task rất tốt . Mặt khác , việc",
    "acronyms": [
      [
        58,
        60
      ]
    ],
    "long-forms": [],
    "ID": "51"
  },
  {
    "text": "véc - tơ 𝑎 , ⨀ là phép nhân Hadamard . Công thức PT 3 . 5 thể hiện rằng biến hỗ trợ ( 𝑙 )",
    "acronyms": [
      [
        49,
        51
      ]
    ],
    "long-forms": [],
    "ID": "52"
  },
  {
    "text": "4 CÁC MÔ HÌNH ĐỀ XUẤT \u0014 onehot \u0015",
    "acronyms": [],
    "long-forms": [],
    "ID": "53"
  },
  {
    "text": "j − ( Eξ [ A(s̃k ) ] − A ( sk ) ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        94,
        105
      ]
    ],
    "long-forms": [],
    "ID": "54"
  },
  {
    "text": "t chúng tôi sẽ chọn f là hàm softmax ) . Khi đó , Θ̃ sẽ là một biến ẩn toàn cục mới ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "55"
  },
  {
    "text": "EWC , VCL và UCL . Trục hoành trên các đồ thị này thể hiện tác vụ tương ứng sau khi học xong và trục tung thể hiện độ chính xác của mô hình trên thử nghiệm với",
    "acronyms": [
      [
        0,
        3
      ],
      [
        6,
        9
      ],
      [
        13,
        16
      ]
    ],
    "long-forms": [],
    "ID": "56"
  },
  {
    "text": "Cụ thể , chúng ta xét họ biến phân mean-field , trong đó giả sử các biến ẩn Z là đôi một độc lập dưới từng tham số biến phân riêng rẽ , tức",
    "acronyms": [],
    "long-forms": [],
    "ID": "57"
  },
  {
    "text": "Số lượng tham số khi áp dụng ALV được tổng hợp sau khi đã học xong toàn bộ các tác vụ trong kịch bản thử nghiệm tương ứng . Bảng 4 . 5 cho biết thông tin về số lượng tham số trên các thử nghiệm ứng với",
    "acronyms": [
      [
        29,
        32
      ]
    ],
    "long-forms": [],
    "ID": "58"
  },
  {
    "text": "được phép truy cập tới ( hoặc chỉ một phần nhỏ ) dữ liệu ở các tác vụ trước đó , nên hàm mục tiêu được viết lại thành : R = RT ( θ ) +",
    "acronyms": [],
    "long-forms": [],
    "ID": "59"
  },
  {
    "text": "2 KIẾN THỨC CƠ SỞ Hình 5 : Neural Collaborative Filtering năng của mô hình dưới góc độ thuần lọc cộng tác .",
    "acronyms": [],
    "long-forms": [],
    "ID": "60"
  },
  {
    "text": "chiều của từ điển và số chiều của biểu diễn từ . Tầng thứ 2 là tầng CNN ( convolutional neural network ) [ 10 ] . Tầng CNN s có",
    "acronyms": [
      [
        68,
        71
      ],
      [
        119,
        122
      ]
    ],
    "long-forms": [],
    "ID": "61"
  },
  {
    "text": "7 2 . 2 Variational Autoencoders ( VAE ) 2 . 2 . 1 VAE là gì ?",
    "acronyms": [
      [
        35,
        38
      ],
      [
        51,
        54
      ]
    ],
    "long-forms": [],
    "ID": "62"
  },
  {
    "text": "Ở mức khái quát , có thể coi mạng nơ-ron lan truyền tiến như là một chuỗi các hàm phi tuyến : hL = g 1 ◦ g 2 ◦ . . . g L",
    "acronyms": [],
    "long-forms": [],
    "ID": "63"
  },
  {
    "text": "xuất phương pháp Group Lasso ( GL ) [ 11 ] để khắc phục điểm yếu của Lasso . Thay vì sử dụng chuẩn L1 làm đại lượng hiệu chỉnh thì Group Lasso sử dụng chuẩn L2 ( khác với hồi",
    "acronyms": [],
    "long-forms": [],
    "ID": "64"
  },
  {
    "text": "• Độ sâu lớn nhất của cây = 4 . SVM : C = 100 Hồi quy Logistic : không có",
    "acronyms": [
      [
        32,
        35
      ]
    ],
    "long-forms": [],
    "ID": "65"
  },
  {
    "text": "Cuối cùng lớp đầu ra sẽ đưa ra giá trị nhãn dự đoán của kiểu dữ liệu tương ứng . các ma trận trọng số và bias được ký hiệu là W và b . Khi đó , bộ tham số cần tối ưu của",
    "acronyms": [],
    "long-forms": [],
    "ID": "66"
  },
  {
    "text": "Xem các bài đã đăng tuyển NTD NTD chọn 1 mục dưới mục quản lý đăng tuyển",
    "acronyms": [
      [
        26,
        29
      ],
      [
        30,
        33
      ]
    ],
    "long-forms": [],
    "ID": "67"
  },
  {
    "text": "tác vụ tiêu đề sau khi đã học xong tác vụ tương ứng trên trục hoành . 35 Hình 4 . 10 Độ chính xác trên từng tác vụ trong thử nghiệm với EWC",
    "acronyms": [
      [
        136,
        139
      ]
    ],
    "long-forms": [],
    "ID": "68"
  },
  {
    "text": "2 . Phép đánh giá P : là thước đo năng lực của một thuật toán học máy . Thông thường ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "69"
  },
  {
    "text": "bào liên kết với mạch đơn của DNA theo nguyên tắc bổ sung tương tự khi DNA nhân đôi , tuy nhiên base Thymine bị thay thế bằng base Uracil . Nguyên tắc bổ sung lúc này sẽ",
    "acronyms": [
      [
        30,
        33
      ],
      [
        71,
        74
      ]
    ],
    "long-forms": [],
    "ID": "70"
  },
  {
    "text": "trong đó I [ n = nd ] là hàm indicator , bằng 1 nếu n = nd và bằng 0 nếu ngược lại . FD là một phân phối population trên tập D văn bản đã cho . Như vậy để sinh ra",
    "acronyms": [],
    "long-forms": [],
    "ID": "71"
  },
  {
    "text": "Dưới đây là hình vẽ mô tả mô hình của bài toán : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03",
    "acronyms": [
      [
        316,
        325
      ]
    ],
    "long-forms": [],
    "ID": "72"
  },
  {
    "text": "Keith đã sử dụng hai phương pháp khác nhau để thu được bộ trích xuất đặc trưng : phương pháp thứ nhất là sử dụng các đặc trưng được xây dựng thủ công với hồi quy logistic ( soft-LR ( EM ) ) , phương pháp thứ hai là sử dụng một mạng nơ-ron tích chập ( soft-CNN ( EM ) ) để tự",
    "acronyms": [
      [
        262,
        264
      ],
      [
        256,
        259
      ],
      [
        178,
        180
      ],
      [
        183,
        185
      ]
    ],
    "long-forms": [
      [
        227,
        248
      ],
      [
        154,
        170
      ]
    ],
    "ID": "73"
  },
  {
    "text": "CHƯƠNG 1 . GIỚI THIỆU ĐỀ TÀI Trong lĩnh vực Trí tuệ nhân tạo nói chung và Học máy , Học sâu nói riêng , mạng",
    "acronyms": [],
    "long-forms": [],
    "ID": "74"
  },
  {
    "text": "Giảng viên hướng dẫn : ThS. Ngô Văn Linh Chữ ký của GVHD",
    "acronyms": [
      [
        23,
        27
      ],
      [
        52,
        56
      ]
    ],
    "long-forms": [],
    "ID": "75"
  },
  {
    "text": "Hình 25 : So sánh các mô hình trên bộ Recobell qua từng vòng lặp với k = 16 ( a ) HR @ 10",
    "acronyms": [
      [
        82,
        84
      ]
    ],
    "long-forms": [],
    "ID": "76"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 43 Thuật toán 4 Phương pháp học PVB cho LDA",
    "acronyms": [
      [
        60,
        71
      ],
      [
        104,
        107
      ],
      [
        112,
        115
      ]
    ],
    "long-forms": [],
    "ID": "77"
  },
  {
    "text": "Số chiều ra của GRU là 100 chiều cho chuỗi mã hóa từ , câu . Vec - tơ ngữ cảnh từ / câu cũng có số chiều là 100 và được khởi tạo ngẫu nhiên .",
    "acronyms": [
      [
        16,
        19
      ]
    ],
    "long-forms": [],
    "ID": "78"
  },
  {
    "text": "Variational Bayesian By Backprop ( BBB ) Xét tập các biến quan sát x ∈ RN và w là các biến ẩn . Mạng nơ ron chính là một mô hình đồ thi",
    "acronyms": [
      [
        35,
        38
      ]
    ],
    "long-forms": [],
    "ID": "79"
  },
  {
    "text": "• Constant learning rate là learning rate schedule mặc định trong thuật toán tối ưu hóa SGD . Mặc đinh , ta cài đặt momentum =0 và decay rate =0 , chọn",
    "acronyms": [
      [
        88,
        91
      ]
    ],
    "long-forms": [],
    "ID": "80"
  },
  {
    "text": "1 . Các thuật toán hồi quy : Hồi quy tuyến tính , Hồi quy logistic , Stepwise regression , . . . 2 . Các thuật toán phân loại : Phân loại tuyên tính , Máy hỗ trợ vector ( Support Vector Machine - SVM ) , Kernel SVM , Sparse Representation-based classification ( SRC ) , . . .",
    "acronyms": [
      [
        211,
        214
      ],
      [
        262,
        265
      ]
    ],
    "long-forms": [],
    "ID": "81"
  },
  {
    "text": "Ngược lại , khi tham số này lớn , mô hình sẽ tập trung nhiều hơn vào việc tiếp nhận thông tin từ dữ liệu mới đến . Không như iDropout , SVB và",
    "acronyms": [
      [
        136,
        139
      ]
    ],
    "long-forms": [],
    "ID": "82"
  },
  {
    "text": "Hàm lỗi của K-medoids tương tự K-means : 𝑁",
    "acronyms": [],
    "long-forms": [],
    "ID": "83"
  },
  {
    "text": "Hình 23 : Kết quả của các cách tính w khác nhau với temperature = 0.1 Hai hình vẽ trên thể hiện sự so sánh giữa SimCLR gốc và phương pháp LCL với 3 view , ta nhận thấy , phương pháp LCL có sự hội tụ nhanh hơn trong quá trình",
    "acronyms": [
      [
        112,
        118
      ],
      [
        138,
        141
      ],
      [
        182,
        185
      ]
    ],
    "long-forms": [],
    "ID": "84"
  },
  {
    "text": "24 CHƯƠNG 4 . THỬ NGHIỆM VÀ ĐÁNH GIÁ",
    "acronyms": [],
    "long-forms": [],
    "ID": "85"
  },
  {
    "text": "Trong phần thử nghiệm , CIFAR100 được chia thành 10 tác vụ , với mỗi tác vụ yêu cầu phân loại 10 vật thể . Split CIFAR10 - 100 : Tập dữ liệu gốc CIFAR10 [ 37 ] tương tự như CIFAR100",
    "acronyms": [
      [
        24,
        32
      ],
      [
        113,
        126
      ],
      [
        145,
        152
      ],
      [
        173,
        181
      ]
    ],
    "long-forms": [],
    "ID": "86"
  },
  {
    "text": "của dữ liệu . Đây cũng chính là hướng nghiên cứu chính của em trong đồ án tốt nghiệp này - Self-supervised learning ( SSL ) , hay nói theo Tiếng Việt là học tự giám",
    "acronyms": [
      [
        118,
        121
      ]
    ],
    "long-forms": [
      [
        153,
        164
      ]
    ],
    "ID": "87"
  },
  {
    "text": "vào các phương pháp hiện tại , em cảm thấy có hứng thú với nhánh Contrastive Learning ( CL ) , một nhánh nhỏ của SSL nhưng đang phát triển một cách vượt bậc . Dựa trên sự gợi ý và sự hướng dẫn của thầy , em nhận thấy mình có thể khai thác",
    "acronyms": [
      [
        88,
        90
      ],
      [
        113,
        116
      ]
    ],
    "long-forms": [],
    "ID": "88"
  },
  {
    "text": "được học đồng thời trong suốt quá trình huấn luyện . 𝛼𝑖𝑡 = 𝑇",
    "acronyms": [],
    "long-forms": [],
    "ID": "89"
  },
  {
    "text": "đủ ( sufficient statistics ), h(x) là một hàm cho trước và A ( θ ) là đại lượng chuẩn hóa . Những mô hình với giả sử này có tính chất \" conjugate exponential \" .",
    "acronyms": [],
    "long-forms": [],
    "ID": "90"
  },
  {
    "text": "diện cho trạng thái ẩn của RNN phụ di chuyển tiến và lùi theo thời gian . Điều này cho phép các đơn vị đầu ra 𝑜 ( 𝑡 ) tính toán một đại diện phụ thuộc vào cả quá khứ và",
    "acronyms": [
      [
        27,
        30
      ]
    ],
    "long-forms": [],
    "ID": "91"
  },
  {
    "text": "Ngoài ra để đánh giá mô hình trong quá trình huấn luyện , ma trận độ chính xác [ R ] T ×T được lưu lại , với T là tổng số tác vụ . Ri , j là độ chính",
    "acronyms": [],
    "long-forms": [],
    "ID": "92"
  },
  {
    "text": "tầng ẩn thứ 𝑙 , 𝐻ℎ( 𝑙)( 𝑥) ( 𝐿̅) là ma trận Hessian của 𝐿̅ tương ứng với ℎ( 𝑙) ( 𝑥) , 〈 . , . 〉 là phép nhân tích vô hướng . Tuy nhiên [ 14 ] chỉ đưa ra phân tích trên phân phối",
    "acronyms": [],
    "long-forms": [],
    "ID": "93"
  },
  {
    "text": "Ta kí hiệu , 𝐹 là hàm đại diện cho mô hình mạng nơ-ron , ℎ ( 𝑙 ) là tầng ẩn thứ 𝑙 trong mạng , 𝐹 ( 𝑙 ) là hàm dựa trên phần kiến trúc mạng từ tầng 𝑙 cho đến tầng đầu ra ( nghĩa là 𝐹 (𝑥) = 𝐹 (𝑙) ( ℎ(𝑙) (𝑥)) ) .",
    "acronyms": [],
    "long-forms": [],
    "ID": "94"
  },
  {
    "text": "42 5 THỬ NGHIỆM VÀ ĐÁNH GIÁ",
    "acronyms": [],
    "long-forms": [],
    "ID": "95"
  },
  {
    "text": "𝑐 với 𝑐 là một hằng số PT 3.4 Khi đó thành phần KL sẽ không có dạng tường minh , nhưng có thể xấp xỉ bằng",
    "acronyms": [
      [
        23,
        25
      ],
      [
        48,
        50
      ]
    ],
    "long-forms": [],
    "ID": "96"
  },
  {
    "text": "Có hai cách để xây dựng word2vec : - Sử dụng các từ ngữ cảnh để dự đoán tự mục tiêu ( CBOW – Continuous Bag-of-word ) - Sử dụng một từ để dự đoán ra ngữ cảnh mục tiêu ( Skip-gram )",
    "acronyms": [
      [
        86,
        90
      ]
    ],
    "long-forms": [
      [
        37,
        83
      ]
    ],
    "ID": "97"
  },
  {
    "text": "Các tác vụ trong cùng một bộ dữ liệu sẽ có số lượng nhãn cần phân loại là như nhau . Thêm vào đó kiến trúc CNN cũng sẽ được sử dụng để đánh giá",
    "acronyms": [
      [
        107,
        110
      ]
    ],
    "long-forms": [],
    "ID": "98"
  },
  {
    "text": "_ alpha = 0.5 , KL - weight = 0.0001 VCL • w/o Dropout : Không cần",
    "acronyms": [
      [
        16,
        18
      ],
      [
        37,
        40
      ],
      [
        43,
        46
      ]
    ],
    "long-forms": [],
    "ID": "99"
  },
  {
    "text": "Hệ đào tạo : Đại học chính quy Đồ án tốt nghiệp ( ĐATN ) được thực hiện tại : Trường Đại học",
    "acronyms": [
      [
        50,
        54
      ]
    ],
    "long-forms": [
      [
        31,
        47
      ]
    ],
    "ID": "100"
  },
  {
    "text": "HAT_1000_0.5 VBD - CL chính xác trung bình",
    "acronyms": [
      [
        13,
        21
      ]
    ],
    "long-forms": [],
    "ID": "101"
  },
  {
    "text": "log 𝑝 ( 𝐷| 𝑤 ) + 𝐾𝐿 ( 𝑞𝜃 , 𝛼 ( 𝑤 ) | |𝑝(𝑤 ) ) PT 3.3 Tuy nhiên trong công thức PT 3.3 , ta thấy rằng đại lượng KL vẫn phụ thuộc",
    "acronyms": [
      [
        46,
        48
      ],
      [
        79,
        81
      ],
      [
        111,
        113
      ]
    ],
    "long-forms": [],
    "ID": "102"
  },
  {
    "text": "liệu . Giả sứ không gian gốc có m điểm dữ liệu xi với 1 ≤ i ≤ m , thuật toán LLE sẽ tìm k lân cận gần nhất với điểm dữ liệu xi và tái thiết lập xi như là một hàm",
    "acronyms": [
      [
        77,
        80
      ]
    ],
    "long-forms": [],
    "ID": "103"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT ( 40 ) 40",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "104"
  },
  {
    "text": "trọng số được giới hạn tùy theo giá trị độ quan trọng của hai nơ -ron ở hai đầu , để can thiệp vào sự thay đổi của trọng số đó . Nhưng HAT chưa hiệu quả trong việc",
    "acronyms": [
      [
        135,
        138
      ]
    ],
    "long-forms": [],
    "ID": "105"
  },
  {
    "text": "3 . 1 . 3 . Ứng dụng của GCN trong xử lý ngôn ngữ tự nhiên ( NLP ) Phân loại văn bản làm một trong những bài toán phổ biến trong NLP . Với bài",
    "acronyms": [
      [
        25,
        28
      ],
      [
        61,
        64
      ],
      [
        129,
        132
      ]
    ],
    "long-forms": [
      [
        35,
        58
      ]
    ],
    "ID": "106"
  },
  {
    "text": "Như vậy , bài toán mới của chúng ta là xấp xỉ phân phối hậu nghiệm population trên một dòng dữ liệu đến liên tục từ phân phối population Fα . Giống như VB",
    "acronyms": [
      [
        152,
        154
      ]
    ],
    "long-forms": [],
    "ID": "107"
  },
  {
    "text": "Chú ý rằng dưới đây sẽ trình bày VBD dưới góc nhìn của việc nén mạng có cấu trúc nên một số định nghĩa sẽ khác so với quy ước trong phần tóm tắt về VD .",
    "acronyms": [
      [
        33,
        36
      ],
      [
        148,
        150
      ]
    ],
    "long-forms": [],
    "ID": "108"
  },
  {
    "text": "𝐶× 𝑊×𝐻 với 𝐶 , 𝑊 , 𝐻 lần lượt là chiều sâu , chiều ngang và chiều dọc của ma trận . Xét tầng 𝑙 có kênh đầu ra là K chiều khi đó số",
    "acronyms": [],
    "long-forms": [],
    "ID": "109"
  },
  {
    "text": "Với 𝐾𝐿 ( 𝑞𝑡 ( 𝜃 ) | | 𝑞𝑡−1 ( 𝜃 ) ) 𝐿 2",
    "acronyms": [],
    "long-forms": [],
    "ID": "110"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 35",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "111"
  },
  {
    "text": "giống như trong các phương pháp cơ sở đã trình bày . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 46",
    "acronyms": [
      [
        113,
        124
      ]
    ],
    "long-forms": [],
    "ID": "112"
  },
  {
    "text": "Vì vậy kiến trúc của mô hình học HAN cũng được thiết kế theo mô hình phân cấp gồm 2 cấp : cấp từ , cấp câu ( được thể hiện rõ ở hình vẽ phía dưới ) . Hình 7 Mạng tập trung phân cấp ( Hierarchical Attention Network )",
    "acronyms": [
      [
        33,
        36
      ]
    ],
    "long-forms": [
      [
        157,
        180
      ]
    ],
    "ID": "113"
  },
  {
    "text": "Một phiên bản ’ cứng ’ của VBD - CL , ký hiệu là HardVBD - CL , được cài đặt khi toàn bộ trọng số nối với cả hai nơ-ron quan trọng được giữ nguyên khi học các tác vụ tiếp theo . Điều này giúp mô hình giữ nguyên tri thức",
    "acronyms": [
      [
        27,
        35
      ],
      [
        49,
        61
      ]
    ],
    "long-forms": [],
    "ID": "114"
  },
  {
    "text": "Hình 2 : Ví dụ về phân lớp ảnh Trong mô hình CNN có hai đặc tính đó là là tính bất biến và tính kết hợp . Ví dụ trong",
    "acronyms": [
      [
        45,
        48
      ]
    ],
    "long-forms": [],
    "ID": "115"
  },
  {
    "text": "v2iT ∈ ℝn x n trong đó FFN là một mạng feed forward được sử dụng cho chiều cuối cùng của từ . w2i = softmax ( Mi ) v2i ∈ ℝn x de",
    "acronyms": [
      [
        23,
        26
      ]
    ],
    "long-forms": [],
    "ID": "116"
  },
  {
    "text": "Giải pháp cho học liên tục mà đồ án đưa ra có tên là Structure Compression - based Continual Learning ( SCCL ) , học liên tục dựa trên nén cấu trúc mạng ; được xây dựng để giải quyết các vấn đề về bộ nhớ sử dụng , mở rộng mạng của nhóm phương pháp dựa trên kiến",
    "acronyms": [
      [
        104,
        108
      ]
    ],
    "long-forms": [
      [
        113,
        152
      ]
    ],
    "ID": "117"
  },
  {
    "text": "• Xác suất mỗi quan sát được sinh bởi từng thành phần 𝛾𝑛𝑘 , 𝑘 = 1 , 2 , … , 𝐾 , n = 1, 2, … , N. 1",
    "acronyms": [],
    "long-forms": [],
    "ID": "118"
  },
  {
    "text": "( 1 , nếu như có click nằm trong top K . HR@K =",
    "acronyms": [
      [
        41,
        45
      ]
    ],
    "long-forms": [],
    "ID": "119"
  },
  {
    "text": "quan hệ thứ tự như đã nói , ngoài ra còn có thêm thông tin mô tả về người dùng hay item . Nhóm thứ hai gồm ba mô hình ITE - 2 , ITE - 3 và ITE - 4 , được xây dựng",
    "acronyms": [
      [
        118,
        125
      ],
      [
        128,
        135
      ],
      [
        139,
        146
      ]
    ],
    "long-forms": [],
    "ID": "120"
  },
  {
    "text": "số chủ đề của tập văn bản V",
    "acronyms": [],
    "long-forms": [],
    "ID": "121"
  },
  {
    "text": "• w/o Dropout : Không cần • Dropout : droprate = 0.5 • ALV : init _ alpha = 0.5 , KL - weight = 0.01",
    "acronyms": [
      [
        2,
        5
      ],
      [
        82,
        84
      ],
      [
        55,
        58
      ]
    ],
    "long-forms": [],
    "ID": "122"
  },
  {
    "text": "Với hi vọng có thể cải thiện hướng nghiên cứu CL nói chung , LCL ra đời và mong muốn của em cùng với nhóm nghiên cứu là có thể đưa được ý tưởng này kết hợp vào rất nhiều phương pháp khác nhau trong hướng nghiên cứu CL như SimCLR ,",
    "acronyms": [
      [
        46,
        48
      ],
      [
        61,
        64
      ],
      [
        215,
        217
      ],
      [
        222,
        228
      ]
    ],
    "long-forms": [],
    "ID": "123"
  },
  {
    "text": "sát ( supervised learning ) , học không giám sát ( unsupervised learning ) , học tăng cường ( reinforcement Learning ) . Có thể nói , việc học kinh nghiệm E chính là việc học các tham số mô hình ( model",
    "acronyms": [],
    "long-forms": [],
    "ID": "124"
  },
  {
    "text": "49.6 49.7 Bảng 2 : Kết quả của biểu diễn ẩn khi thay đổi các yếu tổ ảnh hưởng đến tập hoán vị ( Nguồn : [ NF16 ] )",
    "acronyms": [],
    "long-forms": [],
    "ID": "125"
  },
  {
    "text": "tự bài toán phân loại cho K + 1 lớp . Hàm mất mát được tính như sau : 𝑃",
    "acronyms": [],
    "long-forms": [],
    "ID": "126"
  },
  {
    "text": "vụ cũ 5 . 2 . Từ quan sát trên hai tập dữ liệu , VBD - CL , UCL , AGS - CL cân bằng tính ổn định và",
    "acronyms": [
      [
        49,
        57
      ],
      [
        60,
        63
      ],
      [
        66,
        74
      ]
    ],
    "long-forms": [],
    "ID": "127"
  },
  {
    "text": "2 . 3 . Mô hình chủ đề ẩn ( Latent Dirichlet Allocation ) 2 . 3 . 1 . Ý tưởng Mô hình Latent Dirichlet Allocation ( LDA ) là mô hình sinh xác xuất cho tập",
    "acronyms": [
      [
        116,
        119
      ]
    ],
    "long-forms": [
      [
        8,
        25
      ]
    ],
    "ID": "128"
  },
  {
    "text": "Hình 6 : Hiệu suất LPP của các phương pháp trên mô hình LDA với 6 bộ dữ liệu không có nhãn thời gian Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        19,
        22
      ],
      [
        56,
        59
      ],
      [
        161,
        172
      ]
    ],
    "long-forms": [],
    "ID": "129"
  },
  {
    "text": "− → EFα [ log p( X ] = EFα [ KL ( q ( z , Θ ) | | p ( z , Θ | X ) ) ] + EFα [ Eq [ p ( z , Θ , X ) − q ( z , Θ ) ] ] Theo bất đẳng thức Jensen đối với hàm lõm log ta có :",
    "acronyms": [],
    "long-forms": [],
    "ID": "130"
  },
  {
    "text": "câu nhiều nhất trong một văn bản là MAX _ SENTS , ( ii ) số từ nhiều nhất trong một câu MAX _ SENT _ LENGTH , ( iii ) số bộ trong tập dữ liệu là TRAIN _ SET _ LENGTH .",
    "acronyms": [],
    "long-forms": [],
    "ID": "131"
  },
  {
    "text": "EWC có thể học trong môi trường liên tục nhiều tác vụ , tuy nhiên tác giả diễn giải trên hai tác vụ cho đơn giản và dễ hình dung . Tham số cho việc học tác",
    "acronyms": [
      [
        0,
        3
      ]
    ],
    "long-forms": [],
    "ID": "132"
  },
  {
    "text": "‖ 22 là chuẩn Frobenius của một ma trận . Ta thấy thành phần ( 𝑎 ) trong công thức PT 2. 17 đóng vai trò như một đại lượng ( 𝑙 )",
    "acronyms": [
      [
        83,
        85
      ]
    ],
    "long-forms": [],
    "ID": "133"
  },
  {
    "text": "Chữ ký của GVHD Bộ môn : Viện :",
    "acronyms": [
      [
        11,
        15
      ]
    ],
    "long-forms": [],
    "ID": "134"
  },
  {
    "text": "Một thuật toán A được gọi là mạnh mẽ - ( K , \u000f) , với \u000f( · ) : X m → R nếu với mọi tập học S ∈ X m , ∀s ∈ S , ∀z ∈ X , ∀i ∈ { 1 , ... , K} : s , z ∈ Xi = ⇒ | L ( A ( S) , s ) − L ( A ( S ) , z ) | ≤ \u000f( S )",
    "acronyms": [],
    "long-forms": [],
    "ID": "135"
  },
  {
    "text": "2 . Tác tử thứ hai , tương ứng với ngôn ngữ B , nhận được thông điệp được gửi từ tác tử thứ nhất sang dưới dạng ngôn ngữ B và kiểm tra tính hợp lý của đoạn thông điệp đó",
    "acronyms": [],
    "long-forms": [],
    "ID": "136"
  },
  {
    "text": "EWC HAT Tác v 11",
    "acronyms": [
      [
        4,
        7
      ],
      [
        0,
        3
      ]
    ],
    "long-forms": [],
    "ID": "137"
  },
  {
    "text": "qi Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 8",
    "acronyms": [
      [
        66,
        77
      ]
    ],
    "long-forms": [],
    "ID": "138"
  },
  {
    "text": "L1 làm đại lượng hiệu chỉnh : 𝑁 ∗",
    "acronyms": [],
    "long-forms": [],
    "ID": "139"
  },
  {
    "text": "𝐿𝐷 ( 𝛽 ) trong công thức ( 3 ) . Bài toán hồi quy Logistic cho Group Lasso [ 46 ] được viết dưới dạng :",
    "acronyms": [
      [
        0,
        2
      ]
    ],
    "long-forms": [],
    "ID": "140"
  },
  {
    "text": "( DVAE ) [ 3 ] có thực hiện một ý tưởng đó là sử dụng thành phàn nhiễu ² để thêm vào bức ảnh gốc ban đầu . Với mục tiêu đặt ra là mặc dù ta đưa bức ảnh",
    "acronyms": [
      [
        2,
        6
      ]
    ],
    "long-forms": [],
    "ID": "141"
  },
  {
    "text": "thay đổi C thì kết quả không có quá nhiều chênh lệch nên có thể cách làm này cũng không quá cải thiện được kết quả cuối cùng của mô hình . Song , nó có thể",
    "acronyms": [],
    "long-forms": [],
    "ID": "142"
  },
  {
    "text": "Thông số thực nghiệm chung : • Optimizer : Adam với β1 = 0.5 , β2 = 0.999 • Nhiễu z : gồm 100 chiều tuân theo phân phối chuẩn N ( 0, I )",
    "acronyms": [],
    "long-forms": [],
    "ID": "143"
  },
  {
    "text": "bộ các thử nghiệm . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03",
    "acronyms": [
      [
        287,
        296
      ]
    ],
    "long-forms": [],
    "ID": "144"
  },
  {
    "text": "Quá trình học ra biểu diễn văn bản với đầu vào là TF - IDF của văn bản đó , cố định tham số 𝛽̅ đã học và tối ưu tham số 𝜃 : 24",
    "acronyms": [
      [
        50,
        58
      ]
    ],
    "long-forms": [],
    "ID": "145"
  },
  {
    "text": "Networks ( CNN ) còn có nhiều các mạng nơ-ron kết cấu phức tạp khác như Recurrent Neural Networks ( RNN ) ( LR Medsker , 2001 ) , Transformer ( Ashish Vaswani , 2017 ) , … Trong đồ án này sử dụng hai mạng nơ-ron là MLP và CNN cho bài toán phân loại ảnh để thử nghiệm",
    "acronyms": [
      [
        11,
        14
      ],
      [
        100,
        103
      ],
      [
        215,
        218
      ],
      [
        222,
        225
      ]
    ],
    "long-forms": [],
    "ID": "146"
  },
  {
    "text": "Hình 24 : Kết quả khi so sánh các cách tính w khác nhau với số view là 5 Thêm một thí nghiệm nữa thì số view đã được tăng lên là 5 . Ở thí nghiệm này em",
    "acronyms": [],
    "long-forms": [],
    "ID": "147"
  },
  {
    "text": "Hình 7 : Hiệu suất NPMI của các phương pháp trên mô hình LDA với 6 bộ dữ liệu không có nhãn thời gian Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        19,
        23
      ],
      [
        57,
        60
      ],
      [
        162,
        173
      ]
    ],
    "long-forms": [],
    "ID": "148"
  },
  {
    "text": "PT 3.1 Graph Convolutional Networks ( GCN ) Graph Convolutional Networks ( GCN ) là một mạng neural đa lớp hoạt",
    "acronyms": [
      [
        38,
        41
      ],
      [
        0,
        2
      ],
      [
        75,
        78
      ]
    ],
    "long-forms": [],
    "ID": "149"
  },
  {
    "text": "cũng là một mô hình sinh rất hiệu quả . Phần tiếp theo sẽ trình bày chi tiết cơ sở lý thuyết và phương pháp học cho mô hình LDA để làm rõ những điều này .",
    "acronyms": [
      [
        124,
        127
      ]
    ],
    "long-forms": [],
    "ID": "150"
  },
  {
    "text": "không có nhiều cải thiện so với mô hình ITE - item _ pcat , đôi khi còn thiếu tính ổn định . Nguyên nhân có thể là do cách xây dựng biểu diễn pcat của",
    "acronyms": [
      [
        40,
        57
      ]
    ],
    "long-forms": [],
    "ID": "151"
  },
  {
    "text": "TN : True Negative Độ đo MRR Độ đo MRR có ý nghĩa đánh giá hệ thống gợi ý dựa vào thứ tự sắp xếp các vị trí",
    "acronyms": [
      [
        25,
        28
      ],
      [
        35,
        38
      ],
      [
        0,
        2
      ]
    ],
    "long-forms": [],
    "ID": "152"
  },
  {
    "text": "0.65 Tác v 9 EWC",
    "acronyms": [
      [
        13,
        16
      ]
    ],
    "long-forms": [],
    "ID": "153"
  },
  {
    "text": "để đưa ra đầu ra có cỡ 4 × 4 × K . Tiếp đó , ma trận trọng số ứng W l được tính bằng công thức : ∂L ∂z l",
    "acronyms": [],
    "long-forms": [],
    "ID": "154"
  },
  {
    "text": "Hệ đào tạo : Đại học chính quy Đồ án tốt nghiệp ( ĐATN ) được thực hiện tại : Trường Đại học Bách Khoa",
    "acronyms": [
      [
        50,
        54
      ]
    ],
    "long-forms": [
      [
        31,
        47
      ]
    ],
    "ID": "155"
  },
  {
    "text": "1825K Tuy nhiên giả định trong môi trường học liên tục là số lượng tác vụ có thể vô cùng lớn hay thậm chí là vô hạn , nên đây vẫn là một vấn đề cần phải giải quyết",
    "acronyms": [],
    "long-forms": [],
    "ID": "156"
  },
  {
    "text": "42 7 CÁC CHỮ VIẾT TẮT",
    "acronyms": [],
    "long-forms": [],
    "ID": "157"
  },
  {
    "text": "Hình 5 . 7 : Độ chính xác trung bình khi kết thúc mỗi tác vụ của VBD - CL và HardVBD - CL . 44 HAT_800_0.1",
    "acronyms": [
      [
        65,
        73
      ],
      [
        77,
        89
      ]
    ],
    "long-forms": [],
    "ID": "158"
  },
  {
    "text": "Giả sử phân phối xấp xỉ hậu nghiệm trên 𝜃 là GMF , khi đó ta có 2 ) 𝑞𝜙 ( 𝜃𝑑ℎ ) = 𝑁( 𝜇𝑑ℎ , 𝜎𝑑ℎ",
    "acronyms": [
      [
        45,
        48
      ]
    ],
    "long-forms": [],
    "ID": "159"
  },
  {
    "text": "Cụ thể , vector đầu vào ở lớp thấp nhất sẽ có dạng xi = embeding + P E ( i ) với i là vị trí của từ tương ứng . Cụ thể như sau :",
    "acronyms": [],
    "long-forms": [],
    "ID": "160"
  },
  {
    "text": "5.4.3 Thử nghiệm với mạng CNN Split CIFAR100 và Split CIFAR10 - 100 : VBD - CL cũng thể hiện hiệu quả vượt",
    "acronyms": [
      [
        26,
        29
      ],
      [
        36,
        43
      ],
      [
        54,
        67
      ],
      [
        70,
        78
      ]
    ],
    "long-forms": [],
    "ID": "161"
  },
  {
    "text": "đảm bảo hội tụ đến điểm tối ưu . Từ đây chúng ta có phương pháp học PVB cho mô hình LDA được trình bày chi tiết dưới giải thuật sau :",
    "acronyms": [
      [
        68,
        71
      ],
      [
        84,
        87
      ]
    ],
    "long-forms": [],
    "ID": "162"
  },
  {
    "text": "\u0011 ( 6 ) Trong các hệ thống thực tế , số lượng các người dùng M và số lượng các item",
    "acronyms": [],
    "long-forms": [],
    "ID": "163"
  },
  {
    "text": "tham số tối ưu 𝜃𝐵 ∗ sao cho nằm trong miền tối ưu của 𝜃𝐴 ∗ . EWC cho rằng tồn tại một miền giao thoa giữa các tham số tối ưu của từng tác vụ và mong muốn tìm ra",
    "acronyms": [
      [
        61,
        64
      ]
    ],
    "long-forms": [],
    "ID": "164"
  },
  {
    "text": "sự hướng dẫn của PGS. TS. Đỗ Phan Thuận . Các kết quả trong đồ án là trung thực , không phải sao chép toàn văn của bất kì công trình nào khác .",
    "acronyms": [
      [
        17,
        21
      ],
      [
        22,
        25
      ]
    ],
    "long-forms": [],
    "ID": "165"
  },
  {
    "text": "Giá trị 𝑚=1 𝐿 𝑀",
    "acronyms": [],
    "long-forms": [],
    "ID": "166"
  },
  {
    "text": "𝐴𝑟𝑔𝑚𝑖𝑛𝜃 , 𝛽 𝐿 = ∑ ( 𝛾̅𝑑𝑣 − 𝜃𝑑𝑘 𝑣 Sau quá trình huấn luyện , tham số 𝛽 được cố định và sử dụng cho quá trình suy",
    "acronyms": [],
    "long-forms": [],
    "ID": "167"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 67 Tài liệu tham khảo",
    "acronyms": [
      [
        60,
        71
      ]
    ],
    "long-forms": [],
    "ID": "168"
  },
  {
    "text": "Các phương pháp được huấn luyện với 100 vòng lặp , riêng VBD - CL là 150 . 5.4 5.4.1",
    "acronyms": [
      [
        57,
        65
      ]
    ],
    "long-forms": [],
    "ID": "169"
  },
  {
    "text": "liệu VCCorp được cho ở Bảng 2 . Bộ dữ liệu này được sử dụng để so sánh , đánh giá giữa ba mô hình ITE - 2 , ITE - 3 và ITE - 4 .",
    "acronyms": [
      [
        98,
        105
      ],
      [
        108,
        115
      ],
      [
        119,
        126
      ]
    ],
    "long-forms": [],
    "ID": "170"
  },
  {
    "text": "Tuy nhiên , nếu so sánh với mô hình NAML và NRMS đã trình bày ở phần trên thì mô hình LSTUR có kết quả tốt hơn NRMS nhưng kém hơn so với NAML . Từ đó có thể thấy , cách tiếp cận của mô hình LSTUR là hợp lý nhưng chưa khai thác",
    "acronyms": [
      [
        36,
        40
      ],
      [
        44,
        48
      ],
      [
        86,
        91
      ],
      [
        111,
        115
      ],
      [
        137,
        141
      ],
      [
        190,
        195
      ]
    ],
    "long-forms": [],
    "ID": "171"
  },
  {
    "text": "j−k/2 , . . . , hj+k/2 ] + bw ) + dj với hij là đầu ra của kernel j trong lớp i . Ở vị trí encoder , các đầu ra của lớp trước được đảm bảo có số chiều tương ứng để trở",
    "acronyms": [],
    "long-forms": [],
    "ID": "172"
  },
  {
    "text": "43 Bảng 3 . 2 miêu tả kết quả BLEU của các thử nghiệm áp dụng nhiễu với 3 mô hình ngôn ngữ , có thể nhận thấy mô hình dịch sử dụng nhiều kênh nhiễu có kết quả gần tương đương",
    "acronyms": [
      [
        30,
        34
      ]
    ],
    "long-forms": [],
    "ID": "173"
  },
  {
    "text": "NDCG @ 10 Hình 19 : So sánh các mô hình trên bộ Retailrocket qua từng vòng lặp với k = 64 Hình 19 và Hình 20 là kết quả độ đo trên tập test của các mô hình qua",
    "acronyms": [
      [
        0,
        4
      ]
    ],
    "long-forms": [],
    "ID": "174"
  },
  {
    "text": "với 𝑠𝑖 ∈ 𝑠 sẽ được biểu diễn bằng nhiều phân phối Gauss đơn biến với tỉ lệ trộn khác nhau : 𝑀",
    "acronyms": [],
    "long-forms": [],
    "ID": "175"
  },
  {
    "text": "5 . 3 . 4 Kết quả thử nghiệm 0.80 SVB",
    "acronyms": [
      [
        34,
        37
      ]
    ],
    "long-forms": [],
    "ID": "176"
  },
  {
    "text": "50 % mạng là có thể học xong 5 tác vụ của Split MNIST . Permuted MNIST : Ở tập dữ liệu này , VBD - CL vẫn giữ được sự ổn định của độ chính xác trung bình trong quá trình học và từ tác vụ thứ hai trở đi đều cao hơn các",
    "acronyms": [
      [
        48,
        53
      ],
      [
        65,
        70
      ],
      [
        93,
        101
      ]
    ],
    "long-forms": [],
    "ID": "177"
  },
  {
    "text": "khác nhau , độ chính xác được tính trên tập test với thang đo F1 . Mô hình học Bộ dữ liệu Facebook",
    "acronyms": [],
    "long-forms": [],
    "ID": "178"
  },
  {
    "text": "Với M = 3 , đồ thị mà mô hình dự đoán khá giống với đồ thị thực tế . Tuy nhiên , khi bậc của mô hình dự đoán tăng lên",
    "acronyms": [],
    "long-forms": [],
    "ID": "179"
  },
  {
    "text": "CƠ SỞ LÝ THUYẾT Các khái niệm cơ bản Feature extractor ( FE ) - trích xuất đặc trưng là một quá trình nhằm biến dữ",
    "acronyms": [
      [
        57,
        59
      ]
    ],
    "long-forms": [
      [
        64,
        84
      ]
    ],
    "ID": "180"
  },
  {
    "text": "nữa VCL cần hai tham số ( kỳ vọng và độ lệch chuẩn ) để biểu diễn cho mỗi trọng số của mạng . Do vậy số lượng tham số sẽ gấp đôi số lượng trọng số gốc của mạng ,",
    "acronyms": [
      [
        4,
        7
      ]
    ],
    "long-forms": [],
    "ID": "181"
  },
  {
    "text": "Do đó việc sử dụng một phân phối đơn giản hơn để xấp xỉ 𝑝 ( 𝜃 | 𝐷 ) là cần thiết . Từ những nhận định đó , Suy diễn biến phân ( VI ) là một phương pháp đã",
    "acronyms": [
      [
        128,
        130
      ]
    ],
    "long-forms": [
      [
        107,
        125
      ]
    ],
    "ID": "182"
  },
  {
    "text": "Bảng 5 . 7 : Phần trăm không gian mạng CNN sử dụng qua các tác vụ trên tập Split Omniglot Tác vụ",
    "acronyms": [
      [
        39,
        42
      ]
    ],
    "long-forms": [],
    "ID": "183"
  },
  {
    "text": "AB Tính stochastic gradient của ΘBA : K",
    "acronyms": [],
    "long-forms": [],
    "ID": "184"
  },
  {
    "text": "( 2005 ) [ 64 ] ; Langfelder và cộng sự ( 2008 ) [ 65 ] giới thiệu WGCNA , một công cụ phân nhóm gene đồng biểu hiện dựa trên HAC , hỗ trợ việc chọn lựa cách tính khoảng cách",
    "acronyms": [
      [
        67,
        72
      ],
      [
        126,
        129
      ]
    ],
    "long-forms": [],
    "ID": "185"
  },
  {
    "text": "hiệu quả trong bài toán phân loại cảm xúc văn bản chỉ đạt 78 % , 80 % tương ứng với Naïve Bayes , SVM học trên tập dữ liệu Facebook , và 77 % , 81 % tương ứng với Naïve 25",
    "acronyms": [
      [
        98,
        101
      ]
    ],
    "long-forms": [],
    "ID": "186"
  },
  {
    "text": "34 Bảng 2 . 5 : Điểm BLEU của các thí nghiệm sử dụng mô hình ConvS2S , b là beam size khi thực hiện beam search sử dụng hàm phạt mặc định",
    "acronyms": [
      [
        21,
        25
      ],
      [
        61,
        68
      ]
    ],
    "long-forms": [],
    "ID": "187"
  },
  {
    "text": "Mạng nơ-ron tích chập ( convolutional neural network , CNN ) [ 12 ] , [ 15 ] là một mạng nơ-ron truyền tiến đặc biệt và áp dụng chủ yếu vào lĩnh vực xử lý ảnh . Đầu vào cũng như biểu diễn",
    "acronyms": [
      [
        55,
        58
      ]
    ],
    "long-forms": [
      [
        0,
        21
      ]
    ],
    "ID": "188"
  },
  {
    "text": "hàng xóm của người dùng u thông thường sẽ là những người dùng v mà có mức Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 13",
    "acronyms": [
      [
        137,
        148
      ]
    ],
    "long-forms": [],
    "ID": "189"
  },
  {
    "text": "𝟒1. 𝟑7 ± 0. 𝟑6 Bảng 2 . 3 Kết quả mô hình LSTUR",
    "acronyms": [
      [
        42,
        47
      ]
    ],
    "long-forms": [],
    "ID": "190"
  },
  {
    "text": "Các siêu tham số đó là : λ của EWC , { smax , c } của HAT , { β } của UCL , { λ , µ , ρ } của AGS - CL , { thresh , s } của VBD - CL 35",
    "acronyms": [
      [
        31,
        34
      ],
      [
        54,
        57
      ],
      [
        70,
        73
      ],
      [
        94,
        102
      ],
      [
        124,
        132
      ]
    ],
    "long-forms": [],
    "ID": "191"
  },
  {
    "text": "Từ các tính chất trên chúng ta có nhận xét rằng : Không giống như VB cổ điển , trong đó việc cực đại hoá hàm ELBO này tương đương với cực tiểu khoảng cách",
    "acronyms": [
      [
        66,
        68
      ],
      [
        109,
        113
      ]
    ],
    "long-forms": [],
    "ID": "192"
  },
  {
    "text": "Hà Nội , ngày 10 tháng 6 năm 2021 Tác giả ĐATN Vũ Hồng Phúc",
    "acronyms": [
      [
        42,
        46
      ]
    ],
    "long-forms": [],
    "ID": "193"
  },
  {
    "text": "liên quan đến mô hình LDA đã được trình bày ở phần 2 . 1 . Tuy nhiên có một điều khác biệt là trong các phương pháp học dòng , quá trình sinh được định",
    "acronyms": [
      [
        22,
        25
      ]
    ],
    "long-forms": [],
    "ID": "194"
  },
  {
    "text": "nói một cách khác , việc tối ưu hàm loss NCE đồng nghĩa với việc ta đi cực đại hóa MI giữa các view khác nhau của cùng một bức ảnh . Tuy nhiên , trong phạm",
    "acronyms": [
      [
        41,
        44
      ],
      [
        83,
        85
      ]
    ],
    "long-forms": [],
    "ID": "195"
  },
  {
    "text": "nhau tùy thuộc vào từng miền và lĩnh vực mà bài toán gợi ý đang áp dụng . Để học các tham số của mô hình , có thể sử dụng kĩ thuật stochastic gradient descent ( SGD ) truyền thống .",
    "acronyms": [
      [
        161,
        164
      ]
    ],
    "long-forms": [],
    "ID": "196"
  },
  {
    "text": "Hà Nội , ngày 22 tháng 05 năm 2019 Giảng viên hướng dẫn PGS. TS. Đỗ Phan Thuận",
    "acronyms": [
      [
        56,
        60
      ],
      [
        61,
        64
      ]
    ],
    "long-forms": [],
    "ID": "197"
  },
  {
    "text": "sẽ được học còn Q là tập các mã được tính theo mọt bài toán tối ưu khác không liên quan đến bước học . Sau khi có đủ ba thành phần z , Q , C , sẽ khởi",
    "acronyms": [],
    "long-forms": [],
    "ID": "198"
  },
  {
    "text": "Mặt khác , trong bài báo gốc , các tác giả cũng so sánh HPP với SVB - PP khi tinh chỉnh cẩn thận tham số ρ .",
    "acronyms": [
      [
        56,
        59
      ],
      [
        64,
        72
      ]
    ],
    "long-forms": [],
    "ID": "199"
  },
  {
    "text": "biến phân cũng cần mang tri thức tiên nghiệm . Kỹ thuật Reparameterization : Để có thể tối ưu ELBO với tham số φ , cần phải",
    "acronyms": [
      [
        94,
        98
      ]
    ],
    "long-forms": [],
    "ID": "200"
  },
  {
    "text": ". . . . . . . . . . . . . . . . 31 5 . 1 Độ chính xác trung bình khi kết thúc mỗi tác vụ trên tập Split MNIST",
    "acronyms": [
      [
        104,
        109
      ]
    ],
    "long-forms": [],
    "ID": "201"
  },
  {
    "text": "Dropout biến phân cụ thể được cài đặt , đó là Dropout Bayes biến phân ( Variational Bayesian Dropout , VBD ) [ 1 ] . Các kết quả thử nghiệm tiến hành trên một số bộ dữ",
    "acronyms": [
      [
        103,
        106
      ]
    ],
    "long-forms": [
      [
        46,
        69
      ]
    ],
    "ID": "202"
  },
  {
    "text": "21 2 KIẾN THỨC CƠ SỞ • Hàm sigmoid với công thức",
    "acronyms": [],
    "long-forms": [],
    "ID": "203"
  },
  {
    "text": "không thay đổi quá nhiều theo thành phần ( a ) . UCL thay đổi hai thành phần này dựa trên các nhận xét vừa rồi để xây dựng độ không chắc chắn cho nơ-ron , từ đó",
    "acronyms": [
      [
        49,
        52
      ]
    ],
    "long-forms": [],
    "ID": "204"
  },
  {
    "text": "quá khứ và tính toán xấp xỉ được phân phối hậu nghiệm trên dữ liệu này , tức là p ( Θ | D1 , D2 , ... , Dt−1 , η ) . Khi đó chúng ta có thể tính toán phân phối hậu nghiệm",
    "acronyms": [],
    "long-forms": [],
    "ID": "205"
  },
  {
    "text": "Nói cách khác , mỗi giá trị trong ma trận tương tác R sẽ có công thức rui ≈ pTu qi . Giá trị biểu thức pTu qi sẽ cao nếu như hệ số của",
    "acronyms": [],
    "long-forms": [],
    "ID": "206"
  },
  {
    "text": "Xét mô hình tổng quát B ( Θ , z , x ) trong đó x1 : N là các biến dữ liệu quan sát được , Θ là biến ẩn toàn cục đặc trưng cho mô hình , z1 : N là các biến ẩn cục bộ mà mỗi zi đặc trưng tương ứng với một xi .",
    "acronyms": [],
    "long-forms": [],
    "ID": "207"
  },
  {
    "text": "∗ ) ℒ 𝑡 ( 𝜃 ) = ℒ ( 𝐷𝑡 , 𝜃 ) + 𝑅( 𝜃 , 𝜃𝑡−1 với R là hàm ràng buộc",
    "acronyms": [],
    "long-forms": [],
    "ID": "208"
  },
  {
    "text": "( a ) Cấu trúc Transformer ( b ) Kiến trúc mô-đun User Encoder Hình 3 . 4 Mô hình đề xuất dựa trên BERT",
    "acronyms": [
      [
        99,
        103
      ]
    ],
    "long-forms": [],
    "ID": "209"
  },
  {
    "text": "28.58 31.87 Bảng 2 . 6 : Điểm BLEU của các thí nghiệm sử dụng mô hình Transformer , b là beam size khi thực",
    "acronyms": [
      [
        30,
        34
      ]
    ],
    "long-forms": [],
    "ID": "210"
  },
  {
    "text": "đoán ra các đặc trưng quan trọng nhất . Representation bias Các bias trong MTL được học và chia sẻ chung cho các tác vụ vì thế nó không quá lệch",
    "acronyms": [
      [
        75,
        78
      ]
    ],
    "long-forms": [],
    "ID": "211"
  },
  {
    "text": "dụng Dropout và ALV giúp cho EWC có được sự ổn định hơn trong việc học chuỗi tác vụ liên tục dài . Về tổng thể quá trình , Dropout và ALV đạt hiệu năng ngang",
    "acronyms": [
      [
        16,
        19
      ],
      [
        29,
        32
      ],
      [
        134,
        137
      ]
    ],
    "long-forms": [],
    "ID": "212"
  },
  {
    "text": "0.3 Số subword CNN 2",
    "acronyms": [
      [
        15,
        18
      ]
    ],
    "long-forms": [],
    "ID": "213"
  },
  {
    "text": "đo đánh giá là backward transfer ( BWT ) và độ chính xác trung bình trên tất cả các task ( ACC ) . Backward transfer là một công cụ tốt để đo mức độ ảnh hưởng của việc học task mới",
    "acronyms": [
      [
        35,
        38
      ],
      [
        91,
        94
      ]
    ],
    "long-forms": [
      [
        44,
        88
      ]
    ],
    "ID": "214"
  },
  {
    "text": "Tuy nhiên trong công việc này , LRT sẽ không được sử dụng trên các lớp tích chập mà chỉ sử dụng trên các tầng tuyến tính .",
    "acronyms": [
      [
        32,
        35
      ]
    ],
    "long-forms": [],
    "ID": "215"
  },
  {
    "text": "Số tầng L sẽ được điều chỉnh trong quá trình tinh chỉnh tham số của mô hình như một siêu tham số . Tầng mã hóa ( Embedding Layer )",
    "acronyms": [],
    "long-forms": [],
    "ID": "216"
  },
  {
    "text": "không đổi . Khi đó phân phối hậu nghiệm p ( z , Θ | X ) là một hàm ngẫu nhiên của dữ liệu , và giá trị kỳ vọng của phân phối này được gọi là phân bố hậu nghiệm",
    "acronyms": [],
    "long-forms": [],
    "ID": "217"
  },
  {
    "text": "Mô hình dự đoán của NCF có thể viết thành công thức như sau : y ̂u i = f (P T u , Q T i | P , Q , Θ f )",
    "acronyms": [
      [
        20,
        23
      ]
    ],
    "long-forms": [],
    "ID": "218"
  },
  {
    "text": "Theo cách tiếp cận Bayes , giả sử phân phối của các điểm dữ liệu giữa các tác vụ là độc lập với nhau , 𝑝 ( 𝜃 ) là xác suất tiên nghiệm của tham số θ , xác suất hậu nghiệm sau khi quan sát được dữ liệu từ T tác vụ là :",
    "acronyms": [],
    "long-forms": [],
    "ID": "219"
  },
  {
    "text": "khoảng 3 - 4 % so với SVB - PP và PVB . Trong thử nghiệm này , với bộ dữ liệu được Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        22,
        30
      ],
      [
        34,
        37
      ],
      [
        143,
        154
      ]
    ],
    "long-forms": [],
    "ID": "220"
  },
  {
    "text": "Hình 4 . 2 Bộ dữ liệu CIFAR10 . Nguồn [ 16 ] . CIFAR10 là bộ dữ liệu gồm các ảnh có kích thước 32 × 32 cùng với ba kênh",
    "acronyms": [
      [
        22,
        29
      ],
      [
        47,
        54
      ]
    ],
    "long-forms": [],
    "ID": "221"
  },
  {
    "text": "Độ đo đánh giá cho mô hình đề xuất và các mô hình còn lại là Hit Ratio ( HR ) và Normalized Discounted Cumulative Gain ( NDCG ) . Độ đo HR : Với mỗi người dùng , nếu sản phẩm test xuất hiện trong K sản phẩm",
    "acronyms": [
      [
        73,
        75
      ],
      [
        121,
        125
      ],
      [
        136,
        138
      ]
    ],
    "long-forms": [],
    "ID": "222"
  },
  {
    "text": "Bảng 3 : Kết quả của biểu diễn ẩn tìm được khi cố gắng ngăn chặn các điểm yếu ( Nguồn : [ NF16 ] ) 3.2.5 Một framework đơn giản cho việc tìm biểu diễn ẩn bằng học tương phản",
    "acronyms": [],
    "long-forms": [],
    "ID": "223"
  },
  {
    "text": "tác vụ thứ t sẽ được sử dụng để đánh giá cho tác vụ đó . Học liên tục dựa trên tính không chắc chắn Theo [ 3 ] , trong phương pháp VCL nêu trên , để tính toán được đại lượng khả",
    "acronyms": [
      [
        131,
        134
      ]
    ],
    "long-forms": [],
    "ID": "224"
  },
  {
    "text": "Khi so sánh giữa các mô hình cho thấy mô hình gợi ý đề xuất dựa trên BERT cho kết quả trên cả bốn độ đo cao hơn NAML đến 4 - 6 % , cao hơn LSTUR 2 - 2,5 % , kém mô hình NRMS 0.5 % trên độ đo AUC và MRR , cao hơn 0.5 % trên",
    "acronyms": [
      [
        112,
        116
      ],
      [
        139,
        144
      ],
      [
        169,
        173
      ],
      [
        191,
        194
      ],
      [
        198,
        201
      ]
    ],
    "long-forms": [],
    "ID": "225"
  },
  {
    "text": "λt = λ̃t + λ̃t−1 + ... + λ̃1 + λ̃0 Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT ( 18 )",
    "acronyms": [
      [
        95,
        106
      ]
    ],
    "long-forms": [],
    "ID": "226"
  },
  {
    "text": "Như chúng ta thấy , trong hướng nghiên cứu CL nói chung , cách làm tạo ra các cặp view positive và negative rồi encode về biểu diễn ẩn và cố gắng đưa các biểu diễn ẩn của cặp positive về gần nhau đang gặp phải một vấn đề ,",
    "acronyms": [
      [
        43,
        45
      ]
    ],
    "long-forms": [],
    "ID": "227"
  },
  {
    "text": "Số lượng chủ đề Topics coherence ( NPMI )",
    "acronyms": [
      [
        35,
        39
      ]
    ],
    "long-forms": [],
    "ID": "228"
  },
  {
    "text": "sâu trong kiến trúc của mạng nơ-ron , còn mô hình NMTR tăng về chiều ngang . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        50,
        54
      ],
      [
        140,
        151
      ]
    ],
    "long-forms": [],
    "ID": "229"
  },
  {
    "text": "Các thành phần chính của SSL Từ trong định nghĩa của SSL , ta cũng có thể mường tượng ra , thành phần cơ bản chính của SSL chính là task mà ta định nghĩa , hay còn gọi là một pretext task .",
    "acronyms": [
      [
        25,
        28
      ],
      [
        53,
        56
      ],
      [
        119,
        122
      ]
    ],
    "long-forms": [],
    "ID": "230"
  },
  {
    "text": "Lớp : CNTT 2.03 K59 Hệ đào tạo : Kĩ sư",
    "acronyms": [
      [
        6,
        10
      ]
    ],
    "long-forms": [],
    "ID": "231"
  },
  {
    "text": "• Vec - tơ hóa dữ liệu : Chọn những từ xuất hiện nhiều hơn 5 lần để xây dựng bộ từ vựng và thay những từ không có trong bộ từ vựng bằng token ‘ UNK ’ . • Mô hình HAN :",
    "acronyms": [
      [
        162,
        165
      ]
    ],
    "long-forms": [],
    "ID": "232"
  },
  {
    "text": "Một phần [ X ] ij của đầu vào sẽ tạo ra giá trị lớn với bộ lọc F k nếu chúng tương đồng với nhau . Sau lớp tích chập , chiều không gian d1 × d2 của đầu vào X giảm về",
    "acronyms": [],
    "long-forms": [],
    "ID": "233"
  },
  {
    "text": "Cuối cùng , rất hi vọng bạn đọc sẽ yêu thích những kiến thức mà đồ án mang lại và trong tương lai , mình mong rằng sẽ có nhiều người thích thú và đi theo hướng nghiên cứu SSL để có thể cùng chia sẻ , tìm tòi những kiến thức thú vị về lĩnh này .",
    "acronyms": [
      [
        171,
        174
      ]
    ],
    "long-forms": [],
    "ID": "234"
  },
  {
    "text": "Từ đó ta thu được t giá trị độ chính xác , chỉ số đánh giá sẽ là trung bình của t giá trị này hay độ đo trong công thức PT 4 . 1 . Các phương pháp sẽ được so sánh qua độ chính xác trung bình sau khi đã học",
    "acronyms": [
      [
        120,
        122
      ]
    ],
    "long-forms": [],
    "ID": "235"
  },
  {
    "text": "+ KL ( qφ (θ ) | p (θ ) ) ( 2.14 ) Như vậy tối thiểu hóa KL ( qφ (θ ) | p (θ|D) ) tương đương với tối đa hóa Eqφ ( θ ) [ log p(D|θ ) ] −",
    "acronyms": [],
    "long-forms": [],
    "ID": "236"
  },
  {
    "text": "Ví dụ cả hai mô hình đều dự đoán đúng : Câu 1 : We ' ve been spending a lot of time in Los Angeles talking to TV production people",
    "acronyms": [],
    "long-forms": [],
    "ID": "237"
  },
  {
    "text": "• Lấy X = α − β ∗ log ( − log ( Y ) ) • Trong trường hợp X tuân theo phân phổi Gumbel chuẩn tắc . X = − log ( − log ( Y ) )",
    "acronyms": [],
    "long-forms": [],
    "ID": "238"
  },
  {
    "text": "các lớp trong decoder trước khi tính điểm attention . Cuối cùng , phân phối xác suất của T ứng cử viên cho vị trí thứ đầu ra yj+1 sinh bởi decoder được tính như sau :",
    "acronyms": [],
    "long-forms": [],
    "ID": "239"
  },
  {
    "text": "Y − là tập rỗng hoặc lấy ngẫu nhiên từ tập các cặp ( u , i ) chưa có tương tác rõ ràng . Hàm regular R ( u , i ) được sử dụng là tổng các chuẩn 2 của các vec-tơ tầng",
    "acronyms": [],
    "long-forms": [],
    "ID": "240"
  },
  {
    "text": ". - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03",
    "acronyms": [
      [
        269,
        278
      ]
    ],
    "long-forms": [],
    "ID": "241"
  },
  {
    "text": "kích thước vector embedding dmodel , kích thước lớp ẩn df f của khối FFN , kích thước của dk và dv . Chi tiết về các lựa chọn này được miêu tả trong Bảng 2 . 4 , ngoài ra kích thước của dk",
    "acronyms": [
      [
        69,
        72
      ]
    ],
    "long-forms": [],
    "ID": "242"
  },
  {
    "text": "sử dụng mô hình phân loại , do đó đồ án sẽ tập trung trình bày VCL cho bài toán Học liên tục các tác vụ phân loại . Trong bài toán phân loại , đối với từng tác vụ , dữ liệu được biểu diễn dưới dạng",
    "acronyms": [
      [
        63,
        66
      ]
    ],
    "long-forms": [],
    "ID": "243"
  },
  {
    "text": "còn được gọi là hàm Digamma . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 24",
    "acronyms": [
      [
        90,
        101
      ]
    ],
    "long-forms": [],
    "ID": "244"
  },
  {
    "text": "Bảng 0.4 Tham số tốt nhất cho Split CIFAR10 / 100",
    "acronyms": [
      [
        36,
        49
      ]
    ],
    "long-forms": [],
    "ID": "245"
  },
  {
    "text": "có của người dùng như sở thích dài hạn và ngắn hạn . Hay trong mô hình LSTUR [ 4 ] có đề cập đến sở thích dài hạn của người dùng , tuy nhiên cách tiếp cận dựa",
    "acronyms": [
      [
        71,
        76
      ]
    ],
    "long-forms": [],
    "ID": "246"
  },
  {
    "text": "hình 21 : ACD ca sử dụng đăng ký ứng viên hình 22 : ACD ca sử dụng đăng ký nhà tuyển dụng 43",
    "acronyms": [
      [
        10,
        13
      ],
      [
        52,
        55
      ]
    ],
    "long-forms": [],
    "ID": "247"
  },
  {
    "text": "nào minh chứng cho những điều mà em đã lập luận ở Phần 4 . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 50",
    "acronyms": [
      [
        122,
        133
      ]
    ],
    "long-forms": [],
    "ID": "248"
  },
  {
    "text": "bình để làm giá trị dự đoán cuối cùng . Trong Học liên tục , BNN thường được sử dụng trong các phương pháp tiếp cận theo suy diễn Bayes và xấp xỉ VI , ở đó phân phối xấp xỉ hậu nghiệm 𝑞𝜙 ( 𝜃 ) sẽ",
    "acronyms": [
      [
        61,
        64
      ],
      [
        146,
        148
      ]
    ],
    "long-forms": [],
    "ID": "249"
  },
  {
    "text": "Sinh viên thực hiện : Phan Tuấn Anh - 20150157 • DKL ( P ||Q ) > 0 ∀ P ( X ) , Q ( X ) • DKL ( P ||Q ) = 0 ⇔ P ( x) = Q ( x )",
    "acronyms": [],
    "long-forms": [],
    "ID": "250"
  },
  {
    "text": " E ( X ) = Var ( X ) = λ Hình 4 mô tả phân phối Poisson với các giá trị kỳ vọng khác nhau . Hình 4 : Phân phối Poisson với kỳ vọng 𝜆 khác nhau",
    "acronyms": [],
    "long-forms": [],
    "ID": "251"
  },
  {
    "text": "PT 2.4 5 • Luồng suy diễn biến phân liên tục ( OVI ) : dựa trên suy diễn Bayes , ràng",
    "acronyms": [
      [
        0,
        2
      ],
      [
        47,
        50
      ]
    ],
    "long-forms": [
      [
        11,
        44
      ]
    ],
    "ID": "252"
  },
  {
    "text": "Current Context ( C2 ) : là tất cả các thông tin cung cấp ngữ cảnh cho sự kiện chính , giúp hiểu sự kiện chính trong mối quan",
    "acronyms": [],
    "long-forms": [],
    "ID": "253"
  },
  {
    "text": "32.3 32.8 Bảng 1 : Kết quả của biểu diễn ẩn tìm được ở các không gian màu khác nhau ( Nguồn [ ZIE17 ] )",
    "acronyms": [],
    "long-forms": [],
    "ID": "254"
  },
  {
    "text": "Gauss , và phân tách hoàn toàn : qφ ( E ) = QK",
    "acronyms": [],
    "long-forms": [],
    "ID": "255"
  },
  {
    "text": "repeat t=t +1 ; Hai câu sA và sB tương ứng từ DA và DB ;",
    "acronyms": [],
    "long-forms": [],
    "ID": "256"
  },
  {
    "text": "câu từ 𝑤𝑖𝑡 tới 𝑤𝑖1 : 𝑥𝑖𝑡 = 𝑊𝑒 𝑤𝑖𝑡 , 𝑡 ∈ [ 1, T ] ℎ⃗⃗𝑖𝑡 = ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗",
    "acronyms": [],
    "long-forms": [],
    "ID": "257"
  },
  {
    "text": "Batchsize : Trong thiết lập gốc batch size được để là 512 . Với LCL , do tăng số view lên nên đòi hỏi dung lượng RAM GPU phải lớn hơn , do đó , với số view",
    "acronyms": [
      [
        64,
        67
      ],
      [
        113,
        116
      ],
      [
        117,
        120
      ]
    ],
    "long-forms": [],
    "ID": "258"
  },
  {
    "text": "2 KIẾN THỨC CƠ SỞ Hình 2 : Minh họa gợi ý dùng lọc cộng tác độ quan tâm với các item đã được tương tác bởi cả u và v , gọi là Iuv , tương đồng",
    "acronyms": [],
    "long-forms": [],
    "ID": "259"
  },
  {
    "text": "Factorization – GMF ) với mô hình Multi - layer Perceptron ( MLP ) để tạo thành mô hình Neural Matrix Factorization ( NeuMF ) mới . Mô hình GMF .",
    "acronyms": [
      [
        16,
        18
      ],
      [
        61,
        64
      ],
      [
        118,
        123
      ],
      [
        140,
        143
      ]
    ],
    "long-forms": [],
    "ID": "260"
  },
  {
    "text": "√𝑑 / ℎ ) 𝑉 query Q , key K , và value V là các bộ tham số mô hình , giá trị √𝑑 / ℎ được sử",
    "acronyms": [],
    "long-forms": [],
    "ID": "261"
  },
  {
    "text": "được nhân bản ra thành ma trận 𝛼𝑓𝑢𝑙𝑙 có kích thước 𝑀 × 𝐷 , tương ứng với việc lấy mẫu riêng biệt 𝑀 lần cho từng điểm dữ liệu . ( 𝑙 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "262"
  },
  {
    "text": "log 𝑝 ( 𝑦𝑡𝑚 | 𝑠𝑡 , 𝜃 , 𝑥𝑡𝑚 ) là kỳ vọng của lô-ga-rít khả năng xảy ra của điểm dữ liệu 𝑚 , ta có thể biểu diễn 𝐿 ( 𝑚) = 𝐸𝛾𝑚 , 𝜀𝑚 log 𝑝 ( 𝑦𝑡𝑚 | 𝛼𝑡 , 𝜇𝑡 , 𝜎𝑡 , 𝛾 𝑚 , 𝜀 𝑚 , 𝑥𝑡𝑚 ) . Sau",
    "acronyms": [],
    "long-forms": [],
    "ID": "263"
  },
  {
    "text": "Hình 27 : Kết quả các mô hình ITE ngày 18 / 4 ( a ) HR @ 10",
    "acronyms": [
      [
        30,
        33
      ],
      [
        52,
        54
      ]
    ],
    "long-forms": [],
    "ID": "264"
  },
  {
    "text": "mạch đơn nucleotide . Ở sinh vật nhân sơ ( ví dụ vi khuẩn ) , chuỗi RNA sau khi hình thành có thể đem đi tổng hợp protein được ngay , trong khi ở sinh vật nhân chuẩn ( như động ,",
    "acronyms": [
      [
        68,
        71
      ]
    ],
    "long-forms": [],
    "ID": "265"
  },
  {
    "text": "Độ đo MRR được tính như sau [ 18 ] : 𝑄 1",
    "acronyms": [
      [
        6,
        9
      ]
    ],
    "long-forms": [],
    "ID": "266"
  },
  {
    "text": "PT 3.1 Trong đó db ( database ) là 1 câu đã được chọn , gồm danh sách các từ , query là câu đang được xem xét , 𝑙𝑒𝑛 ( 𝑋 ) trả về số lượng phần tử trong X .",
    "acronyms": [
      [
        16,
        18
      ],
      [
        0,
        2
      ]
    ],
    "long-forms": [],
    "ID": "267"
  },
  {
    "text": "Tóm lại , có thể thấy được mô hình học biểu diễn đề xuất cho kết quả khả quan và có thể áp dụng vào các bài toán khác . Khi đánh giá mô hình gợi ý đề xuất dựa trên ý tưởng của mô hình BERT , do",
    "acronyms": [
      [
        184,
        188
      ]
    ],
    "long-forms": [],
    "ID": "268"
  },
  {
    "text": "Lớp : CNTT 2.03 Email : hieu.lm161522@sis.hust.edu.vn Hệ đào tạo : Kỹ sư chính quy",
    "acronyms": [
      [
        6,
        10
      ]
    ],
    "long-forms": [],
    "ID": "269"
  },
  {
    "text": "Một tư tưởng ban đầu của tác giả đó là việc thiết kế lên một mạng CNN , nhận đầu vào là 9 patch của ảnh xếp chồng các kênh màu lên nhau . ( ví dụ ảnh có 3",
    "acronyms": [
      [
        66,
        69
      ]
    ],
    "long-forms": [],
    "ID": "270"
  },
  {
    "text": "Chú ý rằng không gian thuộc tính LDA của người dùng và banner không nhất thiết phải dùng chung . Như vậy , người dùng",
    "acronyms": [
      [
        33,
        36
      ]
    ],
    "long-forms": [],
    "ID": "271"
  },
  {
    "text": "Các yêu cầu ( AJAX ) sẽ được đưa đến Application . Dựa vào cài đặt các yêu cầu này mà",
    "acronyms": [
      [
        14,
        18
      ]
    ],
    "long-forms": [],
    "ID": "272"
  },
  {
    "text": "ta sẽ sử dụng mạng nơ-ron đơn giản là Multi-layer perceptron ( MLP ) . Ví dụ như ta sử dụng MLP với đầu ra tuân theo phân phối Gaussian cho bộ mã",
    "acronyms": [
      [
        63,
        66
      ],
      [
        92,
        95
      ]
    ],
    "long-forms": [],
    "ID": "273"
  },
  {
    "text": "Độ đo AUC được sử dụng để tỷ lệ , sự tương quan giữa TPR ( True Positive Rate ) và FPR ( False Positive Rate ) khi thay đổi số các item gợi ý bằng cách đo tỷ lệ diện tích phần dưới của biểu đồ ROC [ 18 ] :",
    "acronyms": [
      [
        6,
        9
      ],
      [
        53,
        56
      ],
      [
        83,
        86
      ],
      [
        193,
        196
      ]
    ],
    "long-forms": [],
    "ID": "274"
  },
  {
    "text": "biểu thị cho ma trận ratings với rui biểu thị cho rating của người dùng u dành cho bộ phim i. V ∈ RM ×N , W ∈ RM ×N tương ứng mà hai ma trận dữ liệu hành",
    "acronyms": [],
    "long-forms": [],
    "ID": "275"
  },
  {
    "text": "Hình vẽ 2 sau đây thể hiện một cách cụ thể và rõ ràng hơn về các thành phần chính của SSL Đồ án tốt nghiệp",
    "acronyms": [
      [
        86,
        89
      ]
    ],
    "long-forms": [],
    "ID": "276"
  },
  {
    "text": "diễn cuối cùng của nội dung bài báo được tính bằng tổng trọng số của các từ trong bài báo : 𝑃",
    "acronyms": [],
    "long-forms": [],
    "ID": "277"
  },
  {
    "text": "Tác v 8 Permuted MNIST",
    "acronyms": [
      [
        17,
        22
      ]
    ],
    "long-forms": [],
    "ID": "278"
  },
  {
    "text": "Tôi – Vũ Hồng Phúc cam đoan những nội dung trong đồ án này là của tôi dưới sự hướng dẫn của PGS. TS. Thân Quang Khoát . Các kết quả nêu trong đồ án là trung thực , là thành",
    "acronyms": [
      [
        92,
        96
      ],
      [
        97,
        100
      ]
    ],
    "long-forms": [],
    "ID": "279"
  },
  {
    "text": "Autoencoding Variational Bayes ( AEVB ) là một sự lựa chọn đặc biệt tự nhiên cho mô hình chủ đề , bởi nó học một mạng suy diễn , một mạng nơ ron mà ánh xạ trực tiếp một văn bản tới một phân phối hậu nghiệm xấp xỉ .",
    "acronyms": [
      [
        33,
        37
      ]
    ],
    "long-forms": [],
    "ID": "280"
  },
  {
    "text": "Trong quá trình huấn luyện , ta cần cực tiểu hóa hàm mất mát : L = - log ( Pr[r]) – log ( Pc[c]) ( 6 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "281"
  },
  {
    "text": "Hình 2.1 Mô hình NR MS ....................................................................................... 7",
    "acronyms": [
      [
        17,
        19
      ],
      [
        20,
        22
      ]
    ],
    "long-forms": [],
    "ID": "282"
  },
  {
    "text": "độ chính xác khi tính toán . t Công thức 1. 17 là quan trọng nhất đối với LSTM bởi vì nó đảm bảo giá trị đạo hàm dcdct−1",
    "acronyms": [
      [
        74,
        78
      ]
    ],
    "long-forms": [],
    "ID": "283"
  },
  {
    "text": "áp dụng ràng buộc L1 dạng chuẩn hóa và có trọng số lên các véc - tơ chú ý At . ≤t−1 t",
    "acronyms": [],
    "long-forms": [],
    "ID": "284"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 32",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "285"
  },
  {
    "text": "11 2 . CƠ SỞ LÝ THUYẾT",
    "acronyms": [],
    "long-forms": [],
    "ID": "286"
  },
  {
    "text": "một phân phối mới , gọi là population distribution Fα , với α là số điểm dữ liệu được lấy mẫu . Ở phần này chúng ta sẽ trình bày chi tiết các phương pháp trên , bao gồm :",
    "acronyms": [],
    "long-forms": [],
    "ID": "287"
  },
  {
    "text": "thuộc lớp a True positive ( TP )",
    "acronyms": [
      [
        28,
        30
      ]
    ],
    "long-forms": [],
    "ID": "288"
  },
  {
    "text": "cũng có thể viết dưới dạng là một biến ngẫu nhiên tuân theo phân phối Gauss : 𝑞𝜙 ( 𝑏𝑚ℎ | 𝐴 ) = 𝑁 ( 𝛾𝑚ℎ , 𝛿𝑚ℎ ) 𝐷",
    "acronyms": [],
    "long-forms": [],
    "ID": "289"
  },
  {
    "text": "Với mỗi doc d , hàm mất mát được tính như sau : 𝐿 = ∑ ( 𝛾̅𝑑𝑣 − 𝛾𝑑𝑣 ) 2",
    "acronyms": [],
    "long-forms": [],
    "ID": "290"
  },
  {
    "text": "Với mỗi ma trận tương tác , ta thiết kế một mô hình thuộc tính ẩn với hướng Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 25",
    "acronyms": [
      [
        139,
        150
      ]
    ],
    "long-forms": [],
    "ID": "291"
  },
  {
    "text": "Ahn và các cộng sự [ 3 ] cho biết rằng nó được bắt nguồn từ hai mô hình biến thể của mô hình Autoencoder là VAE và DAE , mô hình Denoising Variational Autoencoder ( DVAE ) [ 3 ] khác với mô hình VAE ở việc thêm nhiễu trước khi tiến hành đưa vào",
    "acronyms": [
      [
        108,
        111
      ],
      [
        115,
        118
      ],
      [
        165,
        169
      ],
      [
        195,
        198
      ]
    ],
    "long-forms": [],
    "ID": "292"
  },
  {
    "text": "Trong đó A là ma trận đầu vào có kích thước 𝑀 × 𝐷 , B là ma trận preactivation có kích thước 𝑀 × 𝐻 , 𝜃 là ma trận trọng số của mạng kích thước 𝐷 × 𝐻 và 𝜉 là ma trận nhiễu cùng kích thước với đầu vào A và được nhân vào đầu vào đóng vai trò là thành phần Dropout .",
    "acronyms": [],
    "long-forms": [],
    "ID": "293"
  },
  {
    "text": ". . . . 51 Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        74,
        85
      ]
    ],
    "long-forms": [],
    "ID": "294"
  },
  {
    "text": "AI - Trí tuệ nhân tạo ( Artificial Intelligence ) là các kỹ thuật giúp cho máy tính thực hiện được những công việc của con người chúng ta . Những năm gần đây , AI là lĩnh vực đang nổi",
    "acronyms": [
      [
        0,
        2
      ],
      [
        160,
        162
      ]
    ],
    "long-forms": [
      [
        5,
        21
      ]
    ],
    "ID": "295"
  },
  {
    "text": "phương giữa giá trị dự đoán và giá trị thực tế như sau : X Lsqr =",
    "acronyms": [],
    "long-forms": [],
    "ID": "296"
  },
  {
    "text": "( 9 ) j=1 Kí hiệu Γ , Ψ theo thứ tự là hàm Gamma và đạo hàm logarit của hàm Gamma ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "297"
  },
  {
    "text": "3 . 3 ALV cho các phương pháp tiếp cận dựa trên ràng buộc trọng số Như đã được giới thiệu , ALV có khả năng áp dụng cho các phương pháp dựa trên hướng tiếp cận ràng buộc trọng số , trong phần này sẽ trình bày chi tiết việc áp",
    "acronyms": [
      [
        6,
        9
      ],
      [
        92,
        95
      ]
    ],
    "long-forms": [],
    "ID": "298"
  },
  {
    "text": "~ N ( 0, I1I K ) ( * * ) Hình dưới mô tả phân rã ma trận Gauss với ràng buộc biến :",
    "acronyms": [],
    "long-forms": [],
    "ID": "299"
  },
  {
    "text": "trên được tính theo công thức sau ( do đã nói từ trước , đồ án này tập trung vào SSL mà không tập trung quá nhiều vào các chứng minh toán học của thuật toán LLE , bạn đọc quan tâm có thể tham khảo bài báo gốc được trích dẫn ở",
    "acronyms": [
      [
        81,
        84
      ],
      [
        157,
        160
      ]
    ],
    "long-forms": [],
    "ID": "300"
  },
  {
    "text": "được gọi là độ đo khoảng cách Kull - back – Leibler giữa hai phân phối q và p . Mặt khác độ đo KL là không âm , tức",
    "acronyms": [],
    "long-forms": [],
    "ID": "301"
  },
  {
    "text": "Mô hình máy dịch dựa trên mạng nơ-ron sử dụng RNN Do mô hình dịch máy sử dụng RNN đã được thử nghiệm nhiều trên các bộ ngôn ngữ khác nhau [ 48 , 51 , 55 ] và đã không còn là mô hình đem lại kết quả cao nhất cho các bộ ngôn ngữ",
    "acronyms": [
      [
        46,
        49
      ],
      [
        78,
        81
      ]
    ],
    "long-forms": [],
    "ID": "302"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 48",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "303"
  },
  {
    "text": "Context-informing Content : cung cấp Previous Event ( C1 ) : là các sự kiện thực thông tin liên quan đến hoàn cảnh thực tế xảy ra ngay trước sự kiện chính , có thể tế mà trong đó sự kiện chính xảy ra .",
    "acronyms": [],
    "long-forms": [],
    "ID": "304"
  },
  {
    "text": "Vì vậy trong công việc này tôi cũng sử dụng 𝛼 ( 𝑙 ) là một véc - tơ 𝐷 chiều . Khi đó số lượng tham số gốc của mô hình",
    "acronyms": [],
    "long-forms": [],
    "ID": "305"
  },
  {
    "text": "Trong quá trình huấn luyện , CNN tự động học các giá trị filter . Ví dụ , trong phân lớp ảnh ở hình 2 , CNN sẽ cố gắng tìm ra các tham số tối ưu cho các filter tương ứng theo",
    "acronyms": [
      [
        29,
        32
      ],
      [
        104,
        107
      ]
    ],
    "long-forms": [],
    "ID": "306"
  },
  {
    "text": "K-medoids HAC Không sử",
    "acronyms": [
      [
        10,
        13
      ]
    ],
    "long-forms": [],
    "ID": "307"
  },
  {
    "text": "── ── ── ── * ─ ── ── ── ĐỒ ÁN TỐT NGHIỆP ĐẠI HỌC",
    "acronyms": [],
    "long-forms": [],
    "ID": "308"
  },
  {
    "text": "Attention Network ( HAN ) . Mục đích thiết kế của nó là để : -",
    "acronyms": [
      [
        20,
        23
      ]
    ],
    "long-forms": [],
    "ID": "309"
  },
  {
    "text": "Hồi quy Logistic Hồi quy Ridge K-means + GL",
    "acronyms": [
      [
        41,
        43
      ]
    ],
    "long-forms": [],
    "ID": "310"
  },
  {
    "text": "38.60 Bảng 5 : Kết quả thử nghiệm ( F1 ) cho binary classification Bảng kết quả 5 cho thấy bài nghiên cứu của em đạt kết quả tốt hơn tất cả các bài",
    "acronyms": [],
    "long-forms": [],
    "ID": "311"
  },
  {
    "text": "Ở phần này , chúng tôi xin chỉ tập trung giới thiệu về multilayer perceptron không áp dụng chuẩn hóa . Một mạng nơ-ron lan truyền tiến bao gồm có 3 loại lớp nơ-ron",
    "acronyms": [],
    "long-forms": [],
    "ID": "312"
  },
  {
    "text": "phương sai lớn , mà số lượng lấy mẫu ( 𝑀 ∗ 𝐻 mẫu ) cũng giảm bớt so với kĩ thuật đổi biến ( 𝑀 ∗ 𝐷 ∗ 𝐻 mẫu ) do số chiều của B ít hơn nhiều so với 𝜃 . Ta có :",
    "acronyms": [],
    "long-forms": [],
    "ID": "313"
  },
  {
    "text": "( FN ) Dự đoán không thuộc lớp a",
    "acronyms": [
      [
        2,
        4
      ]
    ],
    "long-forms": [],
    "ID": "314"
  },
  {
    "text": "Trong mô hình ITE - 3 , em đề xuất thêm vào thông tin về zone . Trong hệ thống quảng cáo trực tuyến , zone đại diện cho",
    "acronyms": [
      [
        14,
        21
      ]
    ],
    "long-forms": [],
    "ID": "315"
  },
  {
    "text": "Thuật toán học cho LDA sử dụng suy diễn biến phân cụ thể như sau : Thuật toán 1 Thuật toán học LDA sử dụng suy diễn biến phân Đầu vào : Tập gồm D văn bản , các tham số tiên nghiệm η , α",
    "acronyms": [
      [
        19,
        22
      ],
      [
        95,
        98
      ]
    ],
    "long-forms": [],
    "ID": "316"
  },
  {
    "text": "với nhau . Trong trường hợp chỉ có phân phối dữ liệu đầu vào p( 𝑥1: 𝑇 ) thay đổi theo",
    "acronyms": [],
    "long-forms": [],
    "ID": "317"
  },
  {
    "text": "phương pháp trong SSL thực sự quan tâm . Do đó , các đánh giá của các thí nghiệm được trình bày trong đồ án này cũng chỉ so sánh về độ mạnh yếu của biểu diễn ẩn",
    "acronyms": [
      [
        18,
        21
      ]
    ],
    "long-forms": [],
    "ID": "318"
  },
  {
    "text": "φ φ λt , φt trong HPP như với một mô hình \" conjugate exponential \" thông",
    "acronyms": [
      [
        18,
        21
      ]
    ],
    "long-forms": [],
    "ID": "319"
  },
  {
    "text": "vecto λ và λ0 là một độ đo không hiệu quả để biểu diễn tính tương đồng của hai phân phối q ( Θ | λ ) và q ( Θ | λ0 ) . Trong khi đó , natural gradient chỉ hướng trong",
    "acronyms": [],
    "long-forms": [],
    "ID": "320"
  },
  {
    "text": "ta xây dựng một hàm mất mát dựa trên những giá trị đã biết trong ma trận R N 1 X X",
    "acronyms": [],
    "long-forms": [],
    "ID": "321"
  },
  {
    "text": "Bài toán có thể phát biểu lại như sau : Input : Dữ liệu tương tác user - item của hành vi mục tiêu YR , và dữ liệu tương tác của các loại hành vi khác { Y1 , Y2 , . . . , YR−1 }",
    "acronyms": [],
    "long-forms": [],
    "ID": "322"
  },
  {
    "text": "Mà đó là một phân phối không có công thức tường minh nên được xấp xỉ bằng phân phối Gauss có kỳ vọng là tham số tối ưu của T1 và ma trận hiệp phương sai chéo ( tức các tham số độc lập với nhau ) :",
    "acronyms": [],
    "long-forms": [],
    "ID": "323"
  },
  {
    "text": "Trong đó , 𝑤 [ 𝑖−𝑀: 𝑖+𝑀 ] là tổng hợp của embedding từ thứ i-M đến từ thứ i+M , C và b là các tham số của mạng CNN và M là kích thước cửa sổ trượt trong mạng CNN .",
    "acronyms": [
      [
        111,
        114
      ],
      [
        158,
        161
      ]
    ],
    "long-forms": [],
    "ID": "324"
  },
  {
    "text": "2 CHƯƠNG 2 . CÁC NGHIÊN CỨU LIÊN QUAN VÀ CƠ SỞ LÝ THUYẾT",
    "acronyms": [],
    "long-forms": [],
    "ID": "325"
  },
  {
    "text": "Tất cả những tham khảo trong ĐATN , bao gồm hình ảnh , bảng biểu , câu trích dẫn đều được ghi rõ ràng nguồn gốc trong danh mục tham khảo .",
    "acronyms": [
      [
        29,
        33
      ]
    ],
    "long-forms": [],
    "ID": "326"
  },
  {
    "text": "Minh họa lời giải tạo bởi Lasso , GL và SGL … . … … … … … … … … … … ....",
    "acronyms": [
      [
        34,
        36
      ],
      [
        40,
        43
      ]
    ],
    "long-forms": [],
    "ID": "327"
  },
  {
    "text": "ĐỒ ÁN TỐT NGHIỆP Kết hợp phân nhóm gene đồng biểu hiện và một số biến thể của Lasso trong bài toán",
    "acronyms": [],
    "long-forms": [],
    "ID": "328"
  },
  {
    "text": "• Công thức 1.13 thể hiện quá trình cập nhật thông tin trong cổng update . Tương tự như RNN chuẩn , quá trình cập nhật nhận véc - tơ đầu vào xt và trạng thái ẩn trước đó ht−1 ,",
    "acronyms": [
      [
        88,
        91
      ]
    ],
    "long-forms": [],
    "ID": "329"
  },
  {
    "text": "mức độ thưa của lời giải của các mô hình Lasso , Group Lasso và Sparse Group Lasso . Hình 5 . Minh họa lời giải tạo bởi Lasso , GL và SGL",
    "acronyms": [
      [
        128,
        130
      ],
      [
        134,
        137
      ]
    ],
    "long-forms": [],
    "ID": "330"
  },
  {
    "text": "Nguồn [ 62 ] . Có hai cách xây dựng dendrogram là tổng hợp ( hay tích tụ - Hierarchical Agglomerative Clustering – HAC) và phân chia ( Hierarchical Divisive Clustering –",
    "acronyms": [
      [
        115,
        118
      ]
    ],
    "long-forms": [],
    "ID": "331"
  },
  {
    "text": "nhiên ALV vẫn vượt trội hơn với kết quả độ chính xác trung bình đạt được là 92.96 % và cao hơn Dropout 0.14 % . Hình 4 . 6 Kết quả thử nghiệm PMNIST trên EWC",
    "acronyms": [
      [
        6,
        9
      ],
      [
        142,
        148
      ],
      [
        154,
        157
      ]
    ],
    "long-forms": [],
    "ID": "332"
  },
  {
    "text": "Mô hình sử dụng hàm kích hoạt và h như trên là mô hình Generalized Matrix Factorization ( GMF ) . Multi - Layer Perceptron",
    "acronyms": [
      [
        90,
        93
      ]
    ],
    "long-forms": [],
    "ID": "333"
  },
  {
    "text": "𝑏𝑚 = 𝑎𝑚 𝑊 với 𝑤𝑑 = 𝑠𝑑 𝜃𝑑 𝑣à 𝑠𝑑 ~ 𝑁 ( 1, 𝛼 ) PT 3.2",
    "acronyms": [
      [
        44,
        46
      ]
    ],
    "long-forms": [],
    "ID": "334"
  },
  {
    "text": "0.20 Bảng 4 . 4 và bảng 4 . 5 biểu thị các giá trị perplexity và độ gắn kết các chủ đề ( NPMI ) của 2 mô hình ProdLDA VAE và Online LDA với từng giá trị k ( số lượng topics )",
    "acronyms": [
      [
        89,
        93
      ],
      [
        118,
        121
      ],
      [
        132,
        135
      ],
      [
        110,
        117
      ]
    ],
    "long-forms": [],
    "ID": "335"
  },
  {
    "text": "46 5 THỬ NGHIỆM VÀ ĐÁNH GIÁ",
    "acronyms": [],
    "long-forms": [],
    "ID": "336"
  },
  {
    "text": "13 Main Content : mô tả nội dung chính của Main Event ( M1 ) : Là sự kiện quan trọng",
    "acronyms": [],
    "long-forms": [],
    "ID": "337"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT ( 47 ) 47",
    "acronyms": [
      [
        60,
        71
      ]
    ],
    "long-forms": [],
    "ID": "338"
  },
  {
    "text": "Trong khi VB cổ điển xấp xỉ phân phối hậu nghiệm bằng cách tối ưu hoá hàm lower bound của hàm log-complete data , còn gọi là ELBO như đề cập ở phần lý thuyết suy diễn biến phân :",
    "acronyms": [
      [
        10,
        12
      ],
      [
        125,
        129
      ]
    ],
    "long-forms": [],
    "ID": "339"
  },
  {
    "text": "tập từ điển các character ta biểu diễn mỗi character thành một vector k chiều được khởi - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03",
    "acronyms": [
      [
        355,
        364
      ]
    ],
    "long-forms": [],
    "ID": "340"
  },
  {
    "text": "( 𝑙 ) ⨀ là phép nhân Hadamard trên ma trận , 𝑠𝑡 ở đây là một ma trận kích thước 𝑀 × 𝐷 ( 𝑙 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "341"
  },
  {
    "text": "Ví dụ , đầu vào mạng CNN là một ảnh kích thước 28x28 tương ứng một ma trận có 28x28 giá trị và mỗi giá trị mỗi điểm ảnh là một ô trong ma trận . Trong một mạng ANN",
    "acronyms": [
      [
        21,
        24
      ],
      [
        160,
        163
      ]
    ],
    "long-forms": [],
    "ID": "342"
  },
  {
    "text": "Thí nghiệm thứ 2 mà đồ án tốt nghiệp này triển khai liên quan đến tham số temperature . Đây là một tham số điều chỉnh trong hàm mất mát , bằng cách thay đổi tham số này trong cả mô hình SimCLR gốc và các mô hình LCL , thì nghiệm muốn",
    "acronyms": [
      [
        186,
        192
      ],
      [
        212,
        215
      ]
    ],
    "long-forms": [],
    "ID": "343"
  },
  {
    "text": "Mạng tập trung phân cấp Natural Language Processing NLP",
    "acronyms": [
      [
        52,
        55
      ]
    ],
    "long-forms": [],
    "ID": "344"
  },
  {
    "text": "của một một chuỗi , có đầu ra phụ thuộc vào những tính toán trước đó . Hình 2 là một mạng nơ-ron hồi quy RNN cơ bản . 5",
    "acronyms": [
      [
        105,
        108
      ]
    ],
    "long-forms": [
      [
        85,
        104
      ]
    ],
    "ID": "345"
  },
  {
    "text": "xỉ . Tồn tại một lớp rộng các thuật toán suy diễn xấp xỉ có thể sử dụng cho mô hình LDA , bao gồm xấp xỉ Laplace , suy diễn biến phân và lấy mẫu Makov chain",
    "acronyms": [
      [
        84,
        87
      ]
    ],
    "long-forms": [],
    "ID": "346"
  },
  {
    "text": "TÀI LIỆU THAM KHẢO ............................................................................................ 43 PHỤ LỤC ..................................................................................................................... 46",
    "acronyms": [],
    "long-forms": [],
    "ID": "347"
  },
  {
    "text": "được xét thêm . Kết quả ở hình 5 . 8 một lần nữa khẳng định VBD - CL tốt hơn HAT",
    "acronyms": [
      [
        60,
        68
      ],
      [
        77,
        80
      ]
    ],
    "long-forms": [],
    "ID": "348"
  },
  {
    "text": "( 29 ) Điểm F1 là trung bình điều hòa của precision và recall : F1 =",
    "acronyms": [],
    "long-forms": [],
    "ID": "349"
  },
  {
    "text": "20 - Newsgroups ( 20NG ) , Ohsumed , R52 và R8 của Reuters , Movie Review ( MR ) :",
    "acronyms": [
      [
        76,
        78
      ]
    ],
    "long-forms": [],
    "ID": "350"
  },
  {
    "text": "được gắn trên đó , gọi là đoạn mồi . Sau đó các đoạn mRNA của đối tượng cần quan tâm sẽ được lấy ra , sử dụng enzyme phiên mã ngược để tái tạo lại các mạch DNA bổ sung",
    "acronyms": [
      [
        53,
        57
      ],
      [
        156,
        159
      ]
    ],
    "long-forms": [],
    "ID": "351"
  },
  {
    "text": "dùng với thuộc tính đó cao . Nếu ta gọi R ∈ RM ×N là ma trận lịch sử tương tác , Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        144,
        155
      ]
    ],
    "long-forms": [],
    "ID": "352"
  },
  {
    "text": "𝑖=1 PT 4.1 trong đó : R j , i là độ chính xác đánh giá trên tác vụ i sau khi",
    "acronyms": [
      [
        4,
        6
      ]
    ],
    "long-forms": [],
    "ID": "353"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT . .",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "354"
  },
  {
    "text": "trong đó ℽ ∈ ℝ và w ∈ ℝ2 là các tham số được cập nhật trong quá trình huấn luyện , dc ’ là chiều của vector đầu ra của ELMo . Cuối cùng , ta đưa hi qua một mạng feed forward",
    "acronyms": [
      [
        119,
        123
      ]
    ],
    "long-forms": [],
    "ID": "355"
  },
  {
    "text": "Variational Inference ( VI ) là một phương pháp suy diễn trong học máy cho phép ta xấp xỉ các hàm mật độ xác suất mà không dễ để tính toán . VI được sử dụng",
    "acronyms": [
      [
        24,
        26
      ],
      [
        141,
        143
      ]
    ],
    "long-forms": [],
    "ID": "356"
  },
  {
    "text": "Factorization ( DMF ) . Với đầu vào là một ma trận xếp hạng rõ ràng không có đặc trưng tiềm ẩn , kiến trúc là tìm hiểu một không gian chiều thấp , tiềm ẩn phổ biến",
    "acronyms": [
      [
        16,
        19
      ]
    ],
    "long-forms": [],
    "ID": "357"
  },
  {
    "text": "dùng và item ; k là số chiều của không gian thuộc tính ẩn . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 18",
    "acronyms": [
      [
        123,
        134
      ]
    ],
    "long-forms": [],
    "ID": "358"
  },
  {
    "text": "∇ ΘAB log P ( s|smid , k ; ΘAB ) ] K k =1 Cập nhật tham số :",
    "acronyms": [],
    "long-forms": [],
    "ID": "359"
  },
  {
    "text": "24 3 BÀI TOÁN HỆ GỢI Ý VỚI DỮ LIỆU HÀNH VI TIỀM ẨN VÀ RÕ",
    "acronyms": [],
    "long-forms": [],
    "ID": "360"
  },
  {
    "text": "Email : phuc.vh173305@sis.hust.edu.vn Lớp : KHMT. 01 – K62 . Hệ đào tạo : Đại học chính quy",
    "acronyms": [
      [
        44,
        48
      ]
    ],
    "long-forms": [],
    "ID": "361"
  },
  {
    "text": "trong Hình 16 . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 37",
    "acronyms": [
      [
        79,
        90
      ]
    ],
    "long-forms": [],
    "ID": "362"
  },
  {
    "text": "Giảng viên hướng dẫn PGS. TS. Thân Quang Khoát 2",
    "acronyms": [
      [
        21,
        25
      ],
      [
        26,
        29
      ]
    ],
    "long-forms": [],
    "ID": "363"
  },
  {
    "text": "CHƯƠNG 6 . KẾT LUẬN 6.1",
    "acronyms": [],
    "long-forms": [],
    "ID": "364"
  },
  {
    "text": "tri thức tiên nghiệm nhúng của từ - Poisson Matrix Factorization using Word Embedding Prior ( PFEP ) . Tương tự với mô hình CTMP , ý tưởng xây dựng mô hình này trên cơ sở",
    "acronyms": [
      [
        94,
        98
      ],
      [
        124,
        128
      ]
    ],
    "long-forms": [],
    "ID": "365"
  },
  {
    "text": "Do đây là phương pháp cuối cùng trong tổ hợp các bài trình bày hướng nghiên cứu SSL , nên bảng 6 sau đây chính là kết quả của việc huấn luyện lớp giữ nguyên các tham số của mạng encoder học được từ",
    "acronyms": [
      [
        80,
        83
      ]
    ],
    "long-forms": [],
    "ID": "366"
  },
  {
    "text": "30 4 CÁC MÔ HÌNH ĐỀ XUẤT",
    "acronyms": [],
    "long-forms": [],
    "ID": "367"
  },
  {
    "text": "PGS. TS. Thân Quang Khoát - - - - - - - - - - - - - - - Chữ ký GVHD",
    "acronyms": [
      [
        63,
        67
      ],
      [
        0,
        4
      ],
      [
        5,
        8
      ]
    ],
    "long-forms": [],
    "ID": "368"
  },
  {
    "text": "tập con khác nhau . 1 . 9 Manifold Learning và thuật toán Local Linear Embedding ( LLE ) Locally Linear Embedding ( LLE ) là một thuật toán giảm chiều dữ liệu phi tuyến",
    "acronyms": [
      [
        83,
        86
      ],
      [
        116,
        119
      ]
    ],
    "long-forms": [],
    "ID": "369"
  },
  {
    "text": "Chúng ta gọi đây là vấn đề vanishing variance . Điều này đồng nghĩa với việc mô Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        140,
        151
      ]
    ],
    "long-forms": [],
    "ID": "370"
  },
  {
    "text": "4 . 2 . 3 Kết quả cài đặt và so sánh với các nghiên cứu trước So sánh với các kết quả nghiên cứu gần đây cho thấy , kết quả trong bài nghiên cứu của em đạt kết quả tốt nhất trên cả PDTB - Lin và PDTB - Ji đối với 11 - way classification .",
    "acronyms": [
      [
        181,
        185
      ],
      [
        195,
        199
      ]
    ],
    "long-forms": [],
    "ID": "371"
  },
  {
    "text": "Đại lượng KL này chính là đại lượng ràng buộc trong hướng tiếp cận dựa trên ràng buộc trọng số đã trình bày ở trên , với mục đích kiểm soát tính ổn định của các trọng số quan trọng và tính",
    "acronyms": [
      [
        10,
        12
      ]
    ],
    "long-forms": [],
    "ID": "372"
  },
  {
    "text": "4 CÁC MÔ HÌNH ĐỀ XUẤT Hình 12 : Mô hình ITE - user _ item _ pcat",
    "acronyms": [
      [
        40,
        64
      ]
    ],
    "long-forms": [],
    "ID": "373"
  },
  {
    "text": "Khi đó chúng ta ước lượng biến ẩn β chính bằng kỳ vọng của phân phối Dirichlet với tham số λ trên , tức là : P",
    "acronyms": [],
    "long-forms": [],
    "ID": "374"
  },
  {
    "text": "4 CÁC MÔ HÌNH ĐỀ XUẤT zone 1 . Kiến trúc của mô hình ITE - 3 ( thêm đầu vào zone so với ITE - 2 ) được",
    "acronyms": [
      [
        88,
        95
      ],
      [
        53,
        60
      ]
    ],
    "long-forms": [],
    "ID": "375"
  },
  {
    "text": "Bảng 5 : Kết quả thử nghiệm ( F1 ) cho binary classification … … … … … … … … … … .. 36",
    "acronyms": [],
    "long-forms": [],
    "ID": "376"
  },
  {
    "text": "đặc trưng và dự đoán bệnh Alzheimer . Xie và cộng sự ( 2018 ) [ 38 ] sử dụng ANN cùng",
    "acronyms": [
      [
        77,
        80
      ]
    ],
    "long-forms": [],
    "ID": "377"
  },
  {
    "text": "sử dụng để vec-tơ hóa biểu diễn của các item khi nội dung các item ở dạng văn bản . Trong khuôn khổ đồ án , em sẽ không đi vào chi tiết mô hình LDA ,",
    "acronyms": [
      [
        144,
        147
      ]
    ],
    "long-forms": [],
    "ID": "378"
  },
  {
    "text": "pháp đi theo hướng tiếp cận Bayesian là VCL và UCB . 3.2 Variational continual learning ( VCL )",
    "acronyms": [
      [
        40,
        43
      ],
      [
        47,
        50
      ],
      [
        90,
        93
      ]
    ],
    "long-forms": [],
    "ID": "379"
  },
  {
    "text": "( các độ đo AUC , MRR , nDCG@ 5 và nDCG@ 10 sẽ được trình bày cụ thể ở chương 4 ) : 9",
    "acronyms": [
      [
        12,
        15
      ],
      [
        18,
        21
      ],
      [
        24,
        28
      ],
      [
        35,
        39
      ]
    ],
    "long-forms": [],
    "ID": "380"
  },
  {
    "text": "= L ( q ) + KL ( q|| p ) ( 2 ) trong đó , chúng ta đang giả định tổng quát rằng các biến ẩn Z là biến ngẫu",
    "acronyms": [],
    "long-forms": [],
    "ID": "381"
  },
  {
    "text": "V = M ( Arg1 , Arg2 ) trong đó M là mô hình encoder chứa toàn bộ ba module đã mô tả trên Arg1 , Arg2 là hai câu đầu vào ban đầu",
    "acronyms": [],
    "long-forms": [],
    "ID": "382"
  },
  {
    "text": "hình 24 : ACD ca sử dụng sửa thông tin cá nhân 44",
    "acronyms": [
      [
        10,
        13
      ]
    ],
    "long-forms": [],
    "ID": "383"
  },
  {
    "text": "2.3 . 1 Đồ thị 8 2 . 3 . 2 Graph Convolutional Networks ( GCN )",
    "acronyms": [
      [
        58,
        61
      ]
    ],
    "long-forms": [],
    "ID": "384"
  },
  {
    "text": "Bên cạnh đó , khi so sánh và đánh giá độ chính xác giữa mô hình HAN với các mô hình khác ( được liệt kê ở phần tiếp theo ) , em sử dụng độ đo F1 . 4 . 2 . 4 Tinh chỉnh trên tập siêu tham số ( hyper parameters )",
    "acronyms": [
      [
        64,
        67
      ]
    ],
    "long-forms": [],
    "ID": "385"
  },
  {
    "text": "HÀ NỘI , 06/2021 Phiếu giao nhiệm vụ đồ án tốt nghiệp 1 . Thông tin về sinh viên :",
    "acronyms": [],
    "long-forms": [],
    "ID": "386"
  },
  {
    "text": "thể hiện bằng công thức sau : 𝐿 = − ∑ 𝑖∈𝑆",
    "acronyms": [],
    "long-forms": [],
    "ID": "387"
  },
  {
    "text": "lưu thành các thư mục 2 - NTD chọn vào 1 thư mục bất kì , sẽ hiện ra danh sách các hồ sơ được lưu trong thư mục đó",
    "acronyms": [
      [
        26,
        29
      ]
    ],
    "long-forms": [],
    "ID": "388"
  },
  {
    "text": "Tuy nhiên , ngoài những điều cải thiện được so với mô hình NRMS thì mô hình NAML vẫn chưa giải quyết được một số vấn đề đặt ra đối với mô hình NRMS đã nêu .",
    "acronyms": [
      [
        59,
        63
      ],
      [
        76,
        80
      ],
      [
        143,
        147
      ]
    ],
    "long-forms": [],
    "ID": "389"
  },
  {
    "text": "𝐺𝑅𝑈 ( 𝑥𝑖𝑡 ) , 𝑡 ∈ [ 𝑇, 1 ] Với từ 𝑤𝑖𝑡 , ta xây dựng được một chuỗi mã hóa bằng cách ghép trạng thái ẩn tiến ℎ⃗⃗𝑖𝑡 với trạng thái ẩn lùi ℎ⃖⃗𝑖𝑡 .",
    "acronyms": [],
    "long-forms": [],
    "ID": "390"
  },
  {
    "text": ") ( 35 ) trong đó LI , LE tương ứng là phần hàm lỗi của dữ liệu hành vi tiềm ẩn và hành",
    "acronyms": [],
    "long-forms": [],
    "ID": "391"
  },
  {
    "text": "Tham số α được đặt là 0.1 . Bảng 3 . 1 : Điểm BLEU của các mô hình học đối ngẫu sử dụng mô hình Transformer , k là số mô hình ngôn ngữ sử dụng cho mỗi ngôn ngữ",
    "acronyms": [
      [
        46,
        50
      ]
    ],
    "long-forms": [],
    "ID": "392"
  },
  {
    "text": "thay đổi các siêu tham số sẽ ảnh hưởng như thế nào đến cả hai phương pháp này ( liệu việc thay đổi siêu tham số ảnh hưởng đến mô hình SimCLR gốc thì có ảnh hưởng đến phương pháp LCL hay không ) .",
    "acronyms": [
      [
        134,
        140
      ],
      [
        178,
        181
      ]
    ],
    "long-forms": [],
    "ID": "393"
  },
  {
    "text": "vậy SVB là một trường hợp đặc biệt của HPP khi ρt = 1 ∀t . Khi đặt toàn bộ ρt cùng là một hằng số cố định trong khoảng ( 0, 1 ) thì HPP được gọi là SVB - PP",
    "acronyms": [
      [
        4,
        7
      ],
      [
        39,
        42
      ],
      [
        132,
        135
      ],
      [
        148,
        156
      ]
    ],
    "long-forms": [],
    "ID": "394"
  },
  {
    "text": "( Θ . T ( zi , Xi ) − A ( Θ ) ) Chúng ta có natural gradient của hàm F- ELBO trên toàn bộ dữ liệu { Xi } αi=1 là :",
    "acronyms": [
      [
        69,
        76
      ]
    ],
    "long-forms": [],
    "ID": "395"
  },
  {
    "text": "và negative mới , lúc này LCL sử dụng một hàm mất mát tương tự với InfoNCE . Xét một batch dữ liệu vào với N ảnh x1 , x2 , ... , xN . Gọi k là số view đang xét đến .",
    "acronyms": [
      [
        26,
        29
      ]
    ],
    "long-forms": [],
    "ID": "396"
  },
  {
    "text": "ước P ( Y t ) , P ( Y t+1 ) lần lượt là phân phối không gian đầu ra của tác vụ t và t + 1 . Khi đó P ( Y t ) 6 = P ( Y t+1 ) trong kịch bản học từng tác vụ ; P ( Y t ) = P ( Y t+1 ) trong",
    "acronyms": [],
    "long-forms": [],
    "ID": "397"
  },
  {
    "text": "chung giữa các tác vụ hơn . ALV khi đó cũng có tính chất này giống như phân tích trong [ 13 ] , nên đảm bảo được việc ràng buộc các tham số toàn cục trong miền tối",
    "acronyms": [
      [
        28,
        31
      ]
    ],
    "long-forms": [],
    "ID": "398"
  },
  {
    "text": "gợi ý Neural Collaborative Filtering ( NCF ) . Sinh viên thực hiện : Bùi Văn Tài , 20143908 , K59 , KSCLC HTTT & TT",
    "acronyms": [
      [
        39,
        42
      ],
      [
        100,
        105
      ],
      [
        106,
        110
      ],
      [
        113,
        115
      ]
    ],
    "long-forms": [],
    "ID": "399"
  },
  {
    "text": "1 . Tập các phép DA : • Các phép DA liên quan đến không gian : cắt ảnh , thay đổi kích thước của",
    "acronyms": [
      [
        17,
        19
      ],
      [
        33,
        35
      ]
    ],
    "long-forms": [],
    "ID": "400"
  },
  {
    "text": "Mỗi chủ đề ẩn k ∈ { 1 , 2 , ... , K} lại là một phân phối xác xuất trên tất cả các từ của tập từ điển với kích thước V , ta biểu diễn phân phối này bởi một vector βk = ( βk1 , βk2 , ... , βkV ) ( còn gọi là topic distribution ), trong đó βkj là",
    "acronyms": [],
    "long-forms": [],
    "ID": "401"
  },
  {
    "text": "𝑘=1 Thuật toán FCM cũng tương tự K-means , ngoại trừ việc ở bước xác định cụm cho các quan sát , ta cần tính xác suất 𝛾𝑛𝑘 theo công thức :",
    "acronyms": [
      [
        15,
        18
      ]
    ],
    "long-forms": [],
    "ID": "402"
  },
  {
    "text": "DANH MỤC BẢNG 5.1 5.2",
    "acronyms": [],
    "long-forms": [],
    "ID": "403"
  },
  {
    "text": "Thí nghiệm tiến hành so sánh việc tăng giảm hệ số C ảnh hưởng như thế nào đến kết quả và so sánh xem việc nới lỏng ràng buộc cho phép w âm thì kết quả có tốt hơn so với cách làm ban đầu không .",
    "acronyms": [],
    "long-forms": [],
    "ID": "404"
  },
  {
    "text": "ALV cho UCL Như đã được trình bày trong 2 . 2 . 3 , quá trình suy diễn của UCL giống hệt so với VCL , điểm khác biệt là UCL tinh chỉnh lại đại lượng KL trong công thức hàm",
    "acronyms": [
      [
        0,
        3
      ],
      [
        8,
        11
      ],
      [
        75,
        78
      ],
      [
        96,
        99
      ],
      [
        120,
        123
      ],
      [
        149,
        151
      ]
    ],
    "long-forms": [],
    "ID": "405"
  },
  {
    "text": "tương ứng với từng bài toán . Trong bài toán gợi ý phim , phim là item ; trong Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        142,
        153
      ]
    ],
    "long-forms": [],
    "ID": "406"
  },
  {
    "text": "for k = 1 , . . . , K do Đặt điểm thưởng bởi mô hình ngôn ngữ của cho quan sát thứ k là r1 , k = LMB ( smid , k ) ;",
    "acronyms": [],
    "long-forms": [],
    "ID": "407"
  },
  {
    "text": "lượng MI giữa X và Y nhưng cũng đồng thời làm cho biểu diễn ẩn tìm ra không có nhiều ý nghĩa .. Đấy chính là lí do mà trong các phương pháp huấn luyện mạng",
    "acronyms": [
      [
        6,
        8
      ]
    ],
    "long-forms": [],
    "ID": "408"
  },
  {
    "text": "đoán càng gần bounding box thực tế . Độ chính xác trung bình ( AP ) là phần diện tích phía dưới được tạo bởi đường cong Precision - Recall khi IoU > = 0.5.",
    "acronyms": [
      [
        63,
        65
      ],
      [
        143,
        146
      ]
    ],
    "long-forms": [
      [
        37,
        60
      ]
    ],
    "ID": "409"
  },
  {
    "text": "đã cho thấy số tác vụ có thể lên tới 50 tác vụ . Do các phương pháp sử dụng mạng BNN nên thực tế số lượng tham số cần học sẽ gấp đôi số lượng trọng số của mạng .",
    "acronyms": [
      [
        81,
        84
      ]
    ],
    "long-forms": [],
    "ID": "410"
  },
  {
    "text": "Hình 12 : Tổng quan về SimCLR ( Nguồn [ Che+20 ] ) Tầm ảnh hưởng của các phép DA : Một trong những đóng góp lớn trong bài",
    "acronyms": [
      [
        23,
        29
      ],
      [
        78,
        80
      ]
    ],
    "long-forms": [],
    "ID": "411"
  },
  {
    "text": "∂L i đã tính được ∂W",
    "acronyms": [],
    "long-forms": [],
    "ID": "412"
  },
  {
    "text": ". . . . 2 . 3 . 3 Phân tách ma trận tổng quát ( GMF - Generalized Matrix",
    "acronyms": [
      [
        48,
        51
      ]
    ],
    "long-forms": [
      [
        18,
        45
      ]
    ],
    "ID": "413"
  },
  {
    "text": "Latent Semantic Indexing ( LSI ) [ 6 ] và probabilistic Latent Semantic Indexing",
    "acronyms": [
      [
        27,
        30
      ]
    ],
    "long-forms": [],
    "ID": "414"
  },
  {
    "text": "Multi Layer Perceptron ĐATN",
    "acronyms": [
      [
        23,
        27
      ]
    ],
    "long-forms": [],
    "ID": "415"
  },
  {
    "text": "Mô hình dual learning trong [ 19 ] được cài đặt dựa trên mô hình dịch chuỗi - chuỗisử dụng RNN đồng thời sử dụng mô hình ngôn ngữ RNN . 40",
    "acronyms": [
      [
        91,
        94
      ],
      [
        130,
        133
      ]
    ],
    "long-forms": [],
    "ID": "416"
  },
  {
    "text": "vec - tơ pu và qi . Có thể thấy rằng , về mặt bản chất mô hình MF giả sử các chiều trong không gian thuộc tính ẩn là độc lập với nhau và tổ hợp tuyến tính chúng",
    "acronyms": [
      [
        63,
        65
      ]
    ],
    "long-forms": [],
    "ID": "417"
  },
  {
    "text": "Với câu cần dịch là X = x1 , x2 , . . . , xm và câu dịch là Y = y1 , y2 , . . . , yn . Encoder sẽ kết nối X",
    "acronyms": [],
    "long-forms": [],
    "ID": "418"
  },
  {
    "text": "hai mô hình còn lại . Ở cả hai độ đo HR @ 10 và NDCG @ 10 , nhìn chung hai mô hình ITE - item_ pcat và ITE - user_item_pcat đều có kết quả cao hơn",
    "acronyms": [
      [
        37,
        39
      ],
      [
        48,
        52
      ],
      [
        83,
        99
      ],
      [
        103,
        123
      ]
    ],
    "long-forms": [],
    "ID": "419"
  },
  {
    "text": "Giải thuật tối ưu cho UCL cũng tương tự như VCL ( Hình 2. 3 ) nhưng sử dụng hàm lỗi được biểu diễn trong công thức PT 2.21 để tối ưu các tham số . 15",
    "acronyms": [
      [
        22,
        25
      ],
      [
        44,
        47
      ],
      [
        115,
        117
      ]
    ],
    "long-forms": [],
    "ID": "420"
  },
  {
    "text": "GMM ( đề xuất ) Số nhóm",
    "acronyms": [
      [
        0,
        3
      ]
    ],
    "long-forms": [],
    "ID": "421"
  },
  {
    "text": "trọng số , UCL sử dụng tính không chắc chắn để xác định một nút trong các tầng mạng có quan trọng hay không . Để giảm số lượng tham số UCL sử dụng chung",
    "acronyms": [
      [
        11,
        14
      ],
      [
        135,
        138
      ]
    ],
    "long-forms": [],
    "ID": "422"
  },
  {
    "text": "Tại thời điểm t ≥ 1 , thu được dữ liệu của t minibatches D1 , D2 , ... , Dt . • Giả sử khi minibatch Dt đến , đã học được mô hình dự đoán M t−1 , khi đó",
    "acronyms": [],
    "long-forms": [],
    "ID": "423"
  },
  {
    "text": "Nén mạng được tiến hành sau khi quá trình tối ưu kết thúc . Tương tự việc sử dụng α để tính tỉ lệ drop trong VD , VBD sử dụng tỉ lệ tín hiệu trên nhiễu ( Signal to noise ,",
    "acronyms": [
      [
        109,
        111
      ],
      [
        114,
        117
      ]
    ],
    "long-forms": [],
    "ID": "424"
  },
  {
    "text": "trên một tập dữ liệu chuẩn , thực tế để đánh giá được tốt nhất các mô hình , tạo điều kiện thuận lợi cho việc nghiên cứu trong lĩnh vực gợi ý tin tức . Tập dữ liệu MIND thu thập khoảng 160 nghìn bài báo tiếng Anh và hơn 15 triệu",
    "acronyms": [
      [
        164,
        168
      ]
    ],
    "long-forms": [],
    "ID": "425"
  },
  {
    "text": "𝐸𝑞𝑡( θ ) log 𝑝( 𝑦𝑡𝑖 | 𝑥𝑡𝑖 , θ ) ≈ ∑𝐾𝑘=1 log 𝑝( 𝑦𝑡𝑖 | 𝑥𝑡𝑖 , 𝜃 𝑘 ) trong đó 𝜃 𝑘 = 𝜇𝑡 + 𝜎𝑡 ⨀ 𝜖 𝑘 với 𝐾 𝜖 𝑘 là một biến ngẫu nhiên được lấy mẫu từ phân phối Gauss đơn vị 𝑁 ( 0,1 ) và",
    "acronyms": [],
    "long-forms": [],
    "ID": "426"
  },
  {
    "text": "| 𝐷𝐴 ) . Từ đó ta có log 𝑝 ( 𝜃 |𝐷𝐴 ) ≈ − ( 𝜃 − 𝜃𝐴 ) 𝑑𝑖𝑎 ( 𝐹 ) ( 𝜃 −",
    "acronyms": [],
    "long-forms": [],
    "ID": "427"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 47",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "428"
  },
  {
    "text": "Trong các phương pháp tiếp cận theo suy diễn VI , công thức PT 2 . 3 yêu cầu phải tính toán hai đại lượng là lô -ga-rít của khả năng xảy ra và KL . Trong nhiều",
    "acronyms": [
      [
        45,
        47
      ],
      [
        60,
        62
      ],
      [
        143,
        145
      ]
    ],
    "long-forms": [],
    "ID": "429"
  },
  {
    "text": "Giáo viên hướng dẫn : PGS. TS. Đỗ Phan Thuận HÀ NỘI Ngày 30 tháng 5 năm 2019",
    "acronyms": [
      [
        22,
        26
      ],
      [
        27,
        30
      ]
    ],
    "long-forms": [],
    "ID": "430"
  },
  {
    "text": "• KNN : K - nearest neighbor s Đồ án tốt nghiệp 5",
    "acronyms": [
      [
        2,
        5
      ]
    ],
    "long-forms": [],
    "ID": "431"
  },
  {
    "text": "được lý giải như sau : việc tối đa hóa ELBO bao gồm việc tối thiểu hóa KL - một hàm số tỉ lệ nghịch với αkd . Như vậy quá trình huấn luyện sẽ có xu hướng làm tăng",
    "acronyms": [
      [
        39,
        43
      ],
      [
        71,
        73
      ]
    ],
    "long-forms": [],
    "ID": "432"
  },
  {
    "text": "Nhìn chung , bất kì một phương pháp SSL nào cũng sẽ gồm 2 bước : Bước 1 giải quyết một pretext task được định nghĩa từ trước và bước 2 đánh giá xem biểu diễn",
    "acronyms": [
      [
        36,
        39
      ]
    ],
    "long-forms": [],
    "ID": "433"
  },
  {
    "text": "ngữ và các thư viện sau : STT Tên Phiên bản",
    "acronyms": [
      [
        26,
        29
      ]
    ],
    "long-forms": [],
    "ID": "434"
  },
  {
    "text": "k - NN : k = 7 Naïve Bayes : không có . 43",
    "acronyms": [],
    "long-forms": [],
    "ID": "435"
  },
  {
    "text": "trước , với tư tưởng kết hợp manifold learning vào các phương pháp trong hướng nghiên cứu CL hiện tại , đây là một cách làm mà chưa có nơi nào trên thế giới đã Đồ án tốt nghiệp",
    "acronyms": [
      [
        90,
        92
      ]
    ],
    "long-forms": [],
    "ID": "436"
  },
  {
    "text": "NGÀNH CÔNG NGHỆ THÔNG TIN TÊN ĐỀ TÀI PHÂN LOẠI CẢM XÚC VĂN BẢN TIẾNG VIỆT",
    "acronyms": [],
    "long-forms": [],
    "ID": "437"
  },
  {
    "text": "Đồng thời có điểm khác biệt ở tầng học chú ý đối với User Encoder so với mô hình NRMS . Có thể thấy , kết quả mô hình NAML so với mô hình NRMS là tốt hơn , do khai",
    "acronyms": [
      [
        81,
        85
      ],
      [
        118,
        122
      ],
      [
        138,
        142
      ]
    ],
    "long-forms": [],
    "ID": "438"
  },
  {
    "text": "ThS. Ngô Văn Linh TRƯỜNG ĐẠI HỌC BÁCH KHOA HÀ NỘI VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG",
    "acronyms": [
      [
        0,
        4
      ]
    ],
    "long-forms": [],
    "ID": "439"
  },
  {
    "text": "3 - NTD xác nhận chọn xóa bài đăng . Các bước thay Không thế",
    "acronyms": [
      [
        4,
        7
      ]
    ],
    "long-forms": [],
    "ID": "440"
  },
  {
    "text": "Với ba mô hình ITE- onehot , ITE- item_ pcat , ITE- user_item_ pcat cũng như hai mô hình MTMF , NMTR , em sẽ xác định trước lr , η và batch - size trên hai Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        219,
        230
      ],
      [
        15,
        26
      ],
      [
        29,
        44
      ],
      [
        47,
        67
      ],
      [
        89,
        93
      ],
      [
        96,
        100
      ]
    ],
    "long-forms": [],
    "ID": "441"
  },
  {
    "text": "Convolutional Neural Network Convolutional Neural Network ( CNN ) là một trong những mô hình học sâu tiên tiến , được sử dụng để xây dựng trong các hệ thống thông minh với độ chính",
    "acronyms": [
      [
        60,
        63
      ]
    ],
    "long-forms": [],
    "ID": "442"
  },
  {
    "text": "Đây chính là tư tưởng chung của VI . Cụ thể , để xấp xỉ được hai phân phối , VI tối thiểu hóa khoảng cách KL ( KL",
    "acronyms": [
      [
        32,
        34
      ],
      [
        77,
        79
      ],
      [
        106,
        108
      ],
      [
        111,
        113
      ]
    ],
    "long-forms": [],
    "ID": "443"
  },
  {
    "text": "RÀNG ( IMPLICIT , EXPLICIT BEHAVIOR ) việc phản ánh sở thích hay mối quan tâm của người dùng , nhưng cũng có thể ngầm định rằng mức độ tin cậy của các loại hành vi tương ứng với thứ tự của",
    "acronyms": [],
    "long-forms": [],
    "ID": "444"
  },
  {
    "text": "Kết quả mô hình NAML được so sánh với các mô hình Deep Learning khác dựa trên các độ đo : AUC , MRR , nDCG@ 5 và nDCG@ 10 ( các độ đo này sẽ được trình bày cụ thể ở chương 4 ) :",
    "acronyms": [
      [
        16,
        20
      ],
      [
        90,
        93
      ],
      [
        96,
        99
      ],
      [
        102,
        106
      ],
      [
        113,
        117
      ]
    ],
    "long-forms": [],
    "ID": "445"
  },
  {
    "text": "thống là sẽ áp dụng mô hình phát hiện đối tượng ( YOLOv5 ) để phát hiện nhân viên , khách hàng và thuật toán theo dõi đối tượng ( SORT ) để theo dõi đối tượng . Sau khi",
    "acronyms": [
      [
        130,
        134
      ]
    ],
    "long-forms": [],
    "ID": "446"
  },
  {
    "text": "Nguyên nhân là do tác vụ B tương tác với tập đặc trưng này một cách khó khăn hơn . Nhưng với MTL , khi hai tác vụ A và B được học cùng nhau thì",
    "acronyms": [
      [
        93,
        96
      ]
    ],
    "long-forms": [],
    "ID": "447"
  },
  {
    "text": "hưởng đến tính chính xác của chương trình . Thư viện pyvi có độ chính xác F1 cho tách từ tiếng Việt là 0.978637686 Em sử dụng pyvi để tiến hành thực hiện tách từ trong phần tiền xử lý dữ liệu .",
    "acronyms": [],
    "long-forms": [],
    "ID": "448"
  },
  {
    "text": "Ta xét hướng tiếp cận này trên một bộ dữ liệu 𝐷 . Mục tiêu là học ra một mô hình với bộ tham số 𝜃 để có thể đưa ra kết quả suy diễn từ dữ liệu này hay giá trị",
    "acronyms": [],
    "long-forms": [],
    "ID": "449"
  },
  {
    "text": "MỘT MÔ HÌNH HỌC SÂU CHO BÀI TOÁN NHẬN DẠNG QUAN HỆ ẨN GIỮA HAI CÂU",
    "acronyms": [],
    "long-forms": [],
    "ID": "450"
  },
  {
    "text": "HAC ( đề xuất ) N/A 277",
    "acronyms": [
      [
        0,
        3
      ]
    ],
    "long-forms": [],
    "ID": "451"
  },
  {
    "text": "2 PT 2.20 Tổng hợp lại từ PT 2.18 , PT 2. 19 và PT 2. 20 ta thu được hàm lỗi sử dụng cho",
    "acronyms": [
      [
        2,
        4
      ],
      [
        26,
        28
      ],
      [
        36,
        38
      ],
      [
        48,
        50
      ]
    ],
    "long-forms": [],
    "ID": "452"
  },
  {
    "text": "Ngữ cảnh của các từ trong câu có ý nghĩa trong việc biểu diễn câu . Vì thế , tầng CNN sẽ học biểu",
    "acronyms": [
      [
        82,
        85
      ]
    ],
    "long-forms": [],
    "ID": "453"
  },
  {
    "text": "Hình 25 : Kết quả khi thay đổi tham số temperature Ta thấy , khi thay đổi temperature = 0.1 lên 0.5 , mặc dù lúc đầu kết quả có vẻ khác nhau song khi hội tụ , các mô hình LCL và SimCLR với temperature = 0.5 đều",
    "acronyms": [
      [
        171,
        174
      ],
      [
        178,
        184
      ]
    ],
    "long-forms": [],
    "ID": "454"
  },
  {
    "text": "Lời đầu tiên , tôi xin bày tỏ lòng biết ơn sâu sắc tới thầy hướng dẫn PGS. TS. Thân Quang Khoát giảng viên tại bộ môn Hệ thống Thông tin - Viện Công nghệ thông tin và truyền thông - Trường Đại học Bách khoa Hà Nội đã định hướng tận tâm",
    "acronyms": [
      [
        70,
        74
      ],
      [
        75,
        78
      ]
    ],
    "long-forms": [],
    "ID": "455"
  },
  {
    "text": "Số hệ số khác 0 LGL",
    "acronyms": [
      [
        16,
        19
      ]
    ],
    "long-forms": [],
    "ID": "456"
  },
  {
    "text": "trận . Tuy nhiên rank ( H ) = rank ( U V T ) ≤ min ( rank ( U ) , rank ( V T ) ) ≤ k",
    "acronyms": [],
    "long-forms": [],
    "ID": "457"
  },
  {
    "text": "Sinh viên thực hiện : Phan Tuấn Anh - 20150157 Bayesian Neural Net ( BNN ) Mô hình mạng nơ ron như đã đề cập ở trên còn được gọi là deterministic neural network , có",
    "acronyms": [
      [
        69,
        72
      ]
    ],
    "long-forms": [],
    "ID": "458"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 15 2 KIẾN THỨC CƠ SỞ",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "459"
  },
  {
    "text": "CHƯƠNG 1 . GIỚI THIỆU ĐỀ TÀI 1 . 1 Đặt vấn đề",
    "acronyms": [],
    "long-forms": [],
    "ID": "460"
  },
  {
    "text": "item cùng loại với nhau . Hình 11 : Mô hình ITE - item _ pcat Công thức biểu diễn và hàm mục tiêu cho các mô hình ITE được đề xuất sau",
    "acronyms": [
      [
        114,
        117
      ],
      [
        44,
        61
      ]
    ],
    "long-forms": [],
    "ID": "461"
  },
  {
    "text": "với khoảng cách càng lớn dần thì RNN bắt đầu không thể nhớ và học được nữa ( hiện tượng vanishing gradient ) [ 4 ] . Từ đó Long short term memory ( LSTM ) ra đời để giải quyết vấn đề này .",
    "acronyms": [
      [
        148,
        152
      ]
    ],
    "long-forms": [],
    "ID": "462"
  },
  {
    "text": "Bảng 2 . 1 Kết quả mô hình NRMS Kết quả mô hình NRMS cho thấy các độ đo tốt hơn các mô hình Deep Learning khác , các độ đo cao hơn mô hình tốt nhất trước đó 2 % trên độ đo nDCG @ 5 và",
    "acronyms": [
      [
        27,
        31
      ],
      [
        48,
        52
      ],
      [
        172,
        176
      ]
    ],
    "long-forms": [],
    "ID": "463"
  },
  {
    "text": "cũng đã phân tích rõ lý thuyết của các phương pháp cơ sở để lập luận những vấn đề trên , trong đó SVB không có cơ chế cân bằng thông tin và gặp phải vấn đề vanishing variance khi nhận được lượng dữ liệu đủ lớn , SVB - PP khắc",
    "acronyms": [
      [
        98,
        101
      ],
      [
        212,
        220
      ]
    ],
    "long-forms": [],
    "ID": "464"
  },
  {
    "text": "𝑝 ( 𝜃 | 𝐷 ) – xác suất có điều kiện của 𝜃 khi biết dữ liệu D . Công thức được khai triển theo suy diễn Bayes sau khi được lô-ga-rít hóa :",
    "acronyms": [],
    "long-forms": [],
    "ID": "465"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 45 PHỤ LỤC",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "466"
  },
  {
    "text": "Nhiều trường hợp các User không tương tác đủ số lượng bài Shortterm sẽ được thêm các giá trị padding 0 để thuận lợi cho quá trình tính toán . Mô hình LSTUR được đánh giá trên tập MIND và cho kết quả như sau : Methords",
    "acronyms": [
      [
        150,
        155
      ],
      [
        179,
        183
      ]
    ],
    "long-forms": [],
    "ID": "467"
  },
  {
    "text": "Bên cạnh đó , mẫu số của ( 2.4 ) là p ( X ) hoàn toàn không phụ thuộc vào θ . Do đó , ta có ( 2.4 ) được viết lại thành :",
    "acronyms": [],
    "long-forms": [],
    "ID": "468"
  },
  {
    "text": "phương pháp rất nổi tiếng của CL . Về mặt tư tưởng MOCO cũng tạo ra các cặp",
    "acronyms": [
      [
        30,
        32
      ],
      [
        51,
        55
      ]
    ],
    "long-forms": [],
    "ID": "469"
  },
  {
    "text": "Tính stochastic gradient của ΘBA : K 1 X",
    "acronyms": [],
    "long-forms": [],
    "ID": "470"
  },
  {
    "text": "– Mô hình vẫn chưa giải quyết được vấn đề cold start 4 . 3 Mô hình ITE - user _ item _ pcat Không nhiều hệ thống thực tế có được những thông tin mô tả về người dùng .",
    "acronyms": [
      [
        67,
        91
      ]
    ],
    "long-forms": [],
    "ID": "471"
  },
  {
    "text": "Thứ hai , phương pháp học batch có nhược điểm Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 14",
    "acronyms": [
      [
        106,
        117
      ]
    ],
    "long-forms": [],
    "ID": "472"
  },
  {
    "text": "Xét một tầng ẩn 𝑙 của mô hình mạng nơ- ron kết nối đầy đủ , có 𝐴 ( 𝑙 ) là ma trận đầu vào có kích thước 𝑀 × 𝐷 , 𝜃 ( 𝑙) là ma trận trọng số có kích thước 𝐷 × 𝐻 , trong 18",
    "acronyms": [],
    "long-forms": [],
    "ID": "473"
  },
  {
    "text": "hoạt là hàm identity ( nghĩa là đầu ra của hàm chính bằng đầu vào của hàm ) và cố định h với tất cả các trọng số đều bằng 1 , ta nhận được chính xác mô hình MF .",
    "acronyms": [
      [
        157,
        159
      ]
    ],
    "long-forms": [],
    "ID": "474"
  },
  {
    "text": "Với 𝑧𝑖 , 𝑙 = 𝜇𝑖 + 𝜎𝑖 ⊙ 𝜀𝑙 và 𝜀𝑙 ~ 𝒩 ( 0, 𝐼 ) 11 Trên đây là những kiến thức cơ sở về mô hình chủ đề LDA và phương pháp học",
    "acronyms": [
      [
        100,
        103
      ]
    ],
    "long-forms": [],
    "ID": "475"
  },
  {
    "text": "Continual Learning ( SCCL ) , dịch là học liên tục dựa trên nén cấu trúc mạng . Phương pháp sử dụng kỹ thuật nén cấu trúc mạng dựa trên Group Lasso regularization làm trọng tâm để",
    "acronyms": [
      [
        21,
        25
      ]
    ],
    "long-forms": [],
    "ID": "476"
  },
  {
    "text": "Đặt s = sA ; Sinh K câu smid , 1 , . . . , smid , K sử dụng beam search từ mô hình dịch P ( .| s ; ΘAB ) ; for k = 1 , . . . , K do",
    "acronyms": [],
    "long-forms": [],
    "ID": "477"
  },
  {
    "text": "PT 2.14 Với t = 1 thì 𝑞0 ( θ ) = 𝑝 ( 𝜃 ) . 𝑍𝑡 là đại lượng hằng số chuẩn hóa trong phép chiếu",
    "acronyms": [
      [
        0,
        2
      ]
    ],
    "long-forms": [],
    "ID": "478"
  },
  {
    "text": "được sử dụng có dạng f L ( aL−1 ) = Act ( ( W L ) T aL−1 + bL ) 31",
    "acronyms": [],
    "long-forms": [],
    "ID": "479"
  },
  {
    "text": "ĐỒ ÁN TỐT NGHIỆP Học tự giám sát NGUYỄN HỒNG QUỐC KHÁNH",
    "acronyms": [],
    "long-forms": [],
    "ID": "480"
  },
  {
    "text": "| B | α , chúng ta xấp xỉ nhiễu cho natural gradient trên dữ liệu của minibatch α",
    "acronyms": [],
    "long-forms": [],
    "ID": "481"
  },
  {
    "text": "Tư tưởng chính của CL đó là tạo ra nhiều cách nhìn khác nhau của cùng một dữ liệu , mà ta tạm gọi là các view . Các view này có thể được tạo ra bởi nhiều cách ,",
    "acronyms": [
      [
        19,
        21
      ]
    ],
    "long-forms": [],
    "ID": "482"
  },
  {
    "text": "dụng và kết quả đã được kiểm chứng . 2 . 1 Mô hình Gợi ý tin tức dựa trên cơ chế tự chú ý đa chiều ( NRMS )",
    "acronyms": [
      [
        101,
        105
      ]
    ],
    "long-forms": [
      [
        51,
        98
      ]
    ],
    "ID": "483"
  },
  {
    "text": "văn bản Consequence ( M2 ) : Là các sự kiện được sinh ra bởi sự kiện chính , nó có thể xảy",
    "acronyms": [],
    "long-forms": [],
    "ID": "484"
  },
  {
    "text": "Các hàm kích hoạt trọng mô hình MLP ta sử dụng hàm ReLU như đã nói ở mục trước . Hàm aout sẽ dùng hàm sigmoid để phù hợp với dữ liệu tương tác dạng binary .",
    "acronyms": [
      [
        32,
        35
      ],
      [
        51,
        55
      ]
    ],
    "long-forms": [],
    "ID": "485"
  },
  {
    "text": "học γ1 , t và γ2 , t , N là số nhiễu áp dụng ; repeat",
    "acronyms": [],
    "long-forms": [],
    "ID": "486"
  },
  {
    "text": "bài báo . Cấu trúc của mô-đun News Encoder được mô tả bằng hình : Hình 2. 4 Mô-dun News Encoder mô hình LSTUR",
    "acronyms": [
      [
        104,
        109
      ]
    ],
    "long-forms": [],
    "ID": "487"
  },
  {
    "text": "0.7 Tác v 7 VBD - CL",
    "acronyms": [
      [
        12,
        20
      ]
    ],
    "long-forms": [],
    "ID": "488"
  },
  {
    "text": "ei = ∑𝑀 𝑖=1 𝛼𝑖 𝑐𝑖 Thành phần tiếp theo của News Encoder là Topic Encoder , được sử dụng để học",
    "acronyms": [],
    "long-forms": [],
    "ID": "489"
  },
  {
    "text": "2 . 1 Mô hình Biterm Topic Model ( BTM ) 3 3",
    "acronyms": [
      [
        35,
        38
      ]
    ],
    "long-forms": [],
    "ID": "490"
  },
  {
    "text": "Gán mỗi quan sát cho một cụm có tâm gần nó nhất . Tính toán giá trị hàm lỗi L trong công thức ( 16 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "491"
  },
  {
    "text": "Tương tự như vậy , trong CL , khi nhìn một bức ảnh đầu vào dưới nhiều view khác nhau , như qua các phép biến hình hay các kênh khác nhau của không gian màu , hay qua các phép DA khác nhau , thì về cơ bản ,",
    "acronyms": [
      [
        25,
        27
      ],
      [
        175,
        177
      ]
    ],
    "long-forms": [],
    "ID": "492"
  },
  {
    "text": "Quá trình đọc dữ liệu và phân chia dữ liệu được thực hiện giống với thử nghiệm trong phương pháp UCL [ 3 ] , trong công việc này mã nguồn đọc và phân chia dữ 29",
    "acronyms": [
      [
        97,
        100
      ]
    ],
    "long-forms": [],
    "ID": "493"
  },
  {
    "text": "thường 478K EWC",
    "acronyms": [
      [
        12,
        15
      ]
    ],
    "long-forms": [],
    "ID": "494"
  },
  {
    "text": "Learning rate Topics coherence ( NPMI ) Hình 4 . 19 Đồ thị biểu diễn giá trị NPMI khi thay đổi tốc độ học mô hình trên tập",
    "acronyms": [
      [
        33,
        37
      ],
      [
        77,
        81
      ]
    ],
    "long-forms": [],
    "ID": "495"
  },
  {
    "text": "xác suất dự đoán các Positive Samples là cao hơn . Cụ thể , xác suất mỗi bài báo được User quan tâm là 𝑦 + , xác suất dự đoán K bài không được User đó quan tâm [ 𝑦1− , 𝑦2 − , . . . , 𝑦𝐾− ] , điểm số được tính bởi công thức :",
    "acronyms": [],
    "long-forms": [],
    "ID": "496"
  },
  {
    "text": "Sự cải thiện tương đối trong L ( w , φ , γ , λ ) > 10−6 do E step : Bước suy diễn biến cục bộ cho từng văn bản for mỗi văn bản d trong Dt do",
    "acronyms": [],
    "long-forms": [],
    "ID": "497"
  },
  {
    "text": "3 . 2 . 3 Mô-đun dự đoán tương tác ( Click Prediction ) Cuối cùng , sau khi qua L block ( tầng ) , biểu diễn cuối cùng của User được học từ",
    "acronyms": [],
    "long-forms": [],
    "ID": "498"
  },
  {
    "text": "như là điểm mạnh và điểm yếu của nó . Cuối cùng đề cập một vài ứng dụng của GCN trong xử lý ngôn ngữ tự nhiên ( NLP ) .",
    "acronyms": [
      [
        76,
        79
      ],
      [
        112,
        115
      ]
    ],
    "long-forms": [
      [
        86,
        109
      ]
    ],
    "ID": "499"
  },
  {
    "text": "có một số pretext task theo hướng dự đoán , ví dụ như trong mô hình colorization , người ta cố xây dựng lên một mạng AE có khả năng tô màu cho ảnh đen trắng . Đồ án tốt nghiệp",
    "acronyms": [
      [
        117,
        119
      ]
    ],
    "long-forms": [],
    "ID": "500"
  },
  {
    "text": "Hình A . 2 Số lượng người dùng theo số bài đọc trung bình trong tuần 42",
    "acronyms": [],
    "long-forms": [],
    "ID": "501"
  },
  {
    "text": "• Sử dụng một tập các phép DA để tạo ra các view các nhau cho dữ liệu . Đây là một bước không mấy xa lạ trong các phương pháp CL từ trước . • Sử dụng một mạng encoder fθ ( x) để tìm các biểu diễn ẩn tương ứng zi của các",
    "acronyms": [
      [
        27,
        29
      ],
      [
        126,
        128
      ]
    ],
    "long-forms": [],
    "ID": "502"
  },
  {
    "text": "cho 𝑠𝑡 , 𝑚 ( 𝑚 ∈ { 1 , … , 𝑀 } và 𝑠𝑡 ,𝑚 ∈ 𝑅1×𝐷 ) . Phân phối của biến ngẫu nhiên cục bộ khi đó sẽ phụ thuộc vào các tham số có thể học được trong quá trình tối ưu , ở đây",
    "acronyms": [],
    "long-forms": [],
    "ID": "503"
  },
  {
    "text": "Trong công thức PT 2.17 , 𝐿 là số lớp trong mạng nơ-ron của mô hình , ( 𝑙 ) ( 𝑙 )",
    "acronyms": [
      [
        16,
        18
      ]
    ],
    "long-forms": [],
    "ID": "504"
  },
  {
    "text": "X C×W ×H ) được coi như một nơ-ron tại một tầng của mạng MLP . Mỗi tầng l , trừ tầng đầu ra , đều được nhân từng phần tử với một tầng nhiễu trước khi được nhân",
    "acronyms": [
      [
        57,
        60
      ]
    ],
    "long-forms": [],
    "ID": "505"
  },
  {
    "text": "j=1 wi , j ∗ yj || 2 Tương tự như phần tìm w , việc tìm Y cũng được giải quyết bằng các thuật toán tối ưu thông thường và đây cũng chính là kết quả cuối cùng của thuật toán LLE .",
    "acronyms": [
      [
        173,
        176
      ]
    ],
    "long-forms": [],
    "ID": "506"
  },
  {
    "text": "2 . 1 . 3 Suy diễn biến phân Xét một mô hình mạng Bayes có các biến quan sát được ký hiệu là X , và tập các biến ẩn ( tham số cần học của mô hình ) ký hiệu là Z. Như đã trình bày ở",
    "acronyms": [],
    "long-forms": [],
    "ID": "507"
  },
  {
    "text": "Đây cũng là một nhược điểm mà mô hình LSTUR chưa giải quyết được . 21",
    "acronyms": [
      [
        38,
        43
      ]
    ],
    "long-forms": [],
    "ID": "508"
  },
  {
    "text": "lượng xấp xỉ này . Tương quan trên pre - activation Trong công thức PT 3. 11 ta thấy mỗi phần tử trong đại lượng pre-activation",
    "acronyms": [
      [
        68,
        70
      ]
    ],
    "long-forms": [],
    "ID": "509"
  },
  {
    "text": "Bảng 5 . 5 : Phần trăm không gian mạng MLP sử dụng qua các tác vụ trên tập Split MNIST và Permuted MNIST Tác vụ",
    "acronyms": [
      [
        39,
        42
      ],
      [
        81,
        86
      ],
      [
        99,
        104
      ]
    ],
    "long-forms": [],
    "ID": "510"
  },
  {
    "text": "2 i=1 N ( µi , σi ) . Khi đó :",
    "acronyms": [],
    "long-forms": [],
    "ID": "511"
  },
  {
    "text": "14 2 KIẾN THỨC CƠ SỞ P ∈ Rk×M là ma trận toàn bộ biểu diễn của M người dùng , Q ∈ Rk×N là ma",
    "acronyms": [],
    "long-forms": [],
    "ID": "512"
  },
  {
    "text": "Quá trình huấn luyện là quá trình tối ưu 2 tham số 𝜃 và 𝛽 , còn quá trình suy diễn là quá trình tối ưu 𝜃 dựa trên biểu diễn TF - IDF và 𝛽 đã được tối ưu .",
    "acronyms": [
      [
        124,
        132
      ]
    ],
    "long-forms": [],
    "ID": "513"
  },
  {
    "text": "Các mô hình khác nhau sẽ tìm ra các biểu diễn khác nhau . Trong mô hình LSI , chúng ta biến đổi các phần tử của ma trận DOC −",
    "acronyms": [
      [
        72,
        75
      ]
    ],
    "long-forms": [],
    "ID": "514"
  },
  {
    "text": "Mục tiêu của em khi thực hiện đề tài là nghiên cứu , hiểu , xây dựng và đánh giá được mô hình cơ bản là Phân tích ma trận phân tử cấu trúc sâu ( DMF ) để dự đoán đánh giá của người dùng đối với sản phẩm mà họ chưa đánh giá , đưa ra xác",
    "acronyms": [
      [
        145,
        148
      ]
    ],
    "long-forms": [
      [
        104,
        142
      ]
    ],
    "ID": "515"
  },
  {
    "text": "Tác v AGS - CL EWC",
    "acronyms": [
      [
        6,
        14
      ],
      [
        15,
        18
      ]
    ],
    "long-forms": [],
    "ID": "516"
  },
  {
    "text": "quy hoá . Tuy nhiên , không như các phương pháp chính quy hóa khác ( chẳng hạn L2 ) , mỗi đại lượng βkj trong iDropout có một hệ số chính quy hoá khác",
    "acronyms": [],
    "long-forms": [],
    "ID": "517"
  },
  {
    "text": "33 mà như đề cập ở trên thì λ̃i là thông tin học được ở minibatch thứ i nên với mô hình LDA ( NB tương tự ) , theo công thức ( 12 ) chúng ta có :",
    "acronyms": [
      [
        88,
        91
      ],
      [
        94,
        96
      ]
    ],
    "long-forms": [],
    "ID": "518"
  },
  {
    "text": "Dt = { X t , Y t } với X t , Y t = { xtn , ynt } N n=1 tuân theo",
    "acronyms": [],
    "long-forms": [],
    "ID": "519"
  },
  {
    "text": "𝛼 : véc − tơ tỉ lệ trộn 𝑀 : là số lượng trộn PT 5.1",
    "acronyms": [
      [
        45,
        47
      ]
    ],
    "long-forms": [],
    "ID": "520"
  },
  {
    "text": "( 14 ) 𝐾 điều kiện 𝑦𝑛𝑘 ∈ { 0, 1 } ∀𝑛 , 𝑘 ; ∑ 𝑦𝑛𝑘 = 1 ∀𝑘",
    "acronyms": [],
    "long-forms": [],
    "ID": "521"
  },
  {
    "text": "Số lần lặp BPE 1000 convolutional encoder",
    "acronyms": [
      [
        11,
        14
      ]
    ],
    "long-forms": [],
    "ID": "522"
  },
  {
    "text": "Tương tự với các item . Như vậy , đối với những người dùng không Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        128,
        139
      ]
    ],
    "long-forms": [],
    "ID": "523"
  },
  {
    "text": "Công thức thông thường của lớp kết nối đầy đủ ( Fully Connected , FC ) là một biến đổi tuyến tính : B = AW .",
    "acronyms": [
      [
        66,
        68
      ]
    ],
    "long-forms": [
      [
        31,
        45
      ]
    ],
    "ID": "524"
  },
  {
    "text": "wk = { w1k , w2k , ... , wtk } gồm t từ có trọng số lớn nhất trong topic distribution βk tương ứng . NPMI của chủ đề k được tính như sau :",
    "acronyms": [
      [
        101,
        105
      ]
    ],
    "long-forms": [],
    "ID": "525"
  },
  {
    "text": "Ước lượng cực đại hóa hậu nghiệm - Maximum A Posterior Ước lượng cực đại hóa hậu nghiệm ( MAP ) được bắt nguồn từ công thức Bayes . Giống như cái tên MAP sẽ làm việc trên xác suất hậu nghiệm , mà không phải",
    "acronyms": [
      [
        90,
        93
      ],
      [
        150,
        153
      ]
    ],
    "long-forms": [
      [
        55,
        87
      ],
      [
        0,
        32
      ]
    ],
    "ID": "526"
  },
  {
    "text": "giúp mô hình có khả năng tránh được hiện tượng học quá khớp . Hình 2 . 1 mô tả trực quan cho hai kiến trúc mạng ANN và BNN kết nối đầy đủ .",
    "acronyms": [
      [
        112,
        115
      ],
      [
        119,
        122
      ]
    ],
    "long-forms": [],
    "ID": "527"
  },
  {
    "text": "− 𝐾𝐿 ( 𝑞𝑡 ( 𝜃 ) | | 𝑞𝑡−1 ( 𝜃 ) ) Trong công thức PT 3.7 , đại lượng 𝐾𝐿 ( 𝑞𝑡 ( 𝜃 ) | | 𝑞𝑡−1 ( 𝜃 ) ) giúp giảm vấn đề quên",
    "acronyms": [
      [
        49,
        51
      ]
    ],
    "long-forms": [],
    "ID": "528"
  },
  {
    "text": "MỘT SỐ THÔNG KÊ KHÁC TRÊN TẬP DỮ LIỆU PEGA Thống kê số lượng người dùng mới mỗi ngày Hình A . 1 Thống kê số lượng User mới trong tháng 10 theo ngày",
    "acronyms": [],
    "long-forms": [],
    "ID": "529"
  },
  {
    "text": "• Tốc độ phát hiện ( FPS ) : Là thời gian phát hiện được đối tượng trong bao nhiêu frame , tính bằng khung hình trên giây . • Độ chính xác trung bình trên một đối tượng ( AP ) : Là độ chính xác được tính",
    "acronyms": [
      [
        21,
        24
      ],
      [
        171,
        173
      ]
    ],
    "long-forms": [
      [
        126,
        168
      ],
      [
        101,
        121
      ]
    ],
    "ID": "530"
  },
  {
    "text": "𝐸 ( 𝑊 ) = 𝐸𝐷 ( 𝑊 ) + 𝜆 ∑ 𝑅 ( 𝑊𝑙 ) 𝑙=1 Công thức 8 : Hàm mục tiêu huấn luyện mạng nơ-ron với regularization",
    "acronyms": [],
    "long-forms": [],
    "ID": "531"
  },
  {
    "text": "ĐỒ ÁN TỐT NGHIÊP ÁP DỤNG DROPOUT BAYES BIẾN PHÂN TRONG BÀI TOÁN HỌC LIÊN TỤC",
    "acronyms": [],
    "long-forms": [],
    "ID": "532"
  },
  {
    "text": "4.1.1 VBD trong học liên tục Độ quan trọng của nơ-ron",
    "acronyms": [
      [
        6,
        9
      ]
    ],
    "long-forms": [],
    "ID": "533"
  },
  {
    "text": "Hàm mất mát : Trong hàm mất mát mà bài báo đưa ra , có một thành phần chính là phần tái cấu trúc ( reconstruct ) gần giống với hàm mất mát của AE truyền thống ,",
    "acronyms": [
      [
        143,
        145
      ]
    ],
    "long-forms": [],
    "ID": "534"
  },
  {
    "text": "Lớp : CNTT - TT 2.03 - K59 Hệ đào tạo : Đại học chính quy",
    "acronyms": [
      [
        6,
        15
      ]
    ],
    "long-forms": [],
    "ID": "535"
  },
  {
    "text": "Thử nghiệm , đánh giá và Phần 6 : Kết luận . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 9",
    "acronyms": [
      [
        108,
        119
      ]
    ],
    "long-forms": [],
    "ID": "536"
  },
  {
    "text": "16 Mạng neural nhân tạo ( Artificial Neural Networks - ANN ) là một kiến trúc trong đó các tầng tuyến tính và phi tuyến được xếp xem kẽ nhau , cho phép dữ liệu đi và mô hình",
    "acronyms": [
      [
        55,
        58
      ]
    ],
    "long-forms": [
      [
        3,
        23
      ]
    ],
    "ID": "537"
  },
  {
    "text": "cứu trong CL hiện tại ) , chứ không tập trung nhiều vào các bản chất toán học sâu xa trong hướng nghiên cứu manifold learning . Bạn đọc có thể xem qua phần này",
    "acronyms": [
      [
        10,
        12
      ]
    ],
    "long-forms": [],
    "ID": "538"
  },
  {
    "text": "hàm softmax . 2 . 2 . 2 Mô hình CBOW Mô hình này tương tự với mô hình skip-gram , tuy nhiên thay vì đầu vào là từ mục tiêu",
    "acronyms": [
      [
        32,
        36
      ]
    ],
    "long-forms": [],
    "ID": "539"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 11 2 KIẾN THỨC CƠ SỞ",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "540"
  },
  {
    "text": "t t Suy diễn các biến cục bộ z từ mô hình gốc B ( Θ̃ , z , Dt ) với Θ̃ , Dt đã",
    "acronyms": [],
    "long-forms": [],
    "ID": "541"
  },
  {
    "text": "Vec - tơ pcat của zone sẽ đi qua một tầng embedding , rồi ghép vào các vec-tơ embedding MLP của người dùng và banner . Phần mô",
    "acronyms": [
      [
        88,
        91
      ]
    ],
    "long-forms": [],
    "ID": "542"
  },
  {
    "text": "Cụ thể như sau : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 28",
    "acronyms": [
      [
        80,
        91
      ]
    ],
    "long-forms": [],
    "ID": "543"
  },
  {
    "text": "ý tưởng từ kỹ thuật Dropout biến phân , cụ thể là VBD , và điều này đem lại thêm hai lợi ích cho phương pháp . Thứ nhất , mô hình có thể phân bổ các nơ -ron không",
    "acronyms": [
      [
        50,
        53
      ]
    ],
    "long-forms": [],
    "ID": "544"
  },
  {
    "text": "Trong đồ án này sẽ gọi chung các bài toán mà con người muốn máy tính giải được với thuật ngữ pretext task . Bản thân thuật ngữ SSL cũng được sử dụng nhiều trong các lĩnh vực khác như",
    "acronyms": [
      [
        127,
        130
      ]
    ],
    "long-forms": [],
    "ID": "545"
  },
  {
    "text": "mô hình , num parameters ( NP ) . Bộ nhớ sử dụng đánh giá xem phương pháp có khả thi để sử dụng trung thực tế hay không .",
    "acronyms": [
      [
        27,
        29
      ]
    ],
    "long-forms": [],
    "ID": "546"
  },
  {
    "text": "iDropout Infinite Dropout Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        86,
        97
      ]
    ],
    "long-forms": [],
    "ID": "547"
  },
  {
    "text": ": Viện Công Nghệ Thông Tin và Truyền Thông Sinh viên thực hiện - Nguyễn Văn Chức - cam kết Đồ án nghiên cứu ( ĐANC )",
    "acronyms": [
      [
        110,
        114
      ]
    ],
    "long-forms": [
      [
        91,
        107
      ]
    ],
    "ID": "548"
  },
  {
    "text": "Trần Thị Hồng 5 . Xác nhận của giáo viên hướng dẫn về mức độ hoàn thành của ĐATN và cho phép bảo vệ : Hà Nội , ngày tháng năm",
    "acronyms": [
      [
        76,
        80
      ]
    ],
    "long-forms": [],
    "ID": "549"
  },
  {
    "text": "Chúng tôi sử dụng mô hình LDA với K = 100 và α = 0.01 để phân tích chủ đề của bộ dữ liệu này và bỏ qua thông tin về nhãn . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        26,
        29
      ],
      [
        183,
        194
      ]
    ],
    "long-forms": [],
    "ID": "550"
  },
  {
    "text": "Cũng ràng buộc sự thay đổi trên trọng số những Variational Continual Learning ( VCL ) ( Cuong V. Nguyen , 2017 ) lại dựa trên học Bayesian trực tuyến , sử dụng mạng nơ-ron Bayesian học ra phân phối của từng",
    "acronyms": [
      [
        80,
        83
      ]
    ],
    "long-forms": [],
    "ID": "551"
  },
  {
    "text": "CHƯƠNG 2 . KIẾN THỨC NỀN TẢNG 2 . 1 Latent Dirichlet Allocation ( LDA )",
    "acronyms": [
      [
        66,
        69
      ]
    ],
    "long-forms": [],
    "ID": "552"
  },
  {
    "text": "1 Mô tả 6 bộ dữ liệu dùng trong thử nghiệm học không giám sát . . 56 Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        129,
        140
      ]
    ],
    "long-forms": [],
    "ID": "553"
  },
  {
    "text": "( RNN ) ra đời nhằm khắc phục nhược điểm trên . Đầu vào của mạng RNN là một chuỗi liên tục theo thời gian ( ví dụ : các khung hình liên tiếp nhau trong một video hay các các chuỗi từ trong một đoạn văn , .. )",
    "acronyms": [
      [
        2,
        5
      ],
      [
        65,
        68
      ]
    ],
    "long-forms": [],
    "ID": "554"
  },
  {
    "text": "hàm likelihood có công thức như sau : Y Y",
    "acronyms": [],
    "long-forms": [],
    "ID": "555"
  },
  {
    "text": "Các công việc ban đầu tập trung đưa ra đánh giá độ quan trọng của tham số . Một số ví dụ điển hình là EWC [ 5 ] , SI [ 7 ] , Bộ nhớ điểm tiếp hợp ( Memory Aware",
    "acronyms": [
      [
        102,
        105
      ],
      [
        114,
        116
      ]
    ],
    "long-forms": [],
    "ID": "556"
  },
  {
    "text": "Trung tâm Nghiên cứu phát triển công nghệ mạng Viettel - VTTEK . Thời gian một năm thực tập ở đây , team đã hỗ trợ em rất nhiều trong việc tiếp cận và",
    "acronyms": [
      [
        57,
        62
      ]
    ],
    "long-forms": [
      [
        0,
        54
      ]
    ],
    "ID": "557"
  },
  {
    "text": "4 CÁC MÔ HÌNH ĐỀ XUẤT ( 34 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "558"
  },
  {
    "text": "là trị riêng lớn nhất của nó thì : xT ∇2 L1 ( w1∗ ) x ≤ 1",
    "acronyms": [],
    "long-forms": [],
    "ID": "559"
  },
  {
    "text": "So sánh hiệu năng mô hình HAN trên tập các thuật toán tối ưu hóa khác nhau Adagrad có learning rate là 0.01 cho độ chính xác trên tập huấn luyện bắt đầu cao",
    "acronyms": [
      [
        26,
        29
      ]
    ],
    "long-forms": [],
    "ID": "560"
  },
  {
    "text": "W2 + b2 . ( 2.15 ) Tuy nhiên , thay vì sử dụng một khối self-attention duy nhất , Transformer sử dụng h khối",
    "acronyms": [],
    "long-forms": [],
    "ID": "561"
  },
  {
    "text": "P∞ 2 P chúng ta có ∞",
    "acronyms": [],
    "long-forms": [],
    "ID": "562"
  },
  {
    "text": "để gán phân phối xác suất trên các chủ đề cho một văn bản chưa biết trước , do đó chúng không được xem là một mô hình sinh ( generative model ) hiệu quả . Mô hình Latent Dirichlet Allocation ( LDA ) được đề xuất sau đó và giải quyết",
    "acronyms": [
      [
        193,
        196
      ]
    ],
    "long-forms": [],
    "ID": "563"
  },
  {
    "text": "mô hình học biểu diễn đề xuất được thử nghiệm trên tập dữ liệu MIND có thể quan sát ở bảng dưới đây : Mô hình News Encoder",
    "acronyms": [
      [
        63,
        67
      ]
    ],
    "long-forms": [],
    "ID": "564"
  },
  {
    "text": "3 . 2 Phương pháp Streaming Variational Bayes 3 . 2 . 1 Cơ sở lý thuyết Streaming Variational Bayes ( SVB ) được đề xuất bởi Broderick và cộng sự [ 4 ] ,",
    "acronyms": [
      [
        102,
        105
      ]
    ],
    "long-forms": [],
    "ID": "565"
  },
  {
    "text": "Câu 1 : As the Dow average ground to its final 190.58 loss Friday , the S & P pit stay ed lock ed at its 30 - point trading limit . Câu 2 : 2,000 S& P contract s were for sale on the close , the equivalent of $ 330 million",
    "acronyms": [],
    "long-forms": [],
    "ID": "566"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 34",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "567"
  },
  {
    "text": "Lương [ 24 ] , vùng intron chiếm khoảng 5 % hệ gene , khoảng 1 % là vùng mã hóa protein , 0.5 % hệ gene là các vùng điều hòa và không mã hóa RNA nằm giữa các gene . 99.9 % hệ",
    "acronyms": [
      [
        141,
        144
      ]
    ],
    "long-forms": [],
    "ID": "568"
  },
  {
    "text": "Lớp CNTT 2.03 – K59 Giảng viên hướng dẫn : HÀ NỘI 05/2019",
    "acronyms": [
      [
        4,
        8
      ]
    ],
    "long-forms": [],
    "ID": "569"
  },
  {
    "text": "• Đầu tiên , mô hình đề xuất một tập hợp các khu vực quan tâm bằng mạng tìm kiếm ( ROI ) hoặc mạng đề xuất khu vực ( RPN ) được chọn . Các vùng được đề",
    "acronyms": [
      [
        83,
        86
      ],
      [
        117,
        120
      ]
    ],
    "long-forms": [
      [
        94,
        114
      ],
      [
        45,
        61
      ]
    ],
    "ID": "570"
  },
  {
    "text": "( 42) Trong đồ án này , em chọn số K bằng 10 trong tất cả các độ đo để đánh giá kết quả .",
    "acronyms": [],
    "long-forms": [],
    "ID": "571"
  },
  {
    "text": "Convolution Neural Network – Mạng tích chập NLP Natural Language Processing – Xử lý ngôn ngữ tự nhiên",
    "acronyms": [
      [
        44,
        47
      ]
    ],
    "long-forms": [
      [
        78,
        101
      ]
    ],
    "ID": "572"
  },
  {
    "text": "Phương pháp đặt điều kiện lên 𝑞𝜙 ( 𝜃 ) như vừa trình bày còn được gọi là Suy diễn biến phân trên trường trung gian ( MFVI ) , nếu như mỗi 𝑞𝜙 ( 𝜃𝑗 ) là một phân phối Gauss thì phương",
    "acronyms": [
      [
        117,
        121
      ]
    ],
    "long-forms": [
      [
        73,
        114
      ]
    ],
    "ID": "573"
  },
  {
    "text": "4 phân kỳ Kullback - Leibler ( KL ) và được tính theo công thức sau 𝐾𝐿 ( 𝑞||𝑝 ) = 𝑞 ( 𝑥 )",
    "acronyms": [
      [
        31,
        33
      ]
    ],
    "long-forms": [
      [
        2,
        28
      ]
    ],
    "ID": "574"
  },
  {
    "text": "thuật toán phân cụm có chồng lấn ( mỗi gene có thể thuộc về nhiều hơn một cụm ) : Fuzzy C-means ( FCM ) và Gaussian Mixture Model ( GMM ) . Kết quả của mô hình",
    "acronyms": [
      [
        98,
        101
      ],
      [
        132,
        135
      ]
    ],
    "long-forms": [],
    "ID": "575"
  },
  {
    "text": "entropy càng cao càng thể hiện độ hỗn loạn và không chắc của tham số Θ , điều này là phù hợp và cần thiết với môi trường động của dữ liệu dòng . Entropy của",
    "acronyms": [],
    "long-forms": [],
    "ID": "576"
  },
  {
    "text": "được mô hình hóa bởi phân phối p ( β t +1 k |β k ) ∼ N ( β k , σ I ) . Vì phương sai này",
    "acronyms": [],
    "long-forms": [],
    "ID": "577"
  },
  {
    "text": "48 Thuật toán 5 Phương pháp học iDropout cho mô hình Bayesian Đầu vào : Dòng dữ liệu với các minibatch { D1 , D2 , ... , Dt , ... } , các tham số drop",
    "acronyms": [],
    "long-forms": [],
    "ID": "578"
  },
  {
    "text": "chính xác trung bình Hình 5 . 1 : Độ chính xác trung bình khi kết thúc mỗi tác vụ trên tập Split MNIST và Permuted MNIST .",
    "acronyms": [
      [
        97,
        102
      ],
      [
        115,
        120
      ]
    ],
    "long-forms": [],
    "ID": "579"
  },
  {
    "text": "không độc lập và đồng nhất ( non-iid ) từ một phân phối Q giả định nào đó . Chú ý rằng phân phối này có thể thay đổi dần dần hoặc đột ngột xuyên suốt quá trình",
    "acronyms": [],
    "long-forms": [],
    "ID": "580"
  },
  {
    "text": "nghiệm ở tác vụ trước sẽ là tri thức tiên nghiệm cho việc học tác vụ hiện tại . ℒ 𝑡 ( 𝜃 ) = ℒ ( 𝐷𝑡 , 𝜃 ) + 𝐾𝐿 ( 𝑞𝑡 (𝜃 ) ||𝑞𝑡−1 (𝜃 ) ) PT 2.5",
    "acronyms": [
      [
        134,
        136
      ]
    ],
    "long-forms": [],
    "ID": "581"
  },
  {
    "text": "𝑔∈𝐺 Để làm rõ cách hoạt động của Latent Group Lasso , xét ví dụ : một tập dữ liệu X bao gồm các quan sát là vector bốn chiều x = ( x1 , x2 , x3 , x4 ) tương ứng với bốn hệ số",
    "acronyms": [],
    "long-forms": [],
    "ID": "582"
  },
  {
    "text": "Trong trường hợp tổng quát với việc học ở tác vụ thứ t , hàm mục tiêu của EWC có thể viết lại như sau : 𝑡−1",
    "acronyms": [
      [
        74,
        77
      ]
    ],
    "long-forms": [],
    "ID": "583"
  },
  {
    "text": "ĐỒ ÁN TỐT NGHIỆP Học liên tục dựa trên tham số cục bộ LÊ HẢI NAM",
    "acronyms": [],
    "long-forms": [],
    "ID": "584"
  },
  {
    "text": "EWC [ 5 ] là phương pháp đầu tiên đề xuất phạt sự thay đổi của các tham số quan trọng của mạng đối với một tác vụ để giữ tri thức của tác vụ đó . Tư tưởng chính",
    "acronyms": [
      [
        0,
        3
      ]
    ],
    "long-forms": [],
    "ID": "585"
  },
  {
    "text": "ITE - onehot và NMTR , tuy nhiên độ đo có ý nghĩa về tính xếp hạng hơn là NDCG @ K thì mô hình MTMF lại kém hơn khi k < = 32 . Nhưng có thể",
    "acronyms": [
      [
        0,
        12
      ],
      [
        16,
        20
      ],
      [
        74,
        78
      ],
      [
        95,
        99
      ]
    ],
    "long-forms": [],
    "ID": "586"
  },
  {
    "text": "Cấu hình máy thực hiện đồ án : • CPU : Intel ( R ) Xeon ( R ) CPU @ 2.30GHz",
    "acronyms": [
      [
        33,
        36
      ],
      [
        62,
        65
      ],
      [
        72,
        75
      ]
    ],
    "long-forms": [],
    "ID": "587"
  },
  {
    "text": "Bảng 3 . 3 : Kết quả của mô hình sử dụng dual learning áp dụng mô hình T ransf ormer − base với beam size 5 và tham số phạt α = 2.0 RNN - base",
    "acronyms": [
      [
        132,
        135
      ]
    ],
    "long-forms": [],
    "ID": "588"
  },
  {
    "text": "kế các hàm mất mát heuristic để ràng buộc sự thay đổi của trọng số và của nơ-ron . • AGS - CL : [ 15 ] tìm độ quan trọng của nơ-ron và sử dụng tính chất thưa của",
    "acronyms": [
      [
        85,
        93
      ]
    ],
    "long-forms": [],
    "ID": "589"
  },
  {
    "text": "1 ) Các khái niệm và kí hiệu Chúng ta định nghĩa một số thuật ngữ sau : • Các từ là phần tử cơ bản tạo thành văn bản , được đánh chỉ số bởi { 1 , 2 , ... , V }",
    "acronyms": [],
    "long-forms": [],
    "ID": "590"
  },
  {
    "text": "11 12 Đầu vào : Hai bộ dữ liệu đơn ngữ DA và DB , trạng thái khởi tạo của hai mô hình",
    "acronyms": [],
    "long-forms": [],
    "ID": "591"
  },
  {
    "text": "Khi quá trình học hội tụ , độ quan trọng của mỗi nơ-ron được định nghĩa thông qua giá trị SNR : SN R ( Ei ) = |µσii |",
    "acronyms": [
      [
        90,
        93
      ]
    ],
    "long-forms": [],
    "ID": "592"
  },
  {
    "text": "khác nhau của những bức ảnh khác nhau thì đó chính là cặp negative . Pretext task chung của CL đó là phân biệt được cặp nào là cặp positive , cặp nào là cặp negative .",
    "acronyms": [
      [
        92,
        94
      ]
    ],
    "long-forms": [],
    "ID": "593"
  },
  {
    "text": "đó với khả năng xảy ra tại tác vụ T hiện tại , và nhân với hệ số chuẩn hóa . Trong hầu hết các trường hợp , phân phối hậu nghiệm thường là các phân phối rất phức",
    "acronyms": [],
    "long-forms": [],
    "ID": "594"
  },
  {
    "text": "• Với mỗi văn bản d = 1 , ... , D : θ̂dk ∝ γdk ( chuẩn hóa vectơ K chiều γd ) • Với mỗi topic k = 1 , ... , K : β ̂kj ∝ λkj ( chuẩn hóa vectơ V chiều λk )",
    "acronyms": [],
    "long-forms": [],
    "ID": "595"
  },
  {
    "text": "Trong một bài báo khác , bài BYOL ( ở [ Gri+20 ] ) , thay vì sử dụng cả cặp positive và negative như thông thường , bài này lược bỏ bớt cặp negative và giải quyết vấn đề các biểu diễn ẩn bị kéo gần về một điểm của biểu diễn ẩn",
    "acronyms": [
      [
        29,
        33
      ]
    ],
    "long-forms": [],
    "ID": "596"
  },
  {
    "text": "Theo định lý Bayes , phân phối tham số của mô hình khi có đã quan sát được tập dữ liệu ( hay còn gọi là phân phối hậu nghiệm ) : p ( θ | D ) =",
    "acronyms": [],
    "long-forms": [],
    "ID": "597"
  },
  {
    "text": "learning , OCI ) [ 20 ] . Với kịch bản ODI thì mô hình có thể cần có cơ chế tìm ra biên giữa các tác vụ để biết khi nào dữ liệu đang đến là của tác vụ mới để có chiến lược",
    "acronyms": [
      [
        11,
        14
      ],
      [
        39,
        42
      ]
    ],
    "long-forms": [],
    "ID": "598"
  },
  {
    "text": "3 BÀI TOÁN HỆ GỢI Ý VỚI DỮ LIỆU HÀNH VI TIỀM ẨN VÀ RÕ RÀNG ( IMPLICIT , EXPLICIT BEHAVIOR )",
    "acronyms": [],
    "long-forms": [],
    "ID": "599"
  },
  {
    "text": "Thuật toán 3 Phương pháp học SVB - PP cho LDA Đầu vào : Dòng dữ liệu với các minibatch { D1 , D2 , ... , Dt , ... } , các tham số η , α , ρ Đầu ra : Tham số biến phân toàn cục λ",
    "acronyms": [
      [
        29,
        37
      ],
      [
        42,
        45
      ]
    ],
    "long-forms": [],
    "ID": "600"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 21",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "601"
  },
  {
    "text": "trúc mạng bị đầy nhanh hơn ở các tập khác 5 . 7 , từ tác vụ thứ 6 trở đi là gần như đầy , tuy nhiên kết quả vẫn cho thấy độ hiệu quả của VBD - CL .",
    "acronyms": [
      [
        137,
        145
      ]
    ],
    "long-forms": [],
    "ID": "602"
  },
  {
    "text": "dòng Bayesian khác chưa giải quyết được . Phần thực nghiệm cũng sẽ chứng tỏ Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        136,
        147
      ]
    ],
    "long-forms": [],
    "ID": "603"
  },
  {
    "text": "hình 30 : : ACD ca sử dụng đánh giá công việc theo số sao 3.4 . 10 . Đăng bài tuyển dụng",
    "acronyms": [
      [
        12,
        15
      ]
    ],
    "long-forms": [],
    "ID": "604"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 26 η",
    "acronyms": [
      [
        60,
        71
      ]
    ],
    "long-forms": [],
    "ID": "605"
  },
  {
    "text": "HÀ NỘI , 6/2021 ĐỀ TÀI TỐT NGHIỆP 1 . Thông tin về sinh viên",
    "acronyms": [],
    "long-forms": [],
    "ID": "606"
  },
  {
    "text": "1 Ở hính 1 . 1 Các từ có kích thước càng lớn thể hiện xác suất văn bản chứa từ đó càng lớn , do đó kích thước các từ trong ảnh thể hiện tính đại diện cho chủ đề .",
    "acronyms": [],
    "long-forms": [],
    "ID": "607"
  },
  {
    "text": "của mô hình , và VBD cung cấp một cơ chế hiệu quả đánh giá độ quan trọng của nơ-ron , đưa ra một cơ chế trực tiếp giảm thiểu mức độ quên . Thứ nhất , Dropout nói chung và VBD nói riêng có khả năng tìm ra được vùng",
    "acronyms": [
      [
        17,
        20
      ],
      [
        171,
        174
      ]
    ],
    "long-forms": [],
    "ID": "608"
  },
  {
    "text": "• Số điện thoại : 0919020291 • Hệ đào tạo : KSCLC - TN - TT - VN - • Lớp : Tài năng Khoa học máy tính",
    "acronyms": [
      [
        44,
        64
      ]
    ],
    "long-forms": [],
    "ID": "609"
  },
  {
    "text": "[ 11 ] giải quyết vấn đề của SVI bằng cách giả sử toàn bộ dòng dữ liệu được sinh ra từ Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        29,
        32
      ],
      [
        147,
        158
      ]
    ],
    "long-forms": [],
    "ID": "610"
  },
  {
    "text": "Ta có các bước cơ bản để xây dựng các vec -tơ biểu diễn cho dữ liệu : • Xây dựng bộ từ vựng , gọi độ lớn bộ từ vựng là VOCABULARY _ SIZE . • Đóng khung dữ liệu : Dữ liệu ban đầu có thể có kích thước khác nhau nhưng đầu",
    "acronyms": [],
    "long-forms": [],
    "ID": "611"
  },
  {
    "text": "đẹp thì nhiều khả năng tất cả các mẫu sinh ra đều trọc đầu hoặc mạng AE sẽ sinh ra những tấm hình như hình 3 Hình 3 : Kết quả sinh ảnh của mạng Auto Encoder",
    "acronyms": [
      [
        69,
        71
      ]
    ],
    "long-forms": [],
    "ID": "612"
  },
  {
    "text": "N M Hình 2 : Biểu diễn đồ thị xác suất của mô hình LDA",
    "acronyms": [
      [
        51,
        54
      ]
    ],
    "long-forms": [],
    "ID": "613"
  },
  {
    "text": "McNicholas ( 2010 ) [ 71 ] giới thiệu EPGMM , một công cụ dựa trên GMM , cho phép mô hình hóa mối tương quan giữa các mức độ biểu hiện gen",
    "acronyms": [
      [
        38,
        43
      ],
      [
        67,
        70
      ]
    ],
    "long-forms": [],
    "ID": "614"
  },
  {
    "text": "GM F và GMF và MLP . Tương tự với qG",
    "acronyms": [
      [
        8,
        11
      ],
      [
        15,
        18
      ],
      [
        0,
        2
      ],
      [
        34,
        36
      ]
    ],
    "long-forms": [],
    "ID": "615"
  },
  {
    "text": "Đặt điểm thưởng toàn phần cho quan sát thứ k là rk = αr1,k + ( 1 − α ) r2 , k ; end Tính stochastic gradient của ΘAB :",
    "acronyms": [],
    "long-forms": [],
    "ID": "616"
  },
  {
    "text": "Việc giới hạn lại số lượng tham số cho ALV sẽ được để lại cho công việc trong tương lai và đề xuất ý tưởng trong 5 . 2 . 2 . 40",
    "acronyms": [
      [
        39,
        42
      ]
    ],
    "long-forms": [],
    "ID": "617"
  },
  {
    "text": "viết quy tắc chuỗi đạo hàm cho tensor . Với Y = g ( X ) và z = f ( Y ) thì : ∇X z =",
    "acronyms": [],
    "long-forms": [],
    "ID": "618"
  },
  {
    "text": ". Hàm mục tiêu : Hàm mục tiêu chung cho các mô hình ITE được phát biểu như sau :",
    "acronyms": [
      [
        52,
        55
      ]
    ],
    "long-forms": [],
    "ID": "619"
  },
  {
    "text": "𝑇 sao cho : 𝛽 ∗ = argmin ‖𝑦 − 𝑋𝛽 ‖ 22 ( 1)",
    "acronyms": [],
    "long-forms": [],
    "ID": "620"
  },
  {
    "text": "W = { w1 , w2 , · · · , wM } tương ứng với tập nhãn lớp C = { c1 , c2 , · · · , cM } . 2 ) Mô hình sinh",
    "acronyms": [],
    "long-forms": [],
    "ID": "621"
  },
  {
    "text": "tự K-means : 𝑁 𝐾",
    "acronyms": [],
    "long-forms": [],
    "ID": "622"
  },
  {
    "text": "trong đó S là ma trận đường chéo gồm K giá trị riêng lớn nhất của ma trận document-term . Với các ma trận DOC [ D×K ] và T OP IC [ K×V ] tìm được , chúng ta",
    "acronyms": [],
    "long-forms": [],
    "ID": "623"
  },
  {
    "text": "Khi đó chúng ta có thể tính toán được đạo hàm của L̂ đối với wt dưới dạng tường minh sau :",
    "acronyms": [],
    "long-forms": [],
    "ID": "624"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 48 finally",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "625"
  },
  {
    "text": "thấy RMS prop cho độ chính xác trên tập validation cao nhất tại epoch 3 là 0.8940 và giảm dần không có dấu hiệu tăng . Đồng thời độ chính xác trên tập huấn luyện tại",
    "acronyms": [
      [
        5,
        8
      ]
    ],
    "long-forms": [],
    "ID": "626"
  },
  {
    "text": "được ký hiệu là L ( θ ) . Bài toán tối thiểu hàm mất mát để tìm tập thâm số của mô hình được 11",
    "acronyms": [],
    "long-forms": [],
    "ID": "627"
  },
  {
    "text": "Split CIFAR100 : Tập dữ liệu gốc CIFAR100 [ 37 ] gồm 100 lớp với mỗi lớp là một vật thể ( ví dụ máy bay , tàu thủy , v.v) . Có tổng cộng 60000 ảnh trong tập huấn luyện",
    "acronyms": [
      [
        6,
        14
      ],
      [
        33,
        41
      ]
    ],
    "long-forms": [],
    "ID": "628"
  },
  {
    "text": "Tầng Word Embedding cũng tương tự như Title Encoder , chuyển chuỗi các từ [ 𝑤1𝑏 , 𝑤2𝑏 , . . . , 𝑤𝑃𝑏 ] thành tập các vector từ tương ứng [ 𝑒1𝑏 , 𝑒2𝑏 , . . . , 𝑒𝑃𝑏 ] , trong đó P là số từ trong nội dung bài báo , giá trị P trong mô hình sẽ được điều chỉnh để lấy P",
    "acronyms": [],
    "long-forms": [],
    "ID": "629"
  },
  {
    "text": "Ma trận đánh giá của người dùng và sản phẩm Ma trận nhân tố ẩn của toàn bộ người dùng P",
    "acronyms": [],
    "long-forms": [],
    "ID": "630"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 27 Sử dụng ELMo đã được huấn luyện với đầu vào là một chuỗi các ký tự cho mỗi từ trong",
    "acronyms": [
      [
        61,
        70
      ],
      [
        87,
        91
      ]
    ],
    "long-forms": [],
    "ID": "631"
  },
  {
    "text": "trong đó Stochastic Variational Inference ( SVI ) [ 8 ] là một ví dụ điển hình . Tuy nhiên , SVI là một phương pháp học trực tuyến ( online ) , nó yêu cầu một bộ dữ",
    "acronyms": [
      [
        44,
        47
      ],
      [
        93,
        96
      ]
    ],
    "long-forms": [],
    "ID": "632"
  },
  {
    "text": "Nhận xét Mô hình NRMS được xây dựng dựa trên Deep Learning đã khai thác khá tốt các đặc điểm của bài toán gợi ý tin tức , mô hình đã tập trung vào các đặc trưng cơ",
    "acronyms": [
      [
        17,
        21
      ]
    ],
    "long-forms": [],
    "ID": "633"
  },
  {
    "text": "đến các mô hình gặp hiện tượng overfitting sớm như vậy . ( a ) HR @ 10",
    "acronyms": [
      [
        63,
        65
      ]
    ],
    "long-forms": [],
    "ID": "634"
  },
  {
    "text": "75.3 Bảng 6 : Kết quả của biểu diễn ẩn khi so sánh các phương pháp SOTA hiện nay ( Nguồn [ Car+20 ] )",
    "acronyms": [
      [
        67,
        71
      ]
    ],
    "long-forms": [],
    "ID": "635"
  },
  {
    "text": "45 5 THỬ NGHIỆM VÀ ĐÁNH GIÁ",
    "acronyms": [],
    "long-forms": [],
    "ID": "636"
  },
  {
    "text": "Hình 4 . 11 Độ chính xác trên từng tác vụ trong thử nghiệm với VCL 36",
    "acronyms": [
      [
        63,
        66
      ]
    ],
    "long-forms": [],
    "ID": "637"
  },
  {
    "text": "Như vây với cách tiếp cận xác suất cho NCF , bài toán gợi ý giống như một bài toán phân loại nhị phân .",
    "acronyms": [
      [
        39,
        42
      ]
    ],
    "long-forms": [],
    "ID": "638"
  },
  {
    "text": "SSL và semi-supervised learning , trước đây cũng đã có một bài người ta đã làm và rất nổi tiếng đó là bài Deep Clustering [ Car + 18 ] . Hình 17 : Mô hình Deep Clustering ( Nguồn [ Car + 18 ] )",
    "acronyms": [
      [
        0,
        3
      ]
    ],
    "long-forms": [],
    "ID": "639"
  },
  {
    "text": "end for * Bước cập nhật tham số toàn cục * Chọn ρt = ( τ0 + t ) κ P",
    "acronyms": [],
    "long-forms": [],
    "ID": "640"
  },
  {
    "text": "số lượng click trong top K tổng số click trong tập test",
    "acronyms": [],
    "long-forms": [],
    "ID": "641"
  },
  {
    "text": ". . . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 16",
    "acronyms": [
      [
        66,
        77
      ]
    ],
    "long-forms": [],
    "ID": "642"
  },
  {
    "text": "Vì vậy trong phạm vi đề tài này , em muốn đề xuất tìm hiểu , cài đặt , đánh giá mô hình HAN cho bài toán phân loại cảm xúc văn bản tiếng Việt .",
    "acronyms": [
      [
        88,
        91
      ]
    ],
    "long-forms": [],
    "ID": "643"
  },
  {
    "text": "Phần này tiến hành hai thử nghiệm tương ứng với hai loại dữ liệu trên bài toán học không giám sát , cụ thể là học chủ đề ẩn của một tập các văn bản dựa trên mô hình LDA .",
    "acronyms": [
      [
        165,
        168
      ]
    ],
    "long-forms": [],
    "ID": "644"
  },
  {
    "text": "2 . 2 Các hệ số a , b , C có thể chọn sao cho các hệ số w có thể âm và kiểm soát độ",
    "acronyms": [],
    "long-forms": [],
    "ID": "645"
  },
  {
    "text": "trong các nhãn đúng được tính là một ví dụ đúng . 4 . 2 . 2 Tham số chi tiết cho mô hình Ở đây , em sử dụng pre-trained word embedding là 300-dim word 2vec ( Mikolov et al. ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "646"
  },
  {
    "text": "\" OccurrenceOffsets \" : [ 35 ] , \" SurfaceForms \" : [ \" PGA Tour \" ] } ] Bảng 4 . 2 Mô tả thuộc tính bài báo tập MIND",
    "acronyms": [
      [
        113,
        117
      ]
    ],
    "long-forms": [],
    "ID": "647"
  },
  {
    "text": "Như đã đề cập ở phần mục tiêu và phạm vi đề tài , mô hình mà em đề xuất để giải quyết bài toán phân loại cảm xúc văn bản là mô hình HAN ( Hierarchical Attention 2",
    "acronyms": [
      [
        132,
        135
      ]
    ],
    "long-forms": [],
    "ID": "648"
  },
  {
    "text": "MỘT SỐ MÔ HÌNH GỢI Ý TIN TỨC Hệ thống gợi ý ( Recommender Systems ) : là một mảng quan trọng trong học",
    "acronyms": [],
    "long-forms": [],
    "ID": "649"
  },
  {
    "text": "• Các từ là phần tử cơ bản tạo thành văn bản , được đánh chỉ số bởi { 1 , 2 , ... , V } trong một tập từ điển có kích thước V . • Mỗi văn bản được kí hiệu là d và có nhãn cd ∈ { 1 , 2 , ... , C } tương ứng , trong",
    "acronyms": [],
    "long-forms": [],
    "ID": "650"
  },
  {
    "text": "( 𝜃 | 𝐷𝐴 , 𝐷𝐵 ) = log 𝑝 ( 𝐷𝐵 | 𝜃 ) + log 𝑝 ( 𝜃 | 𝐷𝐴 ) − log 𝑝 ( 𝐷𝐵 ) PT 2.9 Trong công thức PT 2.9 , đại lượng bên trái là xác suất hậu nghiệm của tham số",
    "acronyms": [
      [
        92,
        94
      ],
      [
        69,
        71
      ]
    ],
    "long-forms": [],
    "ID": "651"
  },
  {
    "text": "Mạng xã hội ( MXH ) là một đồ thị mô tả sự tương tác giữa các cá thể có cùng mối quan tâm , có liên hệ trực tiếp hay gián tiếp . Theo định nghĩa [ 2 ] , Mạng xã",
    "acronyms": [
      [
        14,
        17
      ]
    ],
    "long-forms": [
      [
        0,
        11
      ]
    ],
    "ID": "652"
  },
  {
    "text": "4.2.5 Baseline Sau khi cấu hình cho mô hình HAN để đạt được kết quả tốt nhất trên tập validation , em tiến hành so sánh , đánh giá hiệu năng của mô hình HAN với những mô hình học",
    "acronyms": [
      [
        44,
        47
      ],
      [
        153,
        156
      ]
    ],
    "long-forms": [],
    "ID": "653"
  },
  {
    "text": "Pmột tập dữ liệu . Tham số α sẽ được chia đều cho các mô hình ngôn ngữ , tức là LM (s) = K1 K",
    "acronyms": [],
    "long-forms": [],
    "ID": "654"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 51",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "655"
  },
  {
    "text": "Cũng giống như LSI , pLSI có số lượng tham số của mô hình là tuyến tính theo số lượng văn bản D . Điều này khiến mô hình có thể dễ gặp phải overfitting ,",
    "acronyms": [
      [
        15,
        18
      ],
      [
        21,
        25
      ]
    ],
    "long-forms": [],
    "ID": "656"
  },
  {
    "text": "kết quả của LCL vẫn tốt hơn ổn định so với SimCLR gốc . Kịch bản 3 : Quan sát sự thay đổi mô hình khi đưa thêm các tham số a , b , C vào để điều chỉnh w .",
    "acronyms": [
      [
        12,
        15
      ],
      [
        43,
        49
      ]
    ],
    "long-forms": [],
    "ID": "657"
  },
  {
    "text": "Chúng ta sẽ chứng minh rằng đại lượng Eζ [ A( s̃k ) ] − A ( sk ) ) Vj=1 ukj tương đương với một phép chính quy hoá . Thật vậy , sử dụng xấp xỉ Taylor bậc hai , ta có :",
    "acronyms": [],
    "long-forms": [],
    "ID": "658"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 15",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "659"
  },
  {
    "text": "Bảng 5 . 6 : Phần trăm không gian mạng CNN sử dụng qua các tác vụ trên tập Split CIFAR100 và Split CIFAR10 - 100",
    "acronyms": [
      [
        39,
        42
      ],
      [
        81,
        89
      ],
      [
        99,
        112
      ]
    ],
    "long-forms": [],
    "ID": "660"
  },
  {
    "text": "thực hiện quá trình sinh các từ đầu ra , kết quả biểu diễn của các zi sẽ được đưa trở lại decoder để tiếp tục sinh ra các từ phía sau . Đầu ra của lớp thứ i được ký hiệu là E i = ( ei1 , ei2 , . . . , ein )",
    "acronyms": [],
    "long-forms": [],
    "ID": "661"
  },
  {
    "text": "Khởi tạo ngẫu nhiên Θ = Θ0 for t = 1,2 , ... , T , ... do Nhận dữ liệu ở minibatch thứ t là Dt",
    "acronyms": [],
    "long-forms": [],
    "ID": "662"
  },
  {
    "text": "3.4 Sử dụng mạng Multi-layer perceptron ( MLP ) cho bộ mã hóa và bộ giải mã",
    "acronyms": [
      [
        42,
        45
      ]
    ],
    "long-forms": [],
    "ID": "663"
  },
  {
    "text": "6 Hình 2 . 1 Mô hình NRMS",
    "acronyms": [
      [
        21,
        25
      ]
    ],
    "long-forms": [],
    "ID": "664"
  },
  {
    "text": "Trong các phương pháp của CL mà em được biết , em lựa chọn phương pháp SimCLR . Do đó , đồ án này xoay quanh bài toán kết hợp manifold learning vào",
    "acronyms": [
      [
        26,
        28
      ],
      [
        71,
        77
      ]
    ],
    "long-forms": [],
    "ID": "665"
  },
  {
    "text": "trình bày ở phần đầu . Tuy nhiên chúng ta cũng thấy rằng trên các bộ TMN , TMN - title , The Irish Times thì iDropout với dr = 0 cũng có hiện",
    "acronyms": [
      [
        69,
        72
      ],
      [
        75,
        78
      ]
    ],
    "long-forms": [],
    "ID": "666"
  },
  {
    "text": "Cuối cùng , NPMI cho toàn bộ K chủ đề là : Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 58",
    "acronyms": [
      [
        12,
        16
      ],
      [
        103,
        114
      ]
    ],
    "long-forms": [],
    "ID": "667"
  },
  {
    "text": "Tương tự như đối với tác tử thứ nhất , kênh này sẽ chuyển thông điệp từ ngôn ngữ B sang ngôn ngữ A bằng một mô hình dịch .",
    "acronyms": [],
    "long-forms": [],
    "ID": "668"
  },
  {
    "text": "Bảng 0.1 Tham số tốt nhất cho PMNIST EWC",
    "acronyms": [
      [
        30,
        36
      ],
      [
        37,
        40
      ]
    ],
    "long-forms": [],
    "ID": "669"
  },
  {
    "text": "Đồng thời cũng xin gửi lời cảm ơn tới các em Nguyễn Đức Tùng , Nguyễn Bá Khải - sinh viên lớp CNTT2.04 Khoá 60 và anh Trương Giang Khang - cựu sinh viên lớp KSTN CNTT Khoá 58 , trường đại học Bách Khoa Hà Nội , cũng đều là",
    "acronyms": [
      [
        94,
        98
      ],
      [
        157,
        166
      ]
    ],
    "long-forms": [],
    "ID": "670"
  },
  {
    "text": "( 21 ) 𝐾 điều kiện ∑ 𝜙𝑘 = 1",
    "acronyms": [],
    "long-forms": [],
    "ID": "671"
  },
  {
    "text": "Sử ảnh hưởng của đại lượng σ 2 lên hiệu suất của iDropout là rất rõ ràng trong hình 9 Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        146,
        157
      ]
    ],
    "long-forms": [],
    "ID": "672"
  },
  {
    "text": "Phương pháp Hierarchical Power Priors ( HPP ) được trình bày dưới đây có thể giải quyết tốt các vấn đề đó . Các tác giả của HPP có ý tưởng sử dụng một mô hình chuyển dịch ( transition",
    "acronyms": [
      [
        40,
        43
      ],
      [
        124,
        127
      ]
    ],
    "long-forms": [],
    "ID": "673"
  },
  {
    "text": "những đặc điểm quan trọng trong hành vi người dùng và đưa ra gợi ý thích hợp . Thử nghiệm cho thấy mô hình học biểu diễn đề xuất cho kết quả tốt hơn so với LDA truyền thống , các văn bản có tính diễn giải theo chủ đề , đồng thời áp dụng",
    "acronyms": [
      [
        156,
        159
      ]
    ],
    "long-forms": [],
    "ID": "674"
  },
  {
    "text": "Chữ ký của GVHD Bộ môn : Hệ thống thông tin",
    "acronyms": [
      [
        11,
        15
      ]
    ],
    "long-forms": [],
    "ID": "675"
  },
  {
    "text": "Độ đo HR@K mới chỉ quan tâm đến sự xuất hiện của test item trong top K . Độ đo NDCG quan tâm đến cả xếp hạng của test item trong",
    "acronyms": [
      [
        79,
        83
      ],
      [
        6,
        10
      ]
    ],
    "long-forms": [],
    "ID": "676"
  },
  {
    "text": "45.82 48.6 Bảng 6 : Kết quả nghiên cứu sự ảnh hưởng của các hàm lỗi L1 , L2 và L3 lên kết quả .",
    "acronyms": [],
    "long-forms": [],
    "ID": "677"
  },
  {
    "text": "- Thể loại thường là cụm từ . Từ những lập luận trên , tác giã đã đề xuất mô hình NAML nhằm khai thác được",
    "acronyms": [
      [
        82,
        86
      ]
    ],
    "long-forms": [],
    "ID": "678"
  },
  {
    "text": "TÓM TẮT NỘI DUNG ĐỒ ÁN TỐT NGHIỆP Trong kỷ nguyên phát triển của internet , trong đó phải kể đến các trang mạng xã hội , các diễn đàn , thương mại điện tử hay trang tin",
    "acronyms": [],
    "long-forms": [],
    "ID": "679"
  },
  {
    "text": "Hàm aout và h tương tự như trong mô hình GMF . Đối với các hàm kích hoạt ax , ta có thể tùy ý lựa",
    "acronyms": [
      [
        41,
        44
      ]
    ],
    "long-forms": [],
    "ID": "680"
  },
  {
    "text": "G ( Θt ) và tối ưu nó sử dụng một thuật toán cập nhật dựa trên gradient . Giải thuật sau mô tả phương pháp học của iDropout cho mô hình Bayesian tổng quát .",
    "acronyms": [],
    "long-forms": [],
    "ID": "681"
  },
  {
    "text": "hình : 1T ( ( σtl ) 2 − log ( σtl ) 2 ) . Ràng buộc này cùng với ( b ) đạt cực tiểu khi σtl = 2σt −1 .",
    "acronyms": [],
    "long-forms": [],
    "ID": "682"
  },
  {
    "text": "Dirichlet tiên nghiệm của β t+1 , tức là : p ( β t+1 | D1 , D2 , ... , Dt , η ) ≈ q ( βkt +1 | λt ) = Dir ( βkt +1 | λt ) ( 20 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "683"
  },
  {
    "text": "KL ( q ( z , Θ ) | |p( z , Θ|x) ) là nhỏ nhất . Rõ ràng điều này là không phù hợp với dữ liệu dòng có kích thước không lường trước được .",
    "acronyms": [],
    "long-forms": [],
    "ID": "684"
  },
  {
    "text": "Hình dưới là kết quả huấn luyện với những learning rate schedule s khác nhau . Hình 8 So sánh hiệu năng của mô hình HAN trên các learning rate schedule s khác nhau",
    "acronyms": [
      [
        116,
        119
      ]
    ],
    "long-forms": [],
    "ID": "685"
  },
  {
    "text": "Phần này bao gồm các phương pháp từ rất sơ khai như Denoising auto encoder , split -brain autoencoders cho đến những phương pháp có thể xem là hiện đại nhất ( SOTA ) trong hướng nghiên cứu này trong một năm trờ lại đây như",
    "acronyms": [
      [
        159,
        163
      ]
    ],
    "long-forms": [],
    "ID": "686"
  },
  {
    "text": "Đối với MTL thì việc các tác vụ chia sẻ chung một tầng ẩn nó làm tăng dữ liệu học cho mô hình . Trong khi học một tác vụ , mô hình cố gắng tối ưu sao cho đạt được kết quả",
    "acronyms": [
      [
        8,
        11
      ]
    ],
    "long-forms": [],
    "ID": "687"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT . .",
    "acronyms": [
      [
        60,
        71
      ]
    ],
    "long-forms": [],
    "ID": "688"
  },
  {
    "text": "Đối với tập dữ liệu MIND , các mô hình được thử nghiệm với các phiên bản News Encoder khác nhau , nhằm đánh giá tác động của News Encoder và độ phù hợp của các cách tiếp cận cụ thể :",
    "acronyms": [
      [
        20,
        24
      ]
    ],
    "long-forms": [],
    "ID": "689"
  },
  {
    "text": "3.5 Mô hình học HAN........................................................................................... 15",
    "acronyms": [
      [
        16,
        19
      ]
    ],
    "long-forms": [],
    "ID": "690"
  },
  {
    "text": "xúc văn bản , có thể áp dụng cho các bài toán phân tích đánh giá của người dùng về sản phẩm , đồ ăn , nhà hàng . Bên cạnh đó , giá trị nghiên cứu của mô hình học HAN",
    "acronyms": [
      [
        162,
        165
      ]
    ],
    "long-forms": [],
    "ID": "691"
  },
  {
    "text": "p ( β | η ) p ( W | C , β ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        88,
        99
      ]
    ],
    "long-forms": [],
    "ID": "692"
  },
  {
    "text": "pháp sau : Phương pháp 1 : PDTB - Lin ( Lin et al. , 2009 ) sử dụng các phần 2 - 21 , 22 , 23 tương ứng làm tập dữ liệu training , dev , test .",
    "acronyms": [
      [
        27,
        37
      ],
      [
        131,
        134
      ]
    ],
    "long-forms": [],
    "ID": "693"
  },
  {
    "text": "đó nhóm các phương pháp tiếp cận theo hướng Bayesian Inference đã đạt được nhiều thành tựu đáng kể . Trong nhóm các phương pháp này , hai frame work variational continual learning ( VCL )",
    "acronyms": [
      [
        182,
        185
      ]
    ],
    "long-forms": [],
    "ID": "694"
  },
  {
    "text": "0 trong trường hợp còn lại 1 Chọn ngẫu nhiên K quan sát trong tập dữ liệu làm tâm cụm .",
    "acronyms": [],
    "long-forms": [],
    "ID": "695"
  },
  {
    "text": "Hình 2 . 3 Giải thuật cho VCL 11 VCL là một phương pháp tổng quát trong Học liên tục có khả năng giải quyết",
    "acronyms": [
      [
        26,
        29
      ],
      [
        33,
        36
      ]
    ],
    "long-forms": [],
    "ID": "696"
  },
  {
    "text": "được cập nhật và các key cũng sẽ lưu trữ những thông tin khác so với các key trước ) . Cập nhật bằng momentum : Một điểm khác biệt nữa của mô hình MOCO với",
    "acronyms": [
      [
        147,
        151
      ]
    ],
    "long-forms": [],
    "ID": "697"
  },
  {
    "text": "Đồng thời , phân bố dữ liệu trên tập Pega và tập MIND khác nhau . Do đó , các độ đo trên tập Pega sẽ có sự khác biệt lớn hơn trên",
    "acronyms": [
      [
        49,
        53
      ]
    ],
    "long-forms": [],
    "ID": "698"
  },
  {
    "text": "2.3. 4 Nhận xét Với cách khai thác vào sở thích ngắn hạn và sở thích dài hạn của User , mô hình LSTUR đã cho kết quả tốt hơn các mô hình Deep Learning đơn thuần khác . Tất",
    "acronyms": [
      [
        96,
        101
      ]
    ],
    "long-forms": [],
    "ID": "699"
  },
  {
    "text": "A ( sk ) = log Vi =1 exp ( ski ) . Giả sử ma trận drop π được lấy ngẫu nhiên từ biến thể ξ của phân phối Bernoulli , t = 1 ) = 1 − dr , p ( π t = 0 ) = dr , khi đó ta có :",
    "acronyms": [],
    "long-forms": [],
    "ID": "700"
  },
  {
    "text": "được gọi là ( variational ) cận dưới hay lower bound ( ELBO ) của xác suất biên khả năng xảy ra của điểm dữ liệu x ( i ) . Do đó , từ ( 3. 1 ) ta có thể suy ra :",
    "acronyms": [
      [
        55,
        59
      ]
    ],
    "long-forms": [],
    "ID": "701"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 29 4 CÁC MÔ HÌNH ĐỀ XUẤT",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "702"
  },
  {
    "text": "Các quan điểm trên về cơ bản tương tự mô hình LSTUR , tuy nhiên cách tiếp cận để khai thác các quan điểm sẽ dựa trên mô hình BERT . Kiến trúc tổng quan của mô hình đề xuất được minh họa bằng hình dưới đây :",
    "acronyms": [
      [
        46,
        51
      ],
      [
        125,
        129
      ]
    ],
    "long-forms": [],
    "ID": "703"
  },
  {
    "text": "pair ) . Trong bài báo người ta kết hợp 3 phép DA : randomcropping ( đưa ảnh về một kích cỡ cố định ) , random color distortions ( biến đổi màu của ảnh ) và",
    "acronyms": [
      [
        47,
        49
      ]
    ],
    "long-forms": [],
    "ID": "704"
  },
  {
    "text": "sánh với các phương pháp SSL khác hoặc các phương pháp học có giám sát tương ứng . Có nhiều cách để tính ra kết quả trong bước này , như đưa biểu diễn ẩn vào",
    "acronyms": [
      [
        25,
        28
      ]
    ],
    "long-forms": [],
    "ID": "705"
  },
  {
    "text": "Sau mỗi normalization layer ta sử dụng một kết nối residual với đầu ra là LayerNorm ( X+ SubLayer ( X) )",
    "acronyms": [],
    "long-forms": [],
    "ID": "706"
  },
  {
    "text": "K-means cùng với SOM và Hierarchical Clustering là ba thuật toán phân cụm được sử dụng phổ biến nhất được dùng cho loại dữ liệu này [ 51 ] .",
    "acronyms": [
      [
        17,
        20
      ]
    ],
    "long-forms": [],
    "ID": "707"
  },
  {
    "text": "vẫn nhỉnh hơn đáng kể ở hầu hết các giá trị của k . Mô hình ITE -onehot Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        60,
        71
      ],
      [
        135,
        146
      ]
    ],
    "long-forms": [],
    "ID": "708"
  },
  {
    "text": "Kịch bản gia tăng về miền dữ liệu Trong thử nghiệm này , bộ dữ liệu PMNIST sẽ được sử dụng như mô tả trong 4 . 1 . 1 và một kiến trúc đơn đầu ra sẽ được sử dụng trong quá trình thử nghiệm",
    "acronyms": [
      [
        68,
        74
      ]
    ],
    "long-forms": [],
    "ID": "709"
  },
  {
    "text": "VD chỉ ra một lựa chọn duy nhất có thể thỏa mãn yêu cầu đối với phân phối tiên nghiệm đó là một bất biến log - uniform :",
    "acronyms": [
      [
        0,
        2
      ]
    ],
    "long-forms": [],
    "ID": "710"
  },
  {
    "text": "liệu , nhưng lại chưa được sử dụng với mục đích tạo ra các bài toán liên quan đến CL . Ở các mô hình trước đây , các bài toán cho việc học CL thường được định nghĩa từ việc cắt các biểu diễn ẩn từ mạng để dự đoán địa phương từ toàn cục ( trong bài",
    "acronyms": [
      [
        82,
        84
      ],
      [
        139,
        141
      ]
    ],
    "long-forms": [],
    "ID": "711"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 6",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "712"
  },
  {
    "text": "cỡ embedding , h là số đầu của self-attention , df f là kích thước tầng ẩn đầu tiên của lớp FFN , t là thời gian huấn luyện và params là số lượng tham số cần huấn luyện T ransf ormer − small",
    "acronyms": [
      [
        92,
        95
      ]
    ],
    "long-forms": [],
    "ID": "713"
  },
  {
    "text": "đó ta định nghĩa lên hai thành phần của hàm mất mát là Reconstruction Loss và Adversarial Loss như sau : Lrec = ||M̃",
    "acronyms": [],
    "long-forms": [],
    "ID": "714"
  },
  {
    "text": "4 . 4 Mô hình ITE - 2 Ba mô hình tiếp theo sẽ xây dựng riêng biệt hơn cho hệ thống quảng cáo trực tuyến , nhưng chúng hoàn toàn có thể áp dụng cho những hệ thống có đặc điểm",
    "acronyms": [
      [
        14,
        21
      ]
    ],
    "long-forms": [],
    "ID": "715"
  },
  {
    "text": "VCL và UCL cũng chứng kiến những cải thiện rõ rệt , độ chính xác trung bình đạt được lần lượt là 98.67 % và 99.73 % . 33",
    "acronyms": [
      [
        0,
        3
      ],
      [
        7,
        10
      ]
    ],
    "long-forms": [],
    "ID": "716"
  },
  {
    "text": "Ta có : ( 1 , nếu như test item nằm trong top K .",
    "acronyms": [],
    "long-forms": [],
    "ID": "717"
  },
  {
    "text": "thước batch - size : 500 cho { 20NewsGroups , Grolier , TMN , TMN - title } , 5000 cho { Yahoo - title , NYT- title } . Chúng tôi đặt số topic K = 50 cho { 20NewsGroups ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "718"
  },
  {
    "text": "Với các mô hình học sử dụng mạng nơ-ron nhân tạo như CNN , LSTM không sử dụng cơ chế Attention , mặc dù , thể hiện độ chính xác cao hơn so với các method học máy bình thường , nhưng vẫn thấp hơn 2 - 3 % so với mô hình HAN .",
    "acronyms": [
      [
        53,
        56
      ],
      [
        59,
        63
      ],
      [
        218,
        221
      ]
    ],
    "long-forms": [],
    "ID": "719"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 26",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "720"
  },
  {
    "text": "Mà hàm mục tiêu 𝐸 ( 𝑊 ) gồm có hai phần đó là cực tiểu hóa hàm lỗi 𝐸𝐷 ( 𝑊 ) trên tập dữ liệu huấn luyện 𝐷 và cực tiểu hóa số lượng nơ-ron ở mỗi tầng . Vậy",
    "acronyms": [],
    "long-forms": [],
    "ID": "721"
  },
  {
    "text": "DANH MỤC TỪ VIẾT TẮT VÀ THUẬT NGỮ Attention Cơ chế tập trung ; Cơ chế giám sát có trọng số",
    "acronyms": [],
    "long-forms": [],
    "ID": "722"
  },
  {
    "text": "Từ kết quả trong bảng trên có thể thấy , mô hình NAML cho kết quả trên các độ đo vượt trội so với các mô hình còn lại , cụ thể : AUC vượt trội 3 % , MRR 3 % , nDCG @ 5 và nDCG @ 10 vượt trội 4 % .",
    "acronyms": [
      [
        49,
        53
      ],
      [
        129,
        132
      ],
      [
        149,
        152
      ],
      [
        159,
        163
      ],
      [
        171,
        175
      ]
    ],
    "long-forms": [],
    "ID": "723"
  },
  {
    "text": "học máy , học sâu trên mạng Internet , và rất nhiều tài liệu , blog về các kiến thức cơ bản trong những lĩnh vực này , tuy nhiên , vẫn chưa có một tài liệu cụ thể nào viết sâu và mang tính giới thiệu tổng hợp về hướng nghiên cứu SSL bằng tiếng Việt .",
    "acronyms": [
      [
        229,
        232
      ]
    ],
    "long-forms": [],
    "ID": "724"
  },
  {
    "text": "tăng cường sự ràng buộc dựa trên độ lớn của 𝐿 2",
    "acronyms": [],
    "long-forms": [],
    "ID": "725"
  },
  {
    "text": "Ngoài ra , một độ đo toàn cục mở rộng mới cho trung tâm bậc có tên là Tendency to Make Hub ( TMH ) được định nghĩa như sau : 23",
    "acronyms": [
      [
        93,
        96
      ]
    ],
    "long-forms": [],
    "ID": "726"
  },
  {
    "text": "Vấn đề vanishing variance : Theo cách học của SVB cho LDA , khi dữ liệu của minibatch t + 1 đến , λt sẽ được sử dụng làm tham số trong phân phối",
    "acronyms": [
      [
        46,
        49
      ],
      [
        54,
        57
      ]
    ],
    "long-forms": [],
    "ID": "727"
  },
  {
    "text": "Để phân loại câu vào các loại sự kiện , biểu diễn Si được đưa qua một mạng liên kết đầy đủ ( FC ) hai lớp . 2.3.",
    "acronyms": [
      [
        93,
        95
      ]
    ],
    "long-forms": [
      [
        70,
        90
      ]
    ],
    "ID": "728"
  },
  {
    "text": "hơn , đầy đủ hơn so với kết quả mô hình LDA trong hình 1 . 1 . Các chủ đề đã rõ ràng hơn , chi tiết hơn và các từ tập trung vào cùng một khía cạnh cụ thể thay vì",
    "acronyms": [
      [
        40,
        43
      ]
    ],
    "long-forms": [],
    "ID": "729"
  },
  {
    "text": "• Các mô hình ITE có thể mở rộng từ hai nhóm hành vi thành nhiều loại hành vi tương tự như mô hình NMTR . Như vậy , về mặt bản chất có thể",
    "acronyms": [
      [
        14,
        17
      ],
      [
        99,
        103
      ]
    ],
    "long-forms": [],
    "ID": "730"
  },
  {
    "text": "Topics coherence ( NPMI ) Hình 4 . 1 Đồ thị biểu diễn độ kết hợp các chủ đề khi thay đổi số lượng chủ đề trên tập dữ liệu Associated Press",
    "acronyms": [
      [
        19,
        23
      ]
    ],
    "long-forms": [],
    "ID": "731"
  },
  {
    "text": "• Email : honghaipvu@gmail.com • Điện thoại liên lạc : 035 854 7694 • Lớp : CNTT 2.4 K59",
    "acronyms": [
      [
        76,
        80
      ]
    ],
    "long-forms": [],
    "ID": "732"
  },
  {
    "text": "Trong đó T là số lượng tác vụ cho tới thời điểm hiện tại . Tuy nhiên , mô hình không 8",
    "acronyms": [],
    "long-forms": [],
    "ID": "733"
  },
  {
    "text": "thể tạo ra được một mạng F có biểu diễn ẩn được lấy đầu vào từ toàn bộ ảnh . Từ đó , ta cũng có thể kết hợp việc huấn luyện bằng cách sử dụng một hàm mất mát",
    "acronyms": [],
    "long-forms": [],
    "ID": "734"
  },
  {
    "text": "- Hibernate là một thư viện ORM ( Object Relational Mapping ) mã nguồn mở giúp lập trình viên viết ứng dụng có thể kết nối các đối tượng với hệ quản trị cơ sở dữ liệu quan hệ , và hỗ trợ thực hiện các khái niệm lập trình hướng",
    "acronyms": [
      [
        28,
        31
      ]
    ],
    "long-forms": [],
    "ID": "735"
  },
  {
    "text": "Topics coherence ( NPMI ) Số lần lấy mẫu",
    "acronyms": [
      [
        19,
        23
      ]
    ],
    "long-forms": [],
    "ID": "736"
  },
  {
    "text": "Lời cam đoan của sinh viên Tôi – Trần Thị Hồng - cam kết ĐATN là công trình nghiên cứu của bản thân tôi dưới sự",
    "acronyms": [
      [
        57,
        61
      ]
    ],
    "long-forms": [],
    "ID": "737"
  },
  {
    "text": "phân loại . Và hàm mất mát là hàm Negative Log- likelihood của tất cả các mẫu S , thể hiện bằng công thức sau :",
    "acronyms": [],
    "long-forms": [],
    "ID": "738"
  },
  {
    "text": "CL như MOCO và BYOL , với hi vọng , hướng đi này có thể cải tiến toàn bộ các phương pháp trong CL và mang lại một tư duy mới cho hướng nghiên cứu CL . Đồ án tốt nghiệp",
    "acronyms": [
      [
        0,
        2
      ],
      [
        7,
        11
      ],
      [
        15,
        19
      ],
      [
        95,
        97
      ],
      [
        146,
        148
      ]
    ],
    "long-forms": [],
    "ID": "739"
  },
  {
    "text": "Giảng viên hướng dẫn : ThS. Ngô Văn Linh HÀ NỘI 05/2019",
    "acronyms": [
      [
        23,
        27
      ]
    ],
    "long-forms": [],
    "ID": "740"
  },
  {
    "text": "ràng buộc gradient để giữ lại tri thức cho tác vụ cũ . Nhưng hai hạn chế khác của HAT đó là yêu cầu biết trước số lượng tác vụ , và mạng có xu hướng bị đầy sớm .",
    "acronyms": [
      [
        82,
        85
      ]
    ],
    "long-forms": [],
    "ID": "741"
  },
  {
    "text": "Đối với tập dữ liệu Pega của công ty VCcorp , các kết quả thử nghiệm có thể quan sát ở bảng sau : AUC",
    "acronyms": [
      [
        98,
        101
      ]
    ],
    "long-forms": [],
    "ID": "742"
  },
  {
    "text": "Bây giờ ta xem xét so sánh giữa các mô hình . Trên bộ Retailrocket , dễ dàng thấy rằng ba mô hình ITE đều có kết quả tốt hơn ở tất cả giá trị",
    "acronyms": [
      [
        98,
        101
      ]
    ],
    "long-forms": [],
    "ID": "743"
  },
  {
    "text": "Split CIFAR100 , Split Omniglot Mạng",
    "acronyms": [
      [
        6,
        14
      ]
    ],
    "long-forms": [],
    "ID": "744"
  },
  {
    "text": "β ma trận topic distribution K × V , với mỗi hàng là tỷ lệ của các từ trong một chủ đề",
    "acronyms": [],
    "long-forms": [],
    "ID": "745"
  },
  {
    "text": "Học liên tục dựa trên sự không chắc chắn , UCL UCL [ 33 ] xây dựng độ quan trọng của nơ-ron dựa trên tính không chắc chắn ( uncertainty ) của mạng Bayesian , và điều chỉnh đại lượng KL trong ELBO để cân bằng tính ổn định và mềm dẻo của mô hình .",
    "acronyms": [
      [
        43,
        46
      ],
      [
        47,
        50
      ],
      [
        182,
        184
      ],
      [
        191,
        195
      ]
    ],
    "long-forms": [],
    "ID": "746"
  },
  {
    "text": "SVM dựa trên ý tưởng tìm một siêu phẳng phân tách sao cho mức lề ( tức là khoảng cách gần nhất từ một điểm tới mặt phân cách ) là lớn nhất . SVM chỉ làm việc được với bài",
    "acronyms": [
      [
        0,
        3
      ],
      [
        141,
        144
      ]
    ],
    "long-forms": [],
    "ID": "747"
  },
  {
    "text": "Masked Language Modeling ( MLM ) Mô hình dự đoán giá trị ban đầu của các từ bị che dấu dựa trên ngữ cảnh của các từ không bị che dấu trong chuỗi",
    "acronyms": [
      [
        27,
        30
      ]
    ],
    "long-forms": [],
    "ID": "748"
  },
  {
    "text": "từ biểu diễn đặc trưng của từng tác vụ , từ đó can thiệp vào độ lớn gradient của trọng số để hạn chế sự thay đổi của nơ-ron quan trọng . • UCL : [ 33 ] định nghĩa độ quan trọng của nơ-ron trong mạng BNN và thiết",
    "acronyms": [
      [
        139,
        142
      ],
      [
        199,
        202
      ]
    ],
    "long-forms": [],
    "ID": "749"
  },
  {
    "text": "Ví dụ , từ ‘ additionally ’ chỉ có quan hệ ‘ Expansion.Conjunction ’ . Từ bộ dữ liệu PDTB , em thống kê các quan hệ đi",
    "acronyms": [
      [
        85,
        89
      ]
    ],
    "long-forms": [],
    "ID": "750"
  },
  {
    "text": "10 Tác v 8 AGS - CL",
    "acronyms": [
      [
        11,
        19
      ]
    ],
    "long-forms": [],
    "ID": "751"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 66 Chương 6 Kết luận",
    "acronyms": [
      [
        60,
        71
      ]
    ],
    "long-forms": [],
    "ID": "752"
  },
  {
    "text": "TÓM TẮT NỘI DUNG ĐỒ ÁN Trong ĐATN này , ta thực hiện áp dụng mạng đồ thị tích chập ( GCN ) để phân loại văn bản theo các nhãn cho trước , kết hợp với mô hình chủ đề ( Topic",
    "acronyms": [
      [
        29,
        33
      ],
      [
        85,
        88
      ]
    ],
    "long-forms": [
      [
        61,
        82
      ]
    ],
    "ID": "753"
  },
  {
    "text": "của bài báo . Phép DA này nói một cách đơn giản là cho đầu ra với nhiều view hơn , các view này có thể có kích thước nhỏ hơn so với phép DA crop thông thường , do",
    "acronyms": [
      [
        19,
        21
      ],
      [
        137,
        139
      ]
    ],
    "long-forms": [],
    "ID": "754"
  },
  {
    "text": "hình 23 : ACD ca sử dụng đăng nhập ứng viên 3 . 4 . 3 . Sửa thông tin cá nhân",
    "acronyms": [
      [
        10,
        13
      ]
    ],
    "long-forms": [],
    "ID": "755"
  },
  {
    "text": "Khám phá ra các đặc trưng của bộ dữ liệu PDTB , đồng thời đề ra một số kỹ thuật để khai thác hiệu quả các đặc trưng đó giúp cải thiện chất lượng của mô hình . Kết quả , bài nghiên cứu đã được chấp nhận tại hội nghị Association for Computational",
    "acronyms": [
      [
        41,
        45
      ]
    ],
    "long-forms": [],
    "ID": "756"
  },
  {
    "text": "sát sau , còn ở thử nghiệm này , chúng ta sẽ bỏ qua thông tin về nhãn đó . với một số thống kê chi tiết ở Bảng 1 . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        175,
        186
      ]
    ],
    "long-forms": [],
    "ID": "757"
  },
  {
    "text": "Khi đó ta có thể viết : V V",
    "acronyms": [],
    "long-forms": [],
    "ID": "758"
  },
  {
    "text": ") D Đồ án tốt nghiệp",
    "acronyms": [],
    "long-forms": [],
    "ID": "759"
  },
  {
    "text": "2 KIẾN THỨC CƠ SỞ • Θf là tập các tham số mô hình của hàm ánh xạ f Do hàm ánh xạ f được định nghĩa là một mạng nơ-ron nhiều tầng , công thức",
    "acronyms": [],
    "long-forms": [],
    "ID": "760"
  },
  {
    "text": "Để minh họa cho vấn đề của LDA , hình 1 . 1 thể hiện các chủ đề con của chủ đề “ Thời sự ” : Hình 1 . 1 Các chủ đề con của chủ đề Thời sự",
    "acronyms": [
      [
        27,
        30
      ]
    ],
    "long-forms": [],
    "ID": "761"
  },
  {
    "text": "− 𝐾𝐿 ( 𝑞 ( 𝑠 ) | | 𝑝( 𝑠) ) ≈ hằng số + 0.5 log 𝛼 + 𝑐1 𝛼 + 𝑐2 𝛼 2 + 𝑐3 𝛼 3 Trong đó : 𝑐1 = 1.1614512 , 𝑐2 = −1.50204118 ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "762"
  },
  {
    "text": "GCN , W ( l ) kích thước k x k là ma trận cho lớp biến đổi của thứ l của mạng . Sau khi đi qua GCN có L tầng , ta được ma trận H ( L ) có kích thước là ( n+m ) x k là ma trận biểu diễn của ( n+m ) nốt , mà trong đó , mỗi nốt là tổng hợp thông tin của",
    "acronyms": [
      [
        0,
        3
      ],
      [
        95,
        98
      ]
    ],
    "long-forms": [],
    "ID": "763"
  },
  {
    "text": "19 Đây có thể coi là phương pháp phổ biến nhất của MTL trong học sâu . Ý tưởng của nó",
    "acronyms": [
      [
        51,
        54
      ]
    ],
    "long-forms": [],
    "ID": "764"
  },
  {
    "text": "Giảng viên hướng dẫn : PGS. TS. Thân Quang Khoát Chữ ký của GVHD Bộ môn :",
    "acronyms": [
      [
        23,
        27
      ],
      [
        28,
        31
      ],
      [
        60,
        64
      ]
    ],
    "long-forms": [],
    "ID": "765"
  },
  {
    "text": "đồng thời tâm cụm sẽ được tính là : 𝑘 𝑚 ∑𝑁",
    "acronyms": [],
    "long-forms": [],
    "ID": "766"
  },
  {
    "text": "trong quá trình huấn luyện , 10000 ảnh còn lại được sử dụng trong quá trình kiểm thử . Một số mẫu dữ liệu trong bộ MNIST được mô tả trong Hình 4 . 1 .",
    "acronyms": [
      [
        115,
        120
      ]
    ],
    "long-forms": [],
    "ID": "767"
  },
  {
    "text": "[ logq ( X | Y ) ] θ Đồ án tốt nghiệp",
    "acronyms": [],
    "long-forms": [],
    "ID": "768"
  },
  {
    "text": "• Mở rộng các chủ đề cho bài toán phân loại . • Tìm hiểu việc áp dụng mô hình HAN cho bài toán phân tích cảm xúc mức khía cạnh .",
    "acronyms": [
      [
        78,
        81
      ]
    ],
    "long-forms": [],
    "ID": "769"
  },
  {
    "text": "và không ứng với key nào : exp ( q ∗ k+ / α ) Lq = − log PK",
    "acronyms": [],
    "long-forms": [],
    "ID": "770"
  },
  {
    "text": "Với tham số mô hình là θ , hàm mục tiêu trong công thức PT 2. 15 có thể viết lại là : 𝑁",
    "acronyms": [
      [
        56,
        58
      ]
    ],
    "long-forms": [],
    "ID": "771"
  },
  {
    "text": "Hệ thống được cấu trúc theo mô hình MVC : - Model : Đây là thành phần chứa tất cả các nghiệp vụ logic , phương thức xử lý , truy xuất database , đối tượng mô tả dữ liệu như các Class , hàm xử lý .",
    "acronyms": [
      [
        36,
        39
      ]
    ],
    "long-forms": [],
    "ID": "772"
  },
  {
    "text": "và SVB - PP , đặc biệt là trong môi trường dữ liệu tới vô hạn . 4 . 3 . 2 Cân bằng thông tin cũ và mới Trong mục này , chúng tôi sẽ thảo luận cách iDropout cân bằng giữa thông tin",
    "acronyms": [
      [
        3,
        11
      ]
    ],
    "long-forms": [],
    "ID": "773"
  },
  {
    "text": "Mạng nơ-ron nhân tạo là mô hình cơ bản , cung cấp nền tảng để xây dựng và phát triển những mạng nơ-ron nâng cao , trong đó có mạng nơ-ron hồi quy ( RNN ) . Ta sẽ",
    "acronyms": [
      [
        148,
        151
      ]
    ],
    "long-forms": [
      [
        126,
        145
      ]
    ],
    "ID": "774"
  },
  {
    "text": "• Công thức ( II ) : Closeness centrality được tính bằng bình quân của tổng số khoảng cách ngắn nhất từ một nút đến tất cả các nút còn lại . ∑𝑡𝜖𝑉 \\ 𝑣 𝑑𝐺 ( 𝑣 , 𝑡)",
    "acronyms": [],
    "long-forms": [],
    "ID": "775"
  },
  {
    "text": "tiên nghiệm của nó là p ( Θ | D1 , D2 , ... , Dt−1 , η ) . Mặt khác phân phối tiên nghiệm Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        150,
        161
      ]
    ],
    "long-forms": [],
    "ID": "776"
  },
  {
    "text": "Training epoch s Topics coherence ( NPMI ) Hình 4 . 11 Đồ thị biểu diễn sự thay đổi giá trị NPMI khi thay đổi số lần học mô",
    "acronyms": [
      [
        36,
        40
      ],
      [
        92,
        96
      ]
    ],
    "long-forms": [],
    "ID": "777"
  },
  {
    "text": "{ 𝑦1 , 𝑦2 , … , 𝑦𝑁 }; 𝑦𝑛 ∈ ℝ𝐾 , 𝑦𝑛𝑘 = { 1 nếu 𝑥𝑛 thuộc cụm 𝑘",
    "acronyms": [],
    "long-forms": [],
    "ID": "778"
  },
  {
    "text": "dụng xấp xỉ VI trên đại lượng khả năng xảy ra này : 22 𝑁𝑡",
    "acronyms": [
      [
        12,
        14
      ]
    ],
    "long-forms": [],
    "ID": "779"
  },
  {
    "text": "VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG ─ ── ── ── * ─ ── ── ── ĐỒ ÁN TỐT NGHIỆP ĐẠI HỌC",
    "acronyms": [],
    "long-forms": [],
    "ID": "780"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 33 4 . 2 11 - way classification",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "781"
  },
  {
    "text": "𝐿𝛽 ( 𝛽 ) là đại lượng hiệu chỉnh , hàm lỗi mới được viết thành : 𝐿̃ ( 𝛽 ) = 𝐿𝐷 ( 𝛽 ) + 𝜆𝐿𝛽 ( 𝛽 ) ( 3 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "782"
  },
  {
    "text": "hệ thống gợi ý nói riêng .  Tìm hiểu sâu hơn về mô hình phân tích ma trận ( MF ) ứng dụng trong các hệ thống gợi ý",
    "acronyms": [
      [
        77,
        79
      ]
    ],
    "long-forms": [
      [
        49,
        74
      ]
    ],
    "ID": "783"
  },
  {
    "text": "sử dụng hàm kích hoạt là hàm sigmoid và h được học từ dữ liệu . Mô hình này được gọi GMF - Generalized Matrix Factorization .",
    "acronyms": [
      [
        85,
        88
      ]
    ],
    "long-forms": [],
    "ID": "784"
  },
  {
    "text": "k ∈ { 1 , 2 , ... , K} ( topic distributions ) : βk ∼ Dir ( η ) • Sinh ra Nd từ cho mỗi văn bản d :",
    "acronyms": [],
    "long-forms": [],
    "ID": "785"
  },
  {
    "text": "Nguồn : genome.gov 2 . 2 . 5 Dữ liệu biểu hiện gene Toàn bộ quá trình truyền thông tin di truyền từ DNA tới protein gọi là quá trình biểu",
    "acronyms": [
      [
        100,
        103
      ]
    ],
    "long-forms": [],
    "ID": "786"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 7 Danh mục bảng biểu",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "787"
  },
  {
    "text": "bản . Ta giả sử một văn bản có L câu 𝑠𝑖 và mỗi câu có 𝑇𝑖 từ .",
    "acronyms": [],
    "long-forms": [],
    "ID": "788"
  },
  {
    "text": "Học liên tục dựa trên suy diễn biến phân Học liên tục dựa trên suy diễn biến phân ( VCL ) [ 2 ] là một mô hình tổng quát",
    "acronyms": [
      [
        84,
        87
      ]
    ],
    "long-forms": [
      [
        41,
        81
      ],
      [
        0,
        40
      ]
    ],
    "ID": "789"
  },
  {
    "text": "{ ( 𝑥 , 𝑦𝑖 ) , 𝑖 = 1 , 2 , … 𝑀 } , trong đó 𝑥 𝑖 = ( 𝑥1𝑖 , 𝑥2𝑖 , … , 𝑥𝑁𝑖 ) , y là đại lượng hồi quy tương ứng với quan sát i , N là số đặc trưng ( biến ) của mỗi quan sát . Mục tiêu của bài toán hồi quy",
    "acronyms": [],
    "long-forms": [],
    "ID": "790"
  },
  {
    "text": ") . Phiên bản này đưa ra mối tục tuân theo phân phối Gauss : Emk ∼ N ( 1, α = 1−p",
    "acronyms": [],
    "long-forms": [],
    "ID": "791"
  },
  {
    "text": "1 . Chọn w theo công thức tính của thuật toán LLE thông thường . Ta sử dụng",
    "acronyms": [
      [
        46,
        49
      ]
    ],
    "long-forms": [],
    "ID": "792"
  },
  {
    "text": "trước được coi như phân phối tiên nghiệm cho tác vụ hiện tại . Khi đó ELBO cho tác vụ t có dạng :",
    "acronyms": [
      [
        70,
        74
      ]
    ],
    "long-forms": [],
    "ID": "793"
  },
  {
    "text": "trung của từ ) , Câu ( Bộ mã hóa câu , Độ tập trung của câu ) . Ở đây , độ tập trung có thể hiểu là giá trị thông tin mà từ và câu đóng góp trong việc phân loại văn bản tương",
    "acronyms": [],
    "long-forms": [],
    "ID": "794"
  },
  {
    "text": "• Tốc độ xử lý để đảm bảo tính real -time và tính ứng dụng cao . Bài toán Object Tracking hiện nay có hai hướng tiếp cận chính đấy là Single Object Tracking ( SOT ) , tức là tập chung theo dõi một đối tượng cụ thể trong toàn bộ",
    "acronyms": [
      [
        159,
        162
      ]
    ],
    "long-forms": [],
    "ID": "795"
  },
  {
    "text": "và θ , cụ thể hơn chúng đóng vai trò làm tham số cho phân phối tiên nghiệm Dirichlet của hai biến ẩn này . Điều này giúp mô hình LDA có có tổng quát hoá",
    "acronyms": [
      [
        129,
        132
      ]
    ],
    "long-forms": [],
    "ID": "796"
  },
  {
    "text": "để trọng số có thể cập nhật \" thoải mái \" hơn cho tác vụ mới . Trong quá trình tối ưu , HAT sử dụng một chiến lược heuristic tăng dần s qua các vòng lặp của một epoch , bắt đầu với s → 0 , atl , i → 21 , tức các nơ-ron đều được kích",
    "acronyms": [
      [
        88,
        91
      ]
    ],
    "long-forms": [],
    "ID": "797"
  },
  {
    "text": "Bài nghiên cứu 4 - way PDTB - Lin",
    "acronyms": [
      [
        23,
        27
      ]
    ],
    "long-forms": [],
    "ID": "798"
  },
  {
    "text": "NDCG @ 10 Hình 20 : So sánh các mô hình trên bộ Recobell qua từng vòng lặp với k = 32 Ở hình 19 , ta thấy sau những vòng lặp đầu tiên , hai mô hình ITE - item _ pcat",
    "acronyms": [
      [
        0,
        4
      ],
      [
        148,
        165
      ]
    ],
    "long-forms": [],
    "ID": "799"
  },
  {
    "text": "Sau khi học trên một minibatch t , mô hình sẽ được đánh giá bằng độ đo LPP trên minibatch tiếp theo . 5 . 4 . 2 Kết quả thử nghiệm",
    "acronyms": [
      [
        71,
        74
      ]
    ],
    "long-forms": [],
    "ID": "800"
  },
  {
    "text": "32 Thuật toán 2 Phương pháp học SVB cho LDA Đầu vào : Dòng dữ liệu với các minibatch { D1 , D2 , ... , Dt , ... } , các tham số tiên",
    "acronyms": [
      [
        32,
        35
      ],
      [
        40,
        43
      ]
    ],
    "long-forms": [],
    "ID": "801"
  },
  {
    "text": "Để tính đại lượng khả năng xảy ra trong công thức PT 3.7 , ở đây ta sẽ sử dụng kĩ thuật LRT đã trình bày trong 2 . 1 . 3 . Xét một tầng 𝑙 trong mạng , ta có giá trị preactivation tại tầng 𝑙 với mà trận đầu vào 𝐴 ( 𝑙 ) là :",
    "acronyms": [
      [
        50,
        52
      ],
      [
        88,
        91
      ]
    ],
    "long-forms": [],
    "ID": "802"
  },
  {
    "text": "Trong các nhánh phát triển của hướng nghiên cứu SSL , có một nhánh rất phát triển và đang có rất nhiều mô hình đem lại các kết quả tốt ở nhánh này , thậm chí các phương pháp của hướng tiếp cận này có thể tạo ra các mô hình mạng nơ-ron",
    "acronyms": [
      [
        48,
        51
      ]
    ],
    "long-forms": [],
    "ID": "803"
  },
  {
    "text": "Park và cộng sự ( 2015 ) [ 16 ] sử dụng Sparse Overlapping Group Lasso cho dữ liệu The Cancer Genome Atlas ( TCGA ) của nhiều loại bệnh ung thư",
    "acronyms": [
      [
        109,
        113
      ]
    ],
    "long-forms": [],
    "ID": "804"
  },
  {
    "text": "TỐT NGHIỆP ĐẠI HỌC NGÀNH CÔNG NGHỆ THÔNG TIN TÊN ĐỀ TÀI",
    "acronyms": [],
    "long-forms": [],
    "ID": "805"
  },
  {
    "text": "Dựa theo [ 14 ] , chúng tôi sử dụng thuật toán tối ưu Nesterov ’ s accelerate gradient ( NAG ) [ 47 ] . Trong quá trình huấn luyện , tốc độ học sẽ được giảm từ epoch thứ 24 theo công thức",
    "acronyms": [
      [
        89,
        92
      ]
    ],
    "long-forms": [],
    "ID": "806"
  },
  {
    "text": "Bài toán đó có công thức như sau : min T r ( QT C T Z ) + \u000fH ( Q )",
    "acronyms": [],
    "long-forms": [],
    "ID": "807"
  },
  {
    "text": "Đối với LGL và SOGL , các hệ số 𝛽 cũng được tách thành các hệ số ẩn tương tự công thức ( 11 ) và ( 12 ) . Bài toán ( 13 ) có thể giải bằng một số biến",
    "acronyms": [
      [
        8,
        11
      ],
      [
        15,
        19
      ]
    ],
    "long-forms": [],
    "ID": "808"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 43",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "809"
  },
  {
    "text": "Dropout tương ứng . Như vậy ta có thể nhận định rằng ALV có khả năng áp dụng vào các",
    "acronyms": [
      [
        53,
        56
      ]
    ],
    "long-forms": [],
    "ID": "810"
  },
  {
    "text": "Viện Công nghệ Thông tin và Truyền thông các biểu diễn ẩn mà LCL muốn tìm ra :",
    "acronyms": [
      [
        61,
        64
      ]
    ],
    "long-forms": [],
    "ID": "811"
  },
  {
    "text": "trình nào khác . Hà Nội , ngày tháng năm Tác giả ĐATN",
    "acronyms": [
      [
        49,
        53
      ]
    ],
    "long-forms": [],
    "ID": "812"
  },
  {
    "text": "Trong công việc này , hai bộ dữ liệu mô phỏng có thể áp dụng cho môi trường học liên tục sẽ được xây dựng từ hai bộ dữ liệu gốc này như sau : • Split CIFAR100 : chia bộ dữ liệu CIFAR100 thành 10 bộ dữ liệu tương ứng",
    "acronyms": [
      [
        150,
        158
      ],
      [
        177,
        185
      ]
    ],
    "long-forms": [],
    "ID": "813"
  },
  {
    "text": "× ( d2 − w+1 ) , trong đó O kij = h [ X] ij , F k i =",
    "acronyms": [],
    "long-forms": [],
    "ID": "814"
  },
  {
    "text": "CIFAR10 / 100 Split Omniglot Mô hình gốc",
    "acronyms": [
      [
        0,
        13
      ]
    ],
    "long-forms": [],
    "ID": "815"
  },
  {
    "text": "Thêm vào đó tập trung vào việc cải tiến đại lượng KL trong hàm mục tiêu giúp cải thiện tính ổn định và mềm dẻo của mô hình , giải quyết tốt hơn cho vấn đề quên nhanh chóng của mô hình .",
    "acronyms": [
      [
        50,
        52
      ]
    ],
    "long-forms": [],
    "ID": "816"
  },
  {
    "text": "ẩn của cặp negative vẫn phải cách xa nhau giống như trong CL thông thường . Kết hợp 2 điều này một lần nữa giúp ta hình dung rõ hơn về hình dáng của không gian",
    "acronyms": [
      [
        58,
        60
      ]
    ],
    "long-forms": [],
    "ID": "817"
  },
  {
    "text": "và β = [ β1 β2 ... βV ] trong đó βj là cột thứ j của ma trận β , khi đó : sof tmax ( βk ) j = exp ( skj − A ( sk ) )",
    "acronyms": [],
    "long-forms": [],
    "ID": "818"
  },
  {
    "text": "Hình 4 . 5 Kết quả thử nghiệm cho VCL và UCL trên PMNIST Kết quả thử nghiệm cho thấy việc áp dụng ALV lên các phương pháp tiếp cận theo OVI cũng giúp cải thiện hiệu năng của các phương pháp này trong kịch bản",
    "acronyms": [
      [
        34,
        37
      ],
      [
        41,
        44
      ],
      [
        50,
        56
      ],
      [
        98,
        101
      ],
      [
        136,
        139
      ]
    ],
    "long-forms": [],
    "ID": "819"
  },
  {
    "text": "2.3 Mạng phát hiện khuôn mặt You only look once ( YOLO )",
    "acronyms": [
      [
        50,
        54
      ]
    ],
    "long-forms": [],
    "ID": "820"
  },
  {
    "text": "Để sử dụng bộ dữ liệu MNIST cho bài toán Học liên tục , hai bộ dữ liệu sẽ được tạo ra tương ứng với hai kịch bản thử nghiệm gia tăng số lượng tác vụ và miền dữ liệu , được gọi là Split MNIST và PMNIST .",
    "acronyms": [
      [
        22,
        27
      ],
      [
        194,
        200
      ],
      [
        185,
        190
      ]
    ],
    "long-forms": [],
    "ID": "821"
  },
  {
    "text": "Học liên tục , giúp cải thiện các phương pháp tiếp cận theo hướng ràng buộc trọng số , còn được gọi là Học liên tục dựa trên tham số cục bộ ( ALV ) . Phương",
    "acronyms": [
      [
        142,
        145
      ]
    ],
    "long-forms": [
      [
        103,
        139
      ]
    ],
    "ID": "822"
  },
  {
    "text": "quan trọng hay không , và một thành phần ràng buộc khác được đưa vào : L X",
    "acronyms": [],
    "long-forms": [],
    "ID": "823"
  },
  {
    "text": "Mô hình T ransf ormer − small , T ransf ormer − small ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "824"
  },
  {
    "text": "Giả thiết mà em đặt ra ở đây có thể là với không gian ảnh thì việc tính w theo phương pháp LLE có thể chưa hẳn đã là một cách xấp xỉ tốt , trong khi đó sử dụng hai cách tính w kia mặc dù đơn giản hơn về mặt tính",
    "acronyms": [
      [
        91,
        94
      ]
    ],
    "long-forms": [],
    "ID": "825"
  },
  {
    "text": "và dữ liệu quan sát được x mà thông qua một hàm chuyển đổi ( transformation function ) . Như vậy , biến ẩn toàn cục Θ trong Infinite Dropout không có vai trò",
    "acronyms": [],
    "long-forms": [],
    "ID": "826"
  },
  {
    "text": "nhỏ , µlij , t càng bị ràng buộc để gần µlij , t−1 . Tuy nhiên , BNN có thể học ra những nơ-ron không quan trọng của mạng , cho nên những nơ-ron đó không nhất thiết cần ràng buộc mà có thể cho phép học ở tác vụ",
    "acronyms": [
      [
        65,
        68
      ]
    ],
    "long-forms": [],
    "ID": "827"
  },
  {
    "text": "những kết quả tốt bất ngờ , đó cũng chính là lý do mô hình có tên Naive Bayes . Như vậy chúng ta có : p ( β | W , C , η ) ∝",
    "acronyms": [],
    "long-forms": [],
    "ID": "828"
  },
  {
    "text": "ĐỀ TÀI TỐT NGHIỆP 1 . Thông tin sinh viên Họ và tên : Trần Tùng Lâm - MSSV : 20173226",
    "acronyms": [
      [
        70,
        74
      ]
    ],
    "long-forms": [],
    "ID": "829"
  },
  {
    "text": "cấp độ biểu diễn thông tin từ mức độ thấp đến cao và trừu tượng hơn thông qua convolution từ các filter . Đó là lý do CNN cho ra mô hình với độ chính xác cao .",
    "acronyms": [
      [
        118,
        121
      ]
    ],
    "long-forms": [],
    "ID": "830"
  },
  {
    "text": "Ban và cộng sự ( 2010 ) [ 36 ] dùng SVM trong một nghiên cứu GWAS để tìm ra nhiều SNPs mới liên quan",
    "acronyms": [
      [
        36,
        39
      ],
      [
        61,
        65
      ],
      [
        82,
        86
      ]
    ],
    "long-forms": [],
    "ID": "831"
  },
  {
    "text": "Trong công việc này một biến thể của LRT cũng sẽ được sử dụng để áp dụng vào phương pháp đề xuất cho các phương pháp tiếp cận theo OVI .",
    "acronyms": [
      [
        37,
        40
      ],
      [
        131,
        134
      ]
    ],
    "long-forms": [],
    "ID": "832"
  },
  {
    "text": "Để khắc phục hạn chế về phụ thuộc xa , Long Short Term Memory ( LSTM ) dựa trên RNN ra đời để cải thiện vấn đề này . 2 . 3 Mạng bộ nhớ dài - ngắn ( Long Short Term Memory )",
    "acronyms": [
      [
        64,
        68
      ],
      [
        80,
        83
      ]
    ],
    "long-forms": [
      [
        123,
        145
      ]
    ],
    "ID": "833"
  },
  {
    "text": "đánh giá trước đó . Kết quả mô hình học đề xuất dựa trên BERT ( MHDX ) với News Encoder là biểu diễn đề xuất có thể quan sát trong bảng sau đây :",
    "acronyms": [
      [
        57,
        61
      ],
      [
        64,
        68
      ]
    ],
    "long-forms": [],
    "ID": "834"
  },
  {
    "text": "𝑠𝑡 = 𝑡𝑎𝑛ℎ ( 𝑊 𝑠𝑡−1 + 𝑈 𝑥𝑡 ) 𝑦𝑡 = 𝑉 ℎ𝑡 RNN sử dụng 3 ma trận trọng số 𝑊 , 𝑈 , 𝑉 cho 2 quá trình tính toán .",
    "acronyms": [
      [
        38,
        41
      ]
    ],
    "long-forms": [],
    "ID": "835"
  },
  {
    "text": "trong đó các giá trị kì vọng là : K X",
    "acronyms": [],
    "long-forms": [],
    "ID": "836"
  },
  {
    "text": "Hình 5 . 2 : Sự thay đổi của độ chính xác của các tác vụ trong quá trình học Permuted MNIST .",
    "acronyms": [
      [
        86,
        91
      ]
    ],
    "long-forms": [],
    "ID": "837"
  },
  {
    "text": "26 CHƯƠNG 4 . PHƯƠNG PHÁP ĐỀ XUẤT",
    "acronyms": [],
    "long-forms": [],
    "ID": "838"
  },
  {
    "text": "P ràng buộc là m j =1 wi , j = 1 , điều kiện này cho phép ta tịnh tiến toàn bộ dữ liệu thì",
    "acronyms": [],
    "long-forms": [],
    "ID": "839"
  },
  {
    "text": "2K×V khả năng có thể của Θt khi bị drop thành một mô hình trung bình duy nhất . Đây chính là tính chất ensemble của Dropout mà chúng tôi đã đề cập .",
    "acronyms": [],
    "long-forms": [],
    "ID": "840"
  },
  {
    "text": "∗ Trong công thức PT 2.10 , ta có log 𝑝 ( 𝜃𝐴 , 𝑖 | 𝐷𝐴 ) không phục thuộc vào tham số",
    "acronyms": [
      [
        18,
        20
      ]
    ],
    "long-forms": [],
    "ID": "841"
  },
  {
    "text": "giải trong hệ thống gợi ý tin tức LÊ MINH HIẾU hieu.lm161522@sis.hust.edu.vn",
    "acronyms": [],
    "long-forms": [],
    "ID": "842"
  },
  {
    "text": "độ đo Jaccard làm giá trị thực tế ( ground-truth ) cho tính tương đồng giữa các người dùng , cái mà mô hình MF cần phải khôi phục lại được . • Độ tương đồng Jaccard : Gọi Ru là tập các item mà người dùng u đã",
    "acronyms": [
      [
        108,
        110
      ]
    ],
    "long-forms": [],
    "ID": "843"
  },
  {
    "text": "biến gradient , vấn đề phụ thuộc dài hạn nên cần một kiến trúc mới để giải quyết các vấn đề trên là Long short-term memory ( LSTM ) . Long short-term memory",
    "acronyms": [
      [
        125,
        129
      ]
    ],
    "long-forms": [],
    "ID": "844"
  },
  {
    "text": "bằng thông tin cũ và mới hiệu quả hơn SVB . • Có thể tránh được vấn đề vanishing variance vì theo công thức trên , khi t Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        38,
        41
      ],
      [
        181,
        192
      ]
    ],
    "long-forms": [],
    "ID": "845"
  },
  {
    "text": "phương sai của 𝐿 được biểu diễn như sau : 23 𝑀",
    "acronyms": [],
    "long-forms": [],
    "ID": "846"
  },
  {
    "text": "1 . GIỚI THIỆU ĐỀ TÀI 1 . 1 Đặt vấn đề .",
    "acronyms": [],
    "long-forms": [],
    "ID": "847"
  },
  {
    "text": "KẾT LUẬN 6 Kết luận Phần này sẽ tóm tắt lại những kết quả , kinh nghiệm đã đạt được trong quá",
    "acronyms": [],
    "long-forms": [],
    "ID": "848"
  },
  {
    "text": "Theo công thức PT 2.12 , khi đó ta có hàm mục tiêu cho EWC với ALV : 𝑁𝑡 𝐴𝐿𝑉",
    "acronyms": [
      [
        15,
        17
      ],
      [
        55,
        58
      ],
      [
        63,
        66
      ],
      [
        72,
        75
      ]
    ],
    "long-forms": [],
    "ID": "849"
  },
  {
    "text": "Cụ thể , các tham số của ANN được học dựa trên một tập dữ liệu cho trước . Khi quá trình học hội tụ , những",
    "acronyms": [
      [
        25,
        28
      ]
    ],
    "long-forms": [],
    "ID": "850"
  },
  {
    "text": "Tuy nhiên , ta có thể xem xét một số đặc điểm của các hàm trên : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        128,
        139
      ]
    ],
    "long-forms": [],
    "ID": "851"
  },
  {
    "text": "NTD NTD chọn vào mục hồ sơ ứng viên Nhà tuyển dụng đăng nhập vào hệ thống",
    "acronyms": [
      [
        0,
        3
      ],
      [
        4,
        7
      ]
    ],
    "long-forms": [],
    "ID": "852"
  },
  {
    "text": "Giả sử phân phối biến phân được tham số hóa bởi φ : qφ ( θ ) , khi đó khoảng cách KL giữa qφ ( θ ) và 12",
    "acronyms": [
      [
        82,
        84
      ]
    ],
    "long-forms": [],
    "ID": "853"
  },
  {
    "text": "Khởi tạo ngẫu nhiên λ = λ0 for t = 1,2 , ... , T , ... do Nhận dữ liệu ở minibatch thứ t là Dt",
    "acronyms": [],
    "long-forms": [],
    "ID": "854"
  },
  {
    "text": "trờ thành AE truyền thống . Mạng Split-Brain autoencoder : vận dụng cách làm dự đoán chéo các kênh màu : Như vậy từ mạng AE trong colorization ban đầu , bài báo đã tổng",
    "acronyms": [
      [
        10,
        12
      ],
      [
        121,
        123
      ]
    ],
    "long-forms": [],
    "ID": "855"
  },
  {
    "text": "VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG ── ── ── ── * ─ ── ── ── ĐỒ ÁN",
    "acronyms": [],
    "long-forms": [],
    "ID": "856"
  },
  {
    "text": "cân bằng được tốt tính chất này cũng sẽ giúp việc học được cải thiện . Điển hình có thể thấy trong trường hợp của EWC khi việc sử dụng Dropout cũng giúp cải thiện",
    "acronyms": [
      [
        114,
        117
      ]
    ],
    "long-forms": [],
    "ID": "857"
  },
  {
    "text": "- Split MNIST : Bảng 0.2 Tham số tốt nhất cho Split MNIST",
    "acronyms": [
      [
        8,
        13
      ],
      [
        52,
        57
      ]
    ],
    "long-forms": [],
    "ID": "858"
  },
  {
    "text": "HÀ NỘI , 6/2021 Đại học Bách Khoa Hà Nội",
    "acronyms": [],
    "long-forms": [],
    "ID": "859"
  },
  {
    "text": "KEGG [ 81 ] , Biocarta [ 82 ] . Kết quả được trình bày trong Bảng 4 và Hình 11 . Có thể thấy",
    "acronyms": [
      [
        0,
        4
      ]
    ],
    "long-forms": [],
    "ID": "860"
  },
  {
    "text": "trong đó D là tập huấn luyện . Cơ chế Attention Hình 2 . 2 : Mô hình của cơ chế attention .",
    "acronyms": [],
    "long-forms": [],
    "ID": "861"
  },
  {
    "text": "𝑠𝑡 = 𝑓𝑤 ( 𝑠𝑡 −1 , 𝑥𝑡 ) Hàm 𝑓𝑤 là hàm phi tuyến , ví dụ : 𝑡𝑎𝑛ℎ , 𝑅𝑒𝐿𝑈 . Ở đây , ta sử dụng 𝑡𝑎𝑛ℎ , công thức trên",
    "acronyms": [],
    "long-forms": [],
    "ID": "862"
  },
  {
    "text": "nhiều thông tin từ bài báo một cách có chọn lọc , biểu diễn User bằng lịch sử đọc có trọng số . Hình 2 . 2 Cấu trúc mô hình NAML",
    "acronyms": [
      [
        124,
        128
      ]
    ],
    "long-forms": [],
    "ID": "863"
  },
  {
    "text": "36 Tác động của tham số phạt α lên điểm BLEU của ( a ) ConvS2S- base và ( b ) Transformer - base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "acronyms": [
      [
        40,
        44
      ],
      [
        55,
        62
      ]
    ],
    "long-forms": [],
    "ID": "864"
  },
  {
    "text": "( sim ( z i , z j ) /τ ) ( 10 ) Trong đó , I [ k 6 = i ] là hàm nhận giá trị 0 khi k = i và nhận giá trị 1 khi k 6 = i ) ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "865"
  },
  {
    "text": "Để giúp dễ hiểu và nắm bắt , các nội dung trong mục này sẽ trình bày chi tiết việc áp dụng ALV cho lần lượt các phương pháp này .",
    "acronyms": [
      [
        91,
        94
      ]
    ],
    "long-forms": [],
    "ID": "866"
  },
  {
    "text": "đó những nơ - ron có giá trị SNR nhỏ có thể được bỏ đi bằng cách đặt giá trị đầu ra của chúng bằng 0. Phần trình bày ở trên đang minh họa với mạng liên kết đầy đủ , tức đầu vào có dạng",
    "acronyms": [
      [
        29,
        32
      ]
    ],
    "long-forms": [],
    "ID": "867"
  },
  {
    "text": "còn D là số chiều của dữ liệu . Mỗi hàng là một điểm dữ liệu trong không gian gốc .",
    "acronyms": [],
    "long-forms": [],
    "ID": "868"
  },
  {
    "text": "Cực đại hàm mục tiêu trên bằng việc sử dụng một thuật toán cập nhật dạng gradient ascent . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        151,
        162
      ]
    ],
    "long-forms": [],
    "ID": "869"
  },
  {
    "text": "thức cơ bản về CNN mà em tìm hiểu được , giúp ích trong việc giải quyết bài toán . Trước hết , Convolutional là một cửa sổ trượt trên một ma trận nhằm lấy ra được các thông tin hữu ích nhất từ ma trận đó thông qua các tham số ( kernel ) và cơ chế nhân tích",
    "acronyms": [
      [
        15,
        18
      ]
    ],
    "long-forms": [],
    "ID": "870"
  },
  {
    "text": ". . . Kỹ thuật lấy mẫu đổi biến tham số - Reparameterization Trick . Sử dụng mạng Multi-layer perceptron ( MLP ) cho bộ mã hóa và",
    "acronyms": [
      [
        107,
        110
      ]
    ],
    "long-forms": [],
    "ID": "871"
  },
  {
    "text": "chính là động lực để UCL cải tiến đại lượng này . Sau khi xấp xỉ đại lượng 𝑞𝑡 ( θ ) trong công thức PT 2.16 bằng GMF , ta có công thức tường minh của đại lượng KL :",
    "acronyms": [
      [
        21,
        24
      ],
      [
        100,
        102
      ],
      [
        113,
        116
      ],
      [
        160,
        162
      ]
    ],
    "long-forms": [],
    "ID": "872"
  },
  {
    "text": "18 2 . 3 Multi- task learning Multi-task learning ( MTL ) là một phương pháp học cho phép ta học ra mô hình của",
    "acronyms": [
      [
        52,
        55
      ]
    ],
    "long-forms": [],
    "ID": "873"
  },
  {
    "text": "N91737 - 0 N29160 - 0 Bảng 4 . 1 Mô tả hành vi người dùng tập MIND",
    "acronyms": [
      [
        62,
        66
      ]
    ],
    "long-forms": [],
    "ID": "874"
  },
  {
    "text": "tập dữ liệu huấn luyện VOCABULARY _ SIZE .",
    "acronyms": [],
    "long-forms": [],
    "ID": "875"
  },
  {
    "text": "à Phương pháp PCA sẽ cố gắng tìm phép biến đổi tuyến tính T thỏa mãn y = Tx sao cho trung bình bình phương lỗi ( MSE ) là bé nhất . Phương pháp tìm T :",
    "acronyms": [
      [
        14,
        17
      ],
      [
        113,
        116
      ]
    ],
    "long-forms": [
      [
        84,
        110
      ]
    ],
    "ID": "876"
  },
  {
    "text": "Một thư viện học sâu , là một API bậc cao có thể sử dụng chung với các thư viện deep learning nổi tiếng như tensorflow , CNTK , theano . Keras có một số",
    "acronyms": [
      [
        30,
        33
      ],
      [
        121,
        125
      ]
    ],
    "long-forms": [],
    "ID": "877"
  },
  {
    "text": "the 2015 SIAM International Conference on Data Mining , pages 451 – 459 . SIAM , 2015. Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        147,
        158
      ]
    ],
    "long-forms": [],
    "ID": "878"
  },
  {
    "text": "+ Áp dụng các tính chất của định thức ma trận det ( AB ) = det ( A ) det ( B ) , det ( diag (A ) ) =",
    "acronyms": [],
    "long-forms": [],
    "ID": "879"
  },
  {
    "text": "_ alpha = 0.5 , KL - weight = 1 VCL • w/o Dropout : Không cần",
    "acronyms": [
      [
        16,
        18
      ],
      [
        32,
        35
      ],
      [
        38,
        41
      ]
    ],
    "long-forms": [],
    "ID": "880"
  },
  {
    "text": "K là × 1000 Split MNIST PMNIST",
    "acronyms": [
      [
        18,
        23
      ],
      [
        24,
        30
      ]
    ],
    "long-forms": [],
    "ID": "881"
  },
  {
    "text": "Chúng ta xét một số kí hiệu toán học cơ bản để có thể mô hình hóa các công thức này . Xét một đầu vào x , mạng context Encoder F , xét M̃ là một",
    "acronyms": [],
    "long-forms": [],
    "ID": "882"
  },
  {
    "text": "Hình 2 . 1 a . Mạng ANN kết nối đầy đủ , Mạng BNN kết nối đầy đủ . Nguồn [ 8 ] .",
    "acronyms": [
      [
        20,
        23
      ],
      [
        46,
        49
      ]
    ],
    "long-forms": [],
    "ID": "883"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 20",
    "acronyms": [
      [
        60,
        71
      ]
    ],
    "long-forms": [],
    "ID": "884"
  },
  {
    "text": "( b ) NDCG @ 10 Hình 18 : So sánh các mô hình trên bộ Recobell khi k thay đổi",
    "acronyms": [
      [
        6,
        10
      ]
    ],
    "long-forms": [],
    "ID": "885"
  },
  {
    "text": "chiều và 1 kênh . F k ∈ R3 ×3 ×1 là bộ lọc thứ k với kích cỡ 3 × 3 . Mỗi phần có cỡ 3 × 3 của",
    "acronyms": [],
    "long-forms": [],
    "ID": "886"
  },
  {
    "text": "ta có thể huấn luyện ra hai mạng F1 và F2 thỏa mãn hai hàm mất mát tương ứng : F1 ∗ = arg min l1 ( F1 ( X1 ) , X2 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "887"
  },
  {
    "text": "Đầu tiên , nhúng các từ vào các vec-tơ thông qua một ma trận nhúng 𝑊𝑒 , 𝑥𝑖𝑗 = 𝑊𝑒 𝑤𝑖𝑗 . Tiếp theo , sử dụng GRU 2 chiều để xây dựng chuỗi mã hóa của các từ bằng cách tổng hợp thông tin từ cả hai phía của từ , từ đó kết hợp thông tin theo ngữ cảnh trong chuỗi",
    "acronyms": [
      [
        107,
        110
      ]
    ],
    "long-forms": [],
    "ID": "888"
  },
  {
    "text": "chuẩn hóa thõa mãn điều kiện 26 . Cuối cùng ta sử dụng thêm một hằng số C C. w",
    "acronyms": [],
    "long-forms": [],
    "ID": "889"
  },
  {
    "text": "thấy được rõ rệt ưu điểm ALV . Các kết quả cho thấy ALV giúp các mô hình gốc đạt được hiệu năng cao trên hầu hết tất cả các tác vụ trong cả ba phương pháp",
    "acronyms": [
      [
        25,
        28
      ],
      [
        52,
        55
      ]
    ],
    "long-forms": [],
    "ID": "890"
  },
  {
    "text": "translation - NMT ) và các vấn đề cơ bản của máy dịch dựa trên mạng nơ-ron . Mô hình ngôn ngữ là một khái niệm cơ bản trong xử lý ngôn ngữ tự nhiên , với mô hình ngôn ngữ ta",
    "acronyms": [
      [
        14,
        17
      ]
    ],
    "long-forms": [],
    "ID": "891"
  },
  {
    "text": "NTD đã có bài đăng tuyển Các bước cơ 1 - NTD chọn vào một trong những mục : Việc",
    "acronyms": [
      [
        0,
        3
      ],
      [
        41,
        44
      ]
    ],
    "long-forms": [],
    "ID": "892"
  },
  {
    "text": "Chương 2 : Trình bày về cơ sở lý thuyết được sử dụng làm nền tảng để xây dựng mô hình học HAN cho bài toán phân loại cảm xúc văn bản tiếng Việt . Chương 3 : Mô tả về kiến trúc , phương thức triển khai mô hình học , cách thức các",
    "acronyms": [
      [
        90,
        93
      ]
    ],
    "long-forms": [],
    "ID": "893"
  },
  {
    "text": "dữ liệu . Đó là áp dụng thêm mô hình ngôn ngữ mới hiện nay đó là Bidirectional Encoder Representations from Transformers ( BERT ) , mô hình đem lại cải thiện chất lượng cho",
    "acronyms": [
      [
        123,
        127
      ]
    ],
    "long-forms": [],
    "ID": "894"
  },
  {
    "text": "1 , nếu pi < thresh I [ pi < thresh ] =",
    "acronyms": [],
    "long-forms": [],
    "ID": "895"
  },
  {
    "text": "Phần 3 : Deep Matrix Factorization ( DMF ) . Phần 4 : Thực nghiệm và đánh giá . Phần 5 : Kết luận và định hướng phát triển .",
    "acronyms": [
      [
        37,
        40
      ]
    ],
    "long-forms": [],
    "ID": "896"
  },
  {
    "text": "‖ ) PT 3.12",
    "acronyms": [
      [
        4,
        6
      ]
    ],
    "long-forms": [],
    "ID": "897"
  },
  {
    "text": "NAML trên tất cả các độ đo . Ngoài ra , khi so với mô hình LSTUR , các độ đo cho kết quả tương đương .",
    "acronyms": [
      [
        0,
        4
      ],
      [
        59,
        64
      ]
    ],
    "long-forms": [],
    "ID": "898"
  },
  {
    "text": "Tác vụ chính là tác vụ dự đoán ra quan hệ của hai câu cho bài toán IDRR . Tác vụ còn lại là một tác vụ sử dụng thêm các bộ dữ liệu từ bên",
    "acronyms": [
      [
        67,
        71
      ]
    ],
    "long-forms": [],
    "ID": "899"
  },
  {
    "text": "học được những biểu diễn ẩn có ý nghĩa và có những kết quả tốt hơn hẳn so với AE truyền thống , những phương pháp này vẫn tồn tại một số nhược điểm nhất định . Thứ nhất , mạng nơ-ron trong mô hình được trình bày ở bài [ Pat +16 ] được huấn",
    "acronyms": [
      [
        78,
        80
      ]
    ],
    "long-forms": [],
    "ID": "900"
  },
  {
    "text": "i=2 j=1 trong đó D là tổng số văn bản , D ( wik ) là số văn bản chưa từ wik , D ( wik , wjk ) là số văn bản chứa cả hai từ ( wik , wjk ) và 10−2 là đại lượng tránh zero cho hàm log .",
    "acronyms": [],
    "long-forms": [],
    "ID": "901"
  },
  {
    "text": "ngắn và thưa ) , dẫn đến việc học những thông tin này khiến mô hình dễ bị Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 61",
    "acronyms": [
      [
        134,
        145
      ]
    ],
    "long-forms": [],
    "ID": "902"
  },
  {
    "text": "N là số lượng bài trong lịch sử đọc của User . Cuối cùng , vector biểu diễn của User được",
    "acronyms": [],
    "long-forms": [],
    "ID": "903"
  },
  {
    "text": "Tuy nhiên AGS - CL và UCL vẫn cần lưu lại bộ trọng số của mô hình cho tới tác vụ phía trước . Một",
    "acronyms": [
      [
        10,
        18
      ],
      [
        22,
        25
      ]
    ],
    "long-forms": [],
    "ID": "904"
  },
  {
    "text": "Gọi C , R tương ứng là tập các từ nối và tập các quan hệ . Với mỗi từ nối c i ∈ C , gọi Ri là tập con",
    "acronyms": [],
    "long-forms": [],
    "ID": "905"
  },
  {
    "text": "với các thách thức đến từ bài toán khai phá dòng dữ liệu , cụ thể là cơ chế cân bằng các thông tin cũ và thông tin mới , khả năng thích ứng với sự thay đổi bất Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        220,
        231
      ]
    ],
    "long-forms": [],
    "ID": "906"
  },
  {
    "text": "Do ta muốn dự đoán X2 từ X1 nên có thể xây dựng một hàm mất mát đơn giản như sau : l ( F ( X1 ) , X2 ) =",
    "acronyms": [],
    "long-forms": [],
    "ID": "907"
  },
  {
    "text": "Sửa các bài đã đăng tuyển NTD NTD chọn vào 1 trong các mục Việc làm",
    "acronyms": [
      [
        26,
        29
      ],
      [
        30,
        33
      ]
    ],
    "long-forms": [],
    "ID": "908"
  },
  {
    "text": "𝐿 = ∑ ( 𝛾̅𝑑𝑣 − 𝜃𝑑𝑘 ∗ 𝛽𝑘𝑣 𝑣 Trong đó , các giá trị khởi tạo của 𝜃 ở bước đầu tiên được khởi tạo ngẫu nhiên .",
    "acronyms": [],
    "long-forms": [],
    "ID": "909"
  },
  {
    "text": "0.70 Tác v 7 VBD - CL",
    "acronyms": [
      [
        13,
        21
      ]
    ],
    "long-forms": [],
    "ID": "910"
  },
  {
    "text": "Để hiểu hình ảnh tài liệu , nhiều tác vụ yêu cầu mô hình tạo ra biểu diễn cấp tài liệu chất lượng cao . Tác giả đã sử dụng Multi-label Document Classification ( MDC )",
    "acronyms": [
      [
        161,
        164
      ]
    ],
    "long-forms": [],
    "ID": "911"
  },
  {
    "text": "CIFAR100. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 . 5 Sự thay đổi của độ chính xác của các tác vụ trong quá trình học Split CIFAR10 - 100. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
    "acronyms": [
      [
        0,
        8
      ],
      [
        152,
        165
      ]
    ],
    "long-forms": [],
    "ID": "912"
  },
  {
    "text": "các đặc tính không tương đồng với dữ liệu đến vào thời điểm sau , thậm Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 13",
    "acronyms": [],
    "long-forms": [],
    "ID": "913"
  },
  {
    "text": "cứu SSL . Các phương pháp được trình bày sẽ gần như theo trình tự thời gian mà các bài báo tương ứng được công bố , với mục tiêu giúp cho người đọc có một cái",
    "acronyms": [
      [
        4,
        7
      ]
    ],
    "long-forms": [],
    "ID": "914"
  },
  {
    "text": "Code của SimCLR gốc được tham khảo tại link github . Trong phần triển khai các ý tưởng của LCL , các tham số và phép DA sẽ được dùng giống với các",
    "acronyms": [
      [
        9,
        15
      ],
      [
        91,
        94
      ],
      [
        117,
        119
      ]
    ],
    "long-forms": [],
    "ID": "915"
  },
  {
    "text": "khác ( qua quá trình nguyên phân ) và từ thế hệ này sang thế hệ khác ( thông qua quá trình giảm phân và thụ tinh ) . Gene là một đoạn nằm dọc theo phân tử DNA , mang thông tin di truyền nhất định .",
    "acronyms": [
      [
        155,
        158
      ]
    ],
    "long-forms": [],
    "ID": "916"
  },
  {
    "text": "= 𝑀 ∑𝑗 =1 𝑒𝑥𝑝 ( 𝑎𝑗𝑤 ) Trong đó , 𝑉𝑤 và 𝑣𝑤 là các tham số mô hình , và 𝒒𝑇𝑤 là vector truy vấn .",
    "acronyms": [],
    "long-forms": [],
    "ID": "917"
  },
  {
    "text": "32 Bảng 2 . 2 : Kết quả huấn luyện của mô hình Sequence-to-sequence sử dụng LSTM , trong quá trình decode sử dụng beam search với kích thước 6 .",
    "acronyms": [
      [
        76,
        80
      ]
    ],
    "long-forms": [],
    "ID": "918"
  },
  {
    "text": "* Precision * Recall / ( Precision +Recall ) . F1 nằm trong nửa khoảng ( 0,1 ] . 13",
    "acronyms": [],
    "long-forms": [],
    "ID": "919"
  },
  {
    "text": "phương pháp cực đại hóa kì vọng ( Expectation Maximization - EM ) . Có thể tóm tắt thuật toán như sau :",
    "acronyms": [
      [
        61,
        63
      ]
    ],
    "long-forms": [
      [
        0,
        31
      ]
    ],
    "ID": "920"
  },
  {
    "text": "được khả năng ghi nhớ tốt trên hầu hết các tác vụ . Tuy nhiên ALV vẫn chưa tốt hơn việc áp dụng Dropout trong trường hợp của VCL , nhưng cũng đạt được hiệu",
    "acronyms": [
      [
        62,
        65
      ],
      [
        125,
        128
      ]
    ],
    "long-forms": [],
    "ID": "921"
  },
  {
    "text": "làm đồ án tốt nghiệp . Sinh viên : Nguyễn Trọng Nhật , 20143316 , Lớp KSTN - CNTT K59 Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        70,
        81
      ],
      [
        149,
        160
      ]
    ],
    "long-forms": [],
    "ID": "922"
  },
  {
    "text": "với ndj là số lần xuất hiện từ wj của từ điển trong văn bản d . Chú ý rằng , cũng giống như LDA , biến đổi trên giả thiết các chiều của dữ liệu độc lập với nhau",
    "acronyms": [
      [
        92,
        95
      ]
    ],
    "long-forms": [],
    "ID": "923"
  },
  {
    "text": "Recurrent Neural Network RNN Mạng nơ-ron hồi quy",
    "acronyms": [
      [
        25,
        28
      ]
    ],
    "long-forms": [
      [
        29,
        48
      ]
    ],
    "ID": "924"
  },
  {
    "text": "Đồ án tốt nghiệp SGD Stochastic Gradient Descent",
    "acronyms": [
      [
        17,
        20
      ]
    ],
    "long-forms": [],
    "ID": "925"
  },
  {
    "text": "learning ) [ 19 ] . Ở cả ba kịch bản , mô hình được học với tất cả dữ liệu của một tác vụ tại mỗi thời điểm ( có thể được coi như học liên tục ngoại tuyến - Offline learning ) .",
    "acronyms": [],
    "long-forms": [],
    "ID": "926"
  },
  {
    "text": "công trong tương lai và có thể sẽ mở ra một cách nhìn mới , một hướng đi mới cho CL . Nếu suy nghĩ kĩ hơn , ta có thể hi vọng rằng trong tương lai sẽ có nhiều cách",
    "acronyms": [
      [
        81,
        83
      ]
    ],
    "long-forms": [],
    "ID": "927"
  },
  {
    "text": "Để ước lượng tính toán đại lượng khả năng xảy ra 𝐸𝑞𝑡 (θ) log 𝑝( 𝑦𝑡𝑖 | 𝑥𝑡𝑖 , θ ) , VCL sử dụng kĩ thuật xấp xỉ Monte Carlo và kĩ thuật đổi biến . Khi đó",
    "acronyms": [
      [
        82,
        85
      ]
    ],
    "long-forms": [],
    "ID": "928"
  },
  {
    "text": "Xóa các công việc đã lưu 46 hình 29 : ACD ca sử dụng xóa các công việc đã lưu",
    "acronyms": [
      [
        38,
        41
      ]
    ],
    "long-forms": [],
    "ID": "929"
  },
  {
    "text": "− 1 𝐴𝑇 𝑦 ( 2 ) trong đó A là ma trận của dữ liệu , 𝐴 ∈ ℝ𝑀 ∗ ( 𝑁+1 ) , cột thứ i của ma trận A là 𝐴𝑖 =",
    "acronyms": [],
    "long-forms": [],
    "ID": "930"
  },
  {
    "text": "Các tham số C được sử dụng là 2, 1 , 0.5 và 0. 1 . Kết quả thu được : Khi so sánh việc nới lỏng cách lấy mẫu cho phép w âm và cách lấy w thuộc đoạn",
    "acronyms": [],
    "long-forms": [],
    "ID": "931"
  },
  {
    "text": "mô hình đề xuất có kết quả gợi ý tốt hơn so với những mô hình khác trong các nghiên cứu gần đây . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        161,
        172
      ]
    ],
    "long-forms": [],
    "ID": "932"
  },
  {
    "text": "2.2 Kết quả huấn luyện của hình Sequence-to-sequence sử dụng LSTM . . . . .",
    "acronyms": [
      [
        61,
        65
      ]
    ],
    "long-forms": [],
    "ID": "933"
  },
  {
    "text": "Tuy vậy , phần nghiên cứu chính của đồ án vẫn tập trung trọng tâm vào SSL . Do đó , phần",
    "acronyms": [
      [
        70,
        73
      ]
    ],
    "long-forms": [],
    "ID": "934"
  },
  {
    "text": "Gọi { Y1 , Y2 , . . . , YR } là tập các ma trận tương tác cho cả R loại hành vi . Mỗi ma trận đều có kích thước M × N ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "935"
  },
  {
    "text": "tương tác ) . Ta ký hiệu tập các tương tác quan sát được là Y . Y − là tập dữ liệu",
    "acronyms": [],
    "long-forms": [],
    "ID": "936"
  },
  {
    "text": "VŨ HỒNG PHÚC phuc.vh173305@sis.hust.edu.vn Ngành : Khoa học máy tính",
    "acronyms": [],
    "long-forms": [],
    "ID": "937"
  },
  {
    "text": "KL Kullback - Leibler Divergence Phân kỳ Kullback - Leibler",
    "acronyms": [
      [
        0,
        2
      ]
    ],
    "long-forms": [
      [
        33,
        59
      ]
    ],
    "ID": "938"
  },
  {
    "text": "các bài đăng phù hợp với nội dung . 2 - Các bài đăng sẽ được liệt kê dưới dạng danh sách các cột trong một bảng , NTD có thể kích",
    "acronyms": [
      [
        114,
        117
      ]
    ],
    "long-forms": [],
    "ID": "939"
  },
  {
    "text": "vì mỗi trọng số nối tới một nơ-ron có độ lệch chuẩn σ khác nhau , UCL xét các trọng số đó có cùng một σ , và đó chính là độ không chắc chắn của nơ -ron : σil . UCL định nghĩa hai nguyên nhân dẫn tới việc quên tri thức cũ xảy ra trên một",
    "acronyms": [
      [
        66,
        69
      ],
      [
        160,
        163
      ]
    ],
    "long-forms": [],
    "ID": "940"
  },
  {
    "text": "khi áp dụng ALV lần lượt là 74.23 % , 61.64 % và 73.13 % , cải thiện từ 2 – 3 % so với Dropout . Thêm vào đó để quan sát kĩ hơn khả năng của ALV trong việc hạn",
    "acronyms": [
      [
        12,
        15
      ],
      [
        141,
        144
      ]
    ],
    "long-forms": [],
    "ID": "941"
  },
  {
    "text": "gợi ý đề xuất cũng cho kết quả tốt khi so sánh với các mô hình khác qua đánh giá trên các tập dữ liệu khác nhau . MỤC LỤC",
    "acronyms": [],
    "long-forms": [],
    "ID": "942"
  },
  {
    "text": "ICT • Đồ án tốt nghiệp được thực hiện tại : Bộ môn Hệ thống Thông tin - Viện",
    "acronyms": [
      [
        0,
        3
      ]
    ],
    "long-forms": [],
    "ID": "943"
  },
  {
    "text": "Cụ thể , mô hình có thể viết dưới dạng công thức như sau : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        122,
        133
      ]
    ],
    "long-forms": [],
    "ID": "944"
  },
  {
    "text": "False Negative ( FN ) là số lượng những điểm dữ liệu thực tế có nhãn là positive nhưng được dự đoán là negative . False Positive ( FP ) là số lượng",
    "acronyms": [
      [
        17,
        19
      ],
      [
        131,
        133
      ]
    ],
    "long-forms": [],
    "ID": "945"
  },
  {
    "text": "viết dưới dạng : θ ∗ = argmin L ( θ ) ( 1.1 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "946"
  },
  {
    "text": "Kiến trúc của mô hình ITE - item _ pcat được cho như Hình 11 . Có thể thấy thay đổi của mô hình ITE- item _ pcat so với mô hình ITE- onehot là thay vì",
    "acronyms": [
      [
        22,
        39
      ],
      [
        96,
        112
      ],
      [
        128,
        139
      ]
    ],
    "long-forms": [],
    "ID": "947"
  },
  {
    "text": ". Với ba mô hình ITE - 2 , ITE - 3 và ITE - 4 , em tiến hành tune các siêu tham số lr [ 0.0005, 0.001, 0.01 ] , k [ 32 , 64 , 128 ] , batch-size [ 1024, 2048 ] và η [ 0.01, 0.05,",
    "acronyms": [
      [
        17,
        24
      ],
      [
        27,
        34
      ],
      [
        38,
        45
      ]
    ],
    "long-forms": [],
    "ID": "948"
  },
  {
    "text": "Thứ hai , một mạng AE truyền thống chính là ví dụ điển hình của việc tìm ra một biểu diễn ẩn ít chiều hơn . Mạng nơ-ron này được huấn luyện thông qua việc biểu",
    "acronyms": [
      [
        19,
        21
      ]
    ],
    "long-forms": [],
    "ID": "949"
  },
  {
    "text": "Một số ưu điểm của AJAX : • Cải thiện tốc độ của trang web , tăng hiệu suất và khả năng sử dụng .",
    "acronyms": [
      [
        19,
        23
      ]
    ],
    "long-forms": [],
    "ID": "950"
  },
  {
    "text": "hiệu và biểu diễn toán học cơ bản , tác giả chỉ ra rằng việc tối thiểu hóa hàm mất mát trong mạng AE truyền thống về mặt bản chất cũng chính là cực đại hóa MI giữa input X và biểu diễn ẩn Y , hay nói cách khác cũng tương đương với công thức",
    "acronyms": [
      [
        98,
        100
      ],
      [
        156,
        158
      ]
    ],
    "long-forms": [],
    "ID": "951"
  },
  {
    "text": "Trước hết chúng ta biểu diễn dữ liệu quan sát được là tập văn bản dưới dạng một ma trận document - term DOC − T ERM [ D×V ] , trong đó D là số văn bản , V là kích thước từ điển và phần tử ( d , j ) là số lần xuất hiện từ thứ j của tập từ",
    "acronyms": [],
    "long-forms": [],
    "ID": "952"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 3 TÓM TẮT NỘI DUNG ĐỒ ÁN",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "953"
  },
  {
    "text": "minh họa cho điều đó . Ràng buộc phép chiếu bộ phân loại ( Classifier - Projection Regularization , CPR ) [ 4 ] là một ví dụ về công việc khai thác tính chất này để giải",
    "acronyms": [
      [
        100,
        103
      ]
    ],
    "long-forms": [
      [
        23,
        56
      ]
    ],
    "ID": "954"
  },
  {
    "text": "Hiệu chỉnh thành phần ( 𝒃 ) : Các ràng buộc cho các tham số quan trọng giúp tăng khả năng ghi nhớ của mô hình đã được UCL cải tiến mạnh mẽ , tuy vậy sẽ làm",
    "acronyms": [
      [
        118,
        121
      ]
    ],
    "long-forms": [],
    "ID": "955"
  },
  {
    "text": "K-medoids cho kết quả tốt nhất khi xét cả hai yếu tố là số gene được chọn và hiệu năng . Nguyên nhân có thể do nó ít nhạy cảm với nhiễu hơn so với K-means và HAC nên",
    "acronyms": [
      [
        158,
        161
      ]
    ],
    "long-forms": [],
    "ID": "956"
  },
  {
    "text": "Tập Split CIFAR10 - 100 sau khi học xong , Hard - VBD - CL có kết quả nhỉnh hơn VBD - CL có thể do có những tác vụ \" nhạy cảm \" nên VBD - CL bị quên nhiều tác vụ đó , còn Hard - VBD - CL giữ được độ chính",
    "acronyms": [
      [
        10,
        23
      ],
      [
        80,
        88
      ],
      [
        132,
        140
      ],
      [
        43,
        58
      ],
      [
        171,
        186
      ]
    ],
    "long-forms": [],
    "ID": "957"
  },
  {
    "text": "chia ra gồm lần lượt 50000 và 10000 ảnh tương ứng với mỗi lớp là 5000 và 1000 . Hình 4 . 2 mô tả dữ liệu của CIFAR10 . CIFAR100 cũng tương tự như CIFAR10 , ngoại trừ bộ dữ liệu này bao gồm 100",
    "acronyms": [
      [
        109,
        116
      ],
      [
        119,
        127
      ],
      [
        146,
        153
      ]
    ],
    "long-forms": [],
    "ID": "958"
  },
  {
    "text": "2 LỜI CẢM ƠN Lời đầu tiên , em xin được gửi lời cảm ơn chân thành đến các thầy cô trường Đại học",
    "acronyms": [],
    "long-forms": [],
    "ID": "959"
  },
  {
    "text": "kj đều là các hàm lõm , trong khi hàm log-sum- exp là một hàm lồi khá quen thuộc . Do đó , F ( β tk ) là hàm lõm đối với β tk , nên có thể tìm cực đại bằng việc sử dụng",
    "acronyms": [],
    "long-forms": [],
    "ID": "960"
  },
  {
    "text": "Thay ngược γ ∗ trở lại 2.16 , ta thu được hàm mục tiêu cuối cùng cho VBD : maxw , µ , σ",
    "acronyms": [
      [
        69,
        72
      ]
    ],
    "long-forms": [],
    "ID": "961"
  },
  {
    "text": "Và như vậy , RNN ra đời với ý tưởng chính lá sử dụng một bộ nhớ để lưu lại thông tin từ những bước tính toán xử lý trước để dựa vào nó có thể đưa ra dự đoán chính xác nhất cho bước dự đoán hiện tại .",
    "acronyms": [
      [
        13,
        16
      ]
    ],
    "long-forms": [],
    "ID": "962"
  },
  {
    "text": "Cực đại hoá hàm ELBO này tương đương với cực tiểu khoảng cách KL ( q ( z , Θ ) | |p( z , Θ |x) ) . PVB cũng đi tối ưu một hàm mục tiêu tương tự , gọi",
    "acronyms": [
      [
        16,
        20
      ],
      [
        99,
        102
      ]
    ],
    "long-forms": [],
    "ID": "963"
  },
  {
    "text": "33 CHƯƠNG 4 . THỬ NGHIỆM VÀ ĐÁNH GIÁ",
    "acronyms": [],
    "long-forms": [],
    "ID": "964"
  },
  {
    "text": "Danh mục viết tắt • SSL : Self-supervised learning • DAE : Denoising Auto Encoder",
    "acronyms": [
      [
        20,
        23
      ],
      [
        53,
        56
      ]
    ],
    "long-forms": [],
    "ID": "965"
  },
  {
    "text": "mạng nơ-ron từ pretext task có tốt không . Một trong những cách kinh điển và cũng được xem là tiêu chuẩn để đánh giá chung cho hầu hết các phương pháp SSL đó chính là đưa mạng nơ-ron đã học được này",
    "acronyms": [
      [
        151,
        154
      ]
    ],
    "long-forms": [],
    "ID": "966"
  },
  {
    "text": "Với mô hình LDA , đẳng thức trên có thể viết dưới dạng tường minh : N XX",
    "acronyms": [
      [
        12,
        15
      ]
    ],
    "long-forms": [],
    "ID": "967"
  },
  {
    "text": "Sinh viên thực hiện : Trần Thị Hồng Lớp CNTT 2.03 – K59",
    "acronyms": [
      [
        40,
        44
      ]
    ],
    "long-forms": [],
    "ID": "968"
  },
  {
    "text": "document- term ) để phép truncated SVD có độ chính xác cao , hơn nữa quá trình Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 18",
    "acronyms": [
      [
        35,
        38
      ],
      [
        139,
        150
      ]
    ],
    "long-forms": [],
    "ID": "969"
  },
  {
    "text": "Đầu ra : Tham số biến phân toàn cục λ Khởi tạo ngẫu nhiên λ = λ0 for t = 1,2 , ... , T , ... do",
    "acronyms": [],
    "long-forms": [],
    "ID": "970"
  },
  {
    "text": "PHIẾU GIAO NHIỆM VỤ ĐỒ ÁN TỐT NGHIỆP ( ĐATN ) 1 . Thông tin về sinh viên : •",
    "acronyms": [
      [
        39,
        43
      ]
    ],
    "long-forms": [
      [
        20,
        36
      ]
    ],
    "ID": "971"
  },
  {
    "text": "( 48 ) Chú ý rằng , với mỗi khả năng có thể của π t , thì việc sử dụng Dropout tương đương với việc lấy mẫu một mô hình đơn ( single learner ) từ 2K×V mô hình có",
    "acronyms": [],
    "long-forms": [],
    "ID": "972"
  },
  {
    "text": "log ( 𝑝𝑖 ) Trong đó , S là tập các mẫu Positive trong tập training . Tương tự mô hình NRMS , mô hình NAML cũng được thử nghiệm trên tập dữ",
    "acronyms": [
      [
        86,
        90
      ],
      [
        101,
        105
      ]
    ],
    "long-forms": [],
    "ID": "973"
  },
  {
    "text": "𝑖=1 log 𝑝 ( 𝑦𝑡 | 𝑥𝑡 ) . Sử dụng phương pháp MFVI với hai phân phối biến phân cho tham số toàn cục và cục bộ lần lượt là 𝑞𝑡 ( 𝜃 ) , 𝑞𝑡 ( 𝑠) ta có :",
    "acronyms": [
      [
        44,
        48
      ]
    ],
    "long-forms": [],
    "ID": "974"
  },
  {
    "text": "18 / 4 và 19 / 4 , dễ thấy rằng hiệu năng của hai mô hình ITE - 3 và ITE - 4 cao hơn Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 49",
    "acronyms": [
      [
        58,
        65
      ],
      [
        69,
        76
      ],
      [
        148,
        159
      ]
    ],
    "long-forms": [],
    "ID": "975"
  },
  {
    "text": "định hướng nghiên cứu trong tương lai . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 16",
    "acronyms": [
      [
        100,
        111
      ]
    ],
    "long-forms": [],
    "ID": "976"
  },
  {
    "text": "Hình 4 . 6 : Cải tiến trên JacReg và FrobReg ( 3FC ) Trên 3FC , với JacReg và A_JacReg , A_JacReg chống quá khớp tốt hơn . So với",
    "acronyms": [
      [
        47,
        50
      ],
      [
        58,
        61
      ],
      [
        27,
        33
      ],
      [
        37,
        44
      ],
      [
        68,
        74
      ],
      [
        78,
        86
      ],
      [
        89,
        97
      ]
    ],
    "long-forms": [],
    "ID": "977"
  },
  {
    "text": "- • Tập trung xử lý theo hướng đối tượng , phù hợp sử dụng trong mô hình MVC và cung cấp nhiều API truy vấn cơ sở dữ liệu .",
    "acronyms": [
      [
        73,
        76
      ],
      [
        95,
        98
      ]
    ],
    "long-forms": [],
    "ID": "978"
  },
  {
    "text": "Ngoài ra , ta cũng có thể thấy mô hình ITE - 4 có thể đạt được trạng thái tối ưu hơn so với mô hình ITE - 3 sau chừng hơn 34 vòng lặp . Những kết quả này phần",
    "acronyms": [
      [
        39,
        46
      ],
      [
        100,
        107
      ]
    ],
    "long-forms": [],
    "ID": "979"
  },
  {
    "text": "𝐵 (𝑙) = ( 𝐴(𝑙) ⨀ 𝑠 (𝑙) ) 𝜃 (𝑙) trong đó : 𝑏𝑚ℎ = ∑ ( 𝑙 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "980"
  },
  {
    "text": "Tổng quan : Locally Linear Embedding ( nguồn [ SR03 ] ) là một thuật toán giảm chiều dữ liệu phi tuyến thuộc nhóm Manifold Learning . Ý tưởng chính của thuật",
    "acronyms": [],
    "long-forms": [],
    "ID": "981"
  },
  {
    "text": "Và tiến hành test trên tập này để đưa ra thứ hạng theo dự đoán . Tiếp theo đó là sử dụng 2 độ đo là Hit Ratio ( HR ) và Normalized Discounted",
    "acronyms": [
      [
        112,
        114
      ]
    ],
    "long-forms": [],
    "ID": "982"
  },
  {
    "text": "Hình 7 : Ví dụ về mạng CNN Cách chọn tham số cho CNN Số lượng layer được chọn thường là 3 hoặc 4 .",
    "acronyms": [
      [
        23,
        26
      ],
      [
        49,
        52
      ]
    ],
    "long-forms": [],
    "ID": "983"
  },
  {
    "text": ". Các kết quả nêu trong ĐATN là trung thực , không phải là sao chép toàn văn của bất kỳ công trình nào khác .",
    "acronyms": [
      [
        24,
        28
      ]
    ],
    "long-forms": [],
    "ID": "984"
  },
  {
    "text": "• L̂HP P là một lower bound của LH P P : L̂HP P ( λt , φt , wt | D1 : t , η ) ≤ LHP P ( λt , φt , wt | D1 : t , η )",
    "acronyms": [],
    "long-forms": [],
    "ID": "985"
  },
  {
    "text": "KHOA HỌC MÁY TÍNH Đề tài : Dịch máy sử dụng phương pháp học đối ngẫu Sinh viên thực hiện :",
    "acronyms": [],
    "long-forms": [],
    "ID": "986"
  },
  {
    "text": "3.2.4 Học biểu diễn ẩn thông qua giải bài toán xếp hình Tổ hợp ba bài ở trên liên quan đến các pretext task tạo một mạng AE nhằm khôi",
    "acronyms": [
      [
        121,
        123
      ]
    ],
    "long-forms": [],
    "ID": "987"
  },
  {
    "text": "Parkinson [ 28 ] [ 29 ] , một số loại ung thư [ 30 ] [ 31 ] , tự kỉ [ 32 ] … Nghiên cứu tương quan toàn bộ hệ gene ( Genome-wide association study – GWAS ) và nghiên cứu tương quan toàn bộ hệ phiên mã ( Transcriptome - wide association study –",
    "acronyms": [
      [
        149,
        153
      ]
    ],
    "long-forms": [
      [
        77,
        114
      ]
    ],
    "ID": "988"
  },
  {
    "text": "CIFAR100 và Split CIFAR10 - 100.",
    "acronyms": [
      [
        0,
        8
      ],
      [
        18,
        31
      ]
    ],
    "long-forms": [],
    "ID": "989"
  },
  {
    "text": "của văn bản đầu vào . Từ đây , ta có thể chia mô hình thành 4 công đoạn chính : Tiền xử lý dữ liệu , vec-tơ hóa dữ liệu , Mô hình HAN , Module phân loại .",
    "acronyms": [
      [
        130,
        133
      ]
    ],
    "long-forms": [],
    "ID": "990"
  },
  {
    "text": "7.1 TB Bảng 3 : Cấu hình phần cứng sử dụng Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        4,
        6
      ],
      [
        106,
        117
      ]
    ],
    "long-forms": [],
    "ID": "991"
  },
  {
    "text": "hoá hàm lower bound của hàm log complete- data log p ( w | α, η ) như sau : Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 23",
    "acronyms": [
      [
        136,
        147
      ]
    ],
    "long-forms": [],
    "ID": "992"
  },
  {
    "text": "4 . 1 Dữ liệu . Penn Discourse TreeBank ( PDTB ) : Trong bài toán này , ta sử dụng bộ dữ liệu PDTB",
    "acronyms": [
      [
        42,
        46
      ],
      [
        94,
        98
      ]
    ],
    "long-forms": [],
    "ID": "993"
  },
  {
    "text": "so sánh với phương pháp cũ sử dụng bởi SGL … … … … … … … … … … … … … … … 39",
    "acronyms": [
      [
        39,
        42
      ]
    ],
    "long-forms": [],
    "ID": "994"
  },
  {
    "text": "với item i trong hành vi mục tiêu . Từ đó , nhóm tác giả đề xuất mô hình NMTR với kiến trúc mạng nơ-ron ( Hình 9 ) , có khả năng mô tả tính thứ tự của các hành vi .",
    "acronyms": [
      [
        73,
        77
      ]
    ],
    "long-forms": [],
    "ID": "995"
  },
  {
    "text": "LSTM là một mô hình mạng nơ-ron hồi quy có khả năng học các thông tin phụ thuộc dài hạn ( long-term dependencies ) . LSTM xuất",
    "acronyms": [
      [
        0,
        4
      ],
      [
        117,
        121
      ]
    ],
    "long-forms": [],
    "ID": "996"
  },
  {
    "text": "Hà nội ngày … tháng … năm 2021 Giảng viên hướng dẫn PGS. TS. Thân Quang Khoát",
    "acronyms": [
      [
        52,
        56
      ],
      [
        57,
        60
      ]
    ],
    "long-forms": [],
    "ID": "997"
  },
  {
    "text": "Câu 1 : Shares of UAL , the parent of United Airlines , were extremely active all day Friday , react ing to news and rumor s about the proposed $ 6.79 billion buy - out of the airline by an employee - management group .",
    "acronyms": [],
    "long-forms": [],
    "ID": "998"
  },
  {
    "text": "FID giữa hai tập S1 và S2 là khoảng cách Frechet giữa hai tập I ( S1 ) = { I ( x 11 ) , I ( x 21 ) , ... , I ( x n1 ) } và I ( S2 ) = { I ( x 12 ) , I ( x 22 ) , ... , I ( x n2 ) } . Gọi ( m1 , C1 ) , ( m2 , C2 ) lần lượt là trung bình và ma trận",
    "acronyms": [
      [
        0,
        3
      ]
    ],
    "long-forms": [
      [
        29,
        48
      ]
    ],
    "ID": "999"
  },
  {
    "text": "lên 48.03 % . Tương tự , UCL cũng quan sát một sự cải thiện nhẹ về độ chính xác khi đạt được 64.84 % và cao hơn Dropout 0.52 % .",
    "acronyms": [
      [
        25,
        28
      ]
    ],
    "long-forms": [],
    "ID": "1000"
  },
  {
    "text": "chuyển p ( Θt | Θt−1 ) sao cho khoảng cách KL giữa p ( Θt | D1 : t−1 ) và pδ ( Θt | D1 : t−1 ) ) là không vượt quá κ . Tuy nhiên đây chỉ mới là một điều kiện ràng buộc , chúng",
    "acronyms": [
      [
        43,
        45
      ]
    ],
    "long-forms": [],
    "ID": "1001"
  },
  {
    "text": "và giữa các hệ số trong cùng nhóm . Ý tưởng của tác giả khá đơn giản : đại lượng hiệu chỉnh sẽ là tổng của chuẩn L2 và chuẩn L1 :",
    "acronyms": [],
    "long-forms": [],
    "ID": "1002"
  },
  {
    "text": "dụng LSTM cho kết quả khá tốt ngay cả khi không thực hiện quá trình tinh chỉnh phức tạp . Kết quả huấn luyện của các mô hình sử dụng ConvS2S và Transformer được miêu tả trong Bảng 2 . 5 và 2 .6 .",
    "acronyms": [
      [
        5,
        9
      ],
      [
        133,
        140
      ]
    ],
    "long-forms": [],
    "ID": "1003"
  },
  {
    "text": "( 𝑙 ) Với 𝐿 ̅ là hàm mất mát không phụ thuộc vào 𝛼𝑡 và 𝛼𝑡 là phương sai của phân ( 𝑙 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "1004"
  },
  {
    "text": "Learning ( LCL ) . Hình vẽ dưới đây thể hiện rõ sự khác biệt ở không gian biểu diễn ẩn giữa LCL và CL :",
    "acronyms": [
      [
        11,
        14
      ],
      [
        92,
        95
      ],
      [
        99,
        101
      ]
    ],
    "long-forms": [],
    "ID": "1005"
  },
  {
    "text": "PHỤ LỤC ............................................................................................................ 46 DANH MỤC HÌNH VẼ",
    "acronyms": [],
    "long-forms": [],
    "ID": "1006"
  },
  {
    "text": "Khi đó đầu ra của lớp tích chập sẽ là X̃ với [ X̃ ] kij = ReLU ( O kij ) , ∀i ∈ [ d1 − w + 1 ] , j ∈ [ d2 − w + 1 ] , k ∈ [ dˆ3 ]",
    "acronyms": [],
    "long-forms": [],
    "ID": "1007"
  },
  {
    "text": "phân phối xác suất V chiều của các từ trong tập từ điển này , trong đó những từ có xác suất càng cao sẽ phản ảnh nội dung càng gần với chủ đề đó . Và rõ ràng",
    "acronyms": [],
    "long-forms": [],
    "ID": "1008"
  },
  {
    "text": "Sau khi đã có được thông tin về cấu trúc cục bộ của không gian các view , việc tiếp theo mà LCL cần làm đó là đưa thông tin đó vào mạng để mạng có thể học được . Để làm được điều đó , cặp positive mà LCL xét đến là một biểu diễn ẩn của một",
    "acronyms": [
      [
        92,
        95
      ],
      [
        200,
        203
      ]
    ],
    "long-forms": [],
    "ID": "1009"
  },
  {
    "text": "của VBD - CL khi học tác vụ mới . 34 5.3",
    "acronyms": [
      [
        4,
        12
      ]
    ],
    "long-forms": [],
    "ID": "1010"
  },
  {
    "text": "THỬ NGHIỆM VÀ ĐÁNH GIÁ Hình 16 : Mô hình ITE - 4 Điểm mạnh của mô hình :",
    "acronyms": [
      [
        41,
        48
      ]
    ],
    "long-forms": [],
    "ID": "1011"
  },
  {
    "text": "có tương tác , và 0 ngược lại . Vì mô hình chỉ tập trung vào những dữ liệu quan sát được , ta sử dụng R∗ để biểu thị tập các cặp người dùng - bộ phim quan sát được trong R , và V ∗ để biểu",
    "acronyms": [],
    "long-forms": [],
    "ID": "1012"
  },
  {
    "text": "dung bài báo trực tiếp từ trang web của Microsoft . 4 . 2 . 2 Tập dữ liệu Pega ( VC corp ) Để thử nghiệm và đánh giá các mô hình đối với dữ liệu thực tế của Việt Nam , đồ",
    "acronyms": [],
    "long-forms": [],
    "ID": "1013"
  },
  {
    "text": "Điểm mạnh : • Kết hợp hai mô hình GMF và MLP , mang tính tổng quát hóa cao • Nhờ tính chất tuyến tính của mô hình GMF và tính chất phi tuyến của mô",
    "acronyms": [
      [
        34,
        37
      ],
      [
        41,
        44
      ],
      [
        114,
        117
      ]
    ],
    "long-forms": [],
    "ID": "1014"
  },
  {
    "text": "phần phối population FD như trên và giá trị n sao cho FD ( n ) lớn nhất chính là quan sát được sinh ra . Quay trở lại định nghĩa cho population posterior , hay phân bố hậu nghiệm",
    "acronyms": [],
    "long-forms": [],
    "ID": "1015"
  },
  {
    "text": "Em cũng xin gửi lời cảm ơn tới anh Nguyễn Hữu Thiện – Assistant Professor – University of Oregon , TS. Thân Quang Khoát đã hướng dẫn và giúp đỡ em , cung cấp cho em kiến thức cũng như thiết bị tính toán hiệu năng cao trong suốt quá trình em làm",
    "acronyms": [
      [
        99,
        102
      ]
    ],
    "long-forms": [],
    "ID": "1016"
  },
  {
    "text": "Giả sử sau khi học mô hình trên tập dữ liệu training Dtrain , chúng ta thu được tham số mô hình là Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        159,
        170
      ]
    ],
    "long-forms": [],
    "ID": "1017"
  },
  {
    "text": "ℎ𝑖𝑡 tổng hợp thông tin của cả câu lấy trung tâm là từ 𝑤𝑖𝑡 . Ở đây ta sử dụng trực tiếp ma trận nhúng mức từ . Cuối cùng , ta thu được chuỗi mã hóa của từ ℎ𝑖𝑡 , ℎ𝑖𝑡 sẽ trở thành đầu vào của tầng tiếp",
    "acronyms": [],
    "long-forms": [],
    "ID": "1018"
  },
  {
    "text": "Trong [ 29 ] , các tác giả đã đề xuất phương pháp tìm sự tương đồng giữa các ký tự OOV trong câu nguồn và câu đích trong quá trình hậu xử lý sau khi dịch . Một phương pháp phổ",
    "acronyms": [
      [
        83,
        86
      ]
    ],
    "long-forms": [],
    "ID": "1019"
  },
  {
    "text": "= log 𝑝 ( 𝐷 | 𝜃 ) + log 𝑝 ( 𝜃 ) − log 𝑝 ( 𝐷 ) Đối với trường hợp hai tác vụ A và B tương ứng với bộ dữ liệu 𝐷𝐴 và 𝐷𝐵 , ta viết lại công thức PT 2.8 :",
    "acronyms": [
      [
        141,
        143
      ]
    ],
    "long-forms": [],
    "ID": "1020"
  },
  {
    "text": "Trong trường hợp của mô hình ITE - item _ pcat , sự khác nhau là ở vec-tơ đầu vào của item : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        156,
        167
      ],
      [
        29,
        46
      ]
    ],
    "long-forms": [],
    "ID": "1021"
  },
  {
    "text": "Ở đây các loại bệnh có hệ số di truyền ( heritability ) cao được sử dụng để đánh giá . Hệ số di truyền là một chỉ số thống kê được sử dụng để ước tính mức độ biến đổi của tính trạng kiểu hình trong một quần thể do sự biến đổi di truyền của các cá thể trong quần thể",
    "acronyms": [],
    "long-forms": [],
    "ID": "1022"
  },
  {
    "text": "Khi đó chúng tôi đạt được hàm mục tiêu đối với β tk như sau : F ( β tk ) = log p( β tk | β kt−1 ) + G ( β tk ) V",
    "acronyms": [],
    "long-forms": [],
    "ID": "1023"
  },
  {
    "text": "hay : λt = λt−1 + ( λ̂t − λ0 ) = λt−1 + λ̃t = λ0 + ti=1 ( λ̂i − λ0 ) với λ̃t được xem là thông tin học được từ dữ liệu ở minibatch thứ t . Phương pháp học SVB cho LDA được trình bày trong thuật toán 2 .",
    "acronyms": [
      [
        155,
        158
      ],
      [
        163,
        166
      ]
    ],
    "long-forms": [],
    "ID": "1024"
  },
  {
    "text": "Ngoài max - pooling còn có các loại average- pooling và L2 pooling . Cuối cùng , ta đặt tất cả các lớp với nhau tạo thành CNN với đầu ra gồm các nơ -ron có số lượng tùy",
    "acronyms": [
      [
        122,
        125
      ]
    ],
    "long-forms": [],
    "ID": "1025"
  },
  {
    "text": "nghiên cứu vẫn đang triển khai song song việc đánh giá trên các phương pháp như MOCO và BYOL , tuy nhiên , kết quả không hoàn toàn tốt như trên SimCLR , cần phải có thêm thời gian để điều chỉnh các tham số một cách phù hợp cũng như quan",
    "acronyms": [
      [
        80,
        84
      ],
      [
        88,
        92
      ],
      [
        144,
        150
      ]
    ],
    "long-forms": [],
    "ID": "1026"
  },
  {
    "text": "Mô hình được phát triển từ mô hình MF , tập trung vào việc dự đoán chính xác ratings người dùng dành cho các item , bằng việc sử dụng thêm những tập dữ liệu hành vi tiềm ẩn ( item trong dữ liệu mà phương pháp này sử dụng để",
    "acronyms": [
      [
        35,
        37
      ]
    ],
    "long-forms": [],
    "ID": "1027"
  },
  {
    "text": "Một số dạng của mô hình NCF tổng quát là GMF , MLP , NeuMF , đây cũng là các mô hình cơ sở cho mô hình ITE mà em áp dụng để cải tiến mô hình của mình . Generalized Matrix Factorization ( GMF )",
    "acronyms": [
      [
        24,
        27
      ],
      [
        41,
        44
      ],
      [
        47,
        50
      ],
      [
        53,
        58
      ],
      [
        103,
        106
      ],
      [
        187,
        190
      ]
    ],
    "long-forms": [],
    "ID": "1028"
  },
  {
    "text": "Kết quả của biểu diễn ẩn khi thêm vào lớp cuối ( Nguồn [ Che+20 ] ) So sánh kiến trúc MOCO với các kĩ thuật trước đây ( Nguồn [ He+20 ] )",
    "acronyms": [
      [
        86,
        90
      ]
    ],
    "long-forms": [],
    "ID": "1029"
  },
  {
    "text": "Có một chú ý khi xây dựng ELBO ở hai trường hợp này là phải đảm bảo giá trị W tối ưu bằng huấn luyện 14",
    "acronyms": [
      [
        26,
        30
      ]
    ],
    "long-forms": [],
    "ID": "1030"
  },
  {
    "text": "19 trong một tập từ điển có kích thước V . • Mỗi văn bản được kí hiệu là d , bao gồm Nd từ , được biểu diễn dưới dạng",
    "acronyms": [],
    "long-forms": [],
    "ID": "1031"
  },
  {
    "text": "kết quả tốt nhất trên bộ dữ liệu Split MNIST và giúp EWC đạt được độ chính xác trung bình cao nhất trên cả ba phương pháp . Bảng 4 . 4 tổng hợp lại kết quả của các thử nghiệm sau quá trình học trên 5 tác",
    "acronyms": [
      [
        39,
        44
      ],
      [
        53,
        56
      ]
    ],
    "long-forms": [],
    "ID": "1032"
  },
  {
    "text": "trong đó : p ( β|η ) = Dir ( β|η ) ∝ C Y",
    "acronyms": [],
    "long-forms": [],
    "ID": "1033"
  },
  {
    "text": "} cho SVB - PP ; population size α ∈ { 103 , 104 , 105 , 106 , 5.103 , 5.104 , 5.105 , 5.106 } , dimming factor κ ∈ { 0.5, 0.6 , 0.7 , 0.8 , 0.9 } và τ0 = 1 cho PVB .",
    "acronyms": [
      [
        6,
        14
      ],
      [
        161,
        164
      ]
    ],
    "long-forms": [],
    "ID": "1034"
  },
  {
    "text": "làm việc và nghiên cứu tại đây . Đặc biệt em xin cảm ơn GS. TS. Thân Quang Khoát và anh Phạm Văn Hoàng – hiện đang là sinh viên K61 , thầy và anh là những",
    "acronyms": [
      [
        56,
        59
      ],
      [
        60,
        63
      ]
    ],
    "long-forms": [],
    "ID": "1035"
  },
  {
    "text": "\u0015 tổng các giá trị rời rạc của biến đó . \u0014 phân bằng q ( Z )",
    "acronyms": [],
    "long-forms": [],
    "ID": "1036"
  },
  {
    "text": "và tầng embedding . Trong trường hợp vec-tơ đầu vào là one-hot , P và Q tương ứng chính là các ma trận thuộc tính ẩn của người dùng và item , với",
    "acronyms": [],
    "long-forms": [],
    "ID": "1037"
  },
  {
    "text": "tham số của phân phối biến phân ứng với β Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 10",
    "acronyms": [
      [
        102,
        113
      ]
    ],
    "long-forms": [],
    "ID": "1038"
  },
  {
    "text": "Và phân phối này sẽ được sử dụng làm phân phối tiên nghiệm cho tham số Θ ở minibatch thứ t . Để ý rằng một điều thú vị là nếu tại minibatch thứ t , phương pháp SVB sử",
    "acronyms": [
      [
        160,
        163
      ]
    ],
    "long-forms": [],
    "ID": "1039"
  },
  {
    "text": "ta tổng quát cho trường hợp task thứ T : p ( θ | D1:T ) ≈ qT ( θ ) = proj ( q _ T - 1 ( θ ) p ( DT | θ ) ) Variational Continual Learning ( VCL ) cố gắng đi xây dựng một phép chiếu được định nghĩa thông qua bài toán tối thiểu hóa KL - Divergence Reverse trên không gian phân phối <",
    "acronyms": [
      [
        140,
        143
      ],
      [
        230,
        232
      ]
    ],
    "long-forms": [],
    "ID": "1040"
  },
  {
    "text": "NDCG @ 10 Hình 22 : So sánh các mô hình trên bộ Retailrocket qua từng vòng lặp với k = 16 Hình 21 , 22 tương ứng là kết quả của các mô hình trên bộ dữ liệu Retailrocket qua từng vòng lặp với các giá trị k = 8, 16 .",
    "acronyms": [
      [
        0,
        4
      ]
    ],
    "long-forms": [],
    "ID": "1041"
  },
  {
    "text": "Chú ý rằng , mặc dù theo 3 . 2 . 2 ta lựa chọn 𝛼 (𝑙 ) là một véc-tơ D chiều , tuy nhiên để cho việc viết công thức được mạch lạc và tường minh ở đây ta giả sử 𝛼 ( 𝑙 ) ( 𝑙 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "1042"
  },
  {
    "text": "nghiệm ta sẽ đặt thêm một hệ số cân bằng 𝛽 trước thành phần KL của tham số cục bộ , để cân bằng giữa việc tham số cục bộ 𝑠 sẽ học khớp với dữ liệu và bảo đảm tri thức tiên nghiệm cho tham số này .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1043"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 50",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "1044"
  },
  {
    "text": "* Bước cập nhật tham P số toàn PNdcục *",
    "acronyms": [],
    "long-forms": [],
    "ID": "1045"
  },
  {
    "text": "Kết quả của mô hình ITE - onehot và mô hình NMTR vẫn xấp xỉ nhau , và cao hơn so với mô hình MTMF .",
    "acronyms": [
      [
        20,
        32
      ],
      [
        44,
        48
      ],
      [
        93,
        97
      ]
    ],
    "long-forms": [],
    "ID": "1046"
  },
  {
    "text": "thường được gọi đầy đủ là hàm Evidence Lower BOund ( ELBO ) . Rõ ràng để tìm được phân phối q xấp xỉ tốt nhất cho phân phối hậu nghiệm p , chúng ta mong muốn giá trị KL ( q|| p ) càng gần 0 càng tốt , tức là cần tối",
    "acronyms": [
      [
        53,
        57
      ]
    ],
    "long-forms": [],
    "ID": "1047"
  },
  {
    "text": "* — — — — — — — ĐỒ ÁN TỐT NGHIỆP",
    "acronyms": [],
    "long-forms": [],
    "ID": "1048"
  },
  {
    "text": "trang đăng bài tuyển dụng . Từ đây , NTD có thể sửa nội dung bài đăng của mình .",
    "acronyms": [
      [
        37,
        40
      ]
    ],
    "long-forms": [],
    "ID": "1049"
  },
  {
    "text": "Ký hiệu w1∗ , w2∗ là nghiệm tối ưu sau khi huấn luyện lần lượt hai tác vụ , L1 , L2 là hàm mất mát của hai tác vụ . Độ",
    "acronyms": [],
    "long-forms": [],
    "ID": "1050"
  },
  {
    "text": "collaborative filtering layer s ) , với đầu ra cuối cùng là điểm số dự đoán . Mỗi tầng trong NCF có thể tùy biến để phát hiện các cấu trúc ẩn quan trọng trong tương",
    "acronyms": [
      [
        93,
        96
      ]
    ],
    "long-forms": [],
    "ID": "1051"
  },
  {
    "text": "thuật LRT : 21 ( 𝑙 )",
    "acronyms": [
      [
        6,
        9
      ]
    ],
    "long-forms": [],
    "ID": "1052"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 47",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "1053"
  },
  {
    "text": "Kết hợp với công thức ( 14 ) ta có : q ( Θ | λt ) ≈ q ( Θ | λ̂t )",
    "acronyms": [],
    "long-forms": [],
    "ID": "1054"
  },
  {
    "text": "EWC đề xuất xấp xỉ ma trận hiệp phương sai chéo Σ bằng ma trận Fisher Information chéo F : 1",
    "acronyms": [
      [
        0,
        3
      ]
    ],
    "long-forms": [],
    "ID": "1055"
  },
  {
    "text": "thế Kết quả NTD xem được thông tin hồ sơ về 1 nhân viên",
    "acronyms": [
      [
        12,
        15
      ]
    ],
    "long-forms": [],
    "ID": "1056"
  },
  {
    "text": "Sinh ra từ wdn ∼ M ultinomial ( β̃zdn ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 49",
    "acronyms": [
      [
        100,
        111
      ]
    ],
    "long-forms": [],
    "ID": "1057"
  },
  {
    "text": "NDCG @ 10 Hình 28 : Kết quả các mô hình ITE ngày 19 / 4 Nhìn vào kết quả của ba mô hình ITE - 2 , ITE - 3 và ITE - 4 trong cả hai ngày",
    "acronyms": [
      [
        0,
        4
      ],
      [
        40,
        43
      ],
      [
        88,
        95
      ],
      [
        98,
        105
      ],
      [
        109,
        116
      ]
    ],
    "long-forms": [],
    "ID": "1058"
  },
  {
    "text": "tham số γ . Phân phối Gauss kỳ vọng 0 và phân tách hoàn toàn ( fully factorized ) Q",
    "acronyms": [],
    "long-forms": [],
    "ID": "1059"
  },
  {
    "text": "1 GIỚI THIỆU ĐỀ TÀI 1 Giới thiệu đề tài Dạo gần đây , các hệ thống thương mại điện tử ở Việt Nam đang nổi lên nhanh",
    "acronyms": [],
    "long-forms": [],
    "ID": "1060"
  },
  {
    "text": "Mô hình được tối ưu sử dụng kĩ thuật stochastic gradient descent ( SGD ) . Mô hình Neural Multi-Task Recommendation ( NMTR )",
    "acronyms": [
      [
        67,
        70
      ],
      [
        118,
        122
      ]
    ],
    "long-forms": [],
    "ID": "1061"
  },
  {
    "text": "giúp tránh hiện tượng quên nhanh chóng ) sẽ được kết hợp lại để suy diễn cho dữ liệu tương ứng ở tác vụ này . Trong công việc này , phương pháp xấp xỉ VI sẽ được áp dụng lên phân phối",
    "acronyms": [
      [
        151,
        153
      ]
    ],
    "long-forms": [],
    "ID": "1062"
  },
  {
    "text": "với ảnh thật nhất có thể . Từ đó ta có hàm mất mát tổng hợp như sau : L = λrec Lrec + λadv Ladv",
    "acronyms": [],
    "long-forms": [],
    "ID": "1063"
  },
  {
    "text": "dụng Dropout lại làm giảm hiệu năng của hai mô hình OVI là VCL và UCL . Điều này có thể giải thích là bởi vì bộ dữ liệu Split MNIST có phân phối dữ liệu khá tương đồng nhau , VCL và UCL vốn dĩ đã có tính chất không chắc chắn và việc áp",
    "acronyms": [
      [
        52,
        55
      ],
      [
        59,
        62
      ],
      [
        66,
        69
      ],
      [
        126,
        131
      ],
      [
        175,
        178
      ],
      [
        182,
        185
      ]
    ],
    "long-forms": [],
    "ID": "1064"
  },
  {
    "text": "Trong khuôn khổ ĐATN , em tiến hành triển khai mô hình HAN cho bài toán phân loại cảm xúc văn bản tiếng Việt đồng thời đánh giá mô hình này với những mô hình học khác cho tiếng Việt .",
    "acronyms": [
      [
        16,
        20
      ],
      [
        55,
        58
      ]
    ],
    "long-forms": [],
    "ID": "1065"
  },
  {
    "text": "Sử dụng công thức ( 7 ) ta có thể thấy s23 ( 0.66 ) > s12 ( 0.5 ) > s13 ( 0.4 ) . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 16",
    "acronyms": [
      [
        145,
        156
      ]
    ],
    "long-forms": [],
    "ID": "1066"
  },
  {
    "text": "HAT ở ba tác vụ đầu có độ chính xác cao hơn AGS - CL , nhưng sau đó do quên nhiều tri thức cũ và thậm chí ở những tác vụ sau không đạt độ chính xác 39",
    "acronyms": [
      [
        0,
        3
      ],
      [
        44,
        52
      ]
    ],
    "long-forms": [],
    "ID": "1067"
  },
  {
    "text": "ngẫu nhiên hỗ trợ khác \u000f ∼ p(\u000f) , trong đó p(\u000f) không phụ thuộc vào φ . Ví dụ biến ngẫu nhiên tuân theo phân phối Gauss một chiều x ∼ N ( µ , σ 2 ) , φ = ( µ , σ ) có thể được biểu diễn như sau : x = µ + σ \u000f = g ( φ , \u000f) , \u000f ∼ N ( 0, 1 ) .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1068"
  },
  {
    "text": "Kết quả của việc thử nghiệm so sánh một phép DA và nhiều phép DA được thể hiện trong ma trận ở hình 14 ( đường chéo chính là việc chỉ dùng một phép DA duy nhất ) .",
    "acronyms": [
      [
        45,
        47
      ],
      [
        62,
        64
      ],
      [
        148,
        150
      ]
    ],
    "long-forms": [],
    "ID": "1069"
  },
  {
    "text": "Lời động viên , khích lệ tinh thần đến từ gia đình và bạn bè luôn là động lực để em phấn đấu hết mình và hoàn thành những mục tiêu mà bản thân đề ra . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        211,
        222
      ]
    ],
    "long-forms": [],
    "ID": "1070"
  },
  {
    "text": "Nghiệm của bài toán ( 1 ) được tính bằng công thức : 𝛽 ∗ = ( 𝐴𝑇 𝐴 ) − 1 𝐴𝑇 𝑦",
    "acronyms": [],
    "long-forms": [],
    "ID": "1071"
  },
  {
    "text": "trong khi học MTL với nhiều tác vụ , mô hình có thể học tập trung được các đặc trưng thực sự tốt cho các tác vụ . Vì vậy , nó làm tăng khả năng nắm bắt được các đặc trưng",
    "acronyms": [
      [
        14,
        17
      ]
    ],
    "long-forms": [],
    "ID": "1072"
  },
  {
    "text": "điều này , trước hết chúng ta biểu diễn logarit của hàm phân phối biên tại X, còn gọi là \" log complete -data \" hoặc evidence , dưới dạng như sau : Z",
    "acronyms": [],
    "long-forms": [],
    "ID": "1073"
  },
  {
    "text": "( TRAIN _ SET _ LENGTH , MAX _ SENTS , MAX _ SENT _ LENGTH ) , mỗi đơn vị trong ma trận là số nguyên tương ứng với vị trí của token trong bộ từ vựng . Kết thúc , ta biểu diễn dữ liệu ( văn bản ) huấn luyện ban đầu thành một ma trận dữ liệu",
    "acronyms": [],
    "long-forms": [],
    "ID": "1074"
  },
  {
    "text": "Với thiết lập trong mô hình gốc , tác giả đang để temperature = 0.1 , trong phần thí nghiệm này , em thử đổi temperature = 0.5 và thực hiện quan sát trên cả LCL và SimCLR , kết quả thu được :",
    "acronyms": [
      [
        157,
        160
      ],
      [
        164,
        170
      ]
    ],
    "long-forms": [],
    "ID": "1075"
  },
  {
    "text": "skip - gram cho độ chính xác cao hơn . Vì vậy mà hiện nay nó được sử dụng nhiều hơn so với mô hình CBOW .",
    "acronyms": [
      [
        99,
        103
      ]
    ],
    "long-forms": [],
    "ID": "1076"
  },
  {
    "text": ". . . . Hiệu suất LPP của các phương pháp trên mô hình LDA với bộ",
    "acronyms": [
      [
        18,
        21
      ],
      [
        55,
        58
      ]
    ],
    "long-forms": [],
    "ID": "1077"
  },
  {
    "text": "luyện , tên là Stochastic Gradient Descent ( SGD ) . Khi đó thay vì cập nhật trọng J ( W )",
    "acronyms": [
      [
        45,
        48
      ]
    ],
    "long-forms": [],
    "ID": "1078"
  },
  {
    "text": "sử dụng kết hợp các phép DA có thể khiến cho pretext task trở nên khó hơn nhưng Đồ án tốt nghiệp 29",
    "acronyms": [
      [
        25,
        27
      ]
    ],
    "long-forms": [],
    "ID": "1079"
  },
  {
    "text": "như HAT . Trong các phương pháp , EWC cần một lượng bộ nhớ nhiều nhất vì phải lưu lại cả độ quan trọng của tham số và bộ tham số tối ưu tác vụ phía trước , còn",
    "acronyms": [
      [
        4,
        7
      ],
      [
        34,
        37
      ]
    ],
    "long-forms": [],
    "ID": "1080"
  },
  {
    "text": "9 do vậy EWC xấp xỉ đại lượng này trong biểu thức PT 2.9 qua phép xấp xỉ Taylor bậc 2 tại 𝜃 = 𝜃𝐴 ∗ , khi đó :",
    "acronyms": [
      [
        9,
        12
      ],
      [
        50,
        52
      ]
    ],
    "long-forms": [],
    "ID": "1081"
  },
  {
    "text": "[ 3 ] có thể coi là một biến thể của mạng nơ-ron tích chập ( CNN ) , là một mạng nơ-ron nhiều tầng hoạt động trực tiếp trên đồ thị , trong đó biểu diễn của mỗi nút được cập nhật lặp lại từ các nút lân cận của",
    "acronyms": [
      [
        61,
        64
      ]
    ],
    "long-forms": [
      [
        37,
        58
      ]
    ],
    "ID": "1082"
  },
  {
    "text": "DANH MỤC HÌNH VẼ 3 . 1 Minh họa tư tưởng của nhóm phương pháp dựa trên tri thức . Nguồn [ 5 ] .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1083"
  },
  {
    "text": "KẾT LUẬN VÀ HƯỚNG PHÁT TRIỂN TƯƠNG LAI Phần này sẽ tóm tắt những kết quả cũng như những kinh nghiệm đã đạt được trong quá trình làm đồ án và định hướng phát triển nghiên cứu tiếp theo trong",
    "acronyms": [],
    "long-forms": [],
    "ID": "1084"
  },
  {
    "text": "ở phần sau . Vì vậy em sẽ giới thiệu chi tiết cách xây dựng từ mô hình GMF , MLP cho đến NeuMF , và những ưu , nhược điểm của mô hình NeuMF .",
    "acronyms": [
      [
        71,
        74
      ],
      [
        77,
        80
      ],
      [
        89,
        94
      ],
      [
        134,
        139
      ]
    ],
    "long-forms": [],
    "ID": "1085"
  },
  {
    "text": "dụng gated linear units ( GLU ) ( Dauphin et al. , 2016 ) ta được : zi = Ai ʘ sigmoid ( Bi ) ∈ ℝde Áp dụng residual connection ( Res1 ) ta thu được đầu ra dùng cho đầu vào của tầng tiếp",
    "acronyms": [
      [
        26,
        29
      ]
    ],
    "long-forms": [],
    "ID": "1086"
  },
  {
    "text": "Nhìn chung , có thể nói các phương pháp trong CL nói riêng và SSL nói chung đang rất phát triển . Song , dù có đạt được các kết quả SOTA , thì trong bản thân mỗi",
    "acronyms": [
      [
        46,
        48
      ],
      [
        62,
        65
      ],
      [
        132,
        136
      ]
    ],
    "long-forms": [],
    "ID": "1087"
  },
  {
    "text": "• Trong quá trình dữ liệu đến liên tục theo từng minibatch , SVB sử dụng phân phối hậu nghiệm trên dữ liệu quá khứ làm phân phối tiên nghiệm ( prior distribution ) cho dữ liệu hiện tại .",
    "acronyms": [
      [
        61,
        64
      ]
    ],
    "long-forms": [],
    "ID": "1088"
  },
  {
    "text": "Nhà tuyển dụng đăng nhập vào hệ thống 1 - NTD chọn vào thanh tìm kiếm sẽ được dẫn tới 1 trang web đã hiển thị rất nhiều hồ sơ theo",
    "acronyms": [
      [
        42,
        45
      ]
    ],
    "long-forms": [],
    "ID": "1089"
  },
  {
    "text": "Giáo viên hướng dẫn : ThS. Ngô Văn Linh HÀ NỘI 5 /2019 PHIẾU GIAO NHIỆM VỤ ĐỒ ÁN TỐT NGHIỆP",
    "acronyms": [
      [
        22,
        26
      ]
    ],
    "long-forms": [],
    "ID": "1090"
  },
  {
    "text": "Kế hoạch này có thể sử dụng phương pháp : time - based decay , step decay , exponential decay . Ở",
    "acronyms": [],
    "long-forms": [],
    "ID": "1091"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 22 thông tin từ các bộ dữ liệu bên ngoài như hướng thứ hai thì các nghiên cứu này sử dụng",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "1092"
  },
  {
    "text": "3.2 Mô hình phân loại cảm xúc Amend Representation Module ( ARM ) 3 . 2 . 1 Đặc điểm bạch tạng ( Albino feature )",
    "acronyms": [
      [
        60,
        63
      ]
    ],
    "long-forms": [],
    "ID": "1093"
  },
  {
    "text": "– So với mô hình ITE- onehot , mô hình có thêm vec-tơ pcat đầu vào cho item , giúp mô hình nhận biết các item tương đồng ngay từ đầu vào • Hạn chế",
    "acronyms": [
      [
        17,
        28
      ]
    ],
    "long-forms": [],
    "ID": "1094"
  },
  {
    "text": "thời gian . Sau đó họ giới hạn không gian các khả năng có thể của p ( Θt | D1 : t−1 ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        146,
        157
      ]
    ],
    "long-forms": [],
    "ID": "1095"
  },
  {
    "text": "Kết quả được thể hiện trên Hình 12 . SVB không hoạt động tốt khi đối mặt với concept drift : hiệu suất giảm mạnh khi có sự chuyển tiếp giữa hai lớp và khôi",
    "acronyms": [
      [
        37,
        40
      ]
    ],
    "long-forms": [],
    "ID": "1096"
  },
  {
    "text": "trong đó : s̃kj = ( βi πi ) T xk , A( s̃k ) = log Vi =1 exp ( s̃ki ) . Sử dụng các biến đổi trên , ta có thể viết hàm F dưới dạng tương đương sau :",
    "acronyms": [],
    "long-forms": [],
    "ID": "1097"
  },
  {
    "text": "Ở đây tạo ra một nội dung bộ nhớ mới sử dụng cổng reset gate để lưu trữ thông tin phù hợp từ những trạng thái trước đó . Nếu 𝑟𝑡 có giá trị là 0 , tức là không sử dụng bất kì trạng thái nào trước đó .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1098"
  },
  {
    "text": "rằng bài toán ( 2 ) có thể tách thành N bài toán nhỏ , mỗi bài toán ứng với việc đi tối ưu một cột của ma trận Q : L ( qi ) =",
    "acronyms": [],
    "long-forms": [],
    "ID": "1099"
  },
  {
    "text": "Khi đó mô hình có tính linh hoạt nhờ vào hàm phi tuyến để biểu diễn tương tác giữa người dùng và item , vì vậy có thể học một cách tốt hơn . Các công thức của mô hình MLP được định nghĩa cụ thể như sau :",
    "acronyms": [
      [
        167,
        170
      ]
    ],
    "long-forms": [],
    "ID": "1100"
  },
  {
    "text": "Bảng 4 . 5 Giá trị trung bình độ gắn kết các chủ đề ( NPMI ) cho tập dữ liệu 20Newsgroups . Cao hơn thì tốt hơn",
    "acronyms": [
      [
        54,
        58
      ]
    ],
    "long-forms": [],
    "ID": "1101"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 23 3 . CÁC PHƯƠNG PHÁP GIẢI QUYẾT BÀI TOÁN",
    "acronyms": [
      [
        61,
        70
      ]
    ],
    "long-forms": [],
    "ID": "1102"
  },
  {
    "text": "Ước lượng cực đại hậu nghiệm ( Maximum a Posterior , MAP ) cực đại hóa phân phối hậu nghiệm này : θM AP = argmaxθ p ( θ |D )",
    "acronyms": [
      [
        53,
        56
      ]
    ],
    "long-forms": [
      [
        0,
        28
      ]
    ],
    "ID": "1103"
  },
  {
    "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 31",
    "acronyms": [
      [
        267,
        276
      ]
    ],
    "long-forms": [],
    "ID": "1104"
  },
  {
    "text": "contest này . 4.1.4 Mysql MySQL là một hệ thống quản lý cơ sở dữ liệu quan hệ mã nguồn mở ( RDBMS ) dựa",
    "acronyms": [
      [
        92,
        97
      ]
    ],
    "long-forms": [
      [
        39,
        89
      ]
    ],
    "ID": "1105"
  },
  {
    "text": "( dữ liệu ) E với tác vụ T và được đánh giá bởi độ đo P nếu máy tính khiến tác vụ T này cải thiện được độ chính xác P thông qua dữ liệu E cho trước . Học máy có liên hệ mật thiết",
    "acronyms": [],
    "long-forms": [],
    "ID": "1106"
  },
  {
    "text": "Hình 9 : Hiệu suất LPP của các iDroppout trên 2 bộ 20NewsGroups và TMN title khi thay đổi phương sai σ 2 Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 62",
    "acronyms": [
      [
        19,
        22
      ],
      [
        67,
        70
      ],
      [
        165,
        176
      ]
    ],
    "long-forms": [],
    "ID": "1107"
  },
  {
    "text": "trình học chuỗi tương tác của User cũng chưa tối ưu . Hơn nữa , mô hình LSTUR gặp vấn đề với chuỗi tương tác dài , các hành vi của người dùng càng xa trong",
    "acronyms": [
      [
        72,
        77
      ]
    ],
    "long-forms": [],
    "ID": "1108"
  },
  {
    "text": "Trí tuệ nhân tạo : ( AI - Artificial Intelligence ) là các kỹ thuật giúp cho máy tính có thể thực hiện những công việc thường ngày của con người chúng ta . Các",
    "acronyms": [
      [
        21,
        23
      ]
    ],
    "long-forms": [
      [
        0,
        16
      ]
    ],
    "ID": "1109"
  },
  {
    "text": "ˆ t L̂ . Do đó có thể cập nhật các tham số biến phân • ∇",
    "acronyms": [],
    "long-forms": [],
    "ID": "1110"
  },
  {
    "text": "CIFAR100 và Split CIFAR10 / 100 . Hình 4 . 7 mô tả chi tiết kiến trúc của phương pháp đề xuất được sử dụng trong thử nghiệm này .",
    "acronyms": [
      [
        0,
        8
      ],
      [
        18,
        31
      ]
    ],
    "long-forms": [],
    "ID": "1111"
  },
  {
    "text": "kịch bản học từng miền ; P ( Y t ) = P ( Y t+1 ) và Y t 6 = Y t+1 trong kịch bản học từng lớp . 2.2",
    "acronyms": [],
    "long-forms": [],
    "ID": "1112"
  },
  {
    "text": "Xét sự thay đổi của từng tác vụ ở hình 5 . 4 , AGS - CL duy trì ổn định độ chính xác của các tác vụ , VBD - CL cũng tương tự nhưng trên một số tác vụ như tác vụ đầu tiên có quên nhiều hơn hay thứ năm có cải thiện được độ",
    "acronyms": [
      [
        47,
        55
      ],
      [
        102,
        110
      ]
    ],
    "long-forms": [],
    "ID": "1113"
  },
  {
    "text": "có thể khiến cho số lượng tham số của mô hình tăng lên đáng kể khi số lượng tác vụ ngày càng nhiều . Biến hỗ trợ 𝑠 ( 𝑙 ) là ma trận có kích thước 𝑀 × 𝐷 , tuy nhiên ở",
    "acronyms": [],
    "long-forms": [],
    "ID": "1114"
  },
  {
    "text": "Q : tổng số item gợi ý rank i : Vị trí thứ i Độ đo nDCG @ k",
    "acronyms": [
      [
        51,
        55
      ]
    ],
    "long-forms": [],
    "ID": "1115"
  },
  {
    "text": "Mã nguồn của hai phương pháp EWC và UCL được sử dụng và kế thừa từ mã nguồn công bố trong bài báo gốc UCL [ 3 ] . Phương pháp VCL được cài đặt lại dựa",
    "acronyms": [
      [
        29,
        32
      ],
      [
        36,
        39
      ],
      [
        102,
        105
      ],
      [
        126,
        129
      ]
    ],
    "long-forms": [],
    "ID": "1116"
  },
  {
    "text": "Tìm hiểu về phương pháp Auto-encoding variational bayes ( VAE ) . Áp dụng phương pháp VAE vào mô hình LDA . Thực nghiệm xây dựng mô hình và so sánh với mô hình khác là Online LDA",
    "acronyms": [
      [
        58,
        61
      ],
      [
        86,
        89
      ],
      [
        102,
        105
      ],
      [
        175,
        178
      ]
    ],
    "long-forms": [],
    "ID": "1117"
  },
  {
    "text": "liên tục . Đây chính là động lực cho tác giả của EWC và các phương pháp tiếp cận dựa trên ràng buộc trọng số khác đề ra giải pháp có thể đánh giá được tầm quan",
    "acronyms": [
      [
        49,
        52
      ]
    ],
    "long-forms": [],
    "ID": "1118"
  },
  {
    "text": "Ở đây , ta sẽ quan tâm đến số lượng click mà mô hình có thể dự đoán đúng ở trong top K . Ta gọi độ",
    "acronyms": [],
    "long-forms": [],
    "ID": "1119"
  },
  {
    "text": "tác vụ t được viết như sau : | Dt | X",
    "acronyms": [],
    "long-forms": [],
    "ID": "1120"
  },
  {
    "text": "Mô hình học biểu diễn văn bản dựa trên chủ đề tiềm ẩn của văn bản ( LDA ) [ 1 ] dù biểu diễn văn bản theo",
    "acronyms": [
      [
        68,
        71
      ]
    ],
    "long-forms": [],
    "ID": "1121"
  },
  {
    "text": "Từ kết quả trên , ta có thể thấy biểu diễn từ mô hình học biểu diễn đề xuất khi so với biểu diễn CNN luôn cho kết quả vượt trội , mô hình NRMS cho kết quả tốt hơn 1 % trên các độ đo , mô hình NAML cho kết quả tốt hơn 6 - 8 % và mô hình",
    "acronyms": [
      [
        97,
        100
      ],
      [
        138,
        142
      ],
      [
        192,
        196
      ]
    ],
    "long-forms": [],
    "ID": "1122"
  },
  {
    "text": "Mô hình được gọi là ITE- onehot bởi vì đầu vào của mô hình là các vec - tơ one - hot mã hóa định danh của người dùng và item . Sử dụng các ký hiệu tương tự như trong mô hình NeuMF ( mục 2 . 3 . 5 ) , mô hình",
    "acronyms": [
      [
        174,
        179
      ],
      [
        20,
        31
      ]
    ],
    "long-forms": [],
    "ID": "1123"
  },
  {
    "text": "False negative ( FN ) Dự đoán",
    "acronyms": [
      [
        17,
        19
      ]
    ],
    "long-forms": [],
    "ID": "1124"
  },
  {
    "text": ". Từ đó VD có thể biểu diễn hàm mục tiêu dưới dạng giống như VI trong công thức PT 2. 3 , ta viết lại như sau :",
    "acronyms": [
      [
        8,
        10
      ],
      [
        61,
        63
      ],
      [
        80,
        82
      ]
    ],
    "long-forms": [],
    "ID": "1125"
  },
  {
    "text": "− η − P̂i ( r̂i − P̂i qi ) + λqi s Tương tự , công thức cập nhật cho mỗi cột của P là",
    "acronyms": [],
    "long-forms": [],
    "ID": "1126"
  },
  {
    "text": "Mỗi cột trong ma trận tương ứng với một vec -tơ biểu diễn thuộc tính ẩn cho bộ phim . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        149,
        160
      ]
    ],
    "long-forms": [],
    "ID": "1127"
  },
  {
    "text": "10 2 KIẾN THỨC CƠ SỞ bài toán gợi ý bán hàng , các sản phẩm được bán là item ; trong bài toán gợi ý",
    "acronyms": [],
    "long-forms": [],
    "ID": "1128"
  },
  {
    "text": "2008 ) . Bộ dữ liệu này gồm các cặp câu được trích rút ra từ các bài báo của Wall Street Journal ( WSJ ) được tạo ra từ năm 2008 và được đánh giá là một tập dữ liệu chuẩn cho",
    "acronyms": [
      [
        99,
        102
      ]
    ],
    "long-forms": [],
    "ID": "1129"
  },
  {
    "text": "Trong mô hình ITE - onehot , các đầu ra thực tế nhận các giá trị 0 hoặc 1 . Do đó ta sử dụng hàm lỗi log-loss : X",
    "acronyms": [
      [
        14,
        26
      ]
    ],
    "long-forms": [],
    "ID": "1130"
  },
  {
    "text": "mới , một tư duy mới cho hướng nghiên cứu CL . Đồ án tốt nghiệp",
    "acronyms": [
      [
        42,
        44
      ]
    ],
    "long-forms": [],
    "ID": "1131"
  },
  {
    "text": "Trong cả hai trường hợp nêu trên trọng số của mạng 𝜃 đều có thể diễn giải lại qua một ma trận trọng số mới là W , trong đó các thành phần sẽ là các biến ngẫu 17",
    "acronyms": [],
    "long-forms": [],
    "ID": "1132"
  },
  {
    "text": "index nhỏ hơn – chuỗi lambda được sắp xếp từ lớn đến bé ) , đồng nghĩa với ít biến được chọn hơn . FCM có kích thước các nhóm lớn ( do chỉ có 25 nhóm ) nên số gene được chọn",
    "acronyms": [
      [
        99,
        102
      ]
    ],
    "long-forms": [],
    "ID": "1133"
  },
  {
    "text": "• Giả sử dữ liệu được sinh độc lập , đồng nhất từ một phân phối với tham số Θ và tri thức tiên nghiệm η , tức là p ( x | Θ , η ) . Đồng thời cũng giả sử phân",
    "acronyms": [],
    "long-forms": [],
    "ID": "1134"
  },
  {
    "text": "được cho là sẽ nắm bắt được cấu trúc phức tạp trong tương tác giữa người dùng với item , từ đó cải thiện chất lượng dự đoán . Đầu vào cho mô hình NCF là các vec - tơ u và i tương ứng mô tả cho người",
    "acronyms": [
      [
        146,
        151
      ]
    ],
    "long-forms": [],
    "ID": "1135"
  },
  {
    "text": "là : alpha =0.7, fit _ prior = True • Máy vec - tơ hỗ trợ ( SVM ) : Thư viện sklearn hỗ trợ thuật toán SVM với nhiều nhân khác nhau : SVC với",
    "acronyms": [
      [
        60,
        63
      ],
      [
        103,
        106
      ],
      [
        134,
        137
      ]
    ],
    "long-forms": [
      [
        38,
        57
      ]
    ],
    "ID": "1136"
  },
  {
    "text": "ta thường sử dụng các phiên bản cải tiến của giảm gradient như Stochastic gradient descent ( SGD ) [ 23 ] , AdaGrad [ 9 ] , Adam [ 24 ] , Nesterov ’ s accelerated gradient descent [ 47 ] . . . 1.2.2",
    "acronyms": [
      [
        93,
        96
      ]
    ],
    "long-forms": [],
    "ID": "1137"
  },
  {
    "text": "của LDA và có tính diễn giải cao , gần gũi với con người , từ đó áp dụng vào các mô hình hệ gợi ý để so sánh và đánh giá kết quả . Ngoài ra , đồ án sẽ giới thiệu",
    "acronyms": [
      [
        4,
        7
      ]
    ],
    "long-forms": [],
    "ID": "1138"
  },
  {
    "text": "( 38 ) Ở đây thay x trong VB cố điển bởi X được sinh ra từ phân phối population Fα . Tính chất :",
    "acronyms": [
      [
        26,
        28
      ]
    ],
    "long-forms": [],
    "ID": "1139"
  },
  {
    "text": "hợp với sở thích của User hơn . Cấu trúc mô hình NRMS có thể được quan sát trong hình sau đây [ 2 ] :",
    "acronyms": [
      [
        49,
        53
      ]
    ],
    "long-forms": [],
    "ID": "1140"
  },
  {
    "text": "M ( b ) iDropout cho mô hình LDA",
    "acronyms": [
      [
        29,
        32
      ]
    ],
    "long-forms": [],
    "ID": "1141"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 19 2 KIẾN THỨC CƠ SỞ",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "1142"
  },
  {
    "text": "12GB ( 16GB ) giờ / ngày",
    "acronyms": [
      [
        2,
        4
      ],
      [
        9,
        11
      ]
    ],
    "long-forms": [],
    "ID": "1143"
  },
  {
    "text": "mức văn bản đang là phương pháp cho kết quả tốt nhất tại thời điểm đề xuất . 2.1. Phương pháp tiếp cận dựa trên Support Vector Machine ( SVM )",
    "acronyms": [
      [
        137,
        140
      ]
    ],
    "long-forms": [],
    "ID": "1144"
  },
  {
    "text": "vec -tơ xi , yi , zi biểu thị cho các vec -tơ thuộc tính ẩn của bộ phim i trong các ma trận R , V , W .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1145"
  },
  {
    "text": "𝑊 kết hợp với bộ nhớ trước 𝑠𝑡−1 U kết hợp với 𝑥𝑡 để tính ra bộ nhớ của bước hiện tại 𝑠𝑡 Sau cùng kết hợp với V để tính 𝑦𝑡 .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1146"
  },
  {
    "text": "đầu làm tăng kết quả dịch . Bảng 3 . 2 : Điểm BLEU của các mô hình học đối ngẫu sử dụng nhiễu với 3 mô hình ngôn ngữ trên tập test 2013 .",
    "acronyms": [
      [
        46,
        50
      ]
    ],
    "long-forms": [],
    "ID": "1147"
  },
  {
    "text": "mà người dùng chưa từng tương tác để xếp hạng thay vì lấy toàn bộ . Ta sử dụng hai độ đo Hit Ratio ( HR ) và Normalized Discounted Cumulative Gain ( NDCG ) để đánh giá kết quả vị trí của test item trong danh sách đã được",
    "acronyms": [
      [
        101,
        103
      ],
      [
        149,
        153
      ]
    ],
    "long-forms": [],
    "ID": "1148"
  },
  {
    "text": "lan truyền tiến hay suy diễn . Trọng số của mạng được học dựa trên tập dữ liệu huấn luyện 𝐷 = { ( 𝑥𝑖 , 𝑦𝑖 ) ⁡ |⁡𝑖 = 1, 2, … , 𝑛} bằng cách cực tiểu hóa hàm lỗi 𝐸𝐷 ( 𝑊 ) =",
    "acronyms": [],
    "long-forms": [],
    "ID": "1149"
  },
  {
    "text": "2 . Mục đích nội dung của ĐATN Tìm hiểu , triển khai mô hình Hierarchical Attention Networks cho bài toán phân loại cảm xúc văn bản tiếng Việt .",
    "acronyms": [
      [
        26,
        30
      ]
    ],
    "long-forms": [],
    "ID": "1150"
  },
  {
    "text": "giá trị riêng để tìm ra các biểu diễn DOC [ D×K ] và T OP IC [ K×V ] . Nhắc lại giả sử cơ bản của các mô hình chủ đề , trong đó mỗi văn bản được trộn bởi các chủ đề",
    "acronyms": [],
    "long-forms": [],
    "ID": "1151"
  },
  {
    "text": "bằng thực nghiệm những phương pháp đó . Để hiểu rõ hơn về CL , trước tiên chúng ta sẽ tìm hiểu chi tiết mô hình SimCLR , một trong những mô hình có kết quả SOTA và đại diện cho nhóm các phương pháp",
    "acronyms": [
      [
        58,
        60
      ],
      [
        112,
        118
      ],
      [
        156,
        160
      ]
    ],
    "long-forms": [],
    "ID": "1152"
  },
  {
    "text": "ρ tỷ lệ học với hai tham số κ và τ K",
    "acronyms": [],
    "long-forms": [],
    "ID": "1153"
  },
  {
    "text": "Đặc biệt , khi áp dụng phép DA này vào những mô hình Đồ án tốt nghiệp 34",
    "acronyms": [
      [
        28,
        30
      ]
    ],
    "long-forms": [],
    "ID": "1154"
  },
  {
    "text": "này sẽ được sử dụng để xác định thứ tự các amino axid trong protein ( sẽ trình bày ở phần dưới ) . Sợi DNA có khả năng nhân đôi bằng cách tách mạch xoắn kép thành hai mạch đơn",
    "acronyms": [
      [
        103,
        106
      ]
    ],
    "long-forms": [],
    "ID": "1155"
  },
  {
    "text": "Hình 5 . 8 : Độ chính xác trung bình khi kết thúc mỗi tác vụ trên tập Permuted MNIST với VBD - CL và HAT . 45",
    "acronyms": [
      [
        79,
        84
      ],
      [
        89,
        97
      ],
      [
        101,
        104
      ]
    ],
    "long-forms": [],
    "ID": "1156"
  },
  {
    "text": "𝜃 ở tác vụ hiện tại B , và ∇ log 𝑝 ( 𝜃𝐴 , 𝑖 | 𝐷𝐴 ) = 0 do 𝜃𝐴 , 𝑖 là giá trị tối ưu ở tác vụ A .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1157"
  },
  {
    "text": "13 Với L là số lần lấy mẫu để xấp xỉ kỳ vọng . 2.3.4",
    "acronyms": [],
    "long-forms": [],
    "ID": "1158"
  },
  {
    "text": "số cần học của mạng là các ma trận trọng số W , M và giá trị bias b . 19 h1",
    "acronyms": [],
    "long-forms": [],
    "ID": "1159"
  },
  {
    "text": "Từ đây chúng tôi thấy đại lượng ρ này rất có ý nghĩa trong việc giúp SVB - PP khắc phục những hạn chế của SVB như sau : • Quên đi tri thức cũ với tốc độ hàm mũ , giúp phương pháp có cơ chế cân",
    "acronyms": [
      [
        69,
        77
      ],
      [
        106,
        109
      ]
    ],
    "long-forms": [],
    "ID": "1160"
  },
  {
    "text": "postgresql , AJAX , hibernate và Node.js đây là những công nghệ rất phù hợp với đồ án của em . - Framework spring boot là thư viện sử dụng ngôn ngữ java .",
    "acronyms": [
      [
        13,
        17
      ]
    ],
    "long-forms": [],
    "ID": "1161"
  },
  {
    "text": "tương đương , được gọi là Cận dưới chắc chắn ( ELBO ) : 𝐸𝐿𝐵𝑂 = 𝐸𝑞𝜙 (𝜃 ) log 𝑝 ( 𝐷 |𝜃 ) − 𝐾𝐿 ( 𝑞𝜙 ( 𝜃 ) | |𝑝(𝜃 ) )",
    "acronyms": [
      [
        47,
        51
      ],
      [
        26,
        44
      ]
    ],
    "long-forms": [],
    "ID": "1162"
  },
  {
    "text": "cách tương tự . Ta sử dụng GRU hai chiều để mã hóa các câu trong văn bản ℎ⃗⃗𝑖 = ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗",
    "acronyms": [
      [
        27,
        30
      ]
    ],
    "long-forms": [],
    "ID": "1163"
  },
  {
    "text": "end for Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 44",
    "acronyms": [
      [
        68,
        79
      ]
    ],
    "long-forms": [],
    "ID": "1164"
  },
  {
    "text": "thành một bộ dữ liệu gọi là Split CIFAR10 / 100 và sử dụng cho kịch bản gia tăng số lượng tác vụ trong Học liên tục . Mỗi tác vụ sẽ tương ứng với việc",
    "acronyms": [
      [
        34,
        47
      ]
    ],
    "long-forms": [],
    "ID": "1165"
  },
  {
    "text": "Hard parameter sharing Hình dưới đây mô tả phương pháp hard parameter sharing trong MTL . Hình 10 : Hard parameter sharing",
    "acronyms": [
      [
        84,
        87
      ]
    ],
    "long-forms": [],
    "ID": "1166"
  },
  {
    "text": "và Permuted MNIST cùng chung một kiến trúc MLP hai tầng , Split CIFAR100 và Split CIFAR10 - 100 cùng chung một kiến trúc mạng CNN ( chỉ khác nhau ở số tác",
    "acronyms": [
      [
        12,
        17
      ],
      [
        43,
        46
      ],
      [
        64,
        72
      ],
      [
        82,
        95
      ],
      [
        126,
        129
      ]
    ],
    "long-forms": [],
    "ID": "1167"
  },
  {
    "text": "phương pháp học dòng cho các mô hình Bayesian . Một số công việc như Streaming Variational Bayes ( SVB ) [ 4 ] , Hierarchical Power Priors ( HPP ) [ 10 ] đề xuất các thuật toán cập nhật đệ quy phân tham số biến phân biến toàn cục của",
    "acronyms": [
      [
        99,
        102
      ],
      [
        141,
        144
      ]
    ],
    "long-forms": [],
    "ID": "1168"
  },
  {
    "text": ", log ( i+1 ) nếu như test item ở vị trí i trong top K .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1169"
  },
  {
    "text": "− log ( 𝜎𝑡 ) } Việc lựa chọn phân phối , cũng như quá trình lấy mẫu và áp dụng LRT sẽ được",
    "acronyms": [
      [
        79,
        82
      ]
    ],
    "long-forms": [],
    "ID": "1170"
  },
  {
    "text": "Bảng 2 . 2 : Hiệu năng của các mô hình trước đó với bài toán PKD . Soft-LR ( EM ) tỏ ra vượt trội hơn hẳn khi sử dụng các đặc trưng thủ công làm biểu diễn cho dữ liệu .",
    "acronyms": [
      [
        61,
        64
      ],
      [
        67,
        74
      ],
      [
        77,
        79
      ]
    ],
    "long-forms": [],
    "ID": "1171"
  },
  {
    "text": "Evaluation ( D3 ) : là đánh giá của người có ảnh hưởng lớn , nổi tiếng . Thường",
    "acronyms": [],
    "long-forms": [],
    "ID": "1172"
  },
  {
    "text": "[ wdn = wj ] end while Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        83,
        94
      ]
    ],
    "long-forms": [],
    "ID": "1173"
  },
  {
    "text": "tại là t có tập dữ liệu là Dt = { xti , yit } | D i=1 , độ quan trọng của tham số θj đối với mô hình cho tới tác vụ t − 1 là Ωt−1",
    "acronyms": [],
    "long-forms": [],
    "ID": "1174"
  },
  {
    "text": "AGS - CL được xây dựng trên tư tưởng giảm thiểu sự thay đổi của luồng thông tin truyền qua các nơ-ron của mạng . AGS - CL định nghĩa hai nguyên nhân chính của",
    "acronyms": [
      [
        0,
        8
      ],
      [
        113,
        121
      ]
    ],
    "long-forms": [],
    "ID": "1175"
  },
  {
    "text": "Thứ nhất , cả hai đều tìm độ quan trọng của nơ-ron trong từng tác vụ . Tuy nhiên , HAT tiếp cận theo cơ",
    "acronyms": [
      [
        83,
        86
      ]
    ],
    "long-forms": [],
    "ID": "1176"
  },
  {
    "text": "( SGVB ) là một phương pháp tối ưu hiệu quả cho hàm mục tiêu này , bằng việc ước lượng đại lượng khả năng xảy ra trong quá trình học trên từng tập con có M điểm dữ liệu trong toàn bộ dữ liệu 𝐷 bằng kĩ thuật Monte Carlo [ 9 ] và kĩ thuật đổi biến",
    "acronyms": [
      [
        2,
        6
      ]
    ],
    "long-forms": [],
    "ID": "1177"
  },
  {
    "text": "tới hiện tại : R= Nt",
    "acronyms": [],
    "long-forms": [],
    "ID": "1178"
  },
  {
    "text": "như huấn luyện của kỹ thuật VBD . 2.1 Tổng quan về học liên tục",
    "acronyms": [
      [
        28,
        31
      ]
    ],
    "long-forms": [],
    "ID": "1179"
  },
  {
    "text": "1.2 Mạng nơ- ron Mạng nơ-ron nhân tạo ( Artificial neural network - ANN ) là một phương thức xử lý thông",
    "acronyms": [
      [
        68,
        71
      ]
    ],
    "long-forms": [
      [
        17,
        37
      ]
    ],
    "ID": "1180"
  },
  {
    "text": "bảo rằng các vector embeddings của từ nối có cùng quan hệ phải khác nhau . Cụ thể , với mỗi quan hệ ri ∈ R , gọi tập Ci là tập con của tập từ nối C gồm các từ nối liên kết với",
    "acronyms": [],
    "long-forms": [],
    "ID": "1181"
  },
  {
    "text": "− KL ( q ( θ | ν ) | p ( θ | D1 : T −1 , α ) ) Việc sử dụng Gaussian Mixture Model ( GMM ) cho mô hình là thay vì sử dụng q ( θ | ν ) là một phần",
    "acronyms": [
      [
        85,
        88
      ]
    ],
    "long-forms": [],
    "ID": "1182"
  },
  {
    "text": "VBD - CL được huấn luyện với 150 vòng lặp , và các phương pháp còn lại là 100 vòng lặp .",
    "acronyms": [
      [
        0,
        8
      ]
    ],
    "long-forms": [],
    "ID": "1183"
  },
  {
    "text": "P nếu như dùng P ta đo thấy năng lực thực thi của chương trình có tiến bộ sau khi trải qua E ” ( máy đã học ) [ 32 ] . Một nhánh nhỏ trong học máy gần đây rất được ưu chuộng là học sâu ( deep learning ) .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1184"
  },
  {
    "text": "( 2.8 ) trong đó A , B ∈ Rs , ⊗ là phép nhân từng phân tử . Khi đó , đầu ra v ( [ A B ] ) sẽ là một vector",
    "acronyms": [],
    "long-forms": [],
    "ID": "1185"
  },
  {
    "text": "t bằng cách trích xuất thành phần G ( Θt ) từ đại lượng likelihood log p ( Dt | Θ ̃ ) . Việc này có thể thực hiện bằng cách sử dụng suy diễn biến phân chẳng hạn .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1186"
  },
  {
    "text": "Hình 2 . 3 : Cấu trúc khái quát của mô hình Transformer . Trong đó Q , K và V lần lượt là ma trận của các query , key và value . Trong mô hình",
    "acronyms": [],
    "long-forms": [],
    "ID": "1187"
  },
  {
    "text": "PV Eξ [ A( s̃k )] − A( sk ) ) j=1 ukj tương đương với một phép chính",
    "acronyms": [],
    "long-forms": [],
    "ID": "1188"
  },
  {
    "text": "k , σ I) ( 46 ) trong đó k là chỉ số hàng của Θ ( có kích thước K × V ) và I là một ma trận kích",
    "acronyms": [],
    "long-forms": [],
    "ID": "1189"
  },
  {
    "text": "đổi quá nhiều . ( a ) HR @ 10",
    "acronyms": [
      [
        22,
        24
      ]
    ],
    "long-forms": [],
    "ID": "1190"
  },
  {
    "text": "Pr = softmax ( ErVr ) ∈ ℝn Pc = softmax ( EcVc ) ∈ ℝk trong đó Pr , Pc là phân phối xác suất trên tập quan hệ R và tập các từ nối C .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1191"
  },
  {
    "text": "∇ELBO . ( 4.2 ) Chú ý rằng mô hình cần giữ lại được tri thức của tất cả các tác vụ phía trước , vì",
    "acronyms": [
      [
        1,
        5
      ]
    ],
    "long-forms": [],
    "ID": "1192"
  },
  {
    "text": "min C ( G ) = log 4 , khi và chỉ khi ⇔ p g = pdat a p",
    "acronyms": [],
    "long-forms": [],
    "ID": "1193"
  },
  {
    "text": "thấy sở thích ngắn hạn của người dùng . Và ý tưởng của mô hình BERT được áp dụng để học ra các bài báo quan trọng hơn với người dùng , đồng thời cũng thể",
    "acronyms": [
      [
        63,
        67
      ]
    ],
    "long-forms": [],
    "ID": "1194"
  },
  {
    "text": "RNN hai chiều kết hợp hai RNN với nhau . Trình tự đầu vào được cung cấp theo thứ tự thời gian bình thường cho một mạng và theo thứ tự thời gian ngược lại cho mạng",
    "acronyms": [
      [
        0,
        3
      ],
      [
        26,
        29
      ]
    ],
    "long-forms": [],
    "ID": "1195"
  },
  {
    "text": "74.3 Bảng 5 : Kết quả khi kết hợp phép augment Multi-crop ( Nguồn [ Car +20 ] ) thức số ( 16 ) , song trong lúc training để hạn chê về mặt thời gian , mã Q được lấy",
    "acronyms": [],
    "long-forms": [],
    "ID": "1196"
  },
  {
    "text": "quan giữa người dùng và item được tính bằng : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 31",
    "acronyms": [
      [
        109,
        120
      ]
    ],
    "long-forms": [],
    "ID": "1197"
  },
  {
    "text": "ĐỒ ÁN TỐT NGHIỆP HỆ THỐNG GIẢI PHÁP VIỆC LÀM Nguyễn Đại Dương",
    "acronyms": [],
    "long-forms": [],
    "ID": "1198"
  },
  {
    "text": "Củng cố trọng số mềm dẻo ( Elastic Weight Consolidation , EWC ) [ 5 ] sử dụng ma trận thông",
    "acronyms": [
      [
        58,
        61
      ]
    ],
    "long-forms": [
      [
        0,
        24
      ]
    ],
    "ID": "1199"
  },
  {
    "text": "3.2 Chống quên sử dụng cơ chế chú ý cứng cho tác vụ , HAT HAT [ 16 ] dựa trên biểu diễn riêng biệt của từng tác vụ , xây dựng cơ chế chú ý lên",
    "acronyms": [
      [
        54,
        57
      ],
      [
        58,
        61
      ]
    ],
    "long-forms": [],
    "ID": "1200"
  },
  {
    "text": "Có thể thấy , ConvS2S được lợi ích nhiều nhất từ việc ensemble , điều này làm cho kết quả tăng lên tới 2.6 điểm BLEU ở mô hình base trên tập tst2013 . Trong khi đó , mô hình",
    "acronyms": [
      [
        14,
        21
      ],
      [
        112,
        116
      ]
    ],
    "long-forms": [],
    "ID": "1201"
  },
  {
    "text": "Mô hình dịch máy sử dụng CNN Mô hình dịch máy sử dụng mạng CNN kết hợp RNN được đề xuất lần đầu bởi Gehring [ 13 ]",
    "acronyms": [
      [
        25,
        28
      ],
      [
        59,
        62
      ],
      [
        71,
        74
      ]
    ],
    "long-forms": [],
    "ID": "1202"
  },
  {
    "text": "được viết dưới dạng : 𝑁 𝐾",
    "acronyms": [],
    "long-forms": [],
    "ID": "1203"
  },
  {
    "text": "Bước đầu tiên là tối ưu hàm mục tiêu 𝐸 ( 𝑊 ) cho đến khi kích thước của mạng hội tụ . Bước thứ hai là tinh",
    "acronyms": [],
    "long-forms": [],
    "ID": "1204"
  },
  {
    "text": "( 2.19 ) Trong trường hợp tổng quát với X là một biến ngẫu nhiên nhiều chiều X ∈ RK , biến ngẫu nhiên Z = g ( X ) : RK → RK , khi đó , hàm mật độ xác suất của biến ngẫu nhiên Z nhiều chiều có công",
    "acronyms": [],
    "long-forms": [],
    "ID": "1205"
  },
  {
    "text": "Phạm vi đề tài tập trung nghiên cứu giải quyết bài toán nhận dạng quan hệ ẩn giữa hai câu trên tập dữ liệu PDTB . 1 . 3 Định hướng giải pháp .",
    "acronyms": [
      [
        107,
        111
      ]
    ],
    "long-forms": [],
    "ID": "1206"
  },
  {
    "text": "....................................................................................................... 40 5. KẾT LUẬN .............................................................................................................. 42",
    "acronyms": [],
    "long-forms": [],
    "ID": "1207"
  },
  {
    "text": "Bảng 4 . 4 : Kết quả cải tiến JacReg chỉ bằng 1 giá trị a duy nhất ( 3FC ) Phương pháp JacReg",
    "acronyms": [
      [
        30,
        36
      ],
      [
        69,
        72
      ],
      [
        87,
        93
      ]
    ],
    "long-forms": [],
    "ID": "1208"
  },
  {
    "text": "lượng vi phân . Ở đây , 𝑞 ( 𝜃 | 𝜙 ) còn được gọi là phân phối biến phân . Mục tiêu của VI là cố gắng tìm ra được 𝜙 sao cho 𝑞 ( 𝜃 | 𝜙 ) càng gần với 𝑝 ( 𝜃 |𝐷 )",
    "acronyms": [
      [
        87,
        89
      ]
    ],
    "long-forms": [],
    "ID": "1209"
  },
  {
    "text": "KL ( q ( z , Θ ) | |p ( z , Θ |x) ) , thì đối với PVB , cực đại hoá hàm F- ELBO là không đảm Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 41",
    "acronyms": [
      [
        50,
        53
      ],
      [
        153,
        164
      ],
      [
        72,
        79
      ]
    ],
    "long-forms": [],
    "ID": "1210"
  },
  {
    "text": "vector embeddings của các quan hệ trong tập Ri , đồng nghĩa với việc ta phải tối thiểu hàm lỗi sau : L1 = ∑𝑘𝑖=1 ∑𝑟𝑗 ∈ 𝑅𝑖 ‖ 𝐸𝑐 [𝑐𝑖 ] − 𝐸𝑟 [ 𝑟𝑗 ] ‖",
    "acronyms": [],
    "long-forms": [],
    "ID": "1211"
  },
  {
    "text": "Extensive experiment s show the significant improvement s of iDropout in comparision to other state-of-the-art methods . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 6",
    "acronyms": [
      [
        181,
        192
      ]
    ],
    "long-forms": [],
    "ID": "1212"
  },
  {
    "text": "tham số cục bộ 𝑠𝑡 : 𝐾𝐿 ( 𝑞𝑡 ( 𝑠𝑡 ) | | 𝑝 ( 𝑠𝑡 ) ) . Trong công việc này , việc lựa chọn phân phối tiên nghiệm cho 𝑠𝑡 và thành phần 𝐾𝐿 ( 𝑞𝑡 ( 𝑠𝑡 ) | | 𝑝 ( 𝑠𝑡 ) ) sẽ được xấp xỉ giống",
    "acronyms": [],
    "long-forms": [],
    "ID": "1213"
  },
  {
    "text": "5 .7 Phần trăm không gian mạng CNN sử dụng qua các tác vụ trên tập Split . 35",
    "acronyms": [
      [
        31,
        34
      ]
    ],
    "long-forms": [],
    "ID": "1214"
  },
  {
    "text": "Câu 1 : but ABC Sports also use s the call s as a sales tool Câu 2 : After thanking callers for voting , Frank Gifford offers a football videotape for $ 19.95",
    "acronyms": [],
    "long-forms": [],
    "ID": "1215"
  },
  {
    "text": "0 = a−K Γ (K ) = a−K ( K − 1 ) ! Thế ( 2.25 ) vào ( 2.24 ) , ta được",
    "acronyms": [],
    "long-forms": [],
    "ID": "1216"
  },
  {
    "text": "Tức là đi tìm hàm số yi ≈ f ( xi ) , ∀i = 1, 2 , . . . , N trong đó xi là những điểm dữ liệu trong tập huấn luyện và yi là nhãn tương ứng . Một cách tự",
    "acronyms": [],
    "long-forms": [],
    "ID": "1217"
  },
  {
    "text": "giải trong biểu diễn đối với mô hình học biểu diễn dựa trên Deep Learning , đồng thời , vấn đề trùng lặp , thiếu chủ đề của mô hình LDA cũng được giải quyết . Trong mô hình đề xuất , bằng thực nghiệm có thể thấy các chủ đề con khá đầy đủ ,",
    "acronyms": [
      [
        132,
        135
      ]
    ],
    "long-forms": [],
    "ID": "1218"
  },
  {
    "text": "được hiển thị trong ( H18 ) , trên tập dữ liệu ML 1m lớn , mô hình này với 2 lớp ( DMF - 2layer ) đạt hiệu suất tốt nhất . Các lớp sâu hơn dường như không hữu ích ,",
    "acronyms": [
      [
        47,
        49
      ],
      [
        83,
        86
      ]
    ],
    "long-forms": [],
    "ID": "1219"
  },
  {
    "text": "Topics coherence ( NPMI ) Hình 4 . 9 Đồ thị giá trị NPMI khi thay đổi số lượng dữ liệu đưa vào mỗi lần lặp trên tập dữ liệu Associated Press",
    "acronyms": [
      [
        19,
        23
      ],
      [
        52,
        56
      ]
    ],
    "long-forms": [],
    "ID": "1220"
  },
  {
    "text": "Mức 1 có bốn nhãn quan hệ là : Comparison ( Comp ) , Contigency ( Cont ) , Expansion ( Exp ) , Temporal ( Temp ) . Ở mức 2 , mỗi quan hệ trong mức 1 được",
    "acronyms": [
      [
        44,
        48
      ],
      [
        66,
        70
      ],
      [
        87,
        90
      ],
      [
        106,
        110
      ]
    ],
    "long-forms": [],
    "ID": "1221"
  },
  {
    "text": "sẽ được áp dụng . Tuy nhiên trong các trường hợp p ( 𝑦1 : 𝑇 ) thay đổi theo từng tác vụ ,",
    "acronyms": [],
    "long-forms": [],
    "ID": "1222"
  },
  {
    "text": "- Có thể chạy trên cả CPU và GPU -",
    "acronyms": [
      [
        22,
        25
      ],
      [
        29,
        32
      ]
    ],
    "long-forms": [],
    "ID": "1223"
  },
  {
    "text": "Trong đó 𝐽𝑥 ( 𝐹 ) là ma trận Jacobian của F theo 𝑥 , 𝐻𝑥 ( 𝐹 ) là ma trận Hessian của F theo 𝑥 . Chú ý rằng 𝐸𝛿 [ 𝐽ℎ(𝑙) ( 𝑥) ( 𝐿̅) 𝛿 ] = 0 do 𝛿 tuân theo phân phối Gauss có kỳ",
    "acronyms": [],
    "long-forms": [],
    "ID": "1224"
  },
  {
    "text": "Tổng tham số lớn hơn số lượng tham số của CNN rất nhiều . Vì vậy mà CNN chạy nhanh hơn nhiều so với mạng ANN thông thường .",
    "acronyms": [
      [
        42,
        45
      ],
      [
        68,
        71
      ],
      [
        105,
        108
      ]
    ],
    "long-forms": [],
    "ID": "1225"
  },
  {
    "text": "Collaborative Filtering ( NCF ) [ 6 ] . Theo lý thuyết , NCF sẽ sử dụng một mạng neural để nắm bắt các cấu trúc đặc trưng ẩn của người dùng và sản phẩm để đưa",
    "acronyms": [
      [
        26,
        29
      ],
      [
        57,
        60
      ]
    ],
    "long-forms": [],
    "ID": "1226"
  },
  {
    "text": "Phan Vũ Hồng Hải MSSV : 20141394",
    "acronyms": [
      [
        17,
        21
      ]
    ],
    "long-forms": [],
    "ID": "1227"
  },
  {
    "text": "Biểu diễn của trạng thái cuối cùng sau khi qua các mạng GRU được dùng làm biểu diễn Short-term của User :",
    "acronyms": [
      [
        56,
        59
      ]
    ],
    "long-forms": [],
    "ID": "1228"
  },
  {
    "text": "bản testing Dtest là : P",
    "acronyms": [],
    "long-forms": [],
    "ID": "1229"
  },
  {
    "text": "𝑁 ∑𝑗 =1 𝑒𝑥𝑝 ( 𝑎𝑗 ) Trong đó , v và 𝑣𝑏 là tham số mô hình .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1230"
  },
  {
    "text": "vậy , để tối ưu CTR ( hay nói cách khác là tăng CTR càng cao càng tốt ) , ta cần tăng số lượng click và giảm số lượng view . Hay nói cách khác , ta cần gợi ý hiển",
    "acronyms": [
      [
        16,
        19
      ],
      [
        48,
        51
      ]
    ],
    "long-forms": [],
    "ID": "1231"
  },
  {
    "text": "Trong bài nghiên cứu này , em có đề cập đến ba hàm lỗi L1 , L2 , L3 trong các phương trình ( 7 ) , ( 8 ) và ( 9 ) . Nhằm đánh giá mức độ quan trọng của các hàm lỗi này , dưới đây",
    "acronyms": [],
    "long-forms": [],
    "ID": "1232"
  },
  {
    "text": "rộng của VD , khắc phục được nhược điểm phân phối tiên nghiệm không hợp lệ ( improper prior ) của VD. Một cách tổng quát , thay vì chọn log uniform làm phân",
    "acronyms": [
      [
        9,
        11
      ],
      [
        98,
        100
      ]
    ],
    "long-forms": [],
    "ID": "1233"
  },
  {
    "text": "q ( Θ | λ ) q ( Θ | λ0 ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        85,
        96
      ]
    ],
    "long-forms": [],
    "ID": "1234"
  },
  {
    "text": "mang đậm màu sắc cá nhân Expectation ( D4 ) : suy đoán về hậu quả của sự kiện chính hoặc theo ngữ cảnh .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1235"
  },
  {
    "text": "Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 52",
    "acronyms": [
      [
        63,
        74
      ]
    ],
    "long-forms": [],
    "ID": "1236"
  },
  {
    "text": "trong đó W i và bi lần lượt là ma trận trọng số và bias của lớp ẩn thứ i. Mỗi phần tử wjk trong ma trận trọng số Wi thể hiện sự kết nối từ đơn thứ k của lớp i − 1 tới đơn vị thứ j của",
    "acronyms": [],
    "long-forms": [],
    "ID": "1237"
  },
  {
    "text": "( 𝑙 ) phân phối cho từng phần tử 𝑠𝑡 , 𝑚𝑑 được chọn là 𝑁 ( 𝜇 , 𝛼𝑡, 𝑚𝑑 ) , trong đó 𝜇 là hằng số ( 𝑙 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "1238"
  },
  {
    "text": "= KL ( qφ (E ) | p (E |γ ) ) − Eqφ (E ) [ log p ( D |E ) ] + log p ( D |γ ) . Ta cần tối đa hóa ELBO với tham số { γ , φ } . Chú ý rằng mô hình còn tham số W",
    "acronyms": [
      [
        96,
        100
      ]
    ],
    "long-forms": [],
    "ID": "1239"
  },
  {
    "text": "nhớ T1 mà không học được T2 . Còn với ràng buộc hợp lý như trong 3 . 1 , mô hình có thể tìm được tham số thuộc giao của hai vùng hiệu quả của T1 và T2 .",
    "acronyms": [],
    "long-forms": [],
    "ID": "1240"
  },
  {
    "text": "T ∇2 A ( sk ) ( s̃k − sk ) 2 mà Eξ [ s̃kj ] = skj nên :",
    "acronyms": [],
    "long-forms": [],
    "ID": "1241"
  },
  {
    "text": "Learning cho IDRR Mô hình multi-task learning này nhằm mục đích dự đoán ra các quan hệ và quá trình huẩn luyện cho việc dự đoán các từ nối nhằm bổ sung tri thức cho việc dự đoán các",
    "acronyms": [
      [
        13,
        17
      ]
    ],
    "long-forms": [],
    "ID": "1242"
  },
  {
    "text": "Information. Synaptic Intelligence ( SI ) ( Friedemann Zenke , 2017 ) tính toán độ quan trọng của trọng số một cách trức tuyến bằng cách lưu lại sự thay đổi của hàm lỗi đối với từng trọng",
    "acronyms": [
      [
        37,
        39
      ]
    ],
    "long-forms": [],
    "ID": "1243"
  },
  {
    "text": "T ( Θt ) ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT ( 28 )",
    "acronyms": [],
    "long-forms": [
      [
        71,
        82
      ]
    ],
    "ID": "1244"
  },
  {
    "text": "kĩ thuật DA . Láng giềng của một điểm trong không gian gốc mà nhóm đang định nghĩa chính là các view khác của cùng một dữ liệu gốc ban đầu .",
    "acronyms": [
      [
        9,
        11
      ]
    ],
    "long-forms": [],
    "ID": "1245"
  },
  {
    "text": "4 có được nhờ xấp xỉ Taylor bậc 2 của L1 ( w2 ∗ ) tại w1 ∗ ; 4 . 5 có được vì",
    "acronyms": [],
    "long-forms": [],
    "ID": "1246"
  },
  {
    "text": "phải tình trạng tiêu biến gradient và do đó LSTM có thể nắm bắt được thông tin có sự phụ thuộc dài hạn hiệu quả hơn phiên bản mạng nơ-ron hồi quy đơn giản . Kiến trúc của LSTM",
    "acronyms": [
      [
        44,
        48
      ],
      [
        171,
        175
      ]
    ],
    "long-forms": [],
    "ID": "1247"
  },
  {
    "text": "công việc 3 - Ngoài ra , NTD có thể chọn thêm loại ngành nghề , hoặc chọn thêm địa điểm",
    "acronyms": [
      [
        25,
        28
      ]
    ],
    "long-forms": [],
    "ID": "1248"
  },
  {
    "text": "phối tiên nghiệm 𝑝 ( 𝜃 (𝑙) ) . Giá trị pre-activation khi đó : 𝐵 (𝑙 ) = 𝐴( 𝑙) 𝜃 ( 𝑙 ) . ( 𝑙 )",
    "acronyms": [],
    "long-forms": [],
    "ID": "1249"
  },
  {
    "text": "34 c . Kết quả thử nghiệm trên Split CIFAR10 / 100 :",
    "acronyms": [
      [
        37,
        50
      ]
    ],
    "long-forms": [],
    "ID": "1250"
  },
  {
    "text": "men , tế bào huyết tương và tế bào ung thư ở người . FCM còn được kết hợp với nhiều phương pháp như SVM [ 67 ] hay học bán giám sát [ 68 ] để nâng cao hiệu quả trong bài",
    "acronyms": [
      [
        53,
        56
      ],
      [
        100,
        103
      ]
    ],
    "long-forms": [],
    "ID": "1251"
  },
  {
    "text": "cùng , thông tin về xâu X sẽ được mã hóa vào hm . Decoder được mô tả bởi công thức dưới đây ( d )",
    "acronyms": [],
    "long-forms": [],
    "ID": "1252"
  },
  {
    "text": "năng vượt trội của ALV so với hai trường hợp còn lại qua kết quả được đưa ra trong Hình 4 . 15 . Kết quả trên hai phương pháp này cho thấy , ALV giúp tăng khả",
    "acronyms": [
      [
        19,
        22
      ],
      [
        141,
        144
      ]
    ],
    "long-forms": [],
    "ID": "1253"
  },
  {
    "text": "như sau : 𝑀𝐻 ( 𝐻 𝑙 ) = [ ℎ𝑒𝑎𝑑1 , ℎ𝑒𝑎𝑑2 , . . . , ℎ𝑒𝑎𝑑ℎ ] 𝑊 𝑂",
    "acronyms": [],
    "long-forms": [],
    "ID": "1254"
  },
  {
    "text": "VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG ĐỒ ÁN TỐT NGHIỆP Xây dựng mô hình học sâu giàu tính diễn",
    "acronyms": [],
    "long-forms": [],
    "ID": "1255"
  },
  {
    "text": "Multiview Coding , PIRL , ... Trong khuôn khổ đồ án này , phần chi tiết sẽ chỉ trình bày mô hình SimCLR , MOCO .",
    "acronyms": [
      [
        19,
        23
      ],
      [
        97,
        103
      ],
      [
        106,
        110
      ]
    ],
    "long-forms": [],
    "ID": "1256"
  },
  {
    "text": "Trước hết , do MF ánh xạ các người dùng và item vào cùng một không gian thuộc tính ẩn , mức độ tương đồng giữa hai người dùng cũng có thể được tính bằng tích vô hướng , hay tương đương với việc",
    "acronyms": [
      [
        15,
        17
      ]
    ],
    "long-forms": [],
    "ID": "1257"
  },
  {
    "text": "hạn chế trên của SVB . 3 . 3 Phương pháp học Hierarchical Power Priors",
    "acronyms": [
      [
        17,
        20
      ]
    ],
    "long-forms": [],
    "ID": "1258"
  },
  {
    "text": "Chúng tôi tổng hợp quá trình học của iDropout cho LDA trong giải thuật sau . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        50,
        53
      ],
      [
        137,
        148
      ]
    ],
    "long-forms": [],
    "ID": "1259"
  },
  {
    "text": "Quan hệ dự đoán ( Bai and Zhao , 2018 ) : Temporal Quan hệ đúng : Expansion Câu 1 : NBC has been able to charge premium rates for this ad time",
    "acronyms": [],
    "long-forms": [],
    "ID": "1260"
  },
  {
    "text": "tử đầu vào liên tiếp , mỗi phần tử là một vector thuộc không gian s chiều , đầu ra của kernel là một vector Y ∈ R2s . Mỗi nhóm k phần tử đầu ra của lớp trước sẽ được xử lý tiếp theo",
    "acronyms": [],
    "long-forms": [],
    "ID": "1261"
  },
  {
    "text": "VCcorp . 4 . 2 . 1 Tập dữ liệu MIND Tập dữ liệu MIcrosoft News Dataset ( MIND ) là một tập dữ liệu quy mô lớn về",
    "acronyms": [
      [
        31,
        35
      ],
      [
        73,
        77
      ]
    ],
    "long-forms": [],
    "ID": "1262"
  },
  {
    "text": "CHƯƠNG 3 . PHƯƠNG PHÁP ĐỀ XUẤT 3 . 1 . Lasso và các biến thể của Lasso",
    "acronyms": [],
    "long-forms": [],
    "ID": "1263"
  },
  {
    "text": "khả năng mô hình hoá hiệu quả dữ liệu có cấu trúc rời rạc , đặc biệt là dữ liệu văn bản chữ . LDA được ứng dụng rất thành công nhiều nhiều lĩnh vực bao gồm",
    "acronyms": [
      [
        94,
        97
      ]
    ],
    "long-forms": [],
    "ID": "1264"
  },
  {
    "text": "các tham số của mô hình bằng cách cực đại hóa hàm phân phối hậu nghiệm p ( z , Θ | D1 , D2 , ... , Dt , η ) , với η là một tri thức tiên nghiệm khởi tạo của mô hình . Tuy nhiên , như đã trình bày , việc tính toán trực tiếp phân phối này là bất khả",
    "acronyms": [],
    "long-forms": [],
    "ID": "1265"
  },
  {
    "text": ". Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 39",
    "acronyms": [
      [
        65,
        76
      ]
    ],
    "long-forms": [],
    "ID": "1266"
  },
  {
    "text": "Dù được định nghĩa theo nhiều cách khác nhau , nhưng từ hai khái niệm trên thì về cơ bản ta có thể hiểu được , ý tưởng chính của SSL chính là tự khai phá các tri thức trong dữ liệu bằng việc định nghĩa một vài bài toán mà con người tự đặt ra ,",
    "acronyms": [
      [
        129,
        132
      ]
    ],
    "long-forms": [],
    "ID": "1267"
  },
  {
    "text": "M ( a ) iDropout cho mô hình tổng quát B ( β , z , x )",
    "acronyms": [],
    "long-forms": [],
    "ID": "1268"
  },
  {
    "text": "tham số quan trọng đối với tác vụ A và EWC mong muốn trích xuất được thông tin từ thành phần này . Tuy nhiên xác suất hậu nghiệm log 𝑝 ( 𝜃 | 𝐷𝐴 ) là khó tính toán",
    "acronyms": [
      [
        39,
        42
      ]
    ],
    "long-forms": [],
    "ID": "1269"
  },
  {
    "text": "t ( giờ ) BLEU",
    "acronyms": [
      [
        10,
        14
      ]
    ],
    "long-forms": [],
    "ID": "1270"
  },
  {
    "text": "DA này đều có kết quả tốt hơn đáng kể , được thể hiện ở thực nghiệm trong bảng 5 . Kết quả cuối cùng : Như đã trình bày từ phần giới thiệu về SSL , có thể nói pretext task không phải là kết quả thực sự mà các phương pháp SSL hướng đến .",
    "acronyms": [
      [
        0,
        2
      ],
      [
        142,
        145
      ],
      [
        221,
        224
      ]
    ],
    "long-forms": [],
    "ID": "1271"
  },
  {
    "text": "• Kịch bản gia tăng về miền dữ liệu : bộ dữ liệu PMNIST . • Kịch bản gia tăng về số lượng tác vụ với số nhãn cần phân loại là như nhau trên mọi tác vụ , ta gọi ngắn gọn là “ kịch bản gia tăng tác vụ đồng số nhãn ” :",
    "acronyms": [
      [
        49,
        55
      ]
    ],
    "long-forms": [],
    "ID": "1272"
  },
  {
    "text": ", φM LP Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT",
    "acronyms": [
      [
        71,
        82
      ],
      [
        5,
        7
      ]
    ],
    "long-forms": [],
    "ID": "1273"
  },
  {
    "text": "Ở đây , biểu diễn đầu vào của doc là vec-tơ LDA của nội dung các bài báo , cùng không gian thuộc tính",
    "acronyms": [
      [
        44,
        47
      ]
    ],
    "long-forms": [],
    "ID": "1274"
  }
]