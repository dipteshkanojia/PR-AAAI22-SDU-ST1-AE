{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path = \"../data_tsvs/englishDFDevScientific.tsv\"\n",
    "pred_path = \"../results/finetuned_models/model_all_xb_v1_512/eng_scientific_dev_predictions_xb_512.csv\"\n",
    "op_filename = \"eng_dev_xb_512\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(orig_path, sep=\"\\t\")\n",
    "df_pred = pd.read_csv(pred_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# status = []\n",
    "# flag = True\n",
    "\n",
    "# for orig_text, pred_text in zip(list(df_orig[\"text\"]), list(df_pred[\"Text\"]) ):\n",
    "# #     print(orig_text.strip())\n",
    "# #     print(pred_text.strip())\n",
    "# #     print(\"\\n\\n\")\n",
    "#     #print(type(orig_text))\n",
    "#     orig_text_l = word_tokenize(orig_text.strip())\n",
    "#     pred_text_l = pred_text.strip().split(\" \")\n",
    "#     if(len(orig_text_l) == len(pred_text_l)):\n",
    "#         status.append(True)\n",
    "#     else:\n",
    "#         status.append([False, len(orig_text_l), len(pred_text_l)])\n",
    "# #         print(orig_text.strip())\n",
    "# #         print(pred_text.strip())\n",
    "#         print(orig_text_l)\n",
    "#         print(pred_text_l)\n",
    "#         print(\"\\n\\n\")\n",
    "#         flag = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(df):\n",
    "    df_ = df\n",
    "    text_l   = list(df[\"Text\"])\n",
    "    y_true_l = list(df['Y_true'])\n",
    "    y_pred_l = list(df[\"Y_pred\"])\n",
    "    \n",
    "    cnt_text_l   = []\n",
    "    cnt_y_true_l = []\n",
    "    cnt_y_pred_l = []\n",
    "    for text, y_true, y_pred in zip(text_l, y_true_l, y_pred_l):\n",
    "        cnt_text_l.append(len(text.split(\" \")))\n",
    "        cnt_y_true_l.append(len(ast.literal_eval(y_true)))\n",
    "        cnt_y_pred_l.append(len(ast.literal_eval(y_pred)))\n",
    "        \n",
    "    df_[\"cnt_text\"] = cnt_text_l\n",
    "    df_[\"cnt_y_true\"] = cnt_y_true_l\n",
    "    df_[\"cnt_y_pred\"] = cnt_y_pred_l\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Y_true</th>\n",
       "      <th>Y_pred</th>\n",
       "      <th>cnt_text</th>\n",
       "      <th>cnt_y_true</th>\n",
       "      <th>cnt_y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1 : Mean A...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baselines are a unigram query likelihood ( QL ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'I-LF', 'I-LF', ...</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tion . Then , we extract expansion terms from ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person , mood , voice and case , CATiB uses 6 ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>( LDA ) , Maximum Likelihood Linear Transform ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'I-LF', ...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gold label ( Section 3 ) . Our algorithm is ca...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase markers or words . For simplicity of ma...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When a CFG is associated with probabilistic in...</td>\n",
       "      <td>['B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', ...</td>\n",
       "      <td>['B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', ...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1 ] proposed a language-neutral framework for ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12 Base+FrameNet ( FN ) 61.8 71.9 66.5 59.8 69...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-AN', 'B-O', 'B-O', '...</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model 3 ( Figure 3 ) illustrates how the sourc...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The big blue door. ? In this case , the GrM as...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TI = terse information INT = interrupted TRUN ...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-AN', 'B-O',...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-AN', 'B-O',...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>as in figure 3 . It is parsed as an adverb ( A...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The tagging has been done using a GUI-based to...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Och and Ney ( 2003 ) show that for larger corp...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2 Relation Extraction System In this section ,...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>target question , or zero if the target questi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>discussion in Ritchie ( 1984 ) . Functional un...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bigram Perplex . ( PP ) MDI Missed Samples ( M...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>networks . In Proceedings of the IEEE Conferen...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Therefore , identification methods like Tsuchi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ents ) * 100 % ? F1-score = 2 * P * R / ( P+R ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>eugene @ mathcs.emory.edu Abstract Community q...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'I-LF', 'I...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>means a type of source ? newswire ( NW ) , bro...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'I...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>either lexically encoded , or depends on the i...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Abstract This paper attempts to use an off-the...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>The SemEval ? 2007 task for extracting frame s...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>hauer , haver , haber ) and the corpus does no...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>The ungrammatical distracter , e.g. , are in F...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Words/Phrases as Themselves ( WD ) Symbols/Non...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>DS = Discharge Summary , Echo = Echocardiogram...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-LF',...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-LF',...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>mdiab @ ccls.columbia.edu Abstract We analyze ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>( PERS ) , organization ( ORG ) , geo-politica...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Abstract In this paper , we propose a new synt...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>The second one is a variant that we named Doub...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', '...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>by HG 's ( HL ) . In particular , we show that...</td>\n",
       "      <td>['B-O', 'B-AN', 'I-AN', 'B-O', 'B-AN', 'B-O', ...</td>\n",
       "      <td>['B-O', 'B-AN', 'I-AN', 'B-O', 'B-AN', 'B-O', ...</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>( Ramshaw and Marcus , 1995 ) approached chuck...</td>\n",
       "      <td>['B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>demonstrate such dependencies . The Maximum En...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>dialogues categorized into multiple domains , ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>shared task , namely FLORIAN ( Florian et al. ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1 Introduction Open-domain Question Answering ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>229 Proceedings of the 2014 Conference on Empi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>th fo frture representations abstracts ( AbT )...</td>\n",
       "      <td>['B-LF', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "      <td>['B-LF', 'B-O', 'B-O', 'I-LF', 'I-LF', 'B-O', ...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>787 After labeling the reference BINet , we tr...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>for the annotation process . Topic models ( TM...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Evidence for a text ? s topic and genre comes ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate ( OR ) Co</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>The query ( Figure 4a ) will match adjectives ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>the middle ( Baxendale , 1958 ) . Sentence Pos...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>By contrast , our approach operates at the lev...</td>\n",
       "      <td>['B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0    2 Related Work The availability of emotion-ric...   \n",
       "1    WebDict 0.2919 Backoff 0.3282 Table 1 : Mean A...   \n",
       "2    Baselines are a unigram query likelihood ( QL ...   \n",
       "3    tion . Then , we extract expansion terms from ...   \n",
       "4    person , mood , voice and case , CATiB uses 6 ...   \n",
       "5    ( LDA ) , Maximum Likelihood Linear Transform ...   \n",
       "6    gold label ( Section 3 ) . Our algorithm is ca...   \n",
       "7    phrase markers or words . For simplicity of ma...   \n",
       "8    When a CFG is associated with probabilistic in...   \n",
       "9    1 ] proposed a language-neutral framework for ...   \n",
       "10   12 Base+FrameNet ( FN ) 61.8 71.9 66.5 59.8 69...   \n",
       "11   Model 3 ( Figure 3 ) illustrates how the sourc...   \n",
       "12   in the V column indicates that the verb condit...   \n",
       "13   The big blue door. ? In this case , the GrM as...   \n",
       "14   TI = terse information INT = interrupted TRUN ...   \n",
       "15   classes in both original text and main text co...   \n",
       "16   as in figure 3 . It is parsed as an adverb ( A...   \n",
       "17   can develop after exposure to a terrifying eve...   \n",
       "18   It allows for testing interaction scenarios th...   \n",
       "19   Feature Description lexical the words of the p...   \n",
       "20   The tagging has been done using a GUI-based to...   \n",
       "21   Och and Ney ( 2003 ) show that for larger corp...   \n",
       "22   2 Relation Extraction System In this section ,...   \n",
       "23   target question , or zero if the target questi...   \n",
       "24   discussion in Ritchie ( 1984 ) . Functional un...   \n",
       "25   Bigram Perplex . ( PP ) MDI Missed Samples ( M...   \n",
       "26   networks . In Proceedings of the IEEE Conferen...   \n",
       "27   Therefore , identification methods like Tsuchi...   \n",
       "28   domain-independent mpirical induction algorith...   \n",
       "29   ents ) * 100 % ? F1-score = 2 * P * R / ( P+R ...   \n",
       "..                                                 ...   \n",
       "467  eugene @ mathcs.emory.edu Abstract Community q...   \n",
       "468  means a type of source ? newswire ( NW ) , bro...   \n",
       "469  either lexically encoded , or depends on the i...   \n",
       "470  Abstract This paper attempts to use an off-the...   \n",
       "471  The SemEval ? 2007 task for extracting frame s...   \n",
       "472  hauer , haver , haber ) and the corpus does no...   \n",
       "473  The ungrammatical distracter , e.g. , are in F...   \n",
       "474  Words/Phrases as Themselves ( WD ) Symbols/Non...   \n",
       "475  DS = Discharge Summary , Echo = Echocardiogram...   \n",
       "476  mdiab @ ccls.columbia.edu Abstract We analyze ...   \n",
       "477  ( PERS ) , organization ( ORG ) , geo-politica...   \n",
       "478  Abstract In this paper , we propose a new synt...   \n",
       "479  The second one is a variant that we named Doub...   \n",
       "480  by HG 's ( HL ) . In particular , we show that...   \n",
       "481  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482  ( Ramshaw and Marcus , 1995 ) approached chuck...   \n",
       "483  demonstrate such dependencies . The Maximum En...   \n",
       "484  dialogues categorized into multiple domains , ...   \n",
       "485  shared task , namely FLORIAN ( Florian et al. ...   \n",
       "486  1 Introduction Open-domain Question Answering ...   \n",
       "487  229 Proceedings of the 2014 Conference on Empi...   \n",
       "488  1 Introduction Large-scale open-domain questio...   \n",
       "489  th fo frture representations abstracts ( AbT )...   \n",
       "490  787 After labeling the reference BINet , we tr...   \n",
       "491  for the annotation process . Topic models ( TM...   \n",
       "492  Evidence for a text ? s topic and genre comes ...   \n",
       "493   0.35 0.25 0.50 0.75 1.00 Omission Rate ( OR ) Co   \n",
       "494  The query ( Figure 4a ) will match adjectives ...   \n",
       "495  the middle ( Baxendale , 1958 ) . Sentence Pos...   \n",
       "496  By contrast , our approach operates at the lev...   \n",
       "\n",
       "                                                Y_true  \\\n",
       "0    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "1    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "2    ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "3    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "4    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "5    ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...   \n",
       "6    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "7    ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...   \n",
       "8    ['B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', ...   \n",
       "9    ['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...   \n",
       "10   ['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...   \n",
       "11   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "12   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "13   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "14   ['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-AN', 'B-O',...   \n",
       "15   ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "16   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "17   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...   \n",
       "18   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "19   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "20   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "21   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "22   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "23   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "24   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "25   ['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...   \n",
       "26   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "27   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "28   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "29   ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "..                                                 ...   \n",
       "467  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...   \n",
       "468  ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "469  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "470  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "471  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "472  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "473  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "474  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...   \n",
       "475  ['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-LF',...   \n",
       "476  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "477  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...   \n",
       "478  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "479  ['B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', '...   \n",
       "480  ['B-O', 'B-AN', 'I-AN', 'B-O', 'B-AN', 'B-O', ...   \n",
       "481  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "482  ['B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "483  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "484  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "485  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...   \n",
       "486  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...   \n",
       "487  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "488  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "489  ['B-LF', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', '...   \n",
       "490  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "491  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "492  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...   \n",
       "493  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "494  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "495  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "496  ['B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "\n",
       "                                                Y_pred  cnt_text  cnt_y_true  \\\n",
       "0    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        32          32   \n",
       "1    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        18          18   \n",
       "2    ['B-O', 'B-O', 'B-LF', 'B-O', 'I-LF', 'I-LF', ...        39          39   \n",
       "3    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        23          23   \n",
       "4    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        42          42   \n",
       "5    ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'I-LF', ...        29          29   \n",
       "6    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        31          31   \n",
       "7    ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...        34          34   \n",
       "8    ['B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', ...        29          29   \n",
       "9    ['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...        25          25   \n",
       "10   ['B-O', 'B-LF', 'B-O', 'B-AN', 'B-O', 'B-O', '...        70          70   \n",
       "11   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        35          35   \n",
       "12   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        26          26   \n",
       "13   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        31          31   \n",
       "14   ['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-AN', 'B-O',...        17          17   \n",
       "15   ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...        27          27   \n",
       "16   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        26          26   \n",
       "17   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...        41          41   \n",
       "18   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        18          18   \n",
       "19   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        20          20   \n",
       "20   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        27          27   \n",
       "21   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        28          28   \n",
       "22   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        24          24   \n",
       "23   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        32          32   \n",
       "24   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        21          21   \n",
       "25   ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...        18          18   \n",
       "26   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        18          18   \n",
       "27   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        27          27   \n",
       "28   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        40          40   \n",
       "29   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        24          24   \n",
       "..                                                 ...       ...         ...   \n",
       "467  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'I-LF', 'I...        17          17   \n",
       "468  ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'I...        24          24   \n",
       "469  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        27          27   \n",
       "470  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        17          17   \n",
       "471  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        26          26   \n",
       "472  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        28          28   \n",
       "473  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        26          26   \n",
       "474  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...        19          19   \n",
       "475  ['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-LF',...        31          31   \n",
       "476  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        17          17   \n",
       "477  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...        40          40   \n",
       "478  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        25          25   \n",
       "479  ['B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', '...        26          26   \n",
       "480  ['B-O', 'B-AN', 'I-AN', 'B-O', 'B-AN', 'B-O', ...        47          47   \n",
       "481  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        24          24   \n",
       "482  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        18          18   \n",
       "483  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...        24          24   \n",
       "484  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        31          31   \n",
       "485  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...        22          22   \n",
       "486  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...        16          16   \n",
       "487  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        31          31   \n",
       "488  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        24          24   \n",
       "489  ['B-LF', 'B-O', 'B-O', 'I-LF', 'I-LF', 'B-O', ...        27          27   \n",
       "490  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        28          28   \n",
       "491  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...        15          15   \n",
       "492  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...        42          42   \n",
       "493  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...        11          11   \n",
       "494  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        38          38   \n",
       "495  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        23          23   \n",
       "496  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        28          28   \n",
       "\n",
       "     cnt_y_pred  \n",
       "0            32  \n",
       "1            18  \n",
       "2            39  \n",
       "3            23  \n",
       "4            42  \n",
       "5            29  \n",
       "6            31  \n",
       "7            34  \n",
       "8            29  \n",
       "9            25  \n",
       "10           70  \n",
       "11           35  \n",
       "12           26  \n",
       "13           31  \n",
       "14           17  \n",
       "15           27  \n",
       "16           26  \n",
       "17           41  \n",
       "18           18  \n",
       "19           20  \n",
       "20           27  \n",
       "21           28  \n",
       "22           24  \n",
       "23           32  \n",
       "24           21  \n",
       "25           18  \n",
       "26           18  \n",
       "27           27  \n",
       "28           40  \n",
       "29           24  \n",
       "..          ...  \n",
       "467          17  \n",
       "468          24  \n",
       "469          27  \n",
       "470          17  \n",
       "471          26  \n",
       "472          28  \n",
       "473          26  \n",
       "474          19  \n",
       "475          31  \n",
       "476          17  \n",
       "477          40  \n",
       "478          25  \n",
       "479          26  \n",
       "480          47  \n",
       "481          24  \n",
       "482          18  \n",
       "483          24  \n",
       "484          31  \n",
       "485          22  \n",
       "486          16  \n",
       "487          31  \n",
       "488          24  \n",
       "489          27  \n",
       "490          28  \n",
       "491          15  \n",
       "492          42  \n",
       "493          11  \n",
       "494          38  \n",
       "495          23  \n",
       "496          28  \n",
       "\n",
       "[497 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_cnt = count(df_pred)\n",
    "df_pred_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(df_orig, df_pred):\n",
    "    acronyms = []\n",
    "    long_forms = []\n",
    "    \n",
    "    acronyms_idxs = []\n",
    "    long_forms_idxs = []\n",
    "    \n",
    "    cnt = df_pred.shape[0]\n",
    "    #print(cnt)\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        acronym = []\n",
    "        long_form = []\n",
    "        \n",
    "        an_idx = []\n",
    "        lf_idx = []\n",
    "        \n",
    "        text_l = df_pred.iloc[i][\"Text\"].strip().split(\" \")\n",
    "        text_orig = df_orig.iloc[i][\"text\"].strip()\n",
    "        #print(text_orig)\n",
    "        y_true = df_pred.iloc[i][\"Y_true\"]\n",
    "        y_true_l = ast.literal_eval(y_true)\n",
    "        \n",
    "        y_pred = df_pred.iloc[i][\"Y_pred\"]\n",
    "        y_pred_l = ast.literal_eval(y_pred)\n",
    "        \n",
    "        #print(text_l)\n",
    "        #print(y_true_l)\n",
    "        #print(y_pred_l)\n",
    "        AN_start = False\n",
    "        LF_start = False\n",
    "        \n",
    "        #print(len(y_true))\n",
    "        AN = \"\"\n",
    "        LF = \"\"\n",
    "        for j in range(len(y_pred_l)):\n",
    "            #LF = \"\"\n",
    "            \n",
    "#             if(y_pred_l[j]==\"B-O\"):\n",
    "#                 continue\n",
    "            \n",
    "            if(y_pred_l[j] == \"B-AN\" and AN_start == False ):\n",
    "                AN = AN + text_l[j]\n",
    "                AN_start = True\n",
    "            elif(y_pred_l[j] == \"I-AN\" and AN_start == True):\n",
    "                AN = AN + text_l[j]\n",
    "            elif(y_pred_l[j]==\"B-AN\" and AN_start == True):\n",
    "                acronym.append(AN)\n",
    "                AN = text_l[j]\n",
    "                #print(AN)\n",
    "            elif(AN_start == True and AN != \"\"):\n",
    "                acronym.append(AN)\n",
    "                AN = \"\"\n",
    "                AN_start = False\n",
    "                \n",
    "             \n",
    "            if(y_pred_l[j] == \"B-LF\" and LF_start == False ):\n",
    "                LF = LF + text_l[j]\n",
    "                LF_start = True\n",
    "            elif(y_pred_l[j] == \"I-LF\" and LF_start == False):\n",
    "                LF = LF + text_l[j]\n",
    "                LF_start = True\n",
    "            elif(y_pred_l[j] == \"I-LF\" and LF_start == True):\n",
    "                LF = LF + \" \" + text_l[j]\n",
    "            elif(y_pred_l[j]==\"B-LF\" and LF_start == True):\n",
    "                long_form.append(LF)\n",
    "                LF = text_l[j]\n",
    "            elif(LF_start == True and LF != \"\"):\n",
    "                long_form.append(LF)\n",
    "                LF = \"\"\n",
    "                LF_start = False\n",
    "            \n",
    "#             if(y_pred_l[j]==\"B-AN\" or y_pred_l[j]==\"I-AN\"):\n",
    "#                 acronym.append(str(text_l[j]))\n",
    "#                 #print(acronym)\n",
    "            \n",
    "            \n",
    "#             elif(y_pred_l[j]==\"B-LF\" or y_pred_l[j]==\"I-LF\"):\n",
    "#                 long_form.append(str(text_l[j]))\n",
    "#             elif(y_pred_l[j]==\"B-LF\" and LF_start == False):\n",
    "#                 LF = LF + text_l[j]\n",
    "#                 LF_start = True\n",
    "#             elif(y_pred_l[j]==\"I-LF\" and LF_start == True):\n",
    "#                 LF = LF + \" \"+ texl_l[j]\n",
    "#             elif(y_pred_l[j] == \"B-LF\" and LF_start == True):\n",
    "#                 long_form.append(LF)\n",
    "#                 LF = \"\"\n",
    "#                 LF = LF + text_l[j]\n",
    "        \n",
    "        #print(acronym)\n",
    "        #print(long_form)\n",
    "        acronyms.append(acronym)\n",
    "        long_forms.append(long_form)\n",
    "        \n",
    "        #print(acronym)\n",
    "        for an in acronym:\n",
    "            #print(type(text_orig))\n",
    "            #print(an)\n",
    "            an = an.strip()\n",
    "            start_idx = text_orig.find(an)\n",
    "            end_idx = start_idx + len(an)\n",
    "            \n",
    "            an_idx.append([start_idx, end_idx])\n",
    "        \n",
    "        for lf in long_form:\n",
    "            #print(lf)\n",
    "            lf = lf.strip()\n",
    "            #print(lf)\n",
    "            start_idx = text_orig.find(lf)\n",
    "            end_idx = start_idx + len(lf)\n",
    "            \n",
    "            lf_idx.append([start_idx, end_idx])\n",
    "            \n",
    "        acronyms_idxs.append(an_idx)\n",
    "        long_forms_idxs.append(lf_idx)\n",
    "        \n",
    "    df_orig[\"AN_Pred\"] = acronyms\n",
    "    df_orig[\"LF_Pred\"] = long_forms\n",
    "    df_orig[\"AN_Pred_idxs\"] = acronyms_idxs\n",
    "    df_orig[\"LF_Pred_idxs\"] = long_forms_idxs\n",
    "    return df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "      <th>AN_Pred</th>\n",
       "      <th>LF_Pred</th>\n",
       "      <th>AN_Pred_idxs</th>\n",
       "      <th>LF_Pred_idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[144, 160]]</td>\n",
       "      <td>['TDM']</td>\n",
       "      <td>['Text Data Mining']</td>\n",
       "      <td>[TDM]</td>\n",
       "      <td>[a, Text Data Mining]</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[5, 6], [144, 160]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "      <td>['MAP']</td>\n",
       "      <td>['Mean Average Precision']</td>\n",
       "      <td>[MAP]</td>\n",
       "      <td>[Mean Average Precision]</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Baselines are a unigram query likelihood (QL) ...</td>\n",
       "      <td>[[113, 115], [42, 45], [153, 156]]</td>\n",
       "      <td>[[90, 111], [132, 151]]</td>\n",
       "      <td>['SD', 'QL)', 'MRF']</td>\n",
       "      <td>['sequential dependence', 'Markov random field']</td>\n",
       "      <td>[QL, SD, MRF]</td>\n",
       "      <td>[a, query likelihood, and a, sequential depend...</td>\n",
       "      <td>[[42, 44], [113, 115], [153, 156]]</td>\n",
       "      <td>[[1, 2], [24, 40], [67, 72], [90, 111], [132, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>tion. Then, we extract expansion terms from th...</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90]]</td>\n",
       "      <td>['PRF']</td>\n",
       "      <td>['pseudo relevance feedback']</td>\n",
       "      <td>[PRF]</td>\n",
       "      <td>[pseudo relevance feedback, in]</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90], [61, 63]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>person, mood, voice and case, CATiB uses 6 POS...</td>\n",
       "      <td>[[130, 134], [30, 35], [43, 46], [53, 56], [15...</td>\n",
       "      <td>[[136, 142], [156, 161]]</td>\n",
       "      <td>['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...</td>\n",
       "      <td>['proper', 'verbs']</td>\n",
       "      <td>[CATiB, POS, NOM, PROP, VRB]</td>\n",
       "      <td>[non-proper, nouns, proper nouns, verbs]</td>\n",
       "      <td>[[30, 35], [43, 46], [53, 56], [130, 134], [15...</td>\n",
       "      <td>[[58, 68], [88, 93], [136, 148], [122, 127]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>(LDA), Maximum Likelihood Linear Transform (ML...</td>\n",
       "      <td>[[115, 118], [44, 48], [87, 91]]</td>\n",
       "      <td>[[94, 113], [7, 42], [51, 85]]</td>\n",
       "      <td>['MPE', 'MLLT', 'BMMI']</td>\n",
       "      <td>['Minimum Phone Error', 'Maximum Likelihood Li...</td>\n",
       "      <td>[LDA, MLLT, BMMI, MPE]</td>\n",
       "      <td>[Maximum Likelihood Linear Transform, Boosted ...</td>\n",
       "      <td>[[1, 4], [44, 48], [87, 91], [115, 118]]</td>\n",
       "      <td>[[7, 42], [51, 85], [94, 113]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>gold label (Section 3). Our algorithm is calle...</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93]]</td>\n",
       "      <td>['SWVP']</td>\n",
       "      <td>['Structured Weighted Violations Perceptron']</td>\n",
       "      <td>[SWVP]</td>\n",
       "      <td>[Structured Weighted Violations Perceptron, on a]</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93], [129, 133]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>phrase markers or words. For simplicity of man...</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[128, 147]]</td>\n",
       "      <td>['CNF']</td>\n",
       "      <td>['Chomsky Normal Form']</td>\n",
       "      <td>[CNF]</td>\n",
       "      <td>[or, Chomsky Normal Form]</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[15, 17], [128, 147]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>When a CFG is associated with probabilistic i...</td>\n",
       "      <td>[[85, 89], [8, 11]]</td>\n",
       "      <td>[[66, 83]]</td>\n",
       "      <td>['PCFG', 'CFG']</td>\n",
       "      <td>['Probabilistic CFG']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a CFG is, a Probabilistic CFG, n]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[5, 13], [63, 82], [3, 4]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1] proposed a language-neutral framework for r...</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[103, 126]]</td>\n",
       "      <td>['LNS']</td>\n",
       "      <td>['Language Neutral Syntax']</td>\n",
       "      <td>[LNS]</td>\n",
       "      <td>[a, Language Neutral Syntax]</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[12, 13], [103, 126]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...</td>\n",
       "      <td>[[187, 189], [18, 20], [102, 104]]</td>\n",
       "      <td>[[169, 185], [8, 16], [90, 99]]</td>\n",
       "      <td>['AP', 'FN', 'VP']</td>\n",
       "      <td>['Base+Appositives', 'FrameNet', 'Verb Pair']</td>\n",
       "      <td>[FN, VP, AP]</td>\n",
       "      <td>[Base+FrameNet, Base+Verb Pairs, Base+Appositi...</td>\n",
       "      <td>[[18, 20], [102, 104], [187, 189]]</td>\n",
       "      <td>[[3, 16], [85, 100], [169, 185]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Model 3 (Figure 3) illustrates how the source ...</td>\n",
       "      <td>[[127, 130], [64, 66]]</td>\n",
       "      <td>[[98, 125]]</td>\n",
       "      <td>['NLG', 'MT']</td>\n",
       "      <td>['natural language generation']</td>\n",
       "      <td>[MT, NLG]</td>\n",
       "      <td>[language, a natural language generation, a]</td>\n",
       "      <td>[[64, 66], [127, 130]]</td>\n",
       "      <td>[[46, 54], [96, 125], [26, 27]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "      <td>['LR', 'LP']</td>\n",
       "      <td>['labeled recall', 'labeled precision']</td>\n",
       "      <td>[LR, LP]</td>\n",
       "      <td>[labeled recall, labeled precision]</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>The big blue door.?  In this case, the GrM ask...</td>\n",
       "      <td>[[71, 73], [39, 42], [117, 119], [125, 127]]</td>\n",
       "      <td>[[53, 69]]</td>\n",
       "      <td>['RP', 'GrM', 'UU', 'RP']</td>\n",
       "      <td>['Response Planner']</td>\n",
       "      <td>[GrM, RP, UU]</td>\n",
       "      <td>[Response Planner, an]</td>\n",
       "      <td>[[39, 42], [71, 73], [117, 119]]</td>\n",
       "      <td>[[53, 69], [64, 66]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>TI = terse information  INT = interrupted  TRU...</td>\n",
       "      <td>[[43, 47], [0, 2], [24, 27], [61, 66]]</td>\n",
       "      <td>[[50, 59], [5, 22], [30, 41], [69, 79]]</td>\n",
       "      <td>['TRUN', 'TI', 'INT', 'TRANS']</td>\n",
       "      <td>['truncated', 'terse information', 'interrupte...</td>\n",
       "      <td>[TI, INT, TRUN, TRANS]</td>\n",
       "      <td>[terse information, interrupted, truncated, tr...</td>\n",
       "      <td>[[0, 2], [24, 27], [43, 47], [61, 66]]</td>\n",
       "      <td>[[5, 22], [30, 41], [50, 59], [69, 88]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>[[85, 88], [140, 143]]</td>\n",
       "      <td>[[60, 83]]</td>\n",
       "      <td>['SVM', 'AGI']</td>\n",
       "      <td>['Support Vector Machines']</td>\n",
       "      <td>[SVM]</td>\n",
       "      <td>[in, Support Vector Machines, in]</td>\n",
       "      <td>[[85, 88]]</td>\n",
       "      <td>[[8, 10], [60, 83], [8, 10]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>as in figure 3.  It is parsed as an adverb (AA...</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[[33, 42], [72, 82]]</td>\n",
       "      <td>['AA', 'VG']</td>\n",
       "      <td>['an adverb', 'verb group']</td>\n",
       "      <td>[AA, VG]</td>\n",
       "      <td>[adverb, a verb group]</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[[36, 42], [70, 82]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>[[65, 69]]</td>\n",
       "      <td>[[71, 100]]</td>\n",
       "      <td>['PTSD']</td>\n",
       "      <td>['posttraumatic stress disorder']</td>\n",
       "      <td>[PTSD]</td>\n",
       "      <td>[a, posttraumatic stress disorder, is a, disor...</td>\n",
       "      <td>[[65, 69]]</td>\n",
       "      <td>[[1, 2], [71, 100], [102, 106], [92, 100], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "      <td>['LTC']</td>\n",
       "      <td>['Language Technology Components']</td>\n",
       "      <td>[LTC]</td>\n",
       "      <td>[Language Technology Components]</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>[[63, 65], [71, 74], [96, 98]]</td>\n",
       "      <td>[[45, 61]]</td>\n",
       "      <td>['PA', 'POS', 'PA']</td>\n",
       "      <td>['product attribut']</td>\n",
       "      <td>[PA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[63, 65]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>The tagging has been done using a GUI-based t...</td>\n",
       "      <td>[[86, 92], [35, 38]]</td>\n",
       "      <td>[[62, 84]]</td>\n",
       "      <td>['DTTool', 'GUI']</td>\n",
       "      <td>['Discourse Tagging Tool']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, Discourse Tagging Tool, Discourse Tagging]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[5, 6], [61, 83], [61, 78]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Och and Ney (2003) show that for larger corpor...</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "      <td>['AER']</td>\n",
       "      <td>['Alignment Error Rate']</td>\n",
       "      <td>[AER]</td>\n",
       "      <td>[Alignment Error Rate]</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2 Relation Extraction System In this section, ...</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "      <td>['RE']</td>\n",
       "      <td>['relation extraction']</td>\n",
       "      <td>[RE]</td>\n",
       "      <td>[relation extraction]</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>target question, or zero if the target questio...</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "      <td>['MRR']</td>\n",
       "      <td>['Mean Reciprocal Rank']</td>\n",
       "      <td>[MRR]</td>\n",
       "      <td>[Mean Reciprocal Rank]</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>discussion in Ritchie(1984).  Functional unifi...</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52]]</td>\n",
       "      <td>['FU']</td>\n",
       "      <td>['Functional unification']</td>\n",
       "      <td>[FU]</td>\n",
       "      <td>[Functional unification, a]</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52], [38, 39]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Bigram Perplex. ( PP) MDI Missed Samples (MS) ...</td>\n",
       "      <td>[[42, 44], [18, 20], [22, 25], [69, 71]]</td>\n",
       "      <td>[[26, 40], [7, 14], [53, 67]]</td>\n",
       "      <td>['MS', 'PP', 'MDI', 'MS']</td>\n",
       "      <td>['Missed Samples', 'Perplex', 'Missed Samples']</td>\n",
       "      <td>[PP, MDI, MS]</td>\n",
       "      <td>[Missed Samples, Missed Samples]</td>\n",
       "      <td>[[18, 20], [22, 25], [42, 44]]</td>\n",
       "      <td>[[26, 40], [26, 40]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>networks. In Proceedings of the IEEE Conferenc...</td>\n",
       "      <td>[[92, 96], [32, 36]]</td>\n",
       "      <td>[[51, 90]]</td>\n",
       "      <td>['CVPR', 'IEEE']</td>\n",
       "      <td>['Computer Vision and Pattern Recognition']</td>\n",
       "      <td>[IEEE, CVPR]</td>\n",
       "      <td>[Conference on Computer Vision and Pattern Rec...</td>\n",
       "      <td>[[32, 36], [92, 96]]</td>\n",
       "      <td>[[37, 90]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Therefore, identification methods like Tsuchiy...</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[[72, 94]]</td>\n",
       "      <td>['SVM']</td>\n",
       "      <td>['Support Vector Machine']</td>\n",
       "      <td>[SVM]</td>\n",
       "      <td>[Support Vector Machines, to]</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[[72, 95], [83, 85]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[112, 136]]</td>\n",
       "      <td>['SPE']</td>\n",
       "      <td>['Sound Pattern of English']</td>\n",
       "      <td>[SPE]</td>\n",
       "      <td>[of, Sound Pattern of English]</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[102, 104], [112, 136]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...</td>\n",
       "      <td>[[35, 38]]</td>\n",
       "      <td>[[26, 31]]</td>\n",
       "      <td>['P+R']</td>\n",
       "      <td>['2*P*R']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>eugene@mathcs.emory.edu Abstract Community que...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[33, 61]]</td>\n",
       "      <td>['CQA']</td>\n",
       "      <td>['Community question answering']</td>\n",
       "      <td>[CQA]</td>\n",
       "      <td>[question answering, question]</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[43, 61], [43, 51]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>469</td>\n",
       "      <td>means a type of source ? newswire (NW), broad...</td>\n",
       "      <td>[[36, 38], [57, 59], [86, 88]]</td>\n",
       "      <td>[[26, 34], [41, 55], [62, 84]]</td>\n",
       "      <td>['NW', 'BN', 'BC']</td>\n",
       "      <td>['newswire', 'broadcast news', 'broadcast conv...</td>\n",
       "      <td>[NW, BN, BC]</td>\n",
       "      <td>[a, newswire, broadcast news, broadcast conver...</td>\n",
       "      <td>[[35, 37], [56, 58], [85, 87]]</td>\n",
       "      <td>[[2, 3], [25, 33], [40, 54], [61, 83]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>470</td>\n",
       "      <td>either lexically encoded, or depends on the in...</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[104, 113]]</td>\n",
       "      <td>['VPT']</td>\n",
       "      <td>['viewpoint']</td>\n",
       "      <td>[VPT]</td>\n",
       "      <td>[a, viewpoint]</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[12, 13], [104, 113]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>471</td>\n",
       "      <td>Abstract  This paper attempts to use an off-th...</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[55, 74]]</td>\n",
       "      <td>['AR']</td>\n",
       "      <td>['anaphora resolution']</td>\n",
       "      <td>[AR]</td>\n",
       "      <td>[an, anaphora resolution]</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[37, 39], [55, 74]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>472</td>\n",
       "      <td>The SemEval?2007 task for extracting frame sem...</td>\n",
       "      <td>[[125, 127], [4, 11]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "      <td>['FN', 'SemEval']</td>\n",
       "      <td>['FrameNet']</td>\n",
       "      <td>[FN]</td>\n",
       "      <td>[FrameNet]</td>\n",
       "      <td>[[125, 127]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>473</td>\n",
       "      <td>hauer, haver, haber) and the corpus does not c...</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "      <td>['PoS']</td>\n",
       "      <td>['part of speech']</td>\n",
       "      <td>[PoS]</td>\n",
       "      <td>[part of speech]</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>474</td>\n",
       "      <td>The ungrammatical distracter, e.g., are in F...</td>\n",
       "      <td>[[88, 91]]</td>\n",
       "      <td>[[72, 86]]</td>\n",
       "      <td>['POS']</td>\n",
       "      <td>['part of speech']</td>\n",
       "      <td>[POS]</td>\n",
       "      <td>[a, part of speech]</td>\n",
       "      <td>[[86, 89]]</td>\n",
       "      <td>[[8, 9], [70, 84]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>475</td>\n",
       "      <td>Words/Phrases as Themselves (WD)  Symbols/Nonl...</td>\n",
       "      <td>[[81, 83], [29, 31], [60, 62], [96, 98]]</td>\n",
       "      <td>[[65, 73], [34, 41], [86, 94]]</td>\n",
       "      <td>['PH', 'WD', 'SY', 'SP']</td>\n",
       "      <td>['Phonetic', 'Symbols', 'Spelling']</td>\n",
       "      <td>[WD, SY, PH, SP]</td>\n",
       "      <td>[Phonetic/Sound, Spelling]</td>\n",
       "      <td>[[29, 31], [60, 62], [81, 83], [96, 98]]</td>\n",
       "      <td>[[65, 79], [86, 94]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>476</td>\n",
       "      <td>DS = Discharge Summary,  Echo = Echocardiogram...</td>\n",
       "      <td>[[109, 112], [0, 2], [25, 29], [48, 50], [76, ...</td>\n",
       "      <td>[[115, 128], [5, 22], [32, 46], [53, 73], [91,...</td>\n",
       "      <td>['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']</td>\n",
       "      <td>['Radiology and', 'Discharge Summary', 'Echoca...</td>\n",
       "      <td>[DS, ED, GI, RAD, SP]</td>\n",
       "      <td>[Discharge Summary, Echo, Echocardiogram, Emer...</td>\n",
       "      <td>[[0, 2], [48, 50], [76, 78], [109, 112], [130,...</td>\n",
       "      <td>[[5, 22], [25, 29], [32, 46], [53, 73], [81, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>477</td>\n",
       "      <td>mdiab@ccls.columbia.edu Abstract We analyze ov...</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[[44, 58]]</td>\n",
       "      <td>['ODPs']</td>\n",
       "      <td>['overt displays']</td>\n",
       "      <td>[ODPs]</td>\n",
       "      <td>[overt displays of power, in]</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[[44, 67], [75, 77]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>478</td>\n",
       "      <td>(PERS), organization (ORG), geo-political enti...</td>\n",
       "      <td>[[115, 118], [1, 5], [22, 25], [50, 53], [64, ...</td>\n",
       "      <td>[[105, 113], [8, 20], [28, 48], [56, 62], [70,...</td>\n",
       "      <td>['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'L...</td>\n",
       "      <td>['facility', 'organization', 'geo-political en...</td>\n",
       "      <td>[PERS, ORG, GPE, WEA, VEH, LOC, FAC]</td>\n",
       "      <td>[organization, geo-political entity, weapon, v...</td>\n",
       "      <td>[[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...</td>\n",
       "      <td>[[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>479</td>\n",
       "      <td>Abstract In this paper, we propose a new synta...</td>\n",
       "      <td>[[74, 76], [109, 111]]</td>\n",
       "      <td>[[53, 72]]</td>\n",
       "      <td>['MT', 'MT']</td>\n",
       "      <td>['machine translation']</td>\n",
       "      <td>[MT]</td>\n",
       "      <td>[a, machine translation, on, a]</td>\n",
       "      <td>[[74, 76]]</td>\n",
       "      <td>[[5, 6], [53, 72], [70, 72], [5, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>480</td>\n",
       "      <td>The second one is a variant that we named  D...</td>\n",
       "      <td>[[81, 85]]</td>\n",
       "      <td>[[45, 79]]</td>\n",
       "      <td>['DLED']</td>\n",
       "      <td>['Double Levenshtein?s Edit Distance']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[is a, Levenshtein ? s Edit Distance]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[15, 19], [-1, 28]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>481</td>\n",
       "      <td>by HG's (HL). In particular, we show that HL's...</td>\n",
       "      <td>[[154, 159], [3, 5], [9, 11], [42, 44], [63, 6...</td>\n",
       "      <td>[[130, 152]]</td>\n",
       "      <td>[\"MHG's\", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL...</td>\n",
       "      <td>['Modified Head Grammars']</td>\n",
       "      <td>[HG's, HL, TAL's, TAG's]</td>\n",
       "      <td>[s, Modified Head Grammars]</td>\n",
       "      <td>[[3, 7], [9, 11], [63, 68], [77, 82]]</td>\n",
       "      <td>[[6, 7], [130, 152]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>482</td>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>[[127, 132]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "      <td>[\"CSI's\"]</td>\n",
       "      <td>['concept sequence instances']</td>\n",
       "      <td>[CSI's]</td>\n",
       "      <td>[concept sequence instances]</td>\n",
       "      <td>[[127, 132]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>483</td>\n",
       "      <td>(Ramshaw and Marcus, 1995) approached chuckin...</td>\n",
       "      <td>[[87, 90]]</td>\n",
       "      <td>[[57, 86]]</td>\n",
       "      <td>['TBL']</td>\n",
       "      <td>['Transformation Based Learning']</td>\n",
       "      <td>[TBL]</td>\n",
       "      <td>[Transformation Based Learning]</td>\n",
       "      <td>[[86, 89]]</td>\n",
       "      <td>[[56, 85]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>484</td>\n",
       "      <td>demonstrate such dependencies.  The Maximum En...</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51]]</td>\n",
       "      <td>['MaxEnt']</td>\n",
       "      <td>['Maximum Entropy']</td>\n",
       "      <td>[MaxEnt]</td>\n",
       "      <td>[Maximum Entropy, a]</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51], [8, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>485</td>\n",
       "      <td>dialogues categorized into multiple domains, w...</td>\n",
       "      <td>[[128, 133]]</td>\n",
       "      <td>[[109, 126]]</td>\n",
       "      <td>['CSHMM']</td>\n",
       "      <td>['Class Speaker HMM']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, hidden Markov, HMM, Class Speaker HMM]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[2, 3], [76, 89], [97, 100], [109, 126]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>486</td>\n",
       "      <td>shared task, namely FLORIAN (Florian et al.,  ...</td>\n",
       "      <td>[[56, 64], [20, 27]]</td>\n",
       "      <td>[[66, 78], [29, 36]]</td>\n",
       "      <td>['CHIEU-NG', 'FLORIAN']</td>\n",
       "      <td>['Chieu and Ng', 'Florian']</td>\n",
       "      <td>[FLORIAN, CHIEU-NG]</td>\n",
       "      <td>[Florian, and, Chieu and Ng]</td>\n",
       "      <td>[[20, 27], [56, 64]]</td>\n",
       "      <td>[[29, 36], [52, 55], [66, 78]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>487</td>\n",
       "      <td>1 Introduction Open-domain Question Answering...</td>\n",
       "      <td>[[48, 50]]</td>\n",
       "      <td>[[28, 46]]</td>\n",
       "      <td>['QA']</td>\n",
       "      <td>['Question Answering']</td>\n",
       "      <td>[)]</td>\n",
       "      <td>[Answering]</td>\n",
       "      <td>[[49, 50]]</td>\n",
       "      <td>[[36, 45]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>488</td>\n",
       "      <td>229  Proceedings of the 2014 Conference on Emp...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "      <td>['EMNLP']</td>\n",
       "      <td>['Empirical Methods in Natural Language Proces...</td>\n",
       "      <td>[EMNLP]</td>\n",
       "      <td>[Empirical Methods in Natural Language Process...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>489</td>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>[[90, 92], [154, 156]]</td>\n",
       "      <td>[[74, 88]]</td>\n",
       "      <td>['KB', 'QA']</td>\n",
       "      <td>['Knowledge Base']</td>\n",
       "      <td>[KB]</td>\n",
       "      <td>[Knowledge Base, a]</td>\n",
       "      <td>[[90, 92]]</td>\n",
       "      <td>[[74, 88], [16, 17]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>490</td>\n",
       "      <td>th fo frture representations abstracts (AbT), ...</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73], [86, 89]]</td>\n",
       "      <td>[[29, 38], [46, 52], [61, 68], [76, 84]]</td>\n",
       "      <td>['AbT', 'ArT', 'Aut', 'Jou']</td>\n",
       "      <td>['abstracts', 'titles', 'authors', 'Journals']</td>\n",
       "      <td>[AbT, ArT, Aut]</td>\n",
       "      <td>[th, representations abstracts, titles, author...</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73]]</td>\n",
       "      <td>[[0, 2], [13, 38], [46, 52], [61, 68], [76, 84]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>491</td>\n",
       "      <td>787 After labeling the reference BINet, we tra...</td>\n",
       "      <td>[[69, 72]]</td>\n",
       "      <td>[[51, 67]]</td>\n",
       "      <td>['L2R']</td>\n",
       "      <td>['learning to rank']</td>\n",
       "      <td>[,, )]</td>\n",
       "      <td>[a, to rank, in]</td>\n",
       "      <td>[[38, 39], [72, 73]]</td>\n",
       "      <td>[[11, 12], [60, 67], [15, 17]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>492</td>\n",
       "      <td>for the annotation process.  Topic models (TMs...</td>\n",
       "      <td>[[43, 46]]</td>\n",
       "      <td>[[29, 41]]</td>\n",
       "      <td>['TMs']</td>\n",
       "      <td>['Topic models']</td>\n",
       "      <td>[TMs]</td>\n",
       "      <td>[Topic models, a]</td>\n",
       "      <td>[[43, 46]]</td>\n",
       "      <td>[[29, 41], [8, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>493</td>\n",
       "      <td>Evidence for a text?s topic and genre comes, i...</td>\n",
       "      <td>[[183, 186], [206, 209]]</td>\n",
       "      <td>[[151, 181]]</td>\n",
       "      <td>['AGC', 'AGC']</td>\n",
       "      <td>['Automatic Genre Classification']</td>\n",
       "      <td>[AGC]</td>\n",
       "      <td>[a, Automatic Topic Classification, Automatic ...</td>\n",
       "      <td>[[183, 186]]</td>\n",
       "      <td>[[13, 14], [116, 146], [151, 181]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>494</td>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "      <td>['OR']</td>\n",
       "      <td>['Omission Rate']</td>\n",
       "      <td>[OR]</td>\n",
       "      <td>[Omission Rate]</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>495</td>\n",
       "      <td>The query (Figure 4a) will match adjectives (A...</td>\n",
       "      <td>[[81, 83], [45, 49], [164, 166]]</td>\n",
       "      <td>[[75, 79], [33, 43], [158, 162]]</td>\n",
       "      <td>['NN', 'ADJA', 'NE']</td>\n",
       "      <td>['noun', 'adjectives', 'name']</td>\n",
       "      <td>[ADJA, NN]</td>\n",
       "      <td>[match, noun, noun, name]</td>\n",
       "      <td>[[45, 49], [81, 83]]</td>\n",
       "      <td>[[27, 32], [75, 79], [75, 79], [158, 162]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>the middle (Baxendale, 1958).  Sentence Positi...</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "      <td>['SPY']</td>\n",
       "      <td>['Sentence Position Yield']</td>\n",
       "      <td>[SPY]</td>\n",
       "      <td>[Sentence Position Yield]</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>By contrast, our approach operates at the l...</td>\n",
       "      <td>[[83, 86]]</td>\n",
       "      <td>[[55, 81]]</td>\n",
       "      <td>['IPS']</td>\n",
       "      <td>['inflectional property sets']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[inflectional property sets, inflectional]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[52, 78], [52, 64]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               text  \\\n",
       "0      1  2 Related Work The availability of emotion-ric...   \n",
       "1      2  WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...   \n",
       "2      3  Baselines are a unigram query likelihood (QL) ...   \n",
       "3      4  tion. Then, we extract expansion terms from th...   \n",
       "4      5  person, mood, voice and case, CATiB uses 6 POS...   \n",
       "5      6  (LDA), Maximum Likelihood Linear Transform (ML...   \n",
       "6      7  gold label (Section 3). Our algorithm is calle...   \n",
       "7      8  phrase markers or words. For simplicity of man...   \n",
       "8      9   When a CFG is associated with probabilistic i...   \n",
       "9     10  1] proposed a language-neutral framework for r...   \n",
       "10    11  12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...   \n",
       "11    12  Model 3 (Figure 3) illustrates how the source ...   \n",
       "12    13  in the V column indicates that the verb condit...   \n",
       "13    14  The big blue door.?  In this case, the GrM ask...   \n",
       "14    15  TI = terse information  INT = interrupted  TRU...   \n",
       "15    16  classes in both original text and main text co...   \n",
       "16    17  as in figure 3.  It is parsed as an adverb (AA...   \n",
       "17    18  can develop after exposure to a terrifying eve...   \n",
       "18    19  It allows for testing interaction scenarios th...   \n",
       "19    20  Feature Description lexical the words of the p...   \n",
       "20    21   The tagging has been done using a GUI-based t...   \n",
       "21    22  Och and Ney (2003) show that for larger corpor...   \n",
       "22    23  2 Relation Extraction System In this section, ...   \n",
       "23    24  target question, or zero if the target questio...   \n",
       "24    25  discussion in Ritchie(1984).  Functional unifi...   \n",
       "25    26  Bigram Perplex. ( PP) MDI Missed Samples (MS) ...   \n",
       "26    27  networks. In Proceedings of the IEEE Conferenc...   \n",
       "27    28  Therefore, identification methods like Tsuchiy...   \n",
       "28    29  domain-independent mpirical induction algorith...   \n",
       "29    30  ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...   \n",
       "..   ...                                                ...   \n",
       "467  468  eugene@mathcs.emory.edu Abstract Community que...   \n",
       "468  469   means a type of source ? newswire (NW), broad...   \n",
       "469  470  either lexically encoded, or depends on the in...   \n",
       "470  471  Abstract  This paper attempts to use an off-th...   \n",
       "471  472  The SemEval?2007 task for extracting frame sem...   \n",
       "472  473  hauer, haver, haber) and the corpus does not c...   \n",
       "473  474    The ungrammatical distracter, e.g., are in F...   \n",
       "474  475  Words/Phrases as Themselves (WD)  Symbols/Nonl...   \n",
       "475  476  DS = Discharge Summary,  Echo = Echocardiogram...   \n",
       "476  477  mdiab@ccls.columbia.edu Abstract We analyze ov...   \n",
       "477  478  (PERS), organization (ORG), geo-political enti...   \n",
       "478  479  Abstract In this paper, we propose a new synta...   \n",
       "479  480    The second one is a variant that we named  D...   \n",
       "480  481  by HG's (HL). In particular, we show that HL's...   \n",
       "481  482  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482  483   (Ramshaw and Marcus, 1995) approached chuckin...   \n",
       "483  484  demonstrate such dependencies.  The Maximum En...   \n",
       "484  485  dialogues categorized into multiple domains, w...   \n",
       "485  486  shared task, namely FLORIAN (Florian et al.,  ...   \n",
       "486  487   1 Introduction Open-domain Question Answering...   \n",
       "487  488  229  Proceedings of the 2014 Conference on Emp...   \n",
       "488  489  1 Introduction Large-scale open-domain questio...   \n",
       "489  490  th fo frture representations abstracts (AbT), ...   \n",
       "490  491  787 After labeling the reference BINet, we tra...   \n",
       "491  492  for the annotation process.  Topic models (TMs...   \n",
       "492  493  Evidence for a text?s topic and genre comes, i...   \n",
       "493  494     0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co   \n",
       "494  495  The query (Figure 4a) will match adjectives (A...   \n",
       "495  496  the middle (Baxendale, 1958).  Sentence Positi...   \n",
       "496  497     By contrast, our approach operates at the l...   \n",
       "\n",
       "                                              acronyms  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                   [[113, 115], [42, 45], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[130, 134], [30, 35], [43, 46], [53, 56], [15...   \n",
       "5                     [[115, 118], [44, 48], [87, 91]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                  [[85, 89], [8, 11]]   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[187, 189], [18, 20], [102, 104]]   \n",
       "11                              [[127, 130], [64, 66]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13        [[71, 73], [39, 42], [117, 119], [125, 127]]   \n",
       "14              [[43, 47], [0, 2], [24, 27], [61, 66]]   \n",
       "15                              [[85, 88], [140, 143]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                          [[65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                      [[63, 65], [71, 74], [96, 98]]   \n",
       "20                                [[86, 92], [35, 38]]   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25            [[42, 44], [18, 20], [22, 25], [69, 71]]   \n",
       "26                                [[92, 96], [32, 36]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                          [[35, 38]]   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[36, 38], [57, 59], [86, 88]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                              [[125, 127], [4, 11]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[88, 91]]   \n",
       "474           [[81, 83], [29, 31], [60, 62], [96, 98]]   \n",
       "475  [[109, 112], [0, 2], [25, 29], [48, 50], [76, ...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[115, 118], [1, 5], [22, 25], [50, 53], [64, ...   \n",
       "478                             [[74, 76], [109, 111]]   \n",
       "479                                         [[81, 85]]   \n",
       "480  [[154, 159], [3, 5], [9, 11], [42, 44], [63, 6...   \n",
       "481                                       [[127, 132]]   \n",
       "482                                         [[87, 90]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                                       [[128, 133]]   \n",
       "485                               [[56, 64], [20, 27]]   \n",
       "486                                         [[48, 50]]   \n",
       "487                                         [[93, 98]]   \n",
       "488                             [[90, 92], [154, 156]]   \n",
       "489           [[40, 43], [54, 57], [70, 73], [86, 89]]   \n",
       "490                                         [[69, 72]]   \n",
       "491                                         [[43, 46]]   \n",
       "492                           [[183, 186], [206, 209]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                   [[81, 83], [45, 49], [164, 166]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                         [[83, 86]]   \n",
       "\n",
       "                                            long-forms  \\\n",
       "0                                         [[144, 160]]   \n",
       "1                                           [[39, 61]]   \n",
       "2                              [[90, 111], [132, 151]]   \n",
       "3                                           [[65, 90]]   \n",
       "4                             [[136, 142], [156, 161]]   \n",
       "5                       [[94, 113], [7, 42], [51, 85]]   \n",
       "6                                           [[52, 93]]   \n",
       "7                                         [[128, 147]]   \n",
       "8                                           [[66, 83]]   \n",
       "9                                         [[103, 126]]   \n",
       "10                     [[169, 185], [8, 16], [90, 99]]   \n",
       "11                                         [[98, 125]]   \n",
       "12                             [[91, 105], [112, 129]]   \n",
       "13                                          [[53, 69]]   \n",
       "14             [[50, 59], [5, 22], [30, 41], [69, 79]]   \n",
       "15                                          [[60, 83]]   \n",
       "16                                [[33, 42], [72, 82]]   \n",
       "17                                         [[71, 100]]   \n",
       "18                                          [[68, 98]]   \n",
       "19                                          [[45, 61]]   \n",
       "20                                          [[62, 84]]   \n",
       "21                                         [[83, 103]]   \n",
       "22                                         [[89, 108]]   \n",
       "23                                          [[67, 87]]   \n",
       "24                                          [[30, 52]]   \n",
       "25                       [[26, 40], [7, 14], [53, 67]]   \n",
       "26                                          [[51, 90]]   \n",
       "27                                          [[72, 94]]   \n",
       "28                                        [[112, 136]]   \n",
       "29                                          [[26, 31]]   \n",
       "..                                                 ...   \n",
       "467                                         [[33, 61]]   \n",
       "468                     [[26, 34], [41, 55], [62, 84]]   \n",
       "469                                       [[104, 113]]   \n",
       "470                                         [[55, 74]]   \n",
       "471                                       [[115, 123]]   \n",
       "472                                       [[105, 119]]   \n",
       "473                                         [[72, 86]]   \n",
       "474                     [[65, 73], [34, 41], [86, 94]]   \n",
       "475  [[115, 128], [5, 22], [32, 46], [53, 73], [91,...   \n",
       "476                                         [[44, 58]]   \n",
       "477  [[105, 113], [8, 20], [28, 48], [56, 62], [70,...   \n",
       "478                                         [[53, 72]]   \n",
       "479                                         [[45, 79]]   \n",
       "480                                       [[130, 152]]   \n",
       "481                                        [[99, 125]]   \n",
       "482                                         [[57, 86]]   \n",
       "483                                         [[36, 51]]   \n",
       "484                                       [[109, 126]]   \n",
       "485                               [[66, 78], [29, 36]]   \n",
       "486                                         [[28, 46]]   \n",
       "487                                         [[43, 91]]   \n",
       "488                                         [[74, 88]]   \n",
       "489           [[29, 38], [46, 52], [61, 68], [76, 84]]   \n",
       "490                                         [[51, 67]]   \n",
       "491                                         [[29, 41]]   \n",
       "492                                       [[151, 181]]   \n",
       "493                                         [[25, 38]]   \n",
       "494                   [[75, 79], [33, 43], [158, 162]]   \n",
       "495                                         [[31, 54]]   \n",
       "496                                         [[55, 81]]   \n",
       "\n",
       "                                         acronyms-text  \\\n",
       "0                                              ['TDM']   \n",
       "1                                              ['MAP']   \n",
       "2                                 ['SD', 'QL)', 'MRF']   \n",
       "3                                              ['PRF']   \n",
       "4    ['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...   \n",
       "5                              ['MPE', 'MLLT', 'BMMI']   \n",
       "6                                             ['SWVP']   \n",
       "7                                              ['CNF']   \n",
       "8                                      ['PCFG', 'CFG']   \n",
       "9                                              ['LNS']   \n",
       "10                                  ['AP', 'FN', 'VP']   \n",
       "11                                       ['NLG', 'MT']   \n",
       "12                                        ['LR', 'LP']   \n",
       "13                           ['RP', 'GrM', 'UU', 'RP']   \n",
       "14                      ['TRUN', 'TI', 'INT', 'TRANS']   \n",
       "15                                      ['SVM', 'AGI']   \n",
       "16                                        ['AA', 'VG']   \n",
       "17                                            ['PTSD']   \n",
       "18                                             ['LTC']   \n",
       "19                                 ['PA', 'POS', 'PA']   \n",
       "20                                   ['DTTool', 'GUI']   \n",
       "21                                             ['AER']   \n",
       "22                                              ['RE']   \n",
       "23                                             ['MRR']   \n",
       "24                                              ['FU']   \n",
       "25                           ['MS', 'PP', 'MDI', 'MS']   \n",
       "26                                    ['CVPR', 'IEEE']   \n",
       "27                                             ['SVM']   \n",
       "28                                             ['SPE']   \n",
       "29                                             ['P+R']   \n",
       "..                                                 ...   \n",
       "467                                            ['CQA']   \n",
       "468                                 ['NW', 'BN', 'BC']   \n",
       "469                                            ['VPT']   \n",
       "470                                             ['AR']   \n",
       "471                                  ['FN', 'SemEval']   \n",
       "472                                            ['PoS']   \n",
       "473                                            ['POS']   \n",
       "474                           ['PH', 'WD', 'SY', 'SP']   \n",
       "475            ['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']   \n",
       "476                                           ['ODPs']   \n",
       "477  ['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'L...   \n",
       "478                                       ['MT', 'MT']   \n",
       "479                                           ['DLED']   \n",
       "480  [\"MHG's\", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL...   \n",
       "481                                          [\"CSI's\"]   \n",
       "482                                            ['TBL']   \n",
       "483                                         ['MaxEnt']   \n",
       "484                                          ['CSHMM']   \n",
       "485                            ['CHIEU-NG', 'FLORIAN']   \n",
       "486                                             ['QA']   \n",
       "487                                          ['EMNLP']   \n",
       "488                                       ['KB', 'QA']   \n",
       "489                       ['AbT', 'ArT', 'Aut', 'Jou']   \n",
       "490                                            ['L2R']   \n",
       "491                                            ['TMs']   \n",
       "492                                     ['AGC', 'AGC']   \n",
       "493                                             ['OR']   \n",
       "494                               ['NN', 'ADJA', 'NE']   \n",
       "495                                            ['SPY']   \n",
       "496                                            ['IPS']   \n",
       "\n",
       "                                       long-forms-text  \\\n",
       "0                                 ['Text Data Mining']   \n",
       "1                           ['Mean Average Precision']   \n",
       "2     ['sequential dependence', 'Markov random field']   \n",
       "3                        ['pseudo relevance feedback']   \n",
       "4                                  ['proper', 'verbs']   \n",
       "5    ['Minimum Phone Error', 'Maximum Likelihood Li...   \n",
       "6        ['Structured Weighted Violations Perceptron']   \n",
       "7                              ['Chomsky Normal Form']   \n",
       "8                                ['Probabilistic CFG']   \n",
       "9                          ['Language Neutral Syntax']   \n",
       "10       ['Base+Appositives', 'FrameNet', 'Verb Pair']   \n",
       "11                     ['natural language generation']   \n",
       "12             ['labeled recall', 'labeled precision']   \n",
       "13                                ['Response Planner']   \n",
       "14   ['truncated', 'terse information', 'interrupte...   \n",
       "15                         ['Support Vector Machines']   \n",
       "16                         ['an adverb', 'verb group']   \n",
       "17                   ['posttraumatic stress disorder']   \n",
       "18                  ['Language Technology Components']   \n",
       "19                                ['product attribut']   \n",
       "20                          ['Discourse Tagging Tool']   \n",
       "21                            ['Alignment Error Rate']   \n",
       "22                             ['relation extraction']   \n",
       "23                            ['Mean Reciprocal Rank']   \n",
       "24                          ['Functional unification']   \n",
       "25     ['Missed Samples', 'Perplex', 'Missed Samples']   \n",
       "26         ['Computer Vision and Pattern Recognition']   \n",
       "27                          ['Support Vector Machine']   \n",
       "28                        ['Sound Pattern of English']   \n",
       "29                                           ['2*P*R']   \n",
       "..                                                 ...   \n",
       "467                   ['Community question answering']   \n",
       "468  ['newswire', 'broadcast news', 'broadcast conv...   \n",
       "469                                      ['viewpoint']   \n",
       "470                            ['anaphora resolution']   \n",
       "471                                       ['FrameNet']   \n",
       "472                                 ['part of speech']   \n",
       "473                                 ['part of speech']   \n",
       "474                ['Phonetic', 'Symbols', 'Spelling']   \n",
       "475  ['Radiology and', 'Discharge Summary', 'Echoca...   \n",
       "476                                 ['overt displays']   \n",
       "477  ['facility', 'organization', 'geo-political en...   \n",
       "478                            ['machine translation']   \n",
       "479             ['Double Levenshtein?s Edit Distance']   \n",
       "480                         ['Modified Head Grammars']   \n",
       "481                     ['concept sequence instances']   \n",
       "482                  ['Transformation Based Learning']   \n",
       "483                                ['Maximum Entropy']   \n",
       "484                              ['Class Speaker HMM']   \n",
       "485                        ['Chieu and Ng', 'Florian']   \n",
       "486                             ['Question Answering']   \n",
       "487  ['Empirical Methods in Natural Language Proces...   \n",
       "488                                 ['Knowledge Base']   \n",
       "489     ['abstracts', 'titles', 'authors', 'Journals']   \n",
       "490                               ['learning to rank']   \n",
       "491                                   ['Topic models']   \n",
       "492                 ['Automatic Genre Classification']   \n",
       "493                                  ['Omission Rate']   \n",
       "494                     ['noun', 'adjectives', 'name']   \n",
       "495                        ['Sentence Position Yield']   \n",
       "496                     ['inflectional property sets']   \n",
       "\n",
       "                                  AN_Pred  \\\n",
       "0                                   [TDM]   \n",
       "1                                   [MAP]   \n",
       "2                           [QL, SD, MRF]   \n",
       "3                                   [PRF]   \n",
       "4            [CATiB, POS, NOM, PROP, VRB]   \n",
       "5                  [LDA, MLLT, BMMI, MPE]   \n",
       "6                                  [SWVP]   \n",
       "7                                   [CNF]   \n",
       "8                                      []   \n",
       "9                                   [LNS]   \n",
       "10                           [FN, VP, AP]   \n",
       "11                              [MT, NLG]   \n",
       "12                               [LR, LP]   \n",
       "13                          [GrM, RP, UU]   \n",
       "14                 [TI, INT, TRUN, TRANS]   \n",
       "15                                  [SVM]   \n",
       "16                               [AA, VG]   \n",
       "17                                 [PTSD]   \n",
       "18                                  [LTC]   \n",
       "19                                   [PA]   \n",
       "20                                     []   \n",
       "21                                  [AER]   \n",
       "22                                   [RE]   \n",
       "23                                  [MRR]   \n",
       "24                                   [FU]   \n",
       "25                          [PP, MDI, MS]   \n",
       "26                           [IEEE, CVPR]   \n",
       "27                                  [SVM]   \n",
       "28                                  [SPE]   \n",
       "29                                     []   \n",
       "..                                    ...   \n",
       "467                                 [CQA]   \n",
       "468                          [NW, BN, BC]   \n",
       "469                                 [VPT]   \n",
       "470                                  [AR]   \n",
       "471                                  [FN]   \n",
       "472                                 [PoS]   \n",
       "473                                 [POS]   \n",
       "474                      [WD, SY, PH, SP]   \n",
       "475                 [DS, ED, GI, RAD, SP]   \n",
       "476                                [ODPs]   \n",
       "477  [PERS, ORG, GPE, WEA, VEH, LOC, FAC]   \n",
       "478                                  [MT]   \n",
       "479                                    []   \n",
       "480              [HG's, HL, TAL's, TAG's]   \n",
       "481                               [CSI's]   \n",
       "482                                 [TBL]   \n",
       "483                              [MaxEnt]   \n",
       "484                                    []   \n",
       "485                   [FLORIAN, CHIEU-NG]   \n",
       "486                                   [)]   \n",
       "487                               [EMNLP]   \n",
       "488                                  [KB]   \n",
       "489                       [AbT, ArT, Aut]   \n",
       "490                                [,, )]   \n",
       "491                                 [TMs]   \n",
       "492                                 [AGC]   \n",
       "493                                  [OR]   \n",
       "494                            [ADJA, NN]   \n",
       "495                                 [SPY]   \n",
       "496                                    []   \n",
       "\n",
       "                                               LF_Pred  \\\n",
       "0                                [a, Text Data Mining]   \n",
       "1                             [Mean Average Precision]   \n",
       "2    [a, query likelihood, and a, sequential depend...   \n",
       "3                      [pseudo relevance feedback, in]   \n",
       "4             [non-proper, nouns, proper nouns, verbs]   \n",
       "5    [Maximum Likelihood Linear Transform, Boosted ...   \n",
       "6    [Structured Weighted Violations Perceptron, on a]   \n",
       "7                            [or, Chomsky Normal Form]   \n",
       "8                   [a CFG is, a Probabilistic CFG, n]   \n",
       "9                         [a, Language Neutral Syntax]   \n",
       "10   [Base+FrameNet, Base+Verb Pairs, Base+Appositi...   \n",
       "11        [language, a natural language generation, a]   \n",
       "12                 [labeled recall, labeled precision]   \n",
       "13                              [Response Planner, an]   \n",
       "14   [terse information, interrupted, truncated, tr...   \n",
       "15                   [in, Support Vector Machines, in]   \n",
       "16                              [adverb, a verb group]   \n",
       "17   [a, posttraumatic stress disorder, is a, disor...   \n",
       "18                    [Language Technology Components]   \n",
       "19                                                  []   \n",
       "20      [a, Discourse Tagging Tool, Discourse Tagging]   \n",
       "21                              [Alignment Error Rate]   \n",
       "22                               [relation extraction]   \n",
       "23                              [Mean Reciprocal Rank]   \n",
       "24                         [Functional unification, a]   \n",
       "25                    [Missed Samples, Missed Samples]   \n",
       "26   [Conference on Computer Vision and Pattern Rec...   \n",
       "27                       [Support Vector Machines, to]   \n",
       "28                      [of, Sound Pattern of English]   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "467                     [question answering, question]   \n",
       "468  [a, newswire, broadcast news, broadcast conver...   \n",
       "469                                     [a, viewpoint]   \n",
       "470                          [an, anaphora resolution]   \n",
       "471                                         [FrameNet]   \n",
       "472                                   [part of speech]   \n",
       "473                                [a, part of speech]   \n",
       "474                         [Phonetic/Sound, Spelling]   \n",
       "475  [Discharge Summary, Echo, Echocardiogram, Emer...   \n",
       "476                      [overt displays of power, in]   \n",
       "477  [organization, geo-political entity, weapon, v...   \n",
       "478                    [a, machine translation, on, a]   \n",
       "479              [is a, Levenshtein ? s Edit Distance]   \n",
       "480                        [s, Modified Head Grammars]   \n",
       "481                       [concept sequence instances]   \n",
       "482                    [Transformation Based Learning]   \n",
       "483                               [Maximum Entropy, a]   \n",
       "484         [a, hidden Markov, HMM, Class Speaker HMM]   \n",
       "485                       [Florian, and, Chieu and Ng]   \n",
       "486                                        [Answering]   \n",
       "487  [Empirical Methods in Natural Language Process...   \n",
       "488                                [Knowledge Base, a]   \n",
       "489  [th, representations abstracts, titles, author...   \n",
       "490                                   [a, to rank, in]   \n",
       "491                                  [Topic models, a]   \n",
       "492  [a, Automatic Topic Classification, Automatic ...   \n",
       "493                                    [Omission Rate]   \n",
       "494                          [match, noun, noun, name]   \n",
       "495                          [Sentence Position Yield]   \n",
       "496         [inflectional property sets, inflectional]   \n",
       "\n",
       "                                          AN_Pred_idxs  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                   [[42, 44], [113, 115], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[30, 35], [43, 46], [53, 56], [130, 134], [15...   \n",
       "5             [[1, 4], [44, 48], [87, 91], [115, 118]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                                   []   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[18, 20], [102, 104], [187, 189]]   \n",
       "11                              [[64, 66], [127, 130]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13                    [[39, 42], [71, 73], [117, 119]]   \n",
       "14              [[0, 2], [24, 27], [43, 47], [61, 66]]   \n",
       "15                                          [[85, 88]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                          [[65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                                          [[63, 65]]   \n",
       "20                                                  []   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25                      [[18, 20], [22, 25], [42, 44]]   \n",
       "26                                [[32, 36], [92, 96]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[35, 37], [56, 58], [85, 87]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                                       [[125, 127]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[86, 89]]   \n",
       "474           [[29, 31], [60, 62], [81, 83], [96, 98]]   \n",
       "475  [[0, 2], [48, 50], [76, 78], [109, 112], [130,...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...   \n",
       "478                                         [[74, 76]]   \n",
       "479                                                 []   \n",
       "480              [[3, 7], [9, 11], [63, 68], [77, 82]]   \n",
       "481                                       [[127, 132]]   \n",
       "482                                         [[86, 89]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                                                 []   \n",
       "485                               [[20, 27], [56, 64]]   \n",
       "486                                         [[49, 50]]   \n",
       "487                                         [[93, 98]]   \n",
       "488                                         [[90, 92]]   \n",
       "489                     [[40, 43], [54, 57], [70, 73]]   \n",
       "490                               [[38, 39], [72, 73]]   \n",
       "491                                         [[43, 46]]   \n",
       "492                                       [[183, 186]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                               [[45, 49], [81, 83]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                                 []   \n",
       "\n",
       "                                          LF_Pred_idxs  \n",
       "0                                 [[5, 6], [144, 160]]  \n",
       "1                                           [[39, 61]]  \n",
       "2    [[1, 2], [24, 40], [67, 72], [90, 111], [132, ...  \n",
       "3                                 [[65, 90], [61, 63]]  \n",
       "4         [[58, 68], [88, 93], [136, 148], [122, 127]]  \n",
       "5                       [[7, 42], [51, 85], [94, 113]]  \n",
       "6                               [[52, 93], [129, 133]]  \n",
       "7                               [[15, 17], [128, 147]]  \n",
       "8                          [[5, 13], [63, 82], [3, 4]]  \n",
       "9                               [[12, 13], [103, 126]]  \n",
       "10                    [[3, 16], [85, 100], [169, 185]]  \n",
       "11                     [[46, 54], [96, 125], [26, 27]]  \n",
       "12                             [[91, 105], [112, 129]]  \n",
       "13                                [[53, 69], [64, 66]]  \n",
       "14             [[5, 22], [30, 41], [50, 59], [69, 88]]  \n",
       "15                        [[8, 10], [60, 83], [8, 10]]  \n",
       "16                                [[36, 42], [70, 82]]  \n",
       "17   [[1, 2], [71, 100], [102, 106], [92, 100], [1,...  \n",
       "18                                          [[68, 98]]  \n",
       "19                                                  []  \n",
       "20                        [[5, 6], [61, 83], [61, 78]]  \n",
       "21                                         [[83, 103]]  \n",
       "22                                         [[89, 108]]  \n",
       "23                                          [[67, 87]]  \n",
       "24                                [[30, 52], [38, 39]]  \n",
       "25                                [[26, 40], [26, 40]]  \n",
       "26                                          [[37, 90]]  \n",
       "27                                [[72, 95], [83, 85]]  \n",
       "28                            [[102, 104], [112, 136]]  \n",
       "29                                                  []  \n",
       "..                                                 ...  \n",
       "467                               [[43, 61], [43, 51]]  \n",
       "468             [[2, 3], [25, 33], [40, 54], [61, 83]]  \n",
       "469                             [[12, 13], [104, 113]]  \n",
       "470                               [[37, 39], [55, 74]]  \n",
       "471                                       [[115, 123]]  \n",
       "472                                       [[105, 119]]  \n",
       "473                                 [[8, 9], [70, 84]]  \n",
       "474                               [[65, 79], [86, 94]]  \n",
       "475  [[5, 22], [25, 29], [32, 46], [53, 73], [81, 9...  \n",
       "476                               [[44, 67], [75, 77]]  \n",
       "477  [[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...  \n",
       "478               [[5, 6], [53, 72], [70, 72], [5, 6]]  \n",
       "479                               [[15, 19], [-1, 28]]  \n",
       "480                               [[6, 7], [130, 152]]  \n",
       "481                                        [[99, 125]]  \n",
       "482                                         [[56, 85]]  \n",
       "483                                 [[36, 51], [8, 9]]  \n",
       "484          [[2, 3], [76, 89], [97, 100], [109, 126]]  \n",
       "485                     [[29, 36], [52, 55], [66, 78]]  \n",
       "486                                         [[36, 45]]  \n",
       "487                                         [[43, 91]]  \n",
       "488                               [[74, 88], [16, 17]]  \n",
       "489   [[0, 2], [13, 38], [46, 52], [61, 68], [76, 84]]  \n",
       "490                     [[11, 12], [60, 67], [15, 17]]  \n",
       "491                                 [[29, 41], [8, 9]]  \n",
       "492                 [[13, 14], [116, 146], [151, 181]]  \n",
       "493                                         [[25, 38]]  \n",
       "494         [[27, 32], [75, 79], [75, 79], [158, 162]]  \n",
       "495                                         [[31, 54]]  \n",
       "496                               [[52, 78], [52, 64]]  \n",
       "\n",
       "[497 rows x 10 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig_1 = match(df_orig, df_pred)\n",
    "df_orig_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =[\"text\",\"AN_Pred_idxs\",\"LF_Pred_idxs\",\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/development/prashants/.conda/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[5, 6], [144, 160]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baselines are a unigram query likelihood (QL) ...</td>\n",
       "      <td>[[42, 44], [113, 115], [153, 156]]</td>\n",
       "      <td>[[1, 2], [24, 40], [67, 72], [90, 111], [132, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tion. Then, we extract expansion terms from th...</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90], [61, 63]]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person, mood, voice and case, CATiB uses 6 POS...</td>\n",
       "      <td>[[30, 35], [43, 46], [53, 56], [130, 134], [15...</td>\n",
       "      <td>[[58, 68], [88, 93], [136, 148], [122, 127]]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(LDA), Maximum Likelihood Linear Transform (ML...</td>\n",
       "      <td>[[1, 4], [44, 48], [87, 91], [115, 118]]</td>\n",
       "      <td>[[7, 42], [51, 85], [94, 113]]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gold label (Section 3). Our algorithm is calle...</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93], [129, 133]]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase markers or words. For simplicity of man...</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[15, 17], [128, 147]]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When a CFG is associated with probabilistic i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[5, 13], [63, 82], [3, 4]]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1] proposed a language-neutral framework for r...</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[12, 13], [103, 126]]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...</td>\n",
       "      <td>[[18, 20], [102, 104], [187, 189]]</td>\n",
       "      <td>[[3, 16], [85, 100], [169, 185]]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model 3 (Figure 3) illustrates how the source ...</td>\n",
       "      <td>[[64, 66], [127, 130]]</td>\n",
       "      <td>[[46, 54], [96, 125], [26, 27]]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The big blue door.?  In this case, the GrM ask...</td>\n",
       "      <td>[[39, 42], [71, 73], [117, 119]]</td>\n",
       "      <td>[[53, 69], [64, 66]]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TI = terse information  INT = interrupted  TRU...</td>\n",
       "      <td>[[0, 2], [24, 27], [43, 47], [61, 66]]</td>\n",
       "      <td>[[5, 22], [30, 41], [50, 59], [69, 88]]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>[[85, 88]]</td>\n",
       "      <td>[[8, 10], [60, 83], [8, 10]]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>as in figure 3.  It is parsed as an adverb (AA...</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[[36, 42], [70, 82]]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>[[65, 69]]</td>\n",
       "      <td>[[1, 2], [71, 100], [102, 106], [92, 100], [1,...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>[[63, 65]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The tagging has been done using a GUI-based t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[5, 6], [61, 83], [61, 78]]</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Och and Ney (2003) show that for larger corpor...</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2 Relation Extraction System In this section, ...</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>target question, or zero if the target questio...</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>discussion in Ritchie(1984).  Functional unifi...</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52], [38, 39]]</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bigram Perplex. ( PP) MDI Missed Samples (MS) ...</td>\n",
       "      <td>[[18, 20], [22, 25], [42, 44]]</td>\n",
       "      <td>[[26, 40], [26, 40]]</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>networks. In Proceedings of the IEEE Conferenc...</td>\n",
       "      <td>[[32, 36], [92, 96]]</td>\n",
       "      <td>[[37, 90]]</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Therefore, identification methods like Tsuchiy...</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[[72, 95], [83, 85]]</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[102, 104], [112, 136]]</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>eugene@mathcs.emory.edu Abstract Community que...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[43, 61], [43, 51]]</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>means a type of source ? newswire (NW), broad...</td>\n",
       "      <td>[[35, 37], [56, 58], [85, 87]]</td>\n",
       "      <td>[[2, 3], [25, 33], [40, 54], [61, 83]]</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>either lexically encoded, or depends on the in...</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[12, 13], [104, 113]]</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Abstract  This paper attempts to use an off-th...</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[37, 39], [55, 74]]</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>The SemEval?2007 task for extracting frame sem...</td>\n",
       "      <td>[[125, 127]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>hauer, haver, haber) and the corpus does not c...</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>The ungrammatical distracter, e.g., are in F...</td>\n",
       "      <td>[[86, 89]]</td>\n",
       "      <td>[[8, 9], [70, 84]]</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Words/Phrases as Themselves (WD)  Symbols/Nonl...</td>\n",
       "      <td>[[29, 31], [60, 62], [81, 83], [96, 98]]</td>\n",
       "      <td>[[65, 79], [86, 94]]</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>DS = Discharge Summary,  Echo = Echocardiogram...</td>\n",
       "      <td>[[0, 2], [48, 50], [76, 78], [109, 112], [130,...</td>\n",
       "      <td>[[5, 22], [25, 29], [32, 46], [53, 73], [81, 9...</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>mdiab@ccls.columbia.edu Abstract We analyze ov...</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[[44, 67], [75, 77]]</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>(PERS), organization (ORG), geo-political enti...</td>\n",
       "      <td>[[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...</td>\n",
       "      <td>[[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Abstract In this paper, we propose a new synta...</td>\n",
       "      <td>[[74, 76]]</td>\n",
       "      <td>[[5, 6], [53, 72], [70, 72], [5, 6]]</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>The second one is a variant that we named  D...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[15, 19], [-1, 28]]</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>by HG's (HL). In particular, we show that HL's...</td>\n",
       "      <td>[[3, 7], [9, 11], [63, 68], [77, 82]]</td>\n",
       "      <td>[[6, 7], [130, 152]]</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>[[127, 132]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>(Ramshaw and Marcus, 1995) approached chuckin...</td>\n",
       "      <td>[[86, 89]]</td>\n",
       "      <td>[[56, 85]]</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>demonstrate such dependencies.  The Maximum En...</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51], [8, 9]]</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>dialogues categorized into multiple domains, w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[2, 3], [76, 89], [97, 100], [109, 126]]</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>shared task, namely FLORIAN (Florian et al.,  ...</td>\n",
       "      <td>[[20, 27], [56, 64]]</td>\n",
       "      <td>[[29, 36], [52, 55], [66, 78]]</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1 Introduction Open-domain Question Answering...</td>\n",
       "      <td>[[49, 50]]</td>\n",
       "      <td>[[36, 45]]</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>229  Proceedings of the 2014 Conference on Emp...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>[[90, 92]]</td>\n",
       "      <td>[[74, 88], [16, 17]]</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>th fo frture representations abstracts (AbT), ...</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73]]</td>\n",
       "      <td>[[0, 2], [13, 38], [46, 52], [61, 68], [76, 84]]</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>787 After labeling the reference BINet, we tra...</td>\n",
       "      <td>[[38, 39], [72, 73]]</td>\n",
       "      <td>[[11, 12], [60, 67], [15, 17]]</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>for the annotation process.  Topic models (TMs...</td>\n",
       "      <td>[[43, 46]]</td>\n",
       "      <td>[[29, 41], [8, 9]]</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Evidence for a text?s topic and genre comes, i...</td>\n",
       "      <td>[[183, 186]]</td>\n",
       "      <td>[[13, 14], [116, 146], [151, 181]]</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>The query (Figure 4a) will match adjectives (A...</td>\n",
       "      <td>[[45, 49], [81, 83]]</td>\n",
       "      <td>[[27, 32], [75, 79], [75, 79], [158, 162]]</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>the middle (Baxendale, 1958).  Sentence Positi...</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>By contrast, our approach operates at the l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[52, 78], [52, 64]]</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    2 Related Work The availability of emotion-ric...   \n",
       "1    WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...   \n",
       "2    Baselines are a unigram query likelihood (QL) ...   \n",
       "3    tion. Then, we extract expansion terms from th...   \n",
       "4    person, mood, voice and case, CATiB uses 6 POS...   \n",
       "5    (LDA), Maximum Likelihood Linear Transform (ML...   \n",
       "6    gold label (Section 3). Our algorithm is calle...   \n",
       "7    phrase markers or words. For simplicity of man...   \n",
       "8     When a CFG is associated with probabilistic i...   \n",
       "9    1] proposed a language-neutral framework for r...   \n",
       "10   12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...   \n",
       "11   Model 3 (Figure 3) illustrates how the source ...   \n",
       "12   in the V column indicates that the verb condit...   \n",
       "13   The big blue door.?  In this case, the GrM ask...   \n",
       "14   TI = terse information  INT = interrupted  TRU...   \n",
       "15   classes in both original text and main text co...   \n",
       "16   as in figure 3.  It is parsed as an adverb (AA...   \n",
       "17   can develop after exposure to a terrifying eve...   \n",
       "18   It allows for testing interaction scenarios th...   \n",
       "19   Feature Description lexical the words of the p...   \n",
       "20    The tagging has been done using a GUI-based t...   \n",
       "21   Och and Ney (2003) show that for larger corpor...   \n",
       "22   2 Relation Extraction System In this section, ...   \n",
       "23   target question, or zero if the target questio...   \n",
       "24   discussion in Ritchie(1984).  Functional unifi...   \n",
       "25   Bigram Perplex. ( PP) MDI Missed Samples (MS) ...   \n",
       "26   networks. In Proceedings of the IEEE Conferenc...   \n",
       "27   Therefore, identification methods like Tsuchiy...   \n",
       "28   domain-independent mpirical induction algorith...   \n",
       "29   ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...   \n",
       "..                                                 ...   \n",
       "467  eugene@mathcs.emory.edu Abstract Community que...   \n",
       "468   means a type of source ? newswire (NW), broad...   \n",
       "469  either lexically encoded, or depends on the in...   \n",
       "470  Abstract  This paper attempts to use an off-th...   \n",
       "471  The SemEval?2007 task for extracting frame sem...   \n",
       "472  hauer, haver, haber) and the corpus does not c...   \n",
       "473    The ungrammatical distracter, e.g., are in F...   \n",
       "474  Words/Phrases as Themselves (WD)  Symbols/Nonl...   \n",
       "475  DS = Discharge Summary,  Echo = Echocardiogram...   \n",
       "476  mdiab@ccls.columbia.edu Abstract We analyze ov...   \n",
       "477  (PERS), organization (ORG), geo-political enti...   \n",
       "478  Abstract In this paper, we propose a new synta...   \n",
       "479    The second one is a variant that we named  D...   \n",
       "480  by HG's (HL). In particular, we show that HL's...   \n",
       "481  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482   (Ramshaw and Marcus, 1995) approached chuckin...   \n",
       "483  demonstrate such dependencies.  The Maximum En...   \n",
       "484  dialogues categorized into multiple domains, w...   \n",
       "485  shared task, namely FLORIAN (Florian et al.,  ...   \n",
       "486   1 Introduction Open-domain Question Answering...   \n",
       "487  229  Proceedings of the 2014 Conference on Emp...   \n",
       "488  1 Introduction Large-scale open-domain questio...   \n",
       "489  th fo frture representations abstracts (AbT), ...   \n",
       "490  787 After labeling the reference BINet, we tra...   \n",
       "491  for the annotation process.  Topic models (TMs...   \n",
       "492  Evidence for a text?s topic and genre comes, i...   \n",
       "493     0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co   \n",
       "494  The query (Figure 4a) will match adjectives (A...   \n",
       "495  the middle (Baxendale, 1958).  Sentence Positi...   \n",
       "496     By contrast, our approach operates at the l...   \n",
       "\n",
       "                                              acronyms  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                   [[42, 44], [113, 115], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[30, 35], [43, 46], [53, 56], [130, 134], [15...   \n",
       "5             [[1, 4], [44, 48], [87, 91], [115, 118]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                                   []   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[18, 20], [102, 104], [187, 189]]   \n",
       "11                              [[64, 66], [127, 130]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13                    [[39, 42], [71, 73], [117, 119]]   \n",
       "14              [[0, 2], [24, 27], [43, 47], [61, 66]]   \n",
       "15                                          [[85, 88]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                          [[65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                                          [[63, 65]]   \n",
       "20                                                  []   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25                      [[18, 20], [22, 25], [42, 44]]   \n",
       "26                                [[32, 36], [92, 96]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[35, 37], [56, 58], [85, 87]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                                       [[125, 127]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[86, 89]]   \n",
       "474           [[29, 31], [60, 62], [81, 83], [96, 98]]   \n",
       "475  [[0, 2], [48, 50], [76, 78], [109, 112], [130,...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...   \n",
       "478                                         [[74, 76]]   \n",
       "479                                                 []   \n",
       "480              [[3, 7], [9, 11], [63, 68], [77, 82]]   \n",
       "481                                       [[127, 132]]   \n",
       "482                                         [[86, 89]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                                                 []   \n",
       "485                               [[20, 27], [56, 64]]   \n",
       "486                                         [[49, 50]]   \n",
       "487                                         [[93, 98]]   \n",
       "488                                         [[90, 92]]   \n",
       "489                     [[40, 43], [54, 57], [70, 73]]   \n",
       "490                               [[38, 39], [72, 73]]   \n",
       "491                                         [[43, 46]]   \n",
       "492                                       [[183, 186]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                               [[45, 49], [81, 83]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                                 []   \n",
       "\n",
       "                                            long-forms   ID  \n",
       "0                                 [[5, 6], [144, 160]]    1  \n",
       "1                                           [[39, 61]]    2  \n",
       "2    [[1, 2], [24, 40], [67, 72], [90, 111], [132, ...    3  \n",
       "3                                 [[65, 90], [61, 63]]    4  \n",
       "4         [[58, 68], [88, 93], [136, 148], [122, 127]]    5  \n",
       "5                       [[7, 42], [51, 85], [94, 113]]    6  \n",
       "6                               [[52, 93], [129, 133]]    7  \n",
       "7                               [[15, 17], [128, 147]]    8  \n",
       "8                          [[5, 13], [63, 82], [3, 4]]    9  \n",
       "9                               [[12, 13], [103, 126]]   10  \n",
       "10                    [[3, 16], [85, 100], [169, 185]]   11  \n",
       "11                     [[46, 54], [96, 125], [26, 27]]   12  \n",
       "12                             [[91, 105], [112, 129]]   13  \n",
       "13                                [[53, 69], [64, 66]]   14  \n",
       "14             [[5, 22], [30, 41], [50, 59], [69, 88]]   15  \n",
       "15                        [[8, 10], [60, 83], [8, 10]]   16  \n",
       "16                                [[36, 42], [70, 82]]   17  \n",
       "17   [[1, 2], [71, 100], [102, 106], [92, 100], [1,...   18  \n",
       "18                                          [[68, 98]]   19  \n",
       "19                                                  []   20  \n",
       "20                        [[5, 6], [61, 83], [61, 78]]   21  \n",
       "21                                         [[83, 103]]   22  \n",
       "22                                         [[89, 108]]   23  \n",
       "23                                          [[67, 87]]   24  \n",
       "24                                [[30, 52], [38, 39]]   25  \n",
       "25                                [[26, 40], [26, 40]]   26  \n",
       "26                                          [[37, 90]]   27  \n",
       "27                                [[72, 95], [83, 85]]   28  \n",
       "28                            [[102, 104], [112, 136]]   29  \n",
       "29                                                  []   30  \n",
       "..                                                 ...  ...  \n",
       "467                               [[43, 61], [43, 51]]  468  \n",
       "468             [[2, 3], [25, 33], [40, 54], [61, 83]]  469  \n",
       "469                             [[12, 13], [104, 113]]  470  \n",
       "470                               [[37, 39], [55, 74]]  471  \n",
       "471                                       [[115, 123]]  472  \n",
       "472                                       [[105, 119]]  473  \n",
       "473                                 [[8, 9], [70, 84]]  474  \n",
       "474                               [[65, 79], [86, 94]]  475  \n",
       "475  [[5, 22], [25, 29], [32, 46], [53, 73], [81, 9...  476  \n",
       "476                               [[44, 67], [75, 77]]  477  \n",
       "477  [[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...  478  \n",
       "478               [[5, 6], [53, 72], [70, 72], [5, 6]]  479  \n",
       "479                               [[15, 19], [-1, 28]]  480  \n",
       "480                               [[6, 7], [130, 152]]  481  \n",
       "481                                        [[99, 125]]  482  \n",
       "482                                         [[56, 85]]  483  \n",
       "483                                 [[36, 51], [8, 9]]  484  \n",
       "484          [[2, 3], [76, 89], [97, 100], [109, 126]]  485  \n",
       "485                     [[29, 36], [52, 55], [66, 78]]  486  \n",
       "486                                         [[36, 45]]  487  \n",
       "487                                         [[43, 91]]  488  \n",
       "488                               [[74, 88], [16, 17]]  489  \n",
       "489   [[0, 2], [13, 38], [46, 52], [61, 68], [76, 84]]  490  \n",
       "490                     [[11, 12], [60, 67], [15, 17]]  491  \n",
       "491                                 [[29, 41], [8, 9]]  492  \n",
       "492                 [[13, 14], [116, 146], [151, 181]]  493  \n",
       "493                                         [[25, 38]]  494  \n",
       "494         [[27, 32], [75, 79], [75, 79], [158, 162]]  495  \n",
       "495                                         [[31, 54]]  496  \n",
       "496                               [[52, 78], [52, 64]]  497  \n",
       "\n",
       "[497 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig_json = df_orig_1[cols]\n",
    "df_orig_json.columns = [\"text\",\"acronyms\",\"long-forms\",\"ID\"]\n",
    "df_orig_json[\"ID\"] = df_orig_json[\"ID\"].astype(str)\n",
    "df_orig_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../results/finetuned_models/model_all_xb_v1_512/output\n",
    "df_orig.to_csv(\"../results/finetuned_models/model_all_xb_v1_512/output/\" + op_filename + \".csv\", sep=\"\\t\")\n",
    "df_orig_json.to_json(\"../results/finetuned_models/model_all_xb_v1_512/output/\" + op_filename + \".json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match2(df_orig, df_pred):\n",
    "    acronyms = []\n",
    "    long_forms = []\n",
    "    \n",
    "    acronyms_idxs = []\n",
    "    long_forms_idxs = []\n",
    "    \n",
    "    cnt = df_pred.shape[0]\n",
    "    #print(cnt)\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        acronym = []\n",
    "        long_form = []\n",
    "        \n",
    "        an_idx = []\n",
    "        lf_idx = []\n",
    "        \n",
    "        text_l = df_pred.iloc[i][\"Text\"].strip().split(\" \")\n",
    "        text_orig = df_orig.iloc[i][\"text\"].strip()\n",
    "        #print(text_orig)\n",
    "        y_true = df_pred.iloc[i][\"Y_true\"]\n",
    "        y_true_l = ast.literal_eval(y_true)\n",
    "        \n",
    "        y_pred = df_pred.iloc[i][\"Y_pred\"]\n",
    "        y_pred_l = ast.literal_eval(y_pred)\n",
    "        \n",
    "        an_orig_l = ast.literal_eval(df_orig.iloc[i][\"acronyms-text\"])\n",
    "        an_idx_orig_l = ast.literal_eval(df_orig.iloc[i][\"acronyms\"])\n",
    "        \n",
    "        lf_orig_l = ast.literal_eval(df_orig.iloc[i][\"long-forms-text\"])\n",
    "        lf_idx_orig_l = ast.literal_eval(df_orig.iloc[i][\"long-forms\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(text_l)\n",
    "        #print(y_true_l)\n",
    "        #print(y_pred_l)\n",
    "        AN_start = False\n",
    "        LF_start = False\n",
    "        \n",
    "        #print(len(y_true))\n",
    "        AN = \"\"\n",
    "        LF = \"\"\n",
    "        for j in range(len(y_pred_l)):\n",
    "            \n",
    "            if(y_pred_l[j] == \"B-AN\" and AN_start == False ):\n",
    "                AN = AN + text_l[j]\n",
    "                AN_start = True\n",
    "            elif(y_pred_l[j] == \"I-AN\" and AN_start == True):\n",
    "                AN = AN + text_l[j]\n",
    "            elif(y_pred_l[j]==\"B-AN\" and AN_start == True):\n",
    "                acronym.append(AN)\n",
    "                AN = text_l[j]\n",
    "                #print(AN)\n",
    "            elif(AN_start == True and AN != \"\"):\n",
    "                acronym.append(AN)\n",
    "                AN = \"\"\n",
    "                AN_start = False\n",
    "                \n",
    "             \n",
    "            if(y_pred_l[j] == \"B-LF\" and LF_start == False ):\n",
    "                LF = LF + text_l[j]\n",
    "                LF_start = True\n",
    "            elif(y_pred_l[j] == \"I-LF\" and LF_start == False):\n",
    "                LF = LF + text_l[j]\n",
    "                LF_start = True\n",
    "            elif(y_pred_l[j] == \"I-LF\" and LF_start == True):\n",
    "                LF = LF + \" \" + text_l[j]\n",
    "            elif(y_pred_l[j]==\"B-LF\" and LF_start == True):\n",
    "                long_form.append(LF)\n",
    "                LF = text_l[j]\n",
    "            elif(LF_start == True and LF != \"\"):\n",
    "                long_form.append(LF)\n",
    "                LF = \"\"\n",
    "                LF_start = False\n",
    "        \n",
    "        #print(acronym)\n",
    "        #print(long_form)\n",
    "        acronyms.append(acronym)\n",
    "        long_forms.append(long_form)\n",
    "        \n",
    "        #print(acronym)\n",
    "        for an in acronym:\n",
    "            for an_i in range(len(an_orig_l)):\n",
    "                an_ = an_orig_l[an_i]\n",
    "                if(an == an_):\n",
    "                    an_idx_ = an_idx_orig_l[an_i]\n",
    "                    an_idx.append(an_idx_)  \n",
    "\n",
    "        \n",
    "        for lf in long_form:\n",
    "            for lf_i in range(len(lf_orig_l)):\n",
    "                lf_ = lf_orig_l[lf_i]\n",
    "                if(lf == lf_):\n",
    "                    lf_idx_ = lf_idx_orig_l[lf_i]\n",
    "                    lf_idx.append(lf_idx_) \n",
    "           \n",
    "            \n",
    "        acronyms_idxs.append(an_idx)\n",
    "        long_forms_idxs.append(lf_idx)\n",
    "        \n",
    "    df_orig[\"AN_Pred\"] = acronyms\n",
    "    df_orig[\"LF_Pred\"] = long_forms\n",
    "    df_orig[\"AN_Pred_idxs\"] = acronyms_idxs\n",
    "    df_orig[\"LF_Pred_idxs\"] = long_forms_idxs\n",
    "    return df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "      <th>AN_Pred</th>\n",
       "      <th>LF_Pred</th>\n",
       "      <th>AN_Pred_idxs</th>\n",
       "      <th>LF_Pred_idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[144, 160]]</td>\n",
       "      <td>['TDM']</td>\n",
       "      <td>['Text Data Mining']</td>\n",
       "      <td>[TDM]</td>\n",
       "      <td>[a, Text Data Mining]</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[144, 160]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "      <td>['MAP']</td>\n",
       "      <td>['Mean Average Precision']</td>\n",
       "      <td>[MAP]</td>\n",
       "      <td>[Mean Average Precision]</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Baselines are a unigram query likelihood (QL) ...</td>\n",
       "      <td>[[113, 115], [42, 45], [153, 156]]</td>\n",
       "      <td>[[90, 111], [132, 151]]</td>\n",
       "      <td>['SD', 'QL)', 'MRF']</td>\n",
       "      <td>['sequential dependence', 'Markov random field']</td>\n",
       "      <td>[QL, SD, MRF]</td>\n",
       "      <td>[a, query likelihood, and a, sequential depend...</td>\n",
       "      <td>[[113, 115], [153, 156]]</td>\n",
       "      <td>[[90, 111], [132, 151]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>tion. Then, we extract expansion terms from th...</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90]]</td>\n",
       "      <td>['PRF']</td>\n",
       "      <td>['pseudo relevance feedback']</td>\n",
       "      <td>[PRF]</td>\n",
       "      <td>[pseudo relevance feedback, in]</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>person, mood, voice and case, CATiB uses 6 POS...</td>\n",
       "      <td>[[130, 134], [30, 35], [43, 46], [53, 56], [15...</td>\n",
       "      <td>[[136, 142], [156, 161]]</td>\n",
       "      <td>['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...</td>\n",
       "      <td>['proper', 'verbs']</td>\n",
       "      <td>[CATiB, POS, NOM, PROP, VRB]</td>\n",
       "      <td>[non-proper, nouns, proper nouns, verbs]</td>\n",
       "      <td>[[30, 35], [43, 46], [53, 56], [130, 134], [15...</td>\n",
       "      <td>[[156, 161]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>(LDA), Maximum Likelihood Linear Transform (ML...</td>\n",
       "      <td>[[115, 118], [44, 48], [87, 91]]</td>\n",
       "      <td>[[94, 113], [7, 42], [51, 85]]</td>\n",
       "      <td>['MPE', 'MLLT', 'BMMI']</td>\n",
       "      <td>['Minimum Phone Error', 'Maximum Likelihood Li...</td>\n",
       "      <td>[LDA, MLLT, BMMI, MPE]</td>\n",
       "      <td>[Maximum Likelihood Linear Transform, Boosted ...</td>\n",
       "      <td>[[44, 48], [87, 91], [115, 118]]</td>\n",
       "      <td>[[7, 42], [51, 85], [94, 113]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>gold label (Section 3). Our algorithm is calle...</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93]]</td>\n",
       "      <td>['SWVP']</td>\n",
       "      <td>['Structured Weighted Violations Perceptron']</td>\n",
       "      <td>[SWVP]</td>\n",
       "      <td>[Structured Weighted Violations Perceptron, on a]</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>phrase markers or words. For simplicity of man...</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[128, 147]]</td>\n",
       "      <td>['CNF']</td>\n",
       "      <td>['Chomsky Normal Form']</td>\n",
       "      <td>[CNF]</td>\n",
       "      <td>[or, Chomsky Normal Form]</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[128, 147]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>When a CFG is associated with probabilistic i...</td>\n",
       "      <td>[[85, 89], [8, 11]]</td>\n",
       "      <td>[[66, 83]]</td>\n",
       "      <td>['PCFG', 'CFG']</td>\n",
       "      <td>['Probabilistic CFG']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a CFG is, a Probabilistic CFG, n]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1] proposed a language-neutral framework for r...</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[103, 126]]</td>\n",
       "      <td>['LNS']</td>\n",
       "      <td>['Language Neutral Syntax']</td>\n",
       "      <td>[LNS]</td>\n",
       "      <td>[a, Language Neutral Syntax]</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[103, 126]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...</td>\n",
       "      <td>[[187, 189], [18, 20], [102, 104]]</td>\n",
       "      <td>[[169, 185], [8, 16], [90, 99]]</td>\n",
       "      <td>['AP', 'FN', 'VP']</td>\n",
       "      <td>['Base+Appositives', 'FrameNet', 'Verb Pair']</td>\n",
       "      <td>[FN, VP, AP]</td>\n",
       "      <td>[Base+FrameNet, Base+Verb Pairs, Base+Appositi...</td>\n",
       "      <td>[[18, 20], [102, 104], [187, 189]]</td>\n",
       "      <td>[[169, 185]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Model 3 (Figure 3) illustrates how the source ...</td>\n",
       "      <td>[[127, 130], [64, 66]]</td>\n",
       "      <td>[[98, 125]]</td>\n",
       "      <td>['NLG', 'MT']</td>\n",
       "      <td>['natural language generation']</td>\n",
       "      <td>[MT, NLG]</td>\n",
       "      <td>[language, a natural language generation, a]</td>\n",
       "      <td>[[64, 66], [127, 130]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "      <td>['LR', 'LP']</td>\n",
       "      <td>['labeled recall', 'labeled precision']</td>\n",
       "      <td>[LR, LP]</td>\n",
       "      <td>[labeled recall, labeled precision]</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>The big blue door.?  In this case, the GrM ask...</td>\n",
       "      <td>[[71, 73], [39, 42], [117, 119], [125, 127]]</td>\n",
       "      <td>[[53, 69]]</td>\n",
       "      <td>['RP', 'GrM', 'UU', 'RP']</td>\n",
       "      <td>['Response Planner']</td>\n",
       "      <td>[GrM, RP, UU]</td>\n",
       "      <td>[Response Planner, an]</td>\n",
       "      <td>[[39, 42], [71, 73], [125, 127], [117, 119]]</td>\n",
       "      <td>[[53, 69]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>TI = terse information  INT = interrupted  TRU...</td>\n",
       "      <td>[[43, 47], [0, 2], [24, 27], [61, 66]]</td>\n",
       "      <td>[[50, 59], [5, 22], [30, 41], [69, 79]]</td>\n",
       "      <td>['TRUN', 'TI', 'INT', 'TRANS']</td>\n",
       "      <td>['truncated', 'terse information', 'interrupte...</td>\n",
       "      <td>[TI, INT, TRUN, TRANS]</td>\n",
       "      <td>[terse information, interrupted, truncated, tr...</td>\n",
       "      <td>[[0, 2], [24, 27], [43, 47], [61, 66]]</td>\n",
       "      <td>[[5, 22], [30, 41], [50, 59]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>[[85, 88], [140, 143]]</td>\n",
       "      <td>[[60, 83]]</td>\n",
       "      <td>['SVM', 'AGI']</td>\n",
       "      <td>['Support Vector Machines']</td>\n",
       "      <td>[SVM]</td>\n",
       "      <td>[in, Support Vector Machines, in]</td>\n",
       "      <td>[[85, 88]]</td>\n",
       "      <td>[[60, 83]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>as in figure 3.  It is parsed as an adverb (AA...</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[[33, 42], [72, 82]]</td>\n",
       "      <td>['AA', 'VG']</td>\n",
       "      <td>['an adverb', 'verb group']</td>\n",
       "      <td>[AA, VG]</td>\n",
       "      <td>[adverb, a verb group]</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>[[65, 69]]</td>\n",
       "      <td>[[71, 100]]</td>\n",
       "      <td>['PTSD']</td>\n",
       "      <td>['posttraumatic stress disorder']</td>\n",
       "      <td>[PTSD]</td>\n",
       "      <td>[a, posttraumatic stress disorder, is a, disor...</td>\n",
       "      <td>[[65, 69]]</td>\n",
       "      <td>[[71, 100]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "      <td>['LTC']</td>\n",
       "      <td>['Language Technology Components']</td>\n",
       "      <td>[LTC]</td>\n",
       "      <td>[Language Technology Components]</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>[[63, 65], [71, 74], [96, 98]]</td>\n",
       "      <td>[[45, 61]]</td>\n",
       "      <td>['PA', 'POS', 'PA']</td>\n",
       "      <td>['product attribut']</td>\n",
       "      <td>[PA]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[63, 65], [96, 98]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>The tagging has been done using a GUI-based t...</td>\n",
       "      <td>[[86, 92], [35, 38]]</td>\n",
       "      <td>[[62, 84]]</td>\n",
       "      <td>['DTTool', 'GUI']</td>\n",
       "      <td>['Discourse Tagging Tool']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, Discourse Tagging Tool, Discourse Tagging]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[62, 84]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Och and Ney (2003) show that for larger corpor...</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "      <td>['AER']</td>\n",
       "      <td>['Alignment Error Rate']</td>\n",
       "      <td>[AER]</td>\n",
       "      <td>[Alignment Error Rate]</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2 Relation Extraction System In this section, ...</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "      <td>['RE']</td>\n",
       "      <td>['relation extraction']</td>\n",
       "      <td>[RE]</td>\n",
       "      <td>[relation extraction]</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>target question, or zero if the target questio...</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "      <td>['MRR']</td>\n",
       "      <td>['Mean Reciprocal Rank']</td>\n",
       "      <td>[MRR]</td>\n",
       "      <td>[Mean Reciprocal Rank]</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>discussion in Ritchie(1984).  Functional unifi...</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52]]</td>\n",
       "      <td>['FU']</td>\n",
       "      <td>['Functional unification']</td>\n",
       "      <td>[FU]</td>\n",
       "      <td>[Functional unification, a]</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Bigram Perplex. ( PP) MDI Missed Samples (MS) ...</td>\n",
       "      <td>[[42, 44], [18, 20], [22, 25], [69, 71]]</td>\n",
       "      <td>[[26, 40], [7, 14], [53, 67]]</td>\n",
       "      <td>['MS', 'PP', 'MDI', 'MS']</td>\n",
       "      <td>['Missed Samples', 'Perplex', 'Missed Samples']</td>\n",
       "      <td>[PP, MDI, MS]</td>\n",
       "      <td>[Missed Samples, Missed Samples]</td>\n",
       "      <td>[[18, 20], [22, 25], [42, 44], [69, 71]]</td>\n",
       "      <td>[[26, 40], [53, 67], [26, 40], [53, 67]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>networks. In Proceedings of the IEEE Conferenc...</td>\n",
       "      <td>[[92, 96], [32, 36]]</td>\n",
       "      <td>[[51, 90]]</td>\n",
       "      <td>['CVPR', 'IEEE']</td>\n",
       "      <td>['Computer Vision and Pattern Recognition']</td>\n",
       "      <td>[IEEE, CVPR]</td>\n",
       "      <td>[Conference on Computer Vision and Pattern Rec...</td>\n",
       "      <td>[[32, 36], [92, 96]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Therefore, identification methods like Tsuchiy...</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[[72, 94]]</td>\n",
       "      <td>['SVM']</td>\n",
       "      <td>['Support Vector Machine']</td>\n",
       "      <td>[SVM]</td>\n",
       "      <td>[Support Vector Machines, to]</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[112, 136]]</td>\n",
       "      <td>['SPE']</td>\n",
       "      <td>['Sound Pattern of English']</td>\n",
       "      <td>[SPE]</td>\n",
       "      <td>[of, Sound Pattern of English]</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[112, 136]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...</td>\n",
       "      <td>[[35, 38]]</td>\n",
       "      <td>[[26, 31]]</td>\n",
       "      <td>['P+R']</td>\n",
       "      <td>['2*P*R']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>eugene@mathcs.emory.edu Abstract Community que...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[33, 61]]</td>\n",
       "      <td>['CQA']</td>\n",
       "      <td>['Community question answering']</td>\n",
       "      <td>[CQA]</td>\n",
       "      <td>[question answering, question]</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>469</td>\n",
       "      <td>means a type of source ? newswire (NW), broad...</td>\n",
       "      <td>[[36, 38], [57, 59], [86, 88]]</td>\n",
       "      <td>[[26, 34], [41, 55], [62, 84]]</td>\n",
       "      <td>['NW', 'BN', 'BC']</td>\n",
       "      <td>['newswire', 'broadcast news', 'broadcast conv...</td>\n",
       "      <td>[NW, BN, BC]</td>\n",
       "      <td>[a, newswire, broadcast news, broadcast conver...</td>\n",
       "      <td>[[36, 38], [57, 59], [86, 88]]</td>\n",
       "      <td>[[26, 34], [41, 55], [62, 84]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>470</td>\n",
       "      <td>either lexically encoded, or depends on the in...</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[104, 113]]</td>\n",
       "      <td>['VPT']</td>\n",
       "      <td>['viewpoint']</td>\n",
       "      <td>[VPT]</td>\n",
       "      <td>[a, viewpoint]</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[104, 113]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>471</td>\n",
       "      <td>Abstract  This paper attempts to use an off-th...</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[55, 74]]</td>\n",
       "      <td>['AR']</td>\n",
       "      <td>['anaphora resolution']</td>\n",
       "      <td>[AR]</td>\n",
       "      <td>[an, anaphora resolution]</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[55, 74]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>472</td>\n",
       "      <td>The SemEval?2007 task for extracting frame sem...</td>\n",
       "      <td>[[125, 127], [4, 11]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "      <td>['FN', 'SemEval']</td>\n",
       "      <td>['FrameNet']</td>\n",
       "      <td>[FN]</td>\n",
       "      <td>[FrameNet]</td>\n",
       "      <td>[[125, 127]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>473</td>\n",
       "      <td>hauer, haver, haber) and the corpus does not c...</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "      <td>['PoS']</td>\n",
       "      <td>['part of speech']</td>\n",
       "      <td>[PoS]</td>\n",
       "      <td>[part of speech]</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>474</td>\n",
       "      <td>The ungrammatical distracter, e.g., are in F...</td>\n",
       "      <td>[[88, 91]]</td>\n",
       "      <td>[[72, 86]]</td>\n",
       "      <td>['POS']</td>\n",
       "      <td>['part of speech']</td>\n",
       "      <td>[POS]</td>\n",
       "      <td>[a, part of speech]</td>\n",
       "      <td>[[88, 91]]</td>\n",
       "      <td>[[72, 86]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>475</td>\n",
       "      <td>Words/Phrases as Themselves (WD)  Symbols/Nonl...</td>\n",
       "      <td>[[81, 83], [29, 31], [60, 62], [96, 98]]</td>\n",
       "      <td>[[65, 73], [34, 41], [86, 94]]</td>\n",
       "      <td>['PH', 'WD', 'SY', 'SP']</td>\n",
       "      <td>['Phonetic', 'Symbols', 'Spelling']</td>\n",
       "      <td>[WD, SY, PH, SP]</td>\n",
       "      <td>[Phonetic/Sound, Spelling]</td>\n",
       "      <td>[[29, 31], [60, 62], [81, 83], [96, 98]]</td>\n",
       "      <td>[[86, 94]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>476</td>\n",
       "      <td>DS = Discharge Summary,  Echo = Echocardiogram...</td>\n",
       "      <td>[[109, 112], [0, 2], [25, 29], [48, 50], [76, ...</td>\n",
       "      <td>[[115, 128], [5, 22], [32, 46], [53, 73], [91,...</td>\n",
       "      <td>['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']</td>\n",
       "      <td>['Radiology and', 'Discharge Summary', 'Echoca...</td>\n",
       "      <td>[DS, ED, GI, RAD, SP]</td>\n",
       "      <td>[Discharge Summary, Echo, Echocardiogram, Emer...</td>\n",
       "      <td>[[0, 2], [48, 50], [76, 78], [109, 112], [130,...</td>\n",
       "      <td>[[5, 22], [32, 46], [53, 73], [91, 107], [135,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>477</td>\n",
       "      <td>mdiab@ccls.columbia.edu Abstract We analyze ov...</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[[44, 58]]</td>\n",
       "      <td>['ODPs']</td>\n",
       "      <td>['overt displays']</td>\n",
       "      <td>[ODPs]</td>\n",
       "      <td>[overt displays of power, in]</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>478</td>\n",
       "      <td>(PERS), organization (ORG), geo-political enti...</td>\n",
       "      <td>[[115, 118], [1, 5], [22, 25], [50, 53], [64, ...</td>\n",
       "      <td>[[105, 113], [8, 20], [28, 48], [56, 62], [70,...</td>\n",
       "      <td>['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'L...</td>\n",
       "      <td>['facility', 'organization', 'geo-political en...</td>\n",
       "      <td>[PERS, ORG, GPE, WEA, VEH, LOC, FAC]</td>\n",
       "      <td>[organization, geo-political entity, weapon, v...</td>\n",
       "      <td>[[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...</td>\n",
       "      <td>[[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>479</td>\n",
       "      <td>Abstract In this paper, we propose a new synta...</td>\n",
       "      <td>[[74, 76], [109, 111]]</td>\n",
       "      <td>[[53, 72]]</td>\n",
       "      <td>['MT', 'MT']</td>\n",
       "      <td>['machine translation']</td>\n",
       "      <td>[MT]</td>\n",
       "      <td>[a, machine translation, on, a]</td>\n",
       "      <td>[[74, 76], [109, 111]]</td>\n",
       "      <td>[[53, 72]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>480</td>\n",
       "      <td>The second one is a variant that we named  D...</td>\n",
       "      <td>[[81, 85]]</td>\n",
       "      <td>[[45, 79]]</td>\n",
       "      <td>['DLED']</td>\n",
       "      <td>['Double Levenshtein?s Edit Distance']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[is a, Levenshtein ? s Edit Distance]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>481</td>\n",
       "      <td>by HG's (HL). In particular, we show that HL's...</td>\n",
       "      <td>[[154, 159], [3, 5], [9, 11], [42, 44], [63, 6...</td>\n",
       "      <td>[[130, 152]]</td>\n",
       "      <td>[\"MHG's\", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL...</td>\n",
       "      <td>['Modified Head Grammars']</td>\n",
       "      <td>[HG's, HL, TAL's, TAG's]</td>\n",
       "      <td>[s, Modified Head Grammars]</td>\n",
       "      <td>[[9, 11], [42, 44]]</td>\n",
       "      <td>[[130, 152]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>482</td>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>[[127, 132]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "      <td>[\"CSI's\"]</td>\n",
       "      <td>['concept sequence instances']</td>\n",
       "      <td>[CSI's]</td>\n",
       "      <td>[concept sequence instances]</td>\n",
       "      <td>[[127, 132]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>483</td>\n",
       "      <td>(Ramshaw and Marcus, 1995) approached chuckin...</td>\n",
       "      <td>[[87, 90]]</td>\n",
       "      <td>[[57, 86]]</td>\n",
       "      <td>['TBL']</td>\n",
       "      <td>['Transformation Based Learning']</td>\n",
       "      <td>[TBL]</td>\n",
       "      <td>[Transformation Based Learning]</td>\n",
       "      <td>[[87, 90]]</td>\n",
       "      <td>[[57, 86]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>484</td>\n",
       "      <td>demonstrate such dependencies.  The Maximum En...</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51]]</td>\n",
       "      <td>['MaxEnt']</td>\n",
       "      <td>['Maximum Entropy']</td>\n",
       "      <td>[MaxEnt]</td>\n",
       "      <td>[Maximum Entropy, a]</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>485</td>\n",
       "      <td>dialogues categorized into multiple domains, w...</td>\n",
       "      <td>[[128, 133]]</td>\n",
       "      <td>[[109, 126]]</td>\n",
       "      <td>['CSHMM']</td>\n",
       "      <td>['Class Speaker HMM']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a, hidden Markov, HMM, Class Speaker HMM]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[109, 126]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>486</td>\n",
       "      <td>shared task, namely FLORIAN (Florian et al.,  ...</td>\n",
       "      <td>[[56, 64], [20, 27]]</td>\n",
       "      <td>[[66, 78], [29, 36]]</td>\n",
       "      <td>['CHIEU-NG', 'FLORIAN']</td>\n",
       "      <td>['Chieu and Ng', 'Florian']</td>\n",
       "      <td>[FLORIAN, CHIEU-NG]</td>\n",
       "      <td>[Florian, and, Chieu and Ng]</td>\n",
       "      <td>[[20, 27], [56, 64]]</td>\n",
       "      <td>[[29, 36], [66, 78]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>487</td>\n",
       "      <td>1 Introduction Open-domain Question Answering...</td>\n",
       "      <td>[[48, 50]]</td>\n",
       "      <td>[[28, 46]]</td>\n",
       "      <td>['QA']</td>\n",
       "      <td>['Question Answering']</td>\n",
       "      <td>[)]</td>\n",
       "      <td>[Answering]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>488</td>\n",
       "      <td>229  Proceedings of the 2014 Conference on Emp...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "      <td>['EMNLP']</td>\n",
       "      <td>['Empirical Methods in Natural Language Proces...</td>\n",
       "      <td>[EMNLP]</td>\n",
       "      <td>[Empirical Methods in Natural Language Process...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>489</td>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>[[90, 92], [154, 156]]</td>\n",
       "      <td>[[74, 88]]</td>\n",
       "      <td>['KB', 'QA']</td>\n",
       "      <td>['Knowledge Base']</td>\n",
       "      <td>[KB]</td>\n",
       "      <td>[Knowledge Base, a]</td>\n",
       "      <td>[[90, 92]]</td>\n",
       "      <td>[[74, 88]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>490</td>\n",
       "      <td>th fo frture representations abstracts (AbT), ...</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73], [86, 89]]</td>\n",
       "      <td>[[29, 38], [46, 52], [61, 68], [76, 84]]</td>\n",
       "      <td>['AbT', 'ArT', 'Aut', 'Jou']</td>\n",
       "      <td>['abstracts', 'titles', 'authors', 'Journals']</td>\n",
       "      <td>[AbT, ArT, Aut]</td>\n",
       "      <td>[th, representations abstracts, titles, author...</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73]]</td>\n",
       "      <td>[[46, 52], [61, 68], [76, 84]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>491</td>\n",
       "      <td>787 After labeling the reference BINet, we tra...</td>\n",
       "      <td>[[69, 72]]</td>\n",
       "      <td>[[51, 67]]</td>\n",
       "      <td>['L2R']</td>\n",
       "      <td>['learning to rank']</td>\n",
       "      <td>[,, )]</td>\n",
       "      <td>[a, to rank, in]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>492</td>\n",
       "      <td>for the annotation process.  Topic models (TMs...</td>\n",
       "      <td>[[43, 46]]</td>\n",
       "      <td>[[29, 41]]</td>\n",
       "      <td>['TMs']</td>\n",
       "      <td>['Topic models']</td>\n",
       "      <td>[TMs]</td>\n",
       "      <td>[Topic models, a]</td>\n",
       "      <td>[[43, 46]]</td>\n",
       "      <td>[[29, 41]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>493</td>\n",
       "      <td>Evidence for a text?s topic and genre comes, i...</td>\n",
       "      <td>[[183, 186], [206, 209]]</td>\n",
       "      <td>[[151, 181]]</td>\n",
       "      <td>['AGC', 'AGC']</td>\n",
       "      <td>['Automatic Genre Classification']</td>\n",
       "      <td>[AGC]</td>\n",
       "      <td>[a, Automatic Topic Classification, Automatic ...</td>\n",
       "      <td>[[183, 186], [206, 209]]</td>\n",
       "      <td>[[151, 181]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>494</td>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "      <td>['OR']</td>\n",
       "      <td>['Omission Rate']</td>\n",
       "      <td>[OR]</td>\n",
       "      <td>[Omission Rate]</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>495</td>\n",
       "      <td>The query (Figure 4a) will match adjectives (A...</td>\n",
       "      <td>[[81, 83], [45, 49], [164, 166]]</td>\n",
       "      <td>[[75, 79], [33, 43], [158, 162]]</td>\n",
       "      <td>['NN', 'ADJA', 'NE']</td>\n",
       "      <td>['noun', 'adjectives', 'name']</td>\n",
       "      <td>[ADJA, NN]</td>\n",
       "      <td>[match, noun, noun, name]</td>\n",
       "      <td>[[45, 49], [81, 83]]</td>\n",
       "      <td>[[75, 79], [75, 79], [158, 162]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>the middle (Baxendale, 1958).  Sentence Positi...</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "      <td>['SPY']</td>\n",
       "      <td>['Sentence Position Yield']</td>\n",
       "      <td>[SPY]</td>\n",
       "      <td>[Sentence Position Yield]</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>By contrast, our approach operates at the l...</td>\n",
       "      <td>[[83, 86]]</td>\n",
       "      <td>[[55, 81]]</td>\n",
       "      <td>['IPS']</td>\n",
       "      <td>['inflectional property sets']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[inflectional property sets, inflectional]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[55, 81]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               text  \\\n",
       "0      1  2 Related Work The availability of emotion-ric...   \n",
       "1      2  WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...   \n",
       "2      3  Baselines are a unigram query likelihood (QL) ...   \n",
       "3      4  tion. Then, we extract expansion terms from th...   \n",
       "4      5  person, mood, voice and case, CATiB uses 6 POS...   \n",
       "5      6  (LDA), Maximum Likelihood Linear Transform (ML...   \n",
       "6      7  gold label (Section 3). Our algorithm is calle...   \n",
       "7      8  phrase markers or words. For simplicity of man...   \n",
       "8      9   When a CFG is associated with probabilistic i...   \n",
       "9     10  1] proposed a language-neutral framework for r...   \n",
       "10    11  12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...   \n",
       "11    12  Model 3 (Figure 3) illustrates how the source ...   \n",
       "12    13  in the V column indicates that the verb condit...   \n",
       "13    14  The big blue door.?  In this case, the GrM ask...   \n",
       "14    15  TI = terse information  INT = interrupted  TRU...   \n",
       "15    16  classes in both original text and main text co...   \n",
       "16    17  as in figure 3.  It is parsed as an adverb (AA...   \n",
       "17    18  can develop after exposure to a terrifying eve...   \n",
       "18    19  It allows for testing interaction scenarios th...   \n",
       "19    20  Feature Description lexical the words of the p...   \n",
       "20    21   The tagging has been done using a GUI-based t...   \n",
       "21    22  Och and Ney (2003) show that for larger corpor...   \n",
       "22    23  2 Relation Extraction System In this section, ...   \n",
       "23    24  target question, or zero if the target questio...   \n",
       "24    25  discussion in Ritchie(1984).  Functional unifi...   \n",
       "25    26  Bigram Perplex. ( PP) MDI Missed Samples (MS) ...   \n",
       "26    27  networks. In Proceedings of the IEEE Conferenc...   \n",
       "27    28  Therefore, identification methods like Tsuchiy...   \n",
       "28    29  domain-independent mpirical induction algorith...   \n",
       "29    30  ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...   \n",
       "..   ...                                                ...   \n",
       "467  468  eugene@mathcs.emory.edu Abstract Community que...   \n",
       "468  469   means a type of source ? newswire (NW), broad...   \n",
       "469  470  either lexically encoded, or depends on the in...   \n",
       "470  471  Abstract  This paper attempts to use an off-th...   \n",
       "471  472  The SemEval?2007 task for extracting frame sem...   \n",
       "472  473  hauer, haver, haber) and the corpus does not c...   \n",
       "473  474    The ungrammatical distracter, e.g., are in F...   \n",
       "474  475  Words/Phrases as Themselves (WD)  Symbols/Nonl...   \n",
       "475  476  DS = Discharge Summary,  Echo = Echocardiogram...   \n",
       "476  477  mdiab@ccls.columbia.edu Abstract We analyze ov...   \n",
       "477  478  (PERS), organization (ORG), geo-political enti...   \n",
       "478  479  Abstract In this paper, we propose a new synta...   \n",
       "479  480    The second one is a variant that we named  D...   \n",
       "480  481  by HG's (HL). In particular, we show that HL's...   \n",
       "481  482  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482  483   (Ramshaw and Marcus, 1995) approached chuckin...   \n",
       "483  484  demonstrate such dependencies.  The Maximum En...   \n",
       "484  485  dialogues categorized into multiple domains, w...   \n",
       "485  486  shared task, namely FLORIAN (Florian et al.,  ...   \n",
       "486  487   1 Introduction Open-domain Question Answering...   \n",
       "487  488  229  Proceedings of the 2014 Conference on Emp...   \n",
       "488  489  1 Introduction Large-scale open-domain questio...   \n",
       "489  490  th fo frture representations abstracts (AbT), ...   \n",
       "490  491  787 After labeling the reference BINet, we tra...   \n",
       "491  492  for the annotation process.  Topic models (TMs...   \n",
       "492  493  Evidence for a text?s topic and genre comes, i...   \n",
       "493  494     0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co   \n",
       "494  495  The query (Figure 4a) will match adjectives (A...   \n",
       "495  496  the middle (Baxendale, 1958).  Sentence Positi...   \n",
       "496  497     By contrast, our approach operates at the l...   \n",
       "\n",
       "                                              acronyms  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                   [[113, 115], [42, 45], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[130, 134], [30, 35], [43, 46], [53, 56], [15...   \n",
       "5                     [[115, 118], [44, 48], [87, 91]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                  [[85, 89], [8, 11]]   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[187, 189], [18, 20], [102, 104]]   \n",
       "11                              [[127, 130], [64, 66]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13        [[71, 73], [39, 42], [117, 119], [125, 127]]   \n",
       "14              [[43, 47], [0, 2], [24, 27], [61, 66]]   \n",
       "15                              [[85, 88], [140, 143]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                          [[65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                      [[63, 65], [71, 74], [96, 98]]   \n",
       "20                                [[86, 92], [35, 38]]   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25            [[42, 44], [18, 20], [22, 25], [69, 71]]   \n",
       "26                                [[92, 96], [32, 36]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                          [[35, 38]]   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[36, 38], [57, 59], [86, 88]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                              [[125, 127], [4, 11]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[88, 91]]   \n",
       "474           [[81, 83], [29, 31], [60, 62], [96, 98]]   \n",
       "475  [[109, 112], [0, 2], [25, 29], [48, 50], [76, ...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[115, 118], [1, 5], [22, 25], [50, 53], [64, ...   \n",
       "478                             [[74, 76], [109, 111]]   \n",
       "479                                         [[81, 85]]   \n",
       "480  [[154, 159], [3, 5], [9, 11], [42, 44], [63, 6...   \n",
       "481                                       [[127, 132]]   \n",
       "482                                         [[87, 90]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                                       [[128, 133]]   \n",
       "485                               [[56, 64], [20, 27]]   \n",
       "486                                         [[48, 50]]   \n",
       "487                                         [[93, 98]]   \n",
       "488                             [[90, 92], [154, 156]]   \n",
       "489           [[40, 43], [54, 57], [70, 73], [86, 89]]   \n",
       "490                                         [[69, 72]]   \n",
       "491                                         [[43, 46]]   \n",
       "492                           [[183, 186], [206, 209]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                   [[81, 83], [45, 49], [164, 166]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                         [[83, 86]]   \n",
       "\n",
       "                                            long-forms  \\\n",
       "0                                         [[144, 160]]   \n",
       "1                                           [[39, 61]]   \n",
       "2                              [[90, 111], [132, 151]]   \n",
       "3                                           [[65, 90]]   \n",
       "4                             [[136, 142], [156, 161]]   \n",
       "5                       [[94, 113], [7, 42], [51, 85]]   \n",
       "6                                           [[52, 93]]   \n",
       "7                                         [[128, 147]]   \n",
       "8                                           [[66, 83]]   \n",
       "9                                         [[103, 126]]   \n",
       "10                     [[169, 185], [8, 16], [90, 99]]   \n",
       "11                                         [[98, 125]]   \n",
       "12                             [[91, 105], [112, 129]]   \n",
       "13                                          [[53, 69]]   \n",
       "14             [[50, 59], [5, 22], [30, 41], [69, 79]]   \n",
       "15                                          [[60, 83]]   \n",
       "16                                [[33, 42], [72, 82]]   \n",
       "17                                         [[71, 100]]   \n",
       "18                                          [[68, 98]]   \n",
       "19                                          [[45, 61]]   \n",
       "20                                          [[62, 84]]   \n",
       "21                                         [[83, 103]]   \n",
       "22                                         [[89, 108]]   \n",
       "23                                          [[67, 87]]   \n",
       "24                                          [[30, 52]]   \n",
       "25                       [[26, 40], [7, 14], [53, 67]]   \n",
       "26                                          [[51, 90]]   \n",
       "27                                          [[72, 94]]   \n",
       "28                                        [[112, 136]]   \n",
       "29                                          [[26, 31]]   \n",
       "..                                                 ...   \n",
       "467                                         [[33, 61]]   \n",
       "468                     [[26, 34], [41, 55], [62, 84]]   \n",
       "469                                       [[104, 113]]   \n",
       "470                                         [[55, 74]]   \n",
       "471                                       [[115, 123]]   \n",
       "472                                       [[105, 119]]   \n",
       "473                                         [[72, 86]]   \n",
       "474                     [[65, 73], [34, 41], [86, 94]]   \n",
       "475  [[115, 128], [5, 22], [32, 46], [53, 73], [91,...   \n",
       "476                                         [[44, 58]]   \n",
       "477  [[105, 113], [8, 20], [28, 48], [56, 62], [70,...   \n",
       "478                                         [[53, 72]]   \n",
       "479                                         [[45, 79]]   \n",
       "480                                       [[130, 152]]   \n",
       "481                                        [[99, 125]]   \n",
       "482                                         [[57, 86]]   \n",
       "483                                         [[36, 51]]   \n",
       "484                                       [[109, 126]]   \n",
       "485                               [[66, 78], [29, 36]]   \n",
       "486                                         [[28, 46]]   \n",
       "487                                         [[43, 91]]   \n",
       "488                                         [[74, 88]]   \n",
       "489           [[29, 38], [46, 52], [61, 68], [76, 84]]   \n",
       "490                                         [[51, 67]]   \n",
       "491                                         [[29, 41]]   \n",
       "492                                       [[151, 181]]   \n",
       "493                                         [[25, 38]]   \n",
       "494                   [[75, 79], [33, 43], [158, 162]]   \n",
       "495                                         [[31, 54]]   \n",
       "496                                         [[55, 81]]   \n",
       "\n",
       "                                         acronyms-text  \\\n",
       "0                                              ['TDM']   \n",
       "1                                              ['MAP']   \n",
       "2                                 ['SD', 'QL)', 'MRF']   \n",
       "3                                              ['PRF']   \n",
       "4    ['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...   \n",
       "5                              ['MPE', 'MLLT', 'BMMI']   \n",
       "6                                             ['SWVP']   \n",
       "7                                              ['CNF']   \n",
       "8                                      ['PCFG', 'CFG']   \n",
       "9                                              ['LNS']   \n",
       "10                                  ['AP', 'FN', 'VP']   \n",
       "11                                       ['NLG', 'MT']   \n",
       "12                                        ['LR', 'LP']   \n",
       "13                           ['RP', 'GrM', 'UU', 'RP']   \n",
       "14                      ['TRUN', 'TI', 'INT', 'TRANS']   \n",
       "15                                      ['SVM', 'AGI']   \n",
       "16                                        ['AA', 'VG']   \n",
       "17                                            ['PTSD']   \n",
       "18                                             ['LTC']   \n",
       "19                                 ['PA', 'POS', 'PA']   \n",
       "20                                   ['DTTool', 'GUI']   \n",
       "21                                             ['AER']   \n",
       "22                                              ['RE']   \n",
       "23                                             ['MRR']   \n",
       "24                                              ['FU']   \n",
       "25                           ['MS', 'PP', 'MDI', 'MS']   \n",
       "26                                    ['CVPR', 'IEEE']   \n",
       "27                                             ['SVM']   \n",
       "28                                             ['SPE']   \n",
       "29                                             ['P+R']   \n",
       "..                                                 ...   \n",
       "467                                            ['CQA']   \n",
       "468                                 ['NW', 'BN', 'BC']   \n",
       "469                                            ['VPT']   \n",
       "470                                             ['AR']   \n",
       "471                                  ['FN', 'SemEval']   \n",
       "472                                            ['PoS']   \n",
       "473                                            ['POS']   \n",
       "474                           ['PH', 'WD', 'SY', 'SP']   \n",
       "475            ['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']   \n",
       "476                                           ['ODPs']   \n",
       "477  ['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'L...   \n",
       "478                                       ['MT', 'MT']   \n",
       "479                                           ['DLED']   \n",
       "480  [\"MHG's\", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL...   \n",
       "481                                          [\"CSI's\"]   \n",
       "482                                            ['TBL']   \n",
       "483                                         ['MaxEnt']   \n",
       "484                                          ['CSHMM']   \n",
       "485                            ['CHIEU-NG', 'FLORIAN']   \n",
       "486                                             ['QA']   \n",
       "487                                          ['EMNLP']   \n",
       "488                                       ['KB', 'QA']   \n",
       "489                       ['AbT', 'ArT', 'Aut', 'Jou']   \n",
       "490                                            ['L2R']   \n",
       "491                                            ['TMs']   \n",
       "492                                     ['AGC', 'AGC']   \n",
       "493                                             ['OR']   \n",
       "494                               ['NN', 'ADJA', 'NE']   \n",
       "495                                            ['SPY']   \n",
       "496                                            ['IPS']   \n",
       "\n",
       "                                       long-forms-text  \\\n",
       "0                                 ['Text Data Mining']   \n",
       "1                           ['Mean Average Precision']   \n",
       "2     ['sequential dependence', 'Markov random field']   \n",
       "3                        ['pseudo relevance feedback']   \n",
       "4                                  ['proper', 'verbs']   \n",
       "5    ['Minimum Phone Error', 'Maximum Likelihood Li...   \n",
       "6        ['Structured Weighted Violations Perceptron']   \n",
       "7                              ['Chomsky Normal Form']   \n",
       "8                                ['Probabilistic CFG']   \n",
       "9                          ['Language Neutral Syntax']   \n",
       "10       ['Base+Appositives', 'FrameNet', 'Verb Pair']   \n",
       "11                     ['natural language generation']   \n",
       "12             ['labeled recall', 'labeled precision']   \n",
       "13                                ['Response Planner']   \n",
       "14   ['truncated', 'terse information', 'interrupte...   \n",
       "15                         ['Support Vector Machines']   \n",
       "16                         ['an adverb', 'verb group']   \n",
       "17                   ['posttraumatic stress disorder']   \n",
       "18                  ['Language Technology Components']   \n",
       "19                                ['product attribut']   \n",
       "20                          ['Discourse Tagging Tool']   \n",
       "21                            ['Alignment Error Rate']   \n",
       "22                             ['relation extraction']   \n",
       "23                            ['Mean Reciprocal Rank']   \n",
       "24                          ['Functional unification']   \n",
       "25     ['Missed Samples', 'Perplex', 'Missed Samples']   \n",
       "26         ['Computer Vision and Pattern Recognition']   \n",
       "27                          ['Support Vector Machine']   \n",
       "28                        ['Sound Pattern of English']   \n",
       "29                                           ['2*P*R']   \n",
       "..                                                 ...   \n",
       "467                   ['Community question answering']   \n",
       "468  ['newswire', 'broadcast news', 'broadcast conv...   \n",
       "469                                      ['viewpoint']   \n",
       "470                            ['anaphora resolution']   \n",
       "471                                       ['FrameNet']   \n",
       "472                                 ['part of speech']   \n",
       "473                                 ['part of speech']   \n",
       "474                ['Phonetic', 'Symbols', 'Spelling']   \n",
       "475  ['Radiology and', 'Discharge Summary', 'Echoca...   \n",
       "476                                 ['overt displays']   \n",
       "477  ['facility', 'organization', 'geo-political en...   \n",
       "478                            ['machine translation']   \n",
       "479             ['Double Levenshtein?s Edit Distance']   \n",
       "480                         ['Modified Head Grammars']   \n",
       "481                     ['concept sequence instances']   \n",
       "482                  ['Transformation Based Learning']   \n",
       "483                                ['Maximum Entropy']   \n",
       "484                              ['Class Speaker HMM']   \n",
       "485                        ['Chieu and Ng', 'Florian']   \n",
       "486                             ['Question Answering']   \n",
       "487  ['Empirical Methods in Natural Language Proces...   \n",
       "488                                 ['Knowledge Base']   \n",
       "489     ['abstracts', 'titles', 'authors', 'Journals']   \n",
       "490                               ['learning to rank']   \n",
       "491                                   ['Topic models']   \n",
       "492                 ['Automatic Genre Classification']   \n",
       "493                                  ['Omission Rate']   \n",
       "494                     ['noun', 'adjectives', 'name']   \n",
       "495                        ['Sentence Position Yield']   \n",
       "496                     ['inflectional property sets']   \n",
       "\n",
       "                                  AN_Pred  \\\n",
       "0                                   [TDM]   \n",
       "1                                   [MAP]   \n",
       "2                           [QL, SD, MRF]   \n",
       "3                                   [PRF]   \n",
       "4            [CATiB, POS, NOM, PROP, VRB]   \n",
       "5                  [LDA, MLLT, BMMI, MPE]   \n",
       "6                                  [SWVP]   \n",
       "7                                   [CNF]   \n",
       "8                                      []   \n",
       "9                                   [LNS]   \n",
       "10                           [FN, VP, AP]   \n",
       "11                              [MT, NLG]   \n",
       "12                               [LR, LP]   \n",
       "13                          [GrM, RP, UU]   \n",
       "14                 [TI, INT, TRUN, TRANS]   \n",
       "15                                  [SVM]   \n",
       "16                               [AA, VG]   \n",
       "17                                 [PTSD]   \n",
       "18                                  [LTC]   \n",
       "19                                   [PA]   \n",
       "20                                     []   \n",
       "21                                  [AER]   \n",
       "22                                   [RE]   \n",
       "23                                  [MRR]   \n",
       "24                                   [FU]   \n",
       "25                          [PP, MDI, MS]   \n",
       "26                           [IEEE, CVPR]   \n",
       "27                                  [SVM]   \n",
       "28                                  [SPE]   \n",
       "29                                     []   \n",
       "..                                    ...   \n",
       "467                                 [CQA]   \n",
       "468                          [NW, BN, BC]   \n",
       "469                                 [VPT]   \n",
       "470                                  [AR]   \n",
       "471                                  [FN]   \n",
       "472                                 [PoS]   \n",
       "473                                 [POS]   \n",
       "474                      [WD, SY, PH, SP]   \n",
       "475                 [DS, ED, GI, RAD, SP]   \n",
       "476                                [ODPs]   \n",
       "477  [PERS, ORG, GPE, WEA, VEH, LOC, FAC]   \n",
       "478                                  [MT]   \n",
       "479                                    []   \n",
       "480              [HG's, HL, TAL's, TAG's]   \n",
       "481                               [CSI's]   \n",
       "482                                 [TBL]   \n",
       "483                              [MaxEnt]   \n",
       "484                                    []   \n",
       "485                   [FLORIAN, CHIEU-NG]   \n",
       "486                                   [)]   \n",
       "487                               [EMNLP]   \n",
       "488                                  [KB]   \n",
       "489                       [AbT, ArT, Aut]   \n",
       "490                                [,, )]   \n",
       "491                                 [TMs]   \n",
       "492                                 [AGC]   \n",
       "493                                  [OR]   \n",
       "494                            [ADJA, NN]   \n",
       "495                                 [SPY]   \n",
       "496                                    []   \n",
       "\n",
       "                                               LF_Pred  \\\n",
       "0                                [a, Text Data Mining]   \n",
       "1                             [Mean Average Precision]   \n",
       "2    [a, query likelihood, and a, sequential depend...   \n",
       "3                      [pseudo relevance feedback, in]   \n",
       "4             [non-proper, nouns, proper nouns, verbs]   \n",
       "5    [Maximum Likelihood Linear Transform, Boosted ...   \n",
       "6    [Structured Weighted Violations Perceptron, on a]   \n",
       "7                            [or, Chomsky Normal Form]   \n",
       "8                   [a CFG is, a Probabilistic CFG, n]   \n",
       "9                         [a, Language Neutral Syntax]   \n",
       "10   [Base+FrameNet, Base+Verb Pairs, Base+Appositi...   \n",
       "11        [language, a natural language generation, a]   \n",
       "12                 [labeled recall, labeled precision]   \n",
       "13                              [Response Planner, an]   \n",
       "14   [terse information, interrupted, truncated, tr...   \n",
       "15                   [in, Support Vector Machines, in]   \n",
       "16                              [adverb, a verb group]   \n",
       "17   [a, posttraumatic stress disorder, is a, disor...   \n",
       "18                    [Language Technology Components]   \n",
       "19                                                  []   \n",
       "20      [a, Discourse Tagging Tool, Discourse Tagging]   \n",
       "21                              [Alignment Error Rate]   \n",
       "22                               [relation extraction]   \n",
       "23                              [Mean Reciprocal Rank]   \n",
       "24                         [Functional unification, a]   \n",
       "25                    [Missed Samples, Missed Samples]   \n",
       "26   [Conference on Computer Vision and Pattern Rec...   \n",
       "27                       [Support Vector Machines, to]   \n",
       "28                      [of, Sound Pattern of English]   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "467                     [question answering, question]   \n",
       "468  [a, newswire, broadcast news, broadcast conver...   \n",
       "469                                     [a, viewpoint]   \n",
       "470                          [an, anaphora resolution]   \n",
       "471                                         [FrameNet]   \n",
       "472                                   [part of speech]   \n",
       "473                                [a, part of speech]   \n",
       "474                         [Phonetic/Sound, Spelling]   \n",
       "475  [Discharge Summary, Echo, Echocardiogram, Emer...   \n",
       "476                      [overt displays of power, in]   \n",
       "477  [organization, geo-political entity, weapon, v...   \n",
       "478                    [a, machine translation, on, a]   \n",
       "479              [is a, Levenshtein ? s Edit Distance]   \n",
       "480                        [s, Modified Head Grammars]   \n",
       "481                       [concept sequence instances]   \n",
       "482                    [Transformation Based Learning]   \n",
       "483                               [Maximum Entropy, a]   \n",
       "484         [a, hidden Markov, HMM, Class Speaker HMM]   \n",
       "485                       [Florian, and, Chieu and Ng]   \n",
       "486                                        [Answering]   \n",
       "487  [Empirical Methods in Natural Language Process...   \n",
       "488                                [Knowledge Base, a]   \n",
       "489  [th, representations abstracts, titles, author...   \n",
       "490                                   [a, to rank, in]   \n",
       "491                                  [Topic models, a]   \n",
       "492  [a, Automatic Topic Classification, Automatic ...   \n",
       "493                                    [Omission Rate]   \n",
       "494                          [match, noun, noun, name]   \n",
       "495                          [Sentence Position Yield]   \n",
       "496         [inflectional property sets, inflectional]   \n",
       "\n",
       "                                          AN_Pred_idxs  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                             [[113, 115], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[30, 35], [43, 46], [53, 56], [130, 134], [15...   \n",
       "5                     [[44, 48], [87, 91], [115, 118]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                                   []   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[18, 20], [102, 104], [187, 189]]   \n",
       "11                              [[64, 66], [127, 130]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13        [[39, 42], [71, 73], [125, 127], [117, 119]]   \n",
       "14              [[0, 2], [24, 27], [43, 47], [61, 66]]   \n",
       "15                                          [[85, 88]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                          [[65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                                [[63, 65], [96, 98]]   \n",
       "20                                                  []   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25            [[18, 20], [22, 25], [42, 44], [69, 71]]   \n",
       "26                                [[32, 36], [92, 96]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[36, 38], [57, 59], [86, 88]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                                       [[125, 127]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[88, 91]]   \n",
       "474           [[29, 31], [60, 62], [81, 83], [96, 98]]   \n",
       "475  [[0, 2], [48, 50], [76, 78], [109, 112], [130,...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...   \n",
       "478                             [[74, 76], [109, 111]]   \n",
       "479                                                 []   \n",
       "480                                [[9, 11], [42, 44]]   \n",
       "481                                       [[127, 132]]   \n",
       "482                                         [[87, 90]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                                                 []   \n",
       "485                               [[20, 27], [56, 64]]   \n",
       "486                                                 []   \n",
       "487                                         [[93, 98]]   \n",
       "488                                         [[90, 92]]   \n",
       "489                     [[40, 43], [54, 57], [70, 73]]   \n",
       "490                                                 []   \n",
       "491                                         [[43, 46]]   \n",
       "492                           [[183, 186], [206, 209]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                               [[45, 49], [81, 83]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                                 []   \n",
       "\n",
       "                                          LF_Pred_idxs  \n",
       "0                                         [[144, 160]]  \n",
       "1                                           [[39, 61]]  \n",
       "2                              [[90, 111], [132, 151]]  \n",
       "3                                           [[65, 90]]  \n",
       "4                                         [[156, 161]]  \n",
       "5                       [[7, 42], [51, 85], [94, 113]]  \n",
       "6                                           [[52, 93]]  \n",
       "7                                         [[128, 147]]  \n",
       "8                                                   []  \n",
       "9                                         [[103, 126]]  \n",
       "10                                        [[169, 185]]  \n",
       "11                                                  []  \n",
       "12                             [[91, 105], [112, 129]]  \n",
       "13                                          [[53, 69]]  \n",
       "14                       [[5, 22], [30, 41], [50, 59]]  \n",
       "15                                          [[60, 83]]  \n",
       "16                                                  []  \n",
       "17                                         [[71, 100]]  \n",
       "18                                          [[68, 98]]  \n",
       "19                                                  []  \n",
       "20                                          [[62, 84]]  \n",
       "21                                         [[83, 103]]  \n",
       "22                                         [[89, 108]]  \n",
       "23                                          [[67, 87]]  \n",
       "24                                          [[30, 52]]  \n",
       "25            [[26, 40], [53, 67], [26, 40], [53, 67]]  \n",
       "26                                                  []  \n",
       "27                                                  []  \n",
       "28                                        [[112, 136]]  \n",
       "29                                                  []  \n",
       "..                                                 ...  \n",
       "467                                                 []  \n",
       "468                     [[26, 34], [41, 55], [62, 84]]  \n",
       "469                                       [[104, 113]]  \n",
       "470                                         [[55, 74]]  \n",
       "471                                       [[115, 123]]  \n",
       "472                                       [[105, 119]]  \n",
       "473                                         [[72, 86]]  \n",
       "474                                         [[86, 94]]  \n",
       "475  [[5, 22], [32, 46], [53, 73], [91, 107], [135,...  \n",
       "476                                                 []  \n",
       "477  [[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...  \n",
       "478                                         [[53, 72]]  \n",
       "479                                                 []  \n",
       "480                                       [[130, 152]]  \n",
       "481                                        [[99, 125]]  \n",
       "482                                         [[57, 86]]  \n",
       "483                                         [[36, 51]]  \n",
       "484                                       [[109, 126]]  \n",
       "485                               [[29, 36], [66, 78]]  \n",
       "486                                                 []  \n",
       "487                                         [[43, 91]]  \n",
       "488                                         [[74, 88]]  \n",
       "489                     [[46, 52], [61, 68], [76, 84]]  \n",
       "490                                                 []  \n",
       "491                                         [[29, 41]]  \n",
       "492                                       [[151, 181]]  \n",
       "493                                         [[25, 38]]  \n",
       "494                   [[75, 79], [75, 79], [158, 162]]  \n",
       "495                                         [[31, 54]]  \n",
       "496                                         [[55, 81]]  \n",
       "\n",
       "[497 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig_2 = match2(df_orig, df_pred)\n",
    "df_orig_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/development/prashants/.conda/envs/py3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[144, 160]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baselines are a unigram query likelihood (QL) ...</td>\n",
       "      <td>[[113, 115], [153, 156]]</td>\n",
       "      <td>[[90, 111], [132, 151]]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tion. Then, we extract expansion terms from th...</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90]]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person, mood, voice and case, CATiB uses 6 POS...</td>\n",
       "      <td>[[30, 35], [43, 46], [53, 56], [130, 134], [15...</td>\n",
       "      <td>[[156, 161]]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(LDA), Maximum Likelihood Linear Transform (ML...</td>\n",
       "      <td>[[44, 48], [87, 91], [115, 118]]</td>\n",
       "      <td>[[7, 42], [51, 85], [94, 113]]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gold label (Section 3). Our algorithm is calle...</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93]]</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase markers or words. For simplicity of man...</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[128, 147]]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When a CFG is associated with probabilistic i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1] proposed a language-neutral framework for r...</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[103, 126]]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...</td>\n",
       "      <td>[[18, 20], [102, 104], [187, 189]]</td>\n",
       "      <td>[[169, 185]]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model 3 (Figure 3) illustrates how the source ...</td>\n",
       "      <td>[[64, 66], [127, 130]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The big blue door.?  In this case, the GrM ask...</td>\n",
       "      <td>[[39, 42], [71, 73], [125, 127], [117, 119]]</td>\n",
       "      <td>[[53, 69]]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TI = terse information  INT = interrupted  TRU...</td>\n",
       "      <td>[[0, 2], [24, 27], [43, 47], [61, 66]]</td>\n",
       "      <td>[[5, 22], [30, 41], [50, 59]]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>[[85, 88]]</td>\n",
       "      <td>[[60, 83]]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>as in figure 3.  It is parsed as an adverb (AA...</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>[[65, 69]]</td>\n",
       "      <td>[[71, 100]]</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>[[63, 65], [96, 98]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The tagging has been done using a GUI-based t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[62, 84]]</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Och and Ney (2003) show that for larger corpor...</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2 Relation Extraction System In this section, ...</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>target question, or zero if the target questio...</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>discussion in Ritchie(1984).  Functional unifi...</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52]]</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bigram Perplex. ( PP) MDI Missed Samples (MS) ...</td>\n",
       "      <td>[[18, 20], [22, 25], [42, 44], [69, 71]]</td>\n",
       "      <td>[[26, 40], [53, 67], [26, 40], [53, 67]]</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>networks. In Proceedings of the IEEE Conferenc...</td>\n",
       "      <td>[[32, 36], [92, 96]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Therefore, identification methods like Tsuchiy...</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[112, 136]]</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>eugene@mathcs.emory.edu Abstract Community que...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>means a type of source ? newswire (NW), broad...</td>\n",
       "      <td>[[36, 38], [57, 59], [86, 88]]</td>\n",
       "      <td>[[26, 34], [41, 55], [62, 84]]</td>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>either lexically encoded, or depends on the in...</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[104, 113]]</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Abstract  This paper attempts to use an off-th...</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[55, 74]]</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>The SemEval?2007 task for extracting frame sem...</td>\n",
       "      <td>[[125, 127]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>hauer, haver, haber) and the corpus does not c...</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>The ungrammatical distracter, e.g., are in F...</td>\n",
       "      <td>[[88, 91]]</td>\n",
       "      <td>[[72, 86]]</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Words/Phrases as Themselves (WD)  Symbols/Nonl...</td>\n",
       "      <td>[[29, 31], [60, 62], [81, 83], [96, 98]]</td>\n",
       "      <td>[[86, 94]]</td>\n",
       "      <td>475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>DS = Discharge Summary,  Echo = Echocardiogram...</td>\n",
       "      <td>[[0, 2], [48, 50], [76, 78], [109, 112], [130,...</td>\n",
       "      <td>[[5, 22], [32, 46], [53, 73], [91, 107], [135,...</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>mdiab@ccls.columbia.edu Abstract We analyze ov...</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>(PERS), organization (ORG), geo-political enti...</td>\n",
       "      <td>[[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...</td>\n",
       "      <td>[[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Abstract In this paper, we propose a new synta...</td>\n",
       "      <td>[[74, 76], [109, 111]]</td>\n",
       "      <td>[[53, 72]]</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>The second one is a variant that we named  D...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>by HG's (HL). In particular, we show that HL's...</td>\n",
       "      <td>[[9, 11], [42, 44]]</td>\n",
       "      <td>[[130, 152]]</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>[[127, 132]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>(Ramshaw and Marcus, 1995) approached chuckin...</td>\n",
       "      <td>[[87, 90]]</td>\n",
       "      <td>[[57, 86]]</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>demonstrate such dependencies.  The Maximum En...</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51]]</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>dialogues categorized into multiple domains, w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[109, 126]]</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>shared task, namely FLORIAN (Florian et al.,  ...</td>\n",
       "      <td>[[20, 27], [56, 64]]</td>\n",
       "      <td>[[29, 36], [66, 78]]</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1 Introduction Open-domain Question Answering...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>229  Proceedings of the 2014 Conference on Emp...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>[[90, 92]]</td>\n",
       "      <td>[[74, 88]]</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>th fo frture representations abstracts (AbT), ...</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73]]</td>\n",
       "      <td>[[46, 52], [61, 68], [76, 84]]</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>787 After labeling the reference BINet, we tra...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>for the annotation process.  Topic models (TMs...</td>\n",
       "      <td>[[43, 46]]</td>\n",
       "      <td>[[29, 41]]</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Evidence for a text?s topic and genre comes, i...</td>\n",
       "      <td>[[183, 186], [206, 209]]</td>\n",
       "      <td>[[151, 181]]</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>The query (Figure 4a) will match adjectives (A...</td>\n",
       "      <td>[[45, 49], [81, 83]]</td>\n",
       "      <td>[[75, 79], [75, 79], [158, 162]]</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>the middle (Baxendale, 1958).  Sentence Positi...</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>By contrast, our approach operates at the l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[[55, 81]]</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    2 Related Work The availability of emotion-ric...   \n",
       "1    WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...   \n",
       "2    Baselines are a unigram query likelihood (QL) ...   \n",
       "3    tion. Then, we extract expansion terms from th...   \n",
       "4    person, mood, voice and case, CATiB uses 6 POS...   \n",
       "5    (LDA), Maximum Likelihood Linear Transform (ML...   \n",
       "6    gold label (Section 3). Our algorithm is calle...   \n",
       "7    phrase markers or words. For simplicity of man...   \n",
       "8     When a CFG is associated with probabilistic i...   \n",
       "9    1] proposed a language-neutral framework for r...   \n",
       "10   12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...   \n",
       "11   Model 3 (Figure 3) illustrates how the source ...   \n",
       "12   in the V column indicates that the verb condit...   \n",
       "13   The big blue door.?  In this case, the GrM ask...   \n",
       "14   TI = terse information  INT = interrupted  TRU...   \n",
       "15   classes in both original text and main text co...   \n",
       "16   as in figure 3.  It is parsed as an adverb (AA...   \n",
       "17   can develop after exposure to a terrifying eve...   \n",
       "18   It allows for testing interaction scenarios th...   \n",
       "19   Feature Description lexical the words of the p...   \n",
       "20    The tagging has been done using a GUI-based t...   \n",
       "21   Och and Ney (2003) show that for larger corpor...   \n",
       "22   2 Relation Extraction System In this section, ...   \n",
       "23   target question, or zero if the target questio...   \n",
       "24   discussion in Ritchie(1984).  Functional unifi...   \n",
       "25   Bigram Perplex. ( PP) MDI Missed Samples (MS) ...   \n",
       "26   networks. In Proceedings of the IEEE Conferenc...   \n",
       "27   Therefore, identification methods like Tsuchiy...   \n",
       "28   domain-independent mpirical induction algorith...   \n",
       "29   ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...   \n",
       "..                                                 ...   \n",
       "467  eugene@mathcs.emory.edu Abstract Community que...   \n",
       "468   means a type of source ? newswire (NW), broad...   \n",
       "469  either lexically encoded, or depends on the in...   \n",
       "470  Abstract  This paper attempts to use an off-th...   \n",
       "471  The SemEval?2007 task for extracting frame sem...   \n",
       "472  hauer, haver, haber) and the corpus does not c...   \n",
       "473    The ungrammatical distracter, e.g., are in F...   \n",
       "474  Words/Phrases as Themselves (WD)  Symbols/Nonl...   \n",
       "475  DS = Discharge Summary,  Echo = Echocardiogram...   \n",
       "476  mdiab@ccls.columbia.edu Abstract We analyze ov...   \n",
       "477  (PERS), organization (ORG), geo-political enti...   \n",
       "478  Abstract In this paper, we propose a new synta...   \n",
       "479    The second one is a variant that we named  D...   \n",
       "480  by HG's (HL). In particular, we show that HL's...   \n",
       "481  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482   (Ramshaw and Marcus, 1995) approached chuckin...   \n",
       "483  demonstrate such dependencies.  The Maximum En...   \n",
       "484  dialogues categorized into multiple domains, w...   \n",
       "485  shared task, namely FLORIAN (Florian et al.,  ...   \n",
       "486   1 Introduction Open-domain Question Answering...   \n",
       "487  229  Proceedings of the 2014 Conference on Emp...   \n",
       "488  1 Introduction Large-scale open-domain questio...   \n",
       "489  th fo frture representations abstracts (AbT), ...   \n",
       "490  787 After labeling the reference BINet, we tra...   \n",
       "491  for the annotation process.  Topic models (TMs...   \n",
       "492  Evidence for a text?s topic and genre comes, i...   \n",
       "493     0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co   \n",
       "494  The query (Figure 4a) will match adjectives (A...   \n",
       "495  the middle (Baxendale, 1958).  Sentence Positi...   \n",
       "496     By contrast, our approach operates at the l...   \n",
       "\n",
       "                                              acronyms  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                             [[113, 115], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[30, 35], [43, 46], [53, 56], [130, 134], [15...   \n",
       "5                     [[44, 48], [87, 91], [115, 118]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                                   []   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[18, 20], [102, 104], [187, 189]]   \n",
       "11                              [[64, 66], [127, 130]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13        [[39, 42], [71, 73], [125, 127], [117, 119]]   \n",
       "14              [[0, 2], [24, 27], [43, 47], [61, 66]]   \n",
       "15                                          [[85, 88]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                          [[65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                                [[63, 65], [96, 98]]   \n",
       "20                                                  []   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25            [[18, 20], [22, 25], [42, 44], [69, 71]]   \n",
       "26                                [[32, 36], [92, 96]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[36, 38], [57, 59], [86, 88]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                                       [[125, 127]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[88, 91]]   \n",
       "474           [[29, 31], [60, 62], [81, 83], [96, 98]]   \n",
       "475  [[0, 2], [48, 50], [76, 78], [109, 112], [130,...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...   \n",
       "478                             [[74, 76], [109, 111]]   \n",
       "479                                                 []   \n",
       "480                                [[9, 11], [42, 44]]   \n",
       "481                                       [[127, 132]]   \n",
       "482                                         [[87, 90]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                                                 []   \n",
       "485                               [[20, 27], [56, 64]]   \n",
       "486                                                 []   \n",
       "487                                         [[93, 98]]   \n",
       "488                                         [[90, 92]]   \n",
       "489                     [[40, 43], [54, 57], [70, 73]]   \n",
       "490                                                 []   \n",
       "491                                         [[43, 46]]   \n",
       "492                           [[183, 186], [206, 209]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                               [[45, 49], [81, 83]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                                 []   \n",
       "\n",
       "                                            long-forms   ID  \n",
       "0                                         [[144, 160]]    1  \n",
       "1                                           [[39, 61]]    2  \n",
       "2                              [[90, 111], [132, 151]]    3  \n",
       "3                                           [[65, 90]]    4  \n",
       "4                                         [[156, 161]]    5  \n",
       "5                       [[7, 42], [51, 85], [94, 113]]    6  \n",
       "6                                           [[52, 93]]    7  \n",
       "7                                         [[128, 147]]    8  \n",
       "8                                                   []    9  \n",
       "9                                         [[103, 126]]   10  \n",
       "10                                        [[169, 185]]   11  \n",
       "11                                                  []   12  \n",
       "12                             [[91, 105], [112, 129]]   13  \n",
       "13                                          [[53, 69]]   14  \n",
       "14                       [[5, 22], [30, 41], [50, 59]]   15  \n",
       "15                                          [[60, 83]]   16  \n",
       "16                                                  []   17  \n",
       "17                                         [[71, 100]]   18  \n",
       "18                                          [[68, 98]]   19  \n",
       "19                                                  []   20  \n",
       "20                                          [[62, 84]]   21  \n",
       "21                                         [[83, 103]]   22  \n",
       "22                                         [[89, 108]]   23  \n",
       "23                                          [[67, 87]]   24  \n",
       "24                                          [[30, 52]]   25  \n",
       "25            [[26, 40], [53, 67], [26, 40], [53, 67]]   26  \n",
       "26                                                  []   27  \n",
       "27                                                  []   28  \n",
       "28                                        [[112, 136]]   29  \n",
       "29                                                  []   30  \n",
       "..                                                 ...  ...  \n",
       "467                                                 []  468  \n",
       "468                     [[26, 34], [41, 55], [62, 84]]  469  \n",
       "469                                       [[104, 113]]  470  \n",
       "470                                         [[55, 74]]  471  \n",
       "471                                       [[115, 123]]  472  \n",
       "472                                       [[105, 119]]  473  \n",
       "473                                         [[72, 86]]  474  \n",
       "474                                         [[86, 94]]  475  \n",
       "475  [[5, 22], [32, 46], [53, 73], [91, 107], [135,...  476  \n",
       "476                                                 []  477  \n",
       "477  [[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...  478  \n",
       "478                                         [[53, 72]]  479  \n",
       "479                                                 []  480  \n",
       "480                                       [[130, 152]]  481  \n",
       "481                                        [[99, 125]]  482  \n",
       "482                                         [[57, 86]]  483  \n",
       "483                                         [[36, 51]]  484  \n",
       "484                                       [[109, 126]]  485  \n",
       "485                               [[29, 36], [66, 78]]  486  \n",
       "486                                                 []  487  \n",
       "487                                         [[43, 91]]  488  \n",
       "488                                         [[74, 88]]  489  \n",
       "489                     [[46, 52], [61, 68], [76, 84]]  490  \n",
       "490                                                 []  491  \n",
       "491                                         [[29, 41]]  492  \n",
       "492                                       [[151, 181]]  493  \n",
       "493                                         [[25, 38]]  494  \n",
       "494                   [[75, 79], [75, 79], [158, 162]]  495  \n",
       "495                                         [[31, 54]]  496  \n",
       "496                                         [[55, 81]]  497  \n",
       "\n",
       "[497 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modified Match : Match2()\n",
    "df_orig_json_ = df_orig_2[cols]\n",
    "df_orig_json_.columns = [\"text\",\"acronyms\",\"long-forms\",\"ID\"]\n",
    "df_orig_json_[\"ID\"] = df_orig_json_[\"ID\"].astype(str)\n",
    "df_orig_json_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../results/finetuned_models/model_all_xb_v1_512/output2\n",
    "df_orig_2.to_csv(\"../results/finetuned_models/model_all_xb_v1_512/output2/\" + op_filename + \".csv\", sep=\"\\t\"  )\n",
    "df_orig_json_.to_json(\"../results/finetuned_models/model_all_xb_v1_512/output2/\" + op_filename + \".json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.to_csv(\"../results/finetuned_models/model_eng_scientific_xb_v5/predictions_AN_LF_idxs_Inference.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.iloc[0][\"text\"].find(\"Text Data Mining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_all = \"../data_tsvs/allLangDFTrain.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_all = pd.read_csv(path_all, sep=\"\\t\")\n",
    "# df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = list(df_all['text'])\n",
    "# cnts = [len(str(txt).split(\" \")) for txt in texts]\n",
    "# max(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cnt_outliers = []\n",
    "# for i in range(len( cnts)):\n",
    "#     #print(cnts[i])\n",
    "#     if(cnts[i] > 256):\n",
    "#         print(i,\"\\t\", cnts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all.iloc[13879].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_l = df_all.iloc[13879].text.split()\n",
    "# len(text_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# text_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "267d4488b8371bcb12fb60ff939a960cc6c085e10e5a5111031982adeb143d5c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
