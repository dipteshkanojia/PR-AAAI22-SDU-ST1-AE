{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path = \"../data_tsvs/englishDFDevScientific.tsv\"\n",
    "pred_path = \"../results/finetuned_models/model_eng_scientific_xb_v5/predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(orig_path, sep=\"\\t\")\n",
    "df_pred = pd.read_csv(pred_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[144, 160]]</td>\n",
       "      <td>['TDM']</td>\n",
       "      <td>['Text Data Mining']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "      <td>['MAP']</td>\n",
       "      <td>['Mean Average Precision']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Baselines are a unigram query likelihood (QL) ...</td>\n",
       "      <td>[[113, 115], [42, 45], [153, 156]]</td>\n",
       "      <td>[[90, 111], [132, 151]]</td>\n",
       "      <td>['SD', 'QL)', 'MRF']</td>\n",
       "      <td>['sequential dependence', 'Markov random field']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>tion. Then, we extract expansion terms from th...</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90]]</td>\n",
       "      <td>['PRF']</td>\n",
       "      <td>['pseudo relevance feedback']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>person, mood, voice and case, CATiB uses 6 POS...</td>\n",
       "      <td>[[130, 134], [30, 35], [43, 46], [53, 56], [15...</td>\n",
       "      <td>[[136, 142], [156, 161]]</td>\n",
       "      <td>['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...</td>\n",
       "      <td>['proper', 'verbs']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               text  \\\n",
       "0   1  2 Related Work The availability of emotion-ric...   \n",
       "1   2  WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...   \n",
       "2   3  Baselines are a unigram query likelihood (QL) ...   \n",
       "3   4  tion. Then, we extract expansion terms from th...   \n",
       "4   5  person, mood, voice and case, CATiB uses 6 POS...   \n",
       "\n",
       "                                            acronyms  \\\n",
       "0                                       [[162, 165]]   \n",
       "1                                         [[63, 66]]   \n",
       "2                 [[113, 115], [42, 45], [153, 156]]   \n",
       "3                                         [[92, 95]]   \n",
       "4  [[130, 134], [30, 35], [43, 46], [53, 56], [15...   \n",
       "\n",
       "                 long-forms  \\\n",
       "0              [[144, 160]]   \n",
       "1                [[39, 61]]   \n",
       "2   [[90, 111], [132, 151]]   \n",
       "3                [[65, 90]]   \n",
       "4  [[136, 142], [156, 161]]   \n",
       "\n",
       "                                       acronyms-text  \\\n",
       "0                                            ['TDM']   \n",
       "1                                            ['MAP']   \n",
       "2                               ['SD', 'QL)', 'MRF']   \n",
       "3                                            ['PRF']   \n",
       "4  ['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...   \n",
       "\n",
       "                                    long-forms-text  \n",
       "0                              ['Text Data Mining']  \n",
       "1                        ['Mean Average Precision']  \n",
       "2  ['sequential dependence', 'Markov random field']  \n",
       "3                     ['pseudo relevance feedback']  \n",
       "4                               ['proper', 'verbs']  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Y_true</th>\n",
       "      <th>Y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1 : Mean A...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baselines are a unigram query likelihood ( QL ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'I-LF', 'I-LF', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tion . Then , we extract expansion terms from ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person , mood , voice and case , CATiB uses 6 ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  2 Related Work The availability of emotion-ric...   \n",
       "1  WebDict 0.2919 Backoff 0.3282 Table 1 : Mean A...   \n",
       "2  Baselines are a unigram query likelihood ( QL ...   \n",
       "3  tion . Then , we extract expansion terms from ...   \n",
       "4  person , mood , voice and case , CATiB uses 6 ...   \n",
       "\n",
       "                                              Y_true  \\\n",
       "0  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "1  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "2  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "3  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "4  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "\n",
       "                                              Y_pred  \n",
       "0  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "1  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "2  ['B-O', 'B-O', 'B-LF', 'B-O', 'I-LF', 'I-LF', ...  \n",
       "3  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "4  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  "
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ents', ')', '*100', '%', '?', 'F1-score', '=', '2*P*R', '/', '(', 'P+R', ')', 'Two', 'correctness', 'criteria', 'are', 'used', 'for', 'constitu-']\n",
      "['ents', ')', '*', '100', '%', '?', 'F1-score', '=', '2', '*', 'P', '*', 'R', '/', '(', 'P+R', ')', 'Two', 'correctness', 'criteria', 'are', 'used', 'for', 'constitu-']\n",
      "\n",
      "\n",
      "\n",
      "['XOAJC', '+', 'LC*Ave', '(', 'XOAJC', ')', '/Ave', '(', 'XOAR', ')', 'The', 'second', 'strategy', 'consists', 'in', 'building', 'a', 'prediction', 'model', 'for', 'BLAST', 'bit', 'score', '(', 'BBS', ')', 'using', 'the', 'XOA', 'score', 'and', 'the', 'log-cosine', 'LC', 'as', 'predictors']\n",
      "['XOAJC', '+', 'LC', '*', 'Ave', '(', 'XOAJC', ')', '/Ave', '(', 'XOAR', ')', 'The', 'second', 'strategy', 'consists', 'in', 'building', 'a', 'prediction', 'model', 'for', 'BLAST', 'bit', 'score', '(', 'BBS', ')', 'using', 'the', 'XOA', 'score', 'and', 'the', 'log-cosine', 'LC', 'as', 'predictors']\n",
      "\n",
      "\n",
      "\n",
      "['In', 'the', 'table', ',', 'P', 'is', 'precision', ';', 'R', 'is', 'recall', ';', 'P', '&', 'R', 'is', 'the', 'harmonic', 'mean', 'of', 'precision', 'and', 'recall', '(', 'P', '&', 'R', '=', '(', '2*P*R', ')', '/', '(', 'P+R', ')', ',', 'corresponding', 'to', 'a', 'F-measure', 'with', 'a', '?']\n",
      "['In', 'the', 'table', ',', 'P', 'is', 'precision', ';', 'R', 'is', 'recall', ';', 'P', '&', 'R', 'is', 'the', 'harmonic', 'mean', 'of', 'precision', 'and', 'recall', '(', 'P', '&', 'R', '=', '(', '2', '*', 'P', '*', 'R', ')', '/', '(', 'P+R', ')', ',', 'corresponding', 'to', 'a', 'F-measure', 'with', 'a', '?']\n",
      "\n",
      "\n",
      "\n",
      "['REPRESENTATION', 'OF', 'INTENSIONAI', ',', \"CON'FEXTS\", 'The', 'main', 'extension', 'to', 'the', 'work', 'reported', 'in', '\\\\', '[', 'Di', 'Eugenio', '&', 'Lesnro', \"II'/l\", 'consists', 'in', 'the', 'introduction', 'of', 'CONTEXT', 'SPACES', '(', 'CS', ')', ',', 'which', 'enable', 'us', 'to', 'treat', 'the', 'intensional', 'contexts', 'along', 'the', 'lines', 'pro..']\n",
      "['REPRESENTATION', 'OF', 'INTENSIONAI', ',', \"CON'FEXTS\", 'The', 'main', 'extension', 'to', 'the', 'work', 'reported', 'in', '\\\\', '[', 'Di', 'Eugenio', '&', 'Lesnro', \"II'/l\", 'consists', 'in', 'the', 'introduction', 'of', 'CONTEXT', 'SPACES', '(', 'CS', ')', ',', 'which', 'enable', 'us', 'to', 'treat', 'the', 'intensional', 'contexts', 'along', 'the', 'lines', 'pro', '..']\n",
      "\n",
      "\n",
      "\n",
      "['rlt*s*rle', 'TR', 'AN', 'SFORHATIONS', '*S***', 'SCAN', 'CALLED', 'AT', '1', 'I', 'ANTEST', 'CALLED', 'FOR', '4l', \"'\", 'I', 'NG', 'l1', '(', 'AACC', ')', ',', 'SD=', '5', '.', 'RES=', '0', '.']\n",
      "['rlt', '*', 's', '*', 'rle', 'TR', 'AN', 'SFORHATIONS', '*', 'S', '*', '*', '*', 'SCAN', 'CALLED', 'AT', '1', 'I', 'ANTEST', 'CALLED', 'FOR', '4l', \"'\", 'I', 'NG', 'l1', '(', 'AACC', ')', ',', 'SD=', '5', '.', 'RES=', '0', '.']\n",
      "\n",
      "\n",
      "\n",
      "['metric', 'TF*S~', ',', 'since', 'we', 'base', 'the', 'importance', 'of', 'a', '?', 'SF', '=', 'Segment', 'frequency', '(', 'How', 'many', 'segments', 'does', 'the', 'term', 'occur', 'in', ')']\n",
      "['metric', 'TF', '*', 'S~', ',', 'since', 'we', 'base', 'the', 'importance', 'of', 'a', '?', 'SF', '=', 'Segment', 'frequency', '(', 'How', 'many', 'segments', 'does', 'the', 'term', 'occur', 'in', ')']\n",
      "\n",
      "\n",
      "\n",
      "['procedures', \"''\", '(', 'ReP', ')', '.', 'The', 'RePs', 'work', 'on', 'a', 'memory', 'structure', 'which', 'is', 'adequate', 'for', 'the', 'representation', 'of', 'knowledge', 'about', 'objects', ',', 'the', '``', 'referential', 'net', \"''\", '(', 'RefN', ')', '4*', '.', 'A', 'RefN']\n",
      "['procedures', \"''\", '(', 'ReP', ')', '.', 'The', 'RePs', 'work', 'on', 'a', 'memory', 'structure', 'which', 'is', 'adequate', 'for', 'the', 'representation', 'of', 'knowledge', 'about', 'objects', ',', 'the', '``', 'referential', 'net', \"''\", '(', 'RefN', ')', '4', '*', '.', 'A', 'RefN']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status = []\n",
    "flag = True\n",
    "\n",
    "for orig_text, pred_text in zip(list(df_orig[\"text\"]), list(df_pred[\"Text\"]) ):\n",
    "#     print(orig_text.strip())\n",
    "#     print(pred_text.strip())\n",
    "#     print(\"\\n\\n\")\n",
    "    #print(type(orig_text))\n",
    "    orig_text_l = word_tokenize(orig_text.strip())\n",
    "    pred_text_l = pred_text.strip().split(\" \")\n",
    "    if(len(orig_text_l) == len(pred_text_l)):\n",
    "        status.append(True)\n",
    "    else:\n",
    "        status.append([False, len(orig_text_l), len(pred_text_l)])\n",
    "#         print(orig_text.strip())\n",
    "#         print(pred_text.strip())\n",
    "        print(orig_text_l)\n",
    "        print(pred_text_l)\n",
    "        print(\"\\n\\n\")\n",
    "        flag = False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[144, 160]]</td>\n",
       "      <td>['TDM']</td>\n",
       "      <td>['Text Data Mining']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "      <td>['MAP']</td>\n",
       "      <td>['Mean Average Precision']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Baselines are a unigram query likelihood (QL) ...</td>\n",
       "      <td>[[113, 115], [42, 45], [153, 156]]</td>\n",
       "      <td>[[90, 111], [132, 151]]</td>\n",
       "      <td>['SD', 'QL)', 'MRF']</td>\n",
       "      <td>['sequential dependence', 'Markov random field']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>tion. Then, we extract expansion terms from th...</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90]]</td>\n",
       "      <td>['PRF']</td>\n",
       "      <td>['pseudo relevance feedback']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>person, mood, voice and case, CATiB uses 6 POS...</td>\n",
       "      <td>[[130, 134], [30, 35], [43, 46], [53, 56], [15...</td>\n",
       "      <td>[[136, 142], [156, 161]]</td>\n",
       "      <td>['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...</td>\n",
       "      <td>['proper', 'verbs']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>(LDA), Maximum Likelihood Linear Transform (ML...</td>\n",
       "      <td>[[115, 118], [44, 48], [87, 91]]</td>\n",
       "      <td>[[94, 113], [7, 42], [51, 85]]</td>\n",
       "      <td>['MPE', 'MLLT', 'BMMI']</td>\n",
       "      <td>['Minimum Phone Error', 'Maximum Likelihood Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>gold label (Section 3). Our algorithm is calle...</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93]]</td>\n",
       "      <td>['SWVP']</td>\n",
       "      <td>['Structured Weighted Violations Perceptron']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>phrase markers or words. For simplicity of man...</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[128, 147]]</td>\n",
       "      <td>['CNF']</td>\n",
       "      <td>['Chomsky Normal Form']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>When a CFG is associated with probabilistic i...</td>\n",
       "      <td>[[85, 89], [8, 11]]</td>\n",
       "      <td>[[66, 83]]</td>\n",
       "      <td>['PCFG', 'CFG']</td>\n",
       "      <td>['Probabilistic CFG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1] proposed a language-neutral framework for r...</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[103, 126]]</td>\n",
       "      <td>['LNS']</td>\n",
       "      <td>['Language Neutral Syntax']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...</td>\n",
       "      <td>[[187, 189], [18, 20], [102, 104]]</td>\n",
       "      <td>[[169, 185], [8, 16], [90, 99]]</td>\n",
       "      <td>['AP', 'FN', 'VP']</td>\n",
       "      <td>['Base+Appositives', 'FrameNet', 'Verb Pair']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Model 3 (Figure 3) illustrates how the source ...</td>\n",
       "      <td>[[127, 130], [64, 66]]</td>\n",
       "      <td>[[98, 125]]</td>\n",
       "      <td>['NLG', 'MT']</td>\n",
       "      <td>['natural language generation']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "      <td>['LR', 'LP']</td>\n",
       "      <td>['labeled recall', 'labeled precision']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>The big blue door.?  In this case, the GrM ask...</td>\n",
       "      <td>[[71, 73], [39, 42], [117, 119], [125, 127]]</td>\n",
       "      <td>[[53, 69]]</td>\n",
       "      <td>['RP', 'GrM', 'UU', 'RP']</td>\n",
       "      <td>['Response Planner']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>TI = terse information  INT = interrupted  TRU...</td>\n",
       "      <td>[[43, 47], [0, 2], [24, 27], [61, 66]]</td>\n",
       "      <td>[[50, 59], [5, 22], [30, 41], [69, 79]]</td>\n",
       "      <td>['TRUN', 'TI', 'INT', 'TRANS']</td>\n",
       "      <td>['truncated', 'terse information', 'interrupte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>[[85, 88], [140, 143]]</td>\n",
       "      <td>[[60, 83]]</td>\n",
       "      <td>['SVM', 'AGI']</td>\n",
       "      <td>['Support Vector Machines']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>as in figure 3.  It is parsed as an adverb (AA...</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[[33, 42], [72, 82]]</td>\n",
       "      <td>['AA', 'VG']</td>\n",
       "      <td>['an adverb', 'verb group']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>[[65, 69]]</td>\n",
       "      <td>[[71, 100]]</td>\n",
       "      <td>['PTSD']</td>\n",
       "      <td>['posttraumatic stress disorder']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "      <td>['LTC']</td>\n",
       "      <td>['Language Technology Components']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>[[63, 65], [71, 74], [96, 98]]</td>\n",
       "      <td>[[45, 61]]</td>\n",
       "      <td>['PA', 'POS', 'PA']</td>\n",
       "      <td>['product attribut']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>The tagging has been done using a GUI-based t...</td>\n",
       "      <td>[[86, 92], [35, 38]]</td>\n",
       "      <td>[[62, 84]]</td>\n",
       "      <td>['DTTool', 'GUI']</td>\n",
       "      <td>['Discourse Tagging Tool']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Och and Ney (2003) show that for larger corpor...</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "      <td>['AER']</td>\n",
       "      <td>['Alignment Error Rate']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2 Relation Extraction System In this section, ...</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "      <td>['RE']</td>\n",
       "      <td>['relation extraction']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>target question, or zero if the target questio...</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "      <td>['MRR']</td>\n",
       "      <td>['Mean Reciprocal Rank']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>discussion in Ritchie(1984).  Functional unifi...</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52]]</td>\n",
       "      <td>['FU']</td>\n",
       "      <td>['Functional unification']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Bigram Perplex. ( PP) MDI Missed Samples (MS) ...</td>\n",
       "      <td>[[42, 44], [18, 20], [22, 25], [69, 71]]</td>\n",
       "      <td>[[26, 40], [7, 14], [53, 67]]</td>\n",
       "      <td>['MS', 'PP', 'MDI', 'MS']</td>\n",
       "      <td>['Missed Samples', 'Perplex', 'Missed Samples']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>networks. In Proceedings of the IEEE Conferenc...</td>\n",
       "      <td>[[92, 96], [32, 36]]</td>\n",
       "      <td>[[51, 90]]</td>\n",
       "      <td>['CVPR', 'IEEE']</td>\n",
       "      <td>['Computer Vision and Pattern Recognition']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Therefore, identification methods like Tsuchiy...</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[[72, 94]]</td>\n",
       "      <td>['SVM']</td>\n",
       "      <td>['Support Vector Machine']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[112, 136]]</td>\n",
       "      <td>['SPE']</td>\n",
       "      <td>['Sound Pattern of English']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...</td>\n",
       "      <td>[[35, 38]]</td>\n",
       "      <td>[[26, 31]]</td>\n",
       "      <td>['P+R']</td>\n",
       "      <td>['2*P*R']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>eugene@mathcs.emory.edu Abstract Community que...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[33, 61]]</td>\n",
       "      <td>['CQA']</td>\n",
       "      <td>['Community question answering']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>469</td>\n",
       "      <td>means a type of source ? newswire (NW), broad...</td>\n",
       "      <td>[[36, 38], [57, 59], [86, 88]]</td>\n",
       "      <td>[[26, 34], [41, 55], [62, 84]]</td>\n",
       "      <td>['NW', 'BN', 'BC']</td>\n",
       "      <td>['newswire', 'broadcast news', 'broadcast conv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>470</td>\n",
       "      <td>either lexically encoded, or depends on the in...</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[104, 113]]</td>\n",
       "      <td>['VPT']</td>\n",
       "      <td>['viewpoint']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>471</td>\n",
       "      <td>Abstract  This paper attempts to use an off-th...</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[55, 74]]</td>\n",
       "      <td>['AR']</td>\n",
       "      <td>['anaphora resolution']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>472</td>\n",
       "      <td>The SemEval?2007 task for extracting frame sem...</td>\n",
       "      <td>[[125, 127], [4, 11]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "      <td>['FN', 'SemEval']</td>\n",
       "      <td>['FrameNet']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>473</td>\n",
       "      <td>hauer, haver, haber) and the corpus does not c...</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "      <td>['PoS']</td>\n",
       "      <td>['part of speech']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>474</td>\n",
       "      <td>The ungrammatical distracter, e.g., are in F...</td>\n",
       "      <td>[[88, 91]]</td>\n",
       "      <td>[[72, 86]]</td>\n",
       "      <td>['POS']</td>\n",
       "      <td>['part of speech']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>475</td>\n",
       "      <td>Words/Phrases as Themselves (WD)  Symbols/Nonl...</td>\n",
       "      <td>[[81, 83], [29, 31], [60, 62], [96, 98]]</td>\n",
       "      <td>[[65, 73], [34, 41], [86, 94]]</td>\n",
       "      <td>['PH', 'WD', 'SY', 'SP']</td>\n",
       "      <td>['Phonetic', 'Symbols', 'Spelling']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>476</td>\n",
       "      <td>DS = Discharge Summary,  Echo = Echocardiogram...</td>\n",
       "      <td>[[109, 112], [0, 2], [25, 29], [48, 50], [76, ...</td>\n",
       "      <td>[[115, 128], [5, 22], [32, 46], [53, 73], [91,...</td>\n",
       "      <td>['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']</td>\n",
       "      <td>['Radiology and', 'Discharge Summary', 'Echoca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>477</td>\n",
       "      <td>mdiab@ccls.columbia.edu Abstract We analyze ov...</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[[44, 58]]</td>\n",
       "      <td>['ODPs']</td>\n",
       "      <td>['overt displays']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>478</td>\n",
       "      <td>(PERS), organization (ORG), geo-political enti...</td>\n",
       "      <td>[[115, 118], [1, 5], [22, 25], [50, 53], [64, ...</td>\n",
       "      <td>[[105, 113], [8, 20], [28, 48], [56, 62], [70,...</td>\n",
       "      <td>['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'L...</td>\n",
       "      <td>['facility', 'organization', 'geo-political en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>479</td>\n",
       "      <td>Abstract In this paper, we propose a new synta...</td>\n",
       "      <td>[[74, 76], [109, 111]]</td>\n",
       "      <td>[[53, 72]]</td>\n",
       "      <td>['MT', 'MT']</td>\n",
       "      <td>['machine translation']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>480</td>\n",
       "      <td>The second one is a variant that we named  D...</td>\n",
       "      <td>[[81, 85]]</td>\n",
       "      <td>[[45, 79]]</td>\n",
       "      <td>['DLED']</td>\n",
       "      <td>['Double Levenshtein?s Edit Distance']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>481</td>\n",
       "      <td>by HG's (HL). In particular, we show that HL's...</td>\n",
       "      <td>[[154, 159], [3, 5], [9, 11], [42, 44], [63, 6...</td>\n",
       "      <td>[[130, 152]]</td>\n",
       "      <td>[\"MHG's\", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL...</td>\n",
       "      <td>['Modified Head Grammars']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>482</td>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>[[127, 132]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "      <td>[\"CSI's\"]</td>\n",
       "      <td>['concept sequence instances']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>483</td>\n",
       "      <td>(Ramshaw and Marcus, 1995) approached chuckin...</td>\n",
       "      <td>[[87, 90]]</td>\n",
       "      <td>[[57, 86]]</td>\n",
       "      <td>['TBL']</td>\n",
       "      <td>['Transformation Based Learning']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>484</td>\n",
       "      <td>demonstrate such dependencies.  The Maximum En...</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51]]</td>\n",
       "      <td>['MaxEnt']</td>\n",
       "      <td>['Maximum Entropy']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>485</td>\n",
       "      <td>dialogues categorized into multiple domains, w...</td>\n",
       "      <td>[[128, 133]]</td>\n",
       "      <td>[[109, 126]]</td>\n",
       "      <td>['CSHMM']</td>\n",
       "      <td>['Class Speaker HMM']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>486</td>\n",
       "      <td>shared task, namely FLORIAN (Florian et al.,  ...</td>\n",
       "      <td>[[56, 64], [20, 27]]</td>\n",
       "      <td>[[66, 78], [29, 36]]</td>\n",
       "      <td>['CHIEU-NG', 'FLORIAN']</td>\n",
       "      <td>['Chieu and Ng', 'Florian']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>487</td>\n",
       "      <td>1 Introduction Open-domain Question Answering...</td>\n",
       "      <td>[[48, 50]]</td>\n",
       "      <td>[[28, 46]]</td>\n",
       "      <td>['QA']</td>\n",
       "      <td>['Question Answering']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>488</td>\n",
       "      <td>229  Proceedings of the 2014 Conference on Emp...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "      <td>['EMNLP']</td>\n",
       "      <td>['Empirical Methods in Natural Language Proces...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>489</td>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>[[90, 92], [154, 156]]</td>\n",
       "      <td>[[74, 88]]</td>\n",
       "      <td>['KB', 'QA']</td>\n",
       "      <td>['Knowledge Base']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>490</td>\n",
       "      <td>th fo frture representations abstracts (AbT), ...</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73], [86, 89]]</td>\n",
       "      <td>[[29, 38], [46, 52], [61, 68], [76, 84]]</td>\n",
       "      <td>['AbT', 'ArT', 'Aut', 'Jou']</td>\n",
       "      <td>['abstracts', 'titles', 'authors', 'Journals']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>491</td>\n",
       "      <td>787 After labeling the reference BINet, we tra...</td>\n",
       "      <td>[[69, 72]]</td>\n",
       "      <td>[[51, 67]]</td>\n",
       "      <td>['L2R']</td>\n",
       "      <td>['learning to rank']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>492</td>\n",
       "      <td>for the annotation process.  Topic models (TMs...</td>\n",
       "      <td>[[43, 46]]</td>\n",
       "      <td>[[29, 41]]</td>\n",
       "      <td>['TMs']</td>\n",
       "      <td>['Topic models']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>493</td>\n",
       "      <td>Evidence for a text?s topic and genre comes, i...</td>\n",
       "      <td>[[183, 186], [206, 209]]</td>\n",
       "      <td>[[151, 181]]</td>\n",
       "      <td>['AGC', 'AGC']</td>\n",
       "      <td>['Automatic Genre Classification']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>494</td>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "      <td>['OR']</td>\n",
       "      <td>['Omission Rate']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>495</td>\n",
       "      <td>The query (Figure 4a) will match adjectives (A...</td>\n",
       "      <td>[[81, 83], [45, 49], [164, 166]]</td>\n",
       "      <td>[[75, 79], [33, 43], [158, 162]]</td>\n",
       "      <td>['NN', 'ADJA', 'NE']</td>\n",
       "      <td>['noun', 'adjectives', 'name']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>the middle (Baxendale, 1958).  Sentence Positi...</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "      <td>['SPY']</td>\n",
       "      <td>['Sentence Position Yield']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>By contrast, our approach operates at the l...</td>\n",
       "      <td>[[83, 86]]</td>\n",
       "      <td>[[55, 81]]</td>\n",
       "      <td>['IPS']</td>\n",
       "      <td>['inflectional property sets']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               text  \\\n",
       "0      1  2 Related Work The availability of emotion-ric...   \n",
       "1      2  WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...   \n",
       "2      3  Baselines are a unigram query likelihood (QL) ...   \n",
       "3      4  tion. Then, we extract expansion terms from th...   \n",
       "4      5  person, mood, voice and case, CATiB uses 6 POS...   \n",
       "5      6  (LDA), Maximum Likelihood Linear Transform (ML...   \n",
       "6      7  gold label (Section 3). Our algorithm is calle...   \n",
       "7      8  phrase markers or words. For simplicity of man...   \n",
       "8      9   When a CFG is associated with probabilistic i...   \n",
       "9     10  1] proposed a language-neutral framework for r...   \n",
       "10    11  12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...   \n",
       "11    12  Model 3 (Figure 3) illustrates how the source ...   \n",
       "12    13  in the V column indicates that the verb condit...   \n",
       "13    14  The big blue door.?  In this case, the GrM ask...   \n",
       "14    15  TI = terse information  INT = interrupted  TRU...   \n",
       "15    16  classes in both original text and main text co...   \n",
       "16    17  as in figure 3.  It is parsed as an adverb (AA...   \n",
       "17    18  can develop after exposure to a terrifying eve...   \n",
       "18    19  It allows for testing interaction scenarios th...   \n",
       "19    20  Feature Description lexical the words of the p...   \n",
       "20    21   The tagging has been done using a GUI-based t...   \n",
       "21    22  Och and Ney (2003) show that for larger corpor...   \n",
       "22    23  2 Relation Extraction System In this section, ...   \n",
       "23    24  target question, or zero if the target questio...   \n",
       "24    25  discussion in Ritchie(1984).  Functional unifi...   \n",
       "25    26  Bigram Perplex. ( PP) MDI Missed Samples (MS) ...   \n",
       "26    27  networks. In Proceedings of the IEEE Conferenc...   \n",
       "27    28  Therefore, identification methods like Tsuchiy...   \n",
       "28    29  domain-independent mpirical induction algorith...   \n",
       "29    30  ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...   \n",
       "..   ...                                                ...   \n",
       "467  468  eugene@mathcs.emory.edu Abstract Community que...   \n",
       "468  469   means a type of source ? newswire (NW), broad...   \n",
       "469  470  either lexically encoded, or depends on the in...   \n",
       "470  471  Abstract  This paper attempts to use an off-th...   \n",
       "471  472  The SemEval?2007 task for extracting frame sem...   \n",
       "472  473  hauer, haver, haber) and the corpus does not c...   \n",
       "473  474    The ungrammatical distracter, e.g., are in F...   \n",
       "474  475  Words/Phrases as Themselves (WD)  Symbols/Nonl...   \n",
       "475  476  DS = Discharge Summary,  Echo = Echocardiogram...   \n",
       "476  477  mdiab@ccls.columbia.edu Abstract We analyze ov...   \n",
       "477  478  (PERS), organization (ORG), geo-political enti...   \n",
       "478  479  Abstract In this paper, we propose a new synta...   \n",
       "479  480    The second one is a variant that we named  D...   \n",
       "480  481  by HG's (HL). In particular, we show that HL's...   \n",
       "481  482  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482  483   (Ramshaw and Marcus, 1995) approached chuckin...   \n",
       "483  484  demonstrate such dependencies.  The Maximum En...   \n",
       "484  485  dialogues categorized into multiple domains, w...   \n",
       "485  486  shared task, namely FLORIAN (Florian et al.,  ...   \n",
       "486  487   1 Introduction Open-domain Question Answering...   \n",
       "487  488  229  Proceedings of the 2014 Conference on Emp...   \n",
       "488  489  1 Introduction Large-scale open-domain questio...   \n",
       "489  490  th fo frture representations abstracts (AbT), ...   \n",
       "490  491  787 After labeling the reference BINet, we tra...   \n",
       "491  492  for the annotation process.  Topic models (TMs...   \n",
       "492  493  Evidence for a text?s topic and genre comes, i...   \n",
       "493  494     0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co   \n",
       "494  495  The query (Figure 4a) will match adjectives (A...   \n",
       "495  496  the middle (Baxendale, 1958).  Sentence Positi...   \n",
       "496  497     By contrast, our approach operates at the l...   \n",
       "\n",
       "                                              acronyms  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                   [[113, 115], [42, 45], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[130, 134], [30, 35], [43, 46], [53, 56], [15...   \n",
       "5                     [[115, 118], [44, 48], [87, 91]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                  [[85, 89], [8, 11]]   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[187, 189], [18, 20], [102, 104]]   \n",
       "11                              [[127, 130], [64, 66]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13        [[71, 73], [39, 42], [117, 119], [125, 127]]   \n",
       "14              [[43, 47], [0, 2], [24, 27], [61, 66]]   \n",
       "15                              [[85, 88], [140, 143]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                          [[65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                      [[63, 65], [71, 74], [96, 98]]   \n",
       "20                                [[86, 92], [35, 38]]   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25            [[42, 44], [18, 20], [22, 25], [69, 71]]   \n",
       "26                                [[92, 96], [32, 36]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                          [[35, 38]]   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[36, 38], [57, 59], [86, 88]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                              [[125, 127], [4, 11]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[88, 91]]   \n",
       "474           [[81, 83], [29, 31], [60, 62], [96, 98]]   \n",
       "475  [[109, 112], [0, 2], [25, 29], [48, 50], [76, ...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[115, 118], [1, 5], [22, 25], [50, 53], [64, ...   \n",
       "478                             [[74, 76], [109, 111]]   \n",
       "479                                         [[81, 85]]   \n",
       "480  [[154, 159], [3, 5], [9, 11], [42, 44], [63, 6...   \n",
       "481                                       [[127, 132]]   \n",
       "482                                         [[87, 90]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                                       [[128, 133]]   \n",
       "485                               [[56, 64], [20, 27]]   \n",
       "486                                         [[48, 50]]   \n",
       "487                                         [[93, 98]]   \n",
       "488                             [[90, 92], [154, 156]]   \n",
       "489           [[40, 43], [54, 57], [70, 73], [86, 89]]   \n",
       "490                                         [[69, 72]]   \n",
       "491                                         [[43, 46]]   \n",
       "492                           [[183, 186], [206, 209]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                   [[81, 83], [45, 49], [164, 166]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                         [[83, 86]]   \n",
       "\n",
       "                                            long-forms  \\\n",
       "0                                         [[144, 160]]   \n",
       "1                                           [[39, 61]]   \n",
       "2                              [[90, 111], [132, 151]]   \n",
       "3                                           [[65, 90]]   \n",
       "4                             [[136, 142], [156, 161]]   \n",
       "5                       [[94, 113], [7, 42], [51, 85]]   \n",
       "6                                           [[52, 93]]   \n",
       "7                                         [[128, 147]]   \n",
       "8                                           [[66, 83]]   \n",
       "9                                         [[103, 126]]   \n",
       "10                     [[169, 185], [8, 16], [90, 99]]   \n",
       "11                                         [[98, 125]]   \n",
       "12                             [[91, 105], [112, 129]]   \n",
       "13                                          [[53, 69]]   \n",
       "14             [[50, 59], [5, 22], [30, 41], [69, 79]]   \n",
       "15                                          [[60, 83]]   \n",
       "16                                [[33, 42], [72, 82]]   \n",
       "17                                         [[71, 100]]   \n",
       "18                                          [[68, 98]]   \n",
       "19                                          [[45, 61]]   \n",
       "20                                          [[62, 84]]   \n",
       "21                                         [[83, 103]]   \n",
       "22                                         [[89, 108]]   \n",
       "23                                          [[67, 87]]   \n",
       "24                                          [[30, 52]]   \n",
       "25                       [[26, 40], [7, 14], [53, 67]]   \n",
       "26                                          [[51, 90]]   \n",
       "27                                          [[72, 94]]   \n",
       "28                                        [[112, 136]]   \n",
       "29                                          [[26, 31]]   \n",
       "..                                                 ...   \n",
       "467                                         [[33, 61]]   \n",
       "468                     [[26, 34], [41, 55], [62, 84]]   \n",
       "469                                       [[104, 113]]   \n",
       "470                                         [[55, 74]]   \n",
       "471                                       [[115, 123]]   \n",
       "472                                       [[105, 119]]   \n",
       "473                                         [[72, 86]]   \n",
       "474                     [[65, 73], [34, 41], [86, 94]]   \n",
       "475  [[115, 128], [5, 22], [32, 46], [53, 73], [91,...   \n",
       "476                                         [[44, 58]]   \n",
       "477  [[105, 113], [8, 20], [28, 48], [56, 62], [70,...   \n",
       "478                                         [[53, 72]]   \n",
       "479                                         [[45, 79]]   \n",
       "480                                       [[130, 152]]   \n",
       "481                                        [[99, 125]]   \n",
       "482                                         [[57, 86]]   \n",
       "483                                         [[36, 51]]   \n",
       "484                                       [[109, 126]]   \n",
       "485                               [[66, 78], [29, 36]]   \n",
       "486                                         [[28, 46]]   \n",
       "487                                         [[43, 91]]   \n",
       "488                                         [[74, 88]]   \n",
       "489           [[29, 38], [46, 52], [61, 68], [76, 84]]   \n",
       "490                                         [[51, 67]]   \n",
       "491                                         [[29, 41]]   \n",
       "492                                       [[151, 181]]   \n",
       "493                                         [[25, 38]]   \n",
       "494                   [[75, 79], [33, 43], [158, 162]]   \n",
       "495                                         [[31, 54]]   \n",
       "496                                         [[55, 81]]   \n",
       "\n",
       "                                         acronyms-text  \\\n",
       "0                                              ['TDM']   \n",
       "1                                              ['MAP']   \n",
       "2                                 ['SD', 'QL)', 'MRF']   \n",
       "3                                              ['PRF']   \n",
       "4    ['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...   \n",
       "5                              ['MPE', 'MLLT', 'BMMI']   \n",
       "6                                             ['SWVP']   \n",
       "7                                              ['CNF']   \n",
       "8                                      ['PCFG', 'CFG']   \n",
       "9                                              ['LNS']   \n",
       "10                                  ['AP', 'FN', 'VP']   \n",
       "11                                       ['NLG', 'MT']   \n",
       "12                                        ['LR', 'LP']   \n",
       "13                           ['RP', 'GrM', 'UU', 'RP']   \n",
       "14                      ['TRUN', 'TI', 'INT', 'TRANS']   \n",
       "15                                      ['SVM', 'AGI']   \n",
       "16                                        ['AA', 'VG']   \n",
       "17                                            ['PTSD']   \n",
       "18                                             ['LTC']   \n",
       "19                                 ['PA', 'POS', 'PA']   \n",
       "20                                   ['DTTool', 'GUI']   \n",
       "21                                             ['AER']   \n",
       "22                                              ['RE']   \n",
       "23                                             ['MRR']   \n",
       "24                                              ['FU']   \n",
       "25                           ['MS', 'PP', 'MDI', 'MS']   \n",
       "26                                    ['CVPR', 'IEEE']   \n",
       "27                                             ['SVM']   \n",
       "28                                             ['SPE']   \n",
       "29                                             ['P+R']   \n",
       "..                                                 ...   \n",
       "467                                            ['CQA']   \n",
       "468                                 ['NW', 'BN', 'BC']   \n",
       "469                                            ['VPT']   \n",
       "470                                             ['AR']   \n",
       "471                                  ['FN', 'SemEval']   \n",
       "472                                            ['PoS']   \n",
       "473                                            ['POS']   \n",
       "474                           ['PH', 'WD', 'SY', 'SP']   \n",
       "475            ['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']   \n",
       "476                                           ['ODPs']   \n",
       "477  ['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'L...   \n",
       "478                                       ['MT', 'MT']   \n",
       "479                                           ['DLED']   \n",
       "480  [\"MHG's\", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL...   \n",
       "481                                          [\"CSI's\"]   \n",
       "482                                            ['TBL']   \n",
       "483                                         ['MaxEnt']   \n",
       "484                                          ['CSHMM']   \n",
       "485                            ['CHIEU-NG', 'FLORIAN']   \n",
       "486                                             ['QA']   \n",
       "487                                          ['EMNLP']   \n",
       "488                                       ['KB', 'QA']   \n",
       "489                       ['AbT', 'ArT', 'Aut', 'Jou']   \n",
       "490                                            ['L2R']   \n",
       "491                                            ['TMs']   \n",
       "492                                     ['AGC', 'AGC']   \n",
       "493                                             ['OR']   \n",
       "494                               ['NN', 'ADJA', 'NE']   \n",
       "495                                            ['SPY']   \n",
       "496                                            ['IPS']   \n",
       "\n",
       "                                       long-forms-text  \n",
       "0                                 ['Text Data Mining']  \n",
       "1                           ['Mean Average Precision']  \n",
       "2     ['sequential dependence', 'Markov random field']  \n",
       "3                        ['pseudo relevance feedback']  \n",
       "4                                  ['proper', 'verbs']  \n",
       "5    ['Minimum Phone Error', 'Maximum Likelihood Li...  \n",
       "6        ['Structured Weighted Violations Perceptron']  \n",
       "7                              ['Chomsky Normal Form']  \n",
       "8                                ['Probabilistic CFG']  \n",
       "9                          ['Language Neutral Syntax']  \n",
       "10       ['Base+Appositives', 'FrameNet', 'Verb Pair']  \n",
       "11                     ['natural language generation']  \n",
       "12             ['labeled recall', 'labeled precision']  \n",
       "13                                ['Response Planner']  \n",
       "14   ['truncated', 'terse information', 'interrupte...  \n",
       "15                         ['Support Vector Machines']  \n",
       "16                         ['an adverb', 'verb group']  \n",
       "17                   ['posttraumatic stress disorder']  \n",
       "18                  ['Language Technology Components']  \n",
       "19                                ['product attribut']  \n",
       "20                          ['Discourse Tagging Tool']  \n",
       "21                            ['Alignment Error Rate']  \n",
       "22                             ['relation extraction']  \n",
       "23                            ['Mean Reciprocal Rank']  \n",
       "24                          ['Functional unification']  \n",
       "25     ['Missed Samples', 'Perplex', 'Missed Samples']  \n",
       "26         ['Computer Vision and Pattern Recognition']  \n",
       "27                          ['Support Vector Machine']  \n",
       "28                        ['Sound Pattern of English']  \n",
       "29                                           ['2*P*R']  \n",
       "..                                                 ...  \n",
       "467                   ['Community question answering']  \n",
       "468  ['newswire', 'broadcast news', 'broadcast conv...  \n",
       "469                                      ['viewpoint']  \n",
       "470                            ['anaphora resolution']  \n",
       "471                                       ['FrameNet']  \n",
       "472                                 ['part of speech']  \n",
       "473                                 ['part of speech']  \n",
       "474                ['Phonetic', 'Symbols', 'Spelling']  \n",
       "475  ['Radiology and', 'Discharge Summary', 'Echoca...  \n",
       "476                                 ['overt displays']  \n",
       "477  ['facility', 'organization', 'geo-political en...  \n",
       "478                            ['machine translation']  \n",
       "479             ['Double Levenshtein?s Edit Distance']  \n",
       "480                         ['Modified Head Grammars']  \n",
       "481                     ['concept sequence instances']  \n",
       "482                  ['Transformation Based Learning']  \n",
       "483                                ['Maximum Entropy']  \n",
       "484                              ['Class Speaker HMM']  \n",
       "485                        ['Chieu and Ng', 'Florian']  \n",
       "486                             ['Question Answering']  \n",
       "487  ['Empirical Methods in Natural Language Proces...  \n",
       "488                                 ['Knowledge Base']  \n",
       "489     ['abstracts', 'titles', 'authors', 'Journals']  \n",
       "490                               ['learning to rank']  \n",
       "491                                   ['Topic models']  \n",
       "492                 ['Automatic Genre Classification']  \n",
       "493                                  ['Omission Rate']  \n",
       "494                     ['noun', 'adjectives', 'name']  \n",
       "495                        ['Sentence Position Yield']  \n",
       "496                     ['inflectional property sets']  \n",
       "\n",
       "[497 rows x 6 columns]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Y_true</th>\n",
       "      <th>Y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1 : Mean A...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baselines are a unigram query likelihood ( QL ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'I-LF', 'I-LF', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tion . Then , we extract expansion terms from ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person , mood , voice and case , CATiB uses 6 ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>( LDA ) , Maximum Likelihood Linear Transform ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'I-LF', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gold label ( Section 3 ) . Our algorithm is ca...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase markers or words . For simplicity of ma...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When a CFG is associated with probabilistic in...</td>\n",
       "      <td>['B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', ...</td>\n",
       "      <td>['B-O', 'B-LF', 'I-AN', 'B-O', 'B-O', 'B-O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1 ] proposed a language-neutral framework for ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12 Base+FrameNet ( FN ) 61.8 71.9 66.5 59.8 69...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model 3 ( Figure 3 ) illustrates how the sourc...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The big blue door. ? In this case , the GrM as...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TI = terse information INT = interrupted TRUN ...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-AN', 'B-O',...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'B-LF', 'B-AN', 'B-O',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>as in figure 3 . It is parsed as an adverb ( A...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The tagging has been done using a GUI-based to...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Och and Ney ( 2003 ) show that for larger corp...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2 Relation Extraction System In this section ,...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>target question , or zero if the target questi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>discussion in Ritchie ( 1984 ) . Functional un...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bigram Perplex . ( PP ) MDI Missed Samples ( M...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "      <td>['B-O', 'I-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>networks . In Proceedings of the IEEE Conferen...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Therefore , identification methods like Tsuchi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ents ) * 100 % ? F1-score = 2 * P * R / ( P+R ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>eugene @ mathcs.emory.edu Abstract Community q...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>means a type of source ? newswire ( NW ) , bro...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>either lexically encoded , or depends on the i...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Abstract This paper attempts to use an off-the...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>The SemEval ? 2007 task for extracting frame s...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>hauer , haver , haber ) and the corpus does no...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>The ungrammatical distracter , e.g. , are in F...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Words/Phrases as Themselves ( WD ) Symbols/Non...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>DS = Discharge Summary , Echo = Echocardiogram...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-LF',...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'B-LF', 'B-O', 'B-O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>mdiab @ ccls.columbia.edu Abstract We analyze ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>( PERS ) , organization ( ORG ) , geo-politica...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Abstract In this paper , we propose a new synt...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>The second one is a variant that we named Doub...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>by HG 's ( HL ) . In particular , we show that...</td>\n",
       "      <td>['B-O', 'B-AN', 'I-AN', 'B-O', 'B-AN', 'B-O', ...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>( Ramshaw and Marcus , 1995 ) approached chuck...</td>\n",
       "      <td>['B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>demonstrate such dependencies . The Maximum En...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>dialogues categorized into multiple domains , ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>shared task , namely FLORIAN ( Florian et al. ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1 Introduction Open-domain Question Answering ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'I-LF', 'B-O', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>229 Proceedings of the 2014 Conference on Empi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>th fo frture representations abstracts ( AbT )...</td>\n",
       "      <td>['B-LF', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>787 After labeling the reference BINet , we tr...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>for the annotation process . Topic models ( TM...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Evidence for a text ? s topic and genre comes ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-LF', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate ( OR ) Co</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>The query ( Figure 4a ) will match adjectives ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>the middle ( Baxendale , 1958 ) . Sentence Pos...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>By contrast , our approach operates at the lev...</td>\n",
       "      <td>['B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0    2 Related Work The availability of emotion-ric...   \n",
       "1    WebDict 0.2919 Backoff 0.3282 Table 1 : Mean A...   \n",
       "2    Baselines are a unigram query likelihood ( QL ...   \n",
       "3    tion . Then , we extract expansion terms from ...   \n",
       "4    person , mood , voice and case , CATiB uses 6 ...   \n",
       "5    ( LDA ) , Maximum Likelihood Linear Transform ...   \n",
       "6    gold label ( Section 3 ) . Our algorithm is ca...   \n",
       "7    phrase markers or words . For simplicity of ma...   \n",
       "8    When a CFG is associated with probabilistic in...   \n",
       "9    1 ] proposed a language-neutral framework for ...   \n",
       "10   12 Base+FrameNet ( FN ) 61.8 71.9 66.5 59.8 69...   \n",
       "11   Model 3 ( Figure 3 ) illustrates how the sourc...   \n",
       "12   in the V column indicates that the verb condit...   \n",
       "13   The big blue door. ? In this case , the GrM as...   \n",
       "14   TI = terse information INT = interrupted TRUN ...   \n",
       "15   classes in both original text and main text co...   \n",
       "16   as in figure 3 . It is parsed as an adverb ( A...   \n",
       "17   can develop after exposure to a terrifying eve...   \n",
       "18   It allows for testing interaction scenarios th...   \n",
       "19   Feature Description lexical the words of the p...   \n",
       "20   The tagging has been done using a GUI-based to...   \n",
       "21   Och and Ney ( 2003 ) show that for larger corp...   \n",
       "22   2 Relation Extraction System In this section ,...   \n",
       "23   target question , or zero if the target questi...   \n",
       "24   discussion in Ritchie ( 1984 ) . Functional un...   \n",
       "25   Bigram Perplex . ( PP ) MDI Missed Samples ( M...   \n",
       "26   networks . In Proceedings of the IEEE Conferen...   \n",
       "27   Therefore , identification methods like Tsuchi...   \n",
       "28   domain-independent mpirical induction algorith...   \n",
       "29   ents ) * 100 % ? F1-score = 2 * P * R / ( P+R ...   \n",
       "..                                                 ...   \n",
       "467  eugene @ mathcs.emory.edu Abstract Community q...   \n",
       "468  means a type of source ? newswire ( NW ) , bro...   \n",
       "469  either lexically encoded , or depends on the i...   \n",
       "470  Abstract This paper attempts to use an off-the...   \n",
       "471  The SemEval ? 2007 task for extracting frame s...   \n",
       "472  hauer , haver , haber ) and the corpus does no...   \n",
       "473  The ungrammatical distracter , e.g. , are in F...   \n",
       "474  Words/Phrases as Themselves ( WD ) Symbols/Non...   \n",
       "475  DS = Discharge Summary , Echo = Echocardiogram...   \n",
       "476  mdiab @ ccls.columbia.edu Abstract We analyze ...   \n",
       "477  ( PERS ) , organization ( ORG ) , geo-politica...   \n",
       "478  Abstract In this paper , we propose a new synt...   \n",
       "479  The second one is a variant that we named Doub...   \n",
       "480  by HG 's ( HL ) . In particular , we show that...   \n",
       "481  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482  ( Ramshaw and Marcus , 1995 ) approached chuck...   \n",
       "483  demonstrate such dependencies . The Maximum En...   \n",
       "484  dialogues categorized into multiple domains , ...   \n",
       "485  shared task , namely FLORIAN ( Florian et al. ...   \n",
       "486  1 Introduction Open-domain Question Answering ...   \n",
       "487  229 Proceedings of the 2014 Conference on Empi...   \n",
       "488  1 Introduction Large-scale open-domain questio...   \n",
       "489  th fo frture representations abstracts ( AbT )...   \n",
       "490  787 After labeling the reference BINet , we tr...   \n",
       "491  for the annotation process . Topic models ( TM...   \n",
       "492  Evidence for a text ? s topic and genre comes ...   \n",
       "493   0.35 0.25 0.50 0.75 1.00 Omission Rate ( OR ) Co   \n",
       "494  The query ( Figure 4a ) will match adjectives ...   \n",
       "495  the middle ( Baxendale , 1958 ) . Sentence Pos...   \n",
       "496  By contrast , our approach operates at the lev...   \n",
       "\n",
       "                                                Y_true  \\\n",
       "0    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "1    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "2    ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "3    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "4    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "5    ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...   \n",
       "6    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "7    ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...   \n",
       "8    ['B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', ...   \n",
       "9    ['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...   \n",
       "10   ['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...   \n",
       "11   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "12   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "13   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "14   ['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-AN', 'B-O',...   \n",
       "15   ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "16   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "17   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...   \n",
       "18   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "19   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "20   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "21   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "22   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "23   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "24   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "25   ['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...   \n",
       "26   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "27   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "28   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "29   ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "..                                                 ...   \n",
       "467  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...   \n",
       "468  ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "469  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "470  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "471  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "472  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "473  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "474  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...   \n",
       "475  ['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-LF',...   \n",
       "476  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "477  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...   \n",
       "478  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "479  ['B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', '...   \n",
       "480  ['B-O', 'B-AN', 'I-AN', 'B-O', 'B-AN', 'B-O', ...   \n",
       "481  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "482  ['B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "483  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "484  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "485  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...   \n",
       "486  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...   \n",
       "487  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "488  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "489  ['B-LF', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', '...   \n",
       "490  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "491  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "492  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...   \n",
       "493  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "494  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "495  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "496  ['B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "\n",
       "                                                Y_pred  \n",
       "0    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "1    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "2    ['B-O', 'B-O', 'B-LF', 'B-O', 'I-LF', 'I-LF', ...  \n",
       "3    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "4    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "5    ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'I-LF', ...  \n",
       "6    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "7    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "8    ['B-O', 'B-LF', 'I-AN', 'B-O', 'B-O', 'B-O', '...  \n",
       "9    ['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...  \n",
       "10   ['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...  \n",
       "11   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "12   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "13   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "14   ['B-AN', 'B-O', 'B-LF', 'B-LF', 'B-AN', 'B-O',...  \n",
       "15   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "16   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "17   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...  \n",
       "18   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "19   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "20   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "21   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "22   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "23   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "24   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "25   ['B-O', 'I-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...  \n",
       "26   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "27   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "28   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "29   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "..                                                 ...  \n",
       "467  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...  \n",
       "468  ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...  \n",
       "469  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "470  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "471  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "472  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "473  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "474  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-AN', 'B-O', '...  \n",
       "475  ['B-AN', 'B-O', 'B-LF', 'B-LF', 'B-O', 'B-O', ...  \n",
       "476  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "477  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...  \n",
       "478  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "479  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...  \n",
       "480  ['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...  \n",
       "481  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "482  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "483  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...  \n",
       "484  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "485  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...  \n",
       "486  ['B-O', 'B-O', 'B-O', 'B-O', 'I-LF', 'B-O', 'B...  \n",
       "487  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "488  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "489  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "490  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B...  \n",
       "491  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...  \n",
       "492  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-LF', '...  \n",
       "493  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...  \n",
       "494  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "495  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "496  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...  \n",
       "\n",
       "[497 rows x 3 columns]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred[\"Text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred[\"Y_true\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred[\"Y_pred\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(df):\n",
    "    df_ = df\n",
    "    text_l   = list(df[\"Text\"])\n",
    "    y_true_l = list(df['Y_true'])\n",
    "    y_pred_l = list(df[\"Y_pred\"])\n",
    "    \n",
    "    cnt_text_l   = []\n",
    "    cnt_y_true_l = []\n",
    "    cnt_y_pred_l = []\n",
    "    for text, y_true, y_pred in zip(text_l, y_true_l, y_pred_l):\n",
    "        cnt_text_l.append(len(text.split(\" \")))\n",
    "        cnt_y_true_l.append(len(ast.literal_eval(y_true)))\n",
    "        cnt_y_pred_l.append(len(ast.literal_eval(y_pred)))\n",
    "        \n",
    "    df_[\"cnt_text\"] = cnt_text_l\n",
    "    df_[\"cnt_y_true\"] = cnt_y_true_l\n",
    "    df_[\"cnt_y_pred\"] = cnt_y_pred_l\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Y_true</th>\n",
       "      <th>Y_pred</th>\n",
       "      <th>cnt_text</th>\n",
       "      <th>cnt_y_true</th>\n",
       "      <th>cnt_y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1 : Mean A...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baselines are a unigram query likelihood ( QL ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'I-LF', 'I-LF', ...</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tion . Then , we extract expansion terms from ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person , mood , voice and case , CATiB uses 6 ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>( LDA ) , Maximum Likelihood Linear Transform ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'I-LF', ...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gold label ( Section 3 ) . Our algorithm is ca...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>phrase markers or words . For simplicity of ma...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When a CFG is associated with probabilistic in...</td>\n",
       "      <td>['B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', ...</td>\n",
       "      <td>['B-O', 'B-LF', 'I-AN', 'B-O', 'B-O', 'B-O', '...</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1 ] proposed a language-neutral framework for ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12 Base+FrameNet ( FN ) 61.8 71.9 66.5 59.8 69...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Model 3 ( Figure 3 ) illustrates how the sourc...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The big blue door. ? In this case , the GrM as...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TI = terse information INT = interrupted TRUN ...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-AN', 'B-O',...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'B-LF', 'B-AN', 'B-O',...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>as in figure 3 . It is parsed as an adverb ( A...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The tagging has been done using a GUI-based to...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Och and Ney ( 2003 ) show that for larger corp...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2 Relation Extraction System In this section ,...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>target question , or zero if the target questi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>discussion in Ritchie ( 1984 ) . Functional un...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Bigram Perplex . ( PP ) MDI Missed Samples ( M...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "      <td>['B-O', 'I-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>networks . In Proceedings of the IEEE Conferen...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Therefore , identification methods like Tsuchi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ents ) * 100 % ? F1-score = 2 * P * R / ( P+R ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>eugene @ mathcs.emory.edu Abstract Community q...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>means a type of source ? newswire ( NW ) , bro...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>either lexically encoded , or depends on the i...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Abstract This paper attempts to use an off-the...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>The SemEval ? 2007 task for extracting frame s...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>hauer , haver , haber ) and the corpus does no...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>The ungrammatical distracter , e.g. , are in F...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Words/Phrases as Themselves ( WD ) Symbols/Non...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>DS = Discharge Summary , Echo = Echocardiogram...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-LF',...</td>\n",
       "      <td>['B-AN', 'B-O', 'B-LF', 'B-LF', 'B-O', 'B-O', ...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>mdiab @ ccls.columbia.edu Abstract We analyze ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>( PERS ) , organization ( ORG ) , geo-politica...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Abstract In this paper , we propose a new synt...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>The second one is a variant that we named Doub...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>by HG 's ( HL ) . In particular , we show that...</td>\n",
       "      <td>['B-O', 'B-AN', 'I-AN', 'B-O', 'B-AN', 'B-O', ...</td>\n",
       "      <td>['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>( Ramshaw and Marcus , 1995 ) approached chuck...</td>\n",
       "      <td>['B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>demonstrate such dependencies . The Maximum En...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>dialogues categorized into multiple domains , ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>shared task , namely FLORIAN ( Florian et al. ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1 Introduction Open-domain Question Answering ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'I-LF', 'B-O', 'B...</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>229 Proceedings of the 2014 Conference on Empi...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>th fo frture representations abstracts ( AbT )...</td>\n",
       "      <td>['B-LF', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>787 After labeling the reference BINet , we tr...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>for the annotation process . Topic models ( TM...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Evidence for a text ? s topic and genre comes ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...</td>\n",
       "      <td>['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-LF', '...</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate ( OR ) Co</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>The query ( Figure 4a ) will match adjectives ...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>the middle ( Baxendale , 1958 ) . Sentence Pos...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>By contrast , our approach operates at the lev...</td>\n",
       "      <td>['B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B...</td>\n",
       "      <td>['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "0    2 Related Work The availability of emotion-ric...   \n",
       "1    WebDict 0.2919 Backoff 0.3282 Table 1 : Mean A...   \n",
       "2    Baselines are a unigram query likelihood ( QL ...   \n",
       "3    tion . Then , we extract expansion terms from ...   \n",
       "4    person , mood , voice and case , CATiB uses 6 ...   \n",
       "5    ( LDA ) , Maximum Likelihood Linear Transform ...   \n",
       "6    gold label ( Section 3 ) . Our algorithm is ca...   \n",
       "7    phrase markers or words . For simplicity of ma...   \n",
       "8    When a CFG is associated with probabilistic in...   \n",
       "9    1 ] proposed a language-neutral framework for ...   \n",
       "10   12 Base+FrameNet ( FN ) 61.8 71.9 66.5 59.8 69...   \n",
       "11   Model 3 ( Figure 3 ) illustrates how the sourc...   \n",
       "12   in the V column indicates that the verb condit...   \n",
       "13   The big blue door. ? In this case , the GrM as...   \n",
       "14   TI = terse information INT = interrupted TRUN ...   \n",
       "15   classes in both original text and main text co...   \n",
       "16   as in figure 3 . It is parsed as an adverb ( A...   \n",
       "17   can develop after exposure to a terrifying eve...   \n",
       "18   It allows for testing interaction scenarios th...   \n",
       "19   Feature Description lexical the words of the p...   \n",
       "20   The tagging has been done using a GUI-based to...   \n",
       "21   Och and Ney ( 2003 ) show that for larger corp...   \n",
       "22   2 Relation Extraction System In this section ,...   \n",
       "23   target question , or zero if the target questi...   \n",
       "24   discussion in Ritchie ( 1984 ) . Functional un...   \n",
       "25   Bigram Perplex . ( PP ) MDI Missed Samples ( M...   \n",
       "26   networks . In Proceedings of the IEEE Conferen...   \n",
       "27   Therefore , identification methods like Tsuchi...   \n",
       "28   domain-independent mpirical induction algorith...   \n",
       "29   ents ) * 100 % ? F1-score = 2 * P * R / ( P+R ...   \n",
       "..                                                 ...   \n",
       "467  eugene @ mathcs.emory.edu Abstract Community q...   \n",
       "468  means a type of source ? newswire ( NW ) , bro...   \n",
       "469  either lexically encoded , or depends on the i...   \n",
       "470  Abstract This paper attempts to use an off-the...   \n",
       "471  The SemEval ? 2007 task for extracting frame s...   \n",
       "472  hauer , haver , haber ) and the corpus does no...   \n",
       "473  The ungrammatical distracter , e.g. , are in F...   \n",
       "474  Words/Phrases as Themselves ( WD ) Symbols/Non...   \n",
       "475  DS = Discharge Summary , Echo = Echocardiogram...   \n",
       "476  mdiab @ ccls.columbia.edu Abstract We analyze ...   \n",
       "477  ( PERS ) , organization ( ORG ) , geo-politica...   \n",
       "478  Abstract In this paper , we propose a new synt...   \n",
       "479  The second one is a variant that we named Doub...   \n",
       "480  by HG 's ( HL ) . In particular , we show that...   \n",
       "481  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482  ( Ramshaw and Marcus , 1995 ) approached chuck...   \n",
       "483  demonstrate such dependencies . The Maximum En...   \n",
       "484  dialogues categorized into multiple domains , ...   \n",
       "485  shared task , namely FLORIAN ( Florian et al. ...   \n",
       "486  1 Introduction Open-domain Question Answering ...   \n",
       "487  229 Proceedings of the 2014 Conference on Empi...   \n",
       "488  1 Introduction Large-scale open-domain questio...   \n",
       "489  th fo frture representations abstracts ( AbT )...   \n",
       "490  787 After labeling the reference BINet , we tr...   \n",
       "491  for the annotation process . Topic models ( TM...   \n",
       "492  Evidence for a text ? s topic and genre comes ...   \n",
       "493   0.35 0.25 0.50 0.75 1.00 Omission Rate ( OR ) Co   \n",
       "494  The query ( Figure 4a ) will match adjectives ...   \n",
       "495  the middle ( Baxendale , 1958 ) . Sentence Pos...   \n",
       "496  By contrast , our approach operates at the lev...   \n",
       "\n",
       "                                                Y_true  \\\n",
       "0    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "1    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "2    ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "3    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "4    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "5    ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...   \n",
       "6    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "7    ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...   \n",
       "8    ['B-O', 'B-LF', 'I-LF', 'I-LF', 'B-O', 'B-O', ...   \n",
       "9    ['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...   \n",
       "10   ['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...   \n",
       "11   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "12   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "13   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "14   ['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-AN', 'B-O',...   \n",
       "15   ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "16   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "17   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...   \n",
       "18   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "19   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "20   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "21   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "22   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "23   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "24   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "25   ['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...   \n",
       "26   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "27   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "28   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "29   ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "..                                                 ...   \n",
       "467  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...   \n",
       "468  ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "469  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "470  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "471  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "472  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "473  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "474  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...   \n",
       "475  ['B-AN', 'B-O', 'B-LF', 'I-LF', 'B-O', 'B-LF',...   \n",
       "476  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "477  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...   \n",
       "478  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "479  ['B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', 'B-O', '...   \n",
       "480  ['B-O', 'B-AN', 'I-AN', 'B-O', 'B-AN', 'B-O', ...   \n",
       "481  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "482  ['B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "483  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "484  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "485  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...   \n",
       "486  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...   \n",
       "487  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "488  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "489  ['B-LF', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', '...   \n",
       "490  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "491  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "492  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'I-LF', '...   \n",
       "493  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...   \n",
       "494  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "495  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...   \n",
       "496  ['B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B-O', 'B...   \n",
       "\n",
       "                                                Y_pred  cnt_text  cnt_y_true  \\\n",
       "0    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        32          32   \n",
       "1    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        18          18   \n",
       "2    ['B-O', 'B-O', 'B-LF', 'B-O', 'I-LF', 'I-LF', ...        39          39   \n",
       "3    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        23          23   \n",
       "4    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        42          42   \n",
       "5    ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'I-LF', ...        29          29   \n",
       "6    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        31          31   \n",
       "7    ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        34          34   \n",
       "8    ['B-O', 'B-LF', 'I-AN', 'B-O', 'B-O', 'B-O', '...        29          29   \n",
       "9    ['B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B...        25          25   \n",
       "10   ['B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B-O', 'B...        70          70   \n",
       "11   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        35          35   \n",
       "12   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        26          26   \n",
       "13   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        31          31   \n",
       "14   ['B-AN', 'B-O', 'B-LF', 'B-LF', 'B-AN', 'B-O',...        17          17   \n",
       "15   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        27          27   \n",
       "16   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        26          26   \n",
       "17   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B...        41          41   \n",
       "18   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        18          18   \n",
       "19   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        20          20   \n",
       "20   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        27          27   \n",
       "21   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        28          28   \n",
       "22   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        24          24   \n",
       "23   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        32          32   \n",
       "24   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        21          21   \n",
       "25   ['B-O', 'I-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...        18          18   \n",
       "26   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        18          18   \n",
       "27   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        27          27   \n",
       "28   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        40          40   \n",
       "29   ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        24          24   \n",
       "..                                                 ...       ...         ...   \n",
       "467  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I-LF', '...        17          17   \n",
       "468  ['B-O', 'B-LF', 'B-O', 'B-O', 'B-O', 'B-O', 'B...        24          24   \n",
       "469  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        27          27   \n",
       "470  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        17          17   \n",
       "471  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        26          26   \n",
       "472  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        28          28   \n",
       "473  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        26          26   \n",
       "474  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-AN', 'B-O', '...        19          19   \n",
       "475  ['B-AN', 'B-O', 'B-LF', 'B-LF', 'B-O', 'B-O', ...        31          31   \n",
       "476  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        17          17   \n",
       "477  ['B-O', 'B-AN', 'B-O', 'B-O', 'B-LF', 'B-O', '...        40          40   \n",
       "478  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        25          25   \n",
       "479  ['B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'B-O', 'B...        26          26   \n",
       "480  ['B-O', 'B-LF', 'B-O', 'B-O', 'B-AN', 'B-O', '...        47          47   \n",
       "481  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        24          24   \n",
       "482  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        18          18   \n",
       "483  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...        24          24   \n",
       "484  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        31          31   \n",
       "485  ['B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B-O', 'B...        22          22   \n",
       "486  ['B-O', 'B-O', 'B-O', 'B-O', 'I-LF', 'B-O', 'B...        16          16   \n",
       "487  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        31          31   \n",
       "488  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        24          24   \n",
       "489  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        27          27   \n",
       "490  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-AN', 'B...        28          28   \n",
       "491  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...        15          15   \n",
       "492  ['B-O', 'B-O', 'B-LF', 'B-O', 'B-O', 'B-LF', '...        42          42   \n",
       "493  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-LF', 'I...        11          11   \n",
       "494  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        38          38   \n",
       "495  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        23          23   \n",
       "496  ['B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-O', 'B-...        28          28   \n",
       "\n",
       "     cnt_y_pred  \n",
       "0            32  \n",
       "1            18  \n",
       "2            39  \n",
       "3            23  \n",
       "4            42  \n",
       "5            29  \n",
       "6            31  \n",
       "7            34  \n",
       "8            29  \n",
       "9            25  \n",
       "10           70  \n",
       "11           35  \n",
       "12           26  \n",
       "13           31  \n",
       "14           17  \n",
       "15           27  \n",
       "16           26  \n",
       "17           41  \n",
       "18           18  \n",
       "19           20  \n",
       "20           27  \n",
       "21           28  \n",
       "22           24  \n",
       "23           32  \n",
       "24           21  \n",
       "25           18  \n",
       "26           18  \n",
       "27           27  \n",
       "28           40  \n",
       "29           24  \n",
       "..          ...  \n",
       "467          17  \n",
       "468          24  \n",
       "469          27  \n",
       "470          17  \n",
       "471          26  \n",
       "472          28  \n",
       "473          26  \n",
       "474          19  \n",
       "475          31  \n",
       "476          17  \n",
       "477          40  \n",
       "478          25  \n",
       "479          26  \n",
       "480          47  \n",
       "481          24  \n",
       "482          18  \n",
       "483          24  \n",
       "484          31  \n",
       "485          22  \n",
       "486          16  \n",
       "487          31  \n",
       "488          24  \n",
       "489          27  \n",
       "490          28  \n",
       "491          15  \n",
       "492          42  \n",
       "493          11  \n",
       "494          38  \n",
       "495          23  \n",
       "496          28  \n",
       "\n",
       "[497 rows x 6 columns]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_cnt = count(df_pred)\n",
    "df_pred_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred_cnt.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred_cnt.iloc[2].Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred_cnt.iloc[2].Y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred_cnt.iloc[2].Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [start: end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.iloc[2][\"acronyms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_orig.iloc[2][\"acronyms-text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(list(df_pred[\"cnt_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(list(df_pred[\"cnt_y_true\"])), max(list(df_pred[\"cnt_y_pred\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = '[2, 4, 5]'\n",
    "# type(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ast.literal_eval(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(df_orig, df_pred):\n",
    "    acronyms = []\n",
    "    long_forms = []\n",
    "    \n",
    "    acronyms_idxs = []\n",
    "    long_forms_idxs = []\n",
    "    \n",
    "    cnt = df_pred.shape[0]\n",
    "    #print(cnt)\n",
    "    \n",
    "    for i in range(cnt):\n",
    "        acronym = []\n",
    "        long_form = []\n",
    "        \n",
    "        an_idx = []\n",
    "        lf_idx = []\n",
    "        \n",
    "        text_l = df_pred.iloc[i][\"Text\"].strip().split(\" \")\n",
    "        text_orig = df_orig.iloc[i][\"text\"].strip()\n",
    "        #print(text_orig)\n",
    "        y_true = df_pred.iloc[i][\"Y_true\"]\n",
    "        y_true_l = ast.literal_eval(y_true)\n",
    "        \n",
    "        y_pred = df_pred.iloc[i][\"Y_pred\"]\n",
    "        y_pred_l = ast.literal_eval(y_pred)\n",
    "        \n",
    "        #print(text_l)\n",
    "        #print(y_true_l)\n",
    "        #print(y_pred_l)\n",
    "        AN_start = False\n",
    "        LF_start = False\n",
    "        \n",
    "        #print(len(y_true))\n",
    "        AN = \"\"\n",
    "        LF = \"\"\n",
    "        for j in range(len(y_pred_l)):\n",
    "            #LF = \"\"\n",
    "            \n",
    "#             if(y_pred_l[j]==\"B-O\"):\n",
    "#                 continue\n",
    "            \n",
    "            if(y_pred_l[j] == \"B-AN\" and AN_start == False ):\n",
    "                AN = AN + text_l[j]\n",
    "                AN_start = True\n",
    "            elif(y_pred_l[j] == \"I-AN\" and AN_start == True):\n",
    "                AN = AN + text_l[j]\n",
    "            elif(y_pred_l[j]==\"B-AN\" and AN_start == True):\n",
    "                acronym.append(AN)\n",
    "                AN = text_l[j]\n",
    "                #print(AN)\n",
    "            elif(AN_start == True and AN != \"\"):\n",
    "                acronym.append(AN)\n",
    "                AN = \"\"\n",
    "                AN_start = False\n",
    "                \n",
    "             \n",
    "            if(y_pred_l[j] == \"B-LF\" and LF_start == False ):\n",
    "                LF = LF + text_l[j]\n",
    "                LF_start = True\n",
    "            elif(y_pred_l[j] == \"I-LF\" and LF_start == False):\n",
    "                LF = LF + text_l[j]\n",
    "                LF_start = True\n",
    "            elif(y_pred_l[j] == \"I-LF\" and LF_start == True):\n",
    "                LF = LF + \" \" + text_l[j]\n",
    "            elif(y_pred_l[j]==\"B-LF\" and LF_start == True):\n",
    "                long_form.append(LF)\n",
    "                LF = text_l[j]\n",
    "            elif(LF_start == True and LF != \"\"):\n",
    "                long_form.append(LF)\n",
    "                LF = \"\"\n",
    "                LF_start = False\n",
    "            \n",
    "#             if(y_pred_l[j]==\"B-AN\" or y_pred_l[j]==\"I-AN\"):\n",
    "#                 acronym.append(str(text_l[j]))\n",
    "#                 #print(acronym)\n",
    "            \n",
    "            \n",
    "#             elif(y_pred_l[j]==\"B-LF\" or y_pred_l[j]==\"I-LF\"):\n",
    "#                 long_form.append(str(text_l[j]))\n",
    "#             elif(y_pred_l[j]==\"B-LF\" and LF_start == False):\n",
    "#                 LF = LF + text_l[j]\n",
    "#                 LF_start = True\n",
    "#             elif(y_pred_l[j]==\"I-LF\" and LF_start == True):\n",
    "#                 LF = LF + \" \"+ texl_l[j]\n",
    "#             elif(y_pred_l[j] == \"B-LF\" and LF_start == True):\n",
    "#                 long_form.append(LF)\n",
    "#                 LF = \"\"\n",
    "#                 LF = LF + text_l[j]\n",
    "        \n",
    "        #print(acronym)\n",
    "        #print(long_form)\n",
    "        acronyms.append(acronym)\n",
    "        long_forms.append(long_form)\n",
    "        \n",
    "        #print(acronym)\n",
    "        for an in acronym:\n",
    "            #print(type(text_orig))\n",
    "            #print(an)\n",
    "            an = an.strip()\n",
    "            start_idx = text_orig.find(an)\n",
    "            end_idx = start_idx + len(an)\n",
    "            \n",
    "            an_idx.append([start_idx, end_idx])\n",
    "        \n",
    "        for lf in long_form:\n",
    "            #print(lf)\n",
    "            lf = lf.strip()\n",
    "            #print(lf)\n",
    "            start_idx = text_orig.find(lf)\n",
    "            end_idx = start_idx + len(lf)\n",
    "            \n",
    "            lf_idx.append([start_idx, end_idx])\n",
    "            \n",
    "        acronyms_idxs.append(an_idx)\n",
    "        long_forms_idxs.append(lf_idx)\n",
    "        \n",
    "    df_orig[\"AN_Pred\"] = acronyms\n",
    "    df_orig[\"LF_Pred\"] = long_forms\n",
    "    df_orig[\"AN_Pred_idxs\"] = acronyms_idxs\n",
    "    df_orig[\"LF_Pred_idxs\"] = long_forms_idxs\n",
    "    return df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(8):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "      <th>AN_Pred</th>\n",
       "      <th>LF_Pred</th>\n",
       "      <th>AN_Pred_idxs</th>\n",
       "      <th>LF_Pred_idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2 Related Work The availability of emotion-ric...</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[144, 160]]</td>\n",
       "      <td>['TDM']</td>\n",
       "      <td>['Text Data Mining']</td>\n",
       "      <td>[TDM]</td>\n",
       "      <td>[a, Text Data Mining]</td>\n",
       "      <td>[[162, 165]]</td>\n",
       "      <td>[[5, 6], [144, 160]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "      <td>['MAP']</td>\n",
       "      <td>['Mean Average Precision']</td>\n",
       "      <td>[MAP]</td>\n",
       "      <td>[Mean Average Precision]</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[39, 61]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Baselines are a unigram query likelihood (QL) ...</td>\n",
       "      <td>[[113, 115], [42, 45], [153, 156]]</td>\n",
       "      <td>[[90, 111], [132, 151]]</td>\n",
       "      <td>['SD', 'QL)', 'MRF']</td>\n",
       "      <td>['sequential dependence', 'Markov random field']</td>\n",
       "      <td>[QL, SD, MRF]</td>\n",
       "      <td>[a, query likelihood, a, sequential dependence...</td>\n",
       "      <td>[[42, 44], [113, 115], [153, 156]]</td>\n",
       "      <td>[[1, 2], [24, 40], [1, 2], [90, 111], [132, 151]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>tion. Then, we extract expansion terms from th...</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90]]</td>\n",
       "      <td>['PRF']</td>\n",
       "      <td>['pseudo relevance feedback']</td>\n",
       "      <td>[PRF]</td>\n",
       "      <td>[pseudo relevance feedback]</td>\n",
       "      <td>[[92, 95]]</td>\n",
       "      <td>[[65, 90]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>person, mood, voice and case, CATiB uses 6 POS...</td>\n",
       "      <td>[[130, 134], [30, 35], [43, 46], [53, 56], [15...</td>\n",
       "      <td>[[136, 142], [156, 161]]</td>\n",
       "      <td>['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...</td>\n",
       "      <td>['proper', 'verbs']</td>\n",
       "      <td>[CATiB, POS, NOM, PROP, VRB, VRB-PASS]</td>\n",
       "      <td>[non-proper nominals, proper nouns, verbs]</td>\n",
       "      <td>[[30, 35], [43, 46], [53, 56], [130, 134], [15...</td>\n",
       "      <td>[[58, 77], [136, 148], [122, 127]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>(LDA), Maximum Likelihood Linear Transform (ML...</td>\n",
       "      <td>[[115, 118], [44, 48], [87, 91]]</td>\n",
       "      <td>[[94, 113], [7, 42], [51, 85]]</td>\n",
       "      <td>['MPE', 'MLLT', 'BMMI']</td>\n",
       "      <td>['Minimum Phone Error', 'Maximum Likelihood Li...</td>\n",
       "      <td>[LDA, MLLT, BMMI, MPE]</td>\n",
       "      <td>[Maximum Likelihood Linear Transform, Boosted ...</td>\n",
       "      <td>[[1, 4], [44, 48], [87, 91], [115, 118]]</td>\n",
       "      <td>[[7, 42], [51, 85], [94, 113]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>gold label (Section 3). Our algorithm is calle...</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93]]</td>\n",
       "      <td>['SWVP']</td>\n",
       "      <td>['Structured Weighted Violations Perceptron']</td>\n",
       "      <td>[SWVP]</td>\n",
       "      <td>[Structured Weighted Violations Perceptron, a]</td>\n",
       "      <td>[[95, 99]]</td>\n",
       "      <td>[[52, 93], [6, 7]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>phrase markers or words. For simplicity of man...</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[128, 147]]</td>\n",
       "      <td>['CNF']</td>\n",
       "      <td>['Chomsky Normal Form']</td>\n",
       "      <td>[CNF]</td>\n",
       "      <td>[Chomsky Normal Form]</td>\n",
       "      <td>[[149, 152]]</td>\n",
       "      <td>[[128, 147]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>When a CFG is associated with probabilistic i...</td>\n",
       "      <td>[[85, 89], [8, 11]]</td>\n",
       "      <td>[[66, 83]]</td>\n",
       "      <td>['PCFG', 'CFG']</td>\n",
       "      <td>['Probabilistic CFG']</td>\n",
       "      <td>[PCFG]</td>\n",
       "      <td>[a, a Probabilistic CFG]</td>\n",
       "      <td>[[84, 88]]</td>\n",
       "      <td>[[5, 6], [63, 82]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1] proposed a language-neutral framework for r...</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[103, 126]]</td>\n",
       "      <td>['LNS']</td>\n",
       "      <td>['Language Neutral Syntax']</td>\n",
       "      <td>[LNS]</td>\n",
       "      <td>[a, Language Neutral Syntax]</td>\n",
       "      <td>[[128, 131]]</td>\n",
       "      <td>[[12, 13], [103, 126]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...</td>\n",
       "      <td>[[187, 189], [18, 20], [102, 104]]</td>\n",
       "      <td>[[169, 185], [8, 16], [90, 99]]</td>\n",
       "      <td>['AP', 'FN', 'VP']</td>\n",
       "      <td>['Base+Appositives', 'FrameNet', 'Verb Pair']</td>\n",
       "      <td>[FN, VP, AP]</td>\n",
       "      <td>[Base+Verb Pairs]</td>\n",
       "      <td>[[18, 20], [102, 104], [187, 189]]</td>\n",
       "      <td>[[85, 100]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Model 3 (Figure 3) illustrates how the source ...</td>\n",
       "      <td>[[127, 130], [64, 66]]</td>\n",
       "      <td>[[98, 125]]</td>\n",
       "      <td>['NLG', 'MT']</td>\n",
       "      <td>['natural language generation']</td>\n",
       "      <td>[MT, NLG]</td>\n",
       "      <td>[a natural language generation, a]</td>\n",
       "      <td>[[64, 66], [127, 130]]</td>\n",
       "      <td>[[96, 125], [26, 27]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>in the V column indicates that the verb condit...</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "      <td>['LR', 'LP']</td>\n",
       "      <td>['labeled recall', 'labeled precision']</td>\n",
       "      <td>[LR, LP]</td>\n",
       "      <td>[labeled recall, labeled precision]</td>\n",
       "      <td>[[86, 88], [107, 109]]</td>\n",
       "      <td>[[91, 105], [112, 129]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>The big blue door.?  In this case, the GrM ask...</td>\n",
       "      <td>[[71, 73], [39, 42], [117, 119], [125, 127]]</td>\n",
       "      <td>[[53, 69]]</td>\n",
       "      <td>['RP', 'GrM', 'UU', 'RP']</td>\n",
       "      <td>['Response Planner']</td>\n",
       "      <td>[GrM, RP, UU]</td>\n",
       "      <td>[Response Planner]</td>\n",
       "      <td>[[39, 42], [71, 73], [117, 119]]</td>\n",
       "      <td>[[53, 69]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>TI = terse information  INT = interrupted  TRU...</td>\n",
       "      <td>[[43, 47], [0, 2], [24, 27], [61, 66]]</td>\n",
       "      <td>[[50, 59], [5, 22], [30, 41], [69, 79]]</td>\n",
       "      <td>['TRUN', 'TI', 'INT', 'TRANS']</td>\n",
       "      <td>['truncated', 'terse information', 'interrupte...</td>\n",
       "      <td>[TI, INT, TRUN, TRANS]</td>\n",
       "      <td>[terse, information, interrupted, truncated, t...</td>\n",
       "      <td>[[0, 2], [24, 27], [43, 47], [61, 66]]</td>\n",
       "      <td>[[5, 10], [11, 22], [30, 41], [50, 59], [69, 88]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>classes in both original text and main text co...</td>\n",
       "      <td>[[85, 88], [140, 143]]</td>\n",
       "      <td>[[60, 83]]</td>\n",
       "      <td>['SVM', 'AGI']</td>\n",
       "      <td>['Support Vector Machines']</td>\n",
       "      <td>[SVM]</td>\n",
       "      <td>[Support Vector Machines]</td>\n",
       "      <td>[[85, 88]]</td>\n",
       "      <td>[[60, 83]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>as in figure 3.  It is parsed as an adverb (AA...</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[[33, 42], [72, 82]]</td>\n",
       "      <td>['AA', 'VG']</td>\n",
       "      <td>['an adverb', 'verb group']</td>\n",
       "      <td>[AA, VG]</td>\n",
       "      <td>[adverb, a verb group]</td>\n",
       "      <td>[[44, 46], [84, 86]]</td>\n",
       "      <td>[[36, 42], [70, 82]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>can develop after exposure to a terrifying eve...</td>\n",
       "      <td>[[65, 69]]</td>\n",
       "      <td>[[71, 100]]</td>\n",
       "      <td>['PTSD']</td>\n",
       "      <td>['posttraumatic stress disorder']</td>\n",
       "      <td>[Union, PTSD]</td>\n",
       "      <td>[a, posttraumatic stress disorder, a, a, a]</td>\n",
       "      <td>[[59, 64], [65, 69]]</td>\n",
       "      <td>[[1, 2], [71, 100], [1, 2], [1, 2], [1, 2]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>It allows for testing interaction scenarios th...</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "      <td>['LTC']</td>\n",
       "      <td>['Language Technology Components']</td>\n",
       "      <td>[LTC]</td>\n",
       "      <td>[Language Technology Components]</td>\n",
       "      <td>[[100, 103]]</td>\n",
       "      <td>[[68, 98]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Feature Description lexical the words of the p...</td>\n",
       "      <td>[[63, 65], [71, 74], [96, 98]]</td>\n",
       "      <td>[[45, 61]]</td>\n",
       "      <td>['PA', 'POS', 'PA']</td>\n",
       "      <td>['product attribut']</td>\n",
       "      <td>[PA, ), POS]</td>\n",
       "      <td>[product attribute]</td>\n",
       "      <td>[[63, 65], [65, 66], [71, 74]]</td>\n",
       "      <td>[[45, 62]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>The tagging has been done using a GUI-based t...</td>\n",
       "      <td>[[86, 92], [35, 38]]</td>\n",
       "      <td>[[62, 84]]</td>\n",
       "      <td>['DTTool', 'GUI']</td>\n",
       "      <td>['Discourse Tagging Tool']</td>\n",
       "      <td>[DTTool]</td>\n",
       "      <td>[a, Discourse Tagging Tool, Discourse Tagging]</td>\n",
       "      <td>[[85, 91]]</td>\n",
       "      <td>[[5, 6], [61, 83], [61, 78]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Och and Ney (2003) show that for larger corpor...</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "      <td>['AER']</td>\n",
       "      <td>['Alignment Error Rate']</td>\n",
       "      <td>[AER]</td>\n",
       "      <td>[Alignment Error Rate]</td>\n",
       "      <td>[[105, 108]]</td>\n",
       "      <td>[[83, 103]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>2 Relation Extraction System In this section, ...</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "      <td>['RE']</td>\n",
       "      <td>['relation extraction']</td>\n",
       "      <td>[RE]</td>\n",
       "      <td>[relation extraction]</td>\n",
       "      <td>[[110, 112]]</td>\n",
       "      <td>[[89, 108]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>target question, or zero if the target questio...</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "      <td>['MRR']</td>\n",
       "      <td>['Mean Reciprocal Rank']</td>\n",
       "      <td>[MRR]</td>\n",
       "      <td>[Mean Reciprocal Rank]</td>\n",
       "      <td>[[89, 92]]</td>\n",
       "      <td>[[67, 87]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>discussion in Ritchie(1984).  Functional unifi...</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52]]</td>\n",
       "      <td>['FU']</td>\n",
       "      <td>['Functional unification']</td>\n",
       "      <td>[FU]</td>\n",
       "      <td>[Functional unification, a]</td>\n",
       "      <td>[[54, 56]]</td>\n",
       "      <td>[[30, 52], [38, 39]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Bigram Perplex. ( PP) MDI Missed Samples (MS) ...</td>\n",
       "      <td>[[42, 44], [18, 20], [22, 25], [69, 71]]</td>\n",
       "      <td>[[26, 40], [7, 14], [53, 67]]</td>\n",
       "      <td>['MS', 'PP', 'MDI', 'MS']</td>\n",
       "      <td>['Missed Samples', 'Perplex', 'Missed Samples']</td>\n",
       "      <td>[PP, MS, MS]</td>\n",
       "      <td>[Perplex, Missed Samples, Missed Samples]</td>\n",
       "      <td>[[18, 20], [42, 44], [42, 44]]</td>\n",
       "      <td>[[7, 14], [26, 40], [26, 40]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>networks. In Proceedings of the IEEE Conferenc...</td>\n",
       "      <td>[[92, 96], [32, 36]]</td>\n",
       "      <td>[[51, 90]]</td>\n",
       "      <td>['CVPR', 'IEEE']</td>\n",
       "      <td>['Computer Vision and Pattern Recognition']</td>\n",
       "      <td>[IEEE, CVPR]</td>\n",
       "      <td>[on Computer Vision and Pattern Recognition]</td>\n",
       "      <td>[[32, 36], [92, 96]]</td>\n",
       "      <td>[[48, 90]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Therefore, identification methods like Tsuchiy...</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[[72, 94]]</td>\n",
       "      <td>['SVM']</td>\n",
       "      <td>['Support Vector Machine']</td>\n",
       "      <td>[SVM]</td>\n",
       "      <td>[Support Vector Machines]</td>\n",
       "      <td>[[96, 99]]</td>\n",
       "      <td>[[72, 95]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>domain-independent mpirical induction algorith...</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[112, 136]]</td>\n",
       "      <td>['SPE']</td>\n",
       "      <td>['Sound Pattern of English']</td>\n",
       "      <td>[SPE]</td>\n",
       "      <td>[Sound Pattern of English]</td>\n",
       "      <td>[[138, 141]]</td>\n",
       "      <td>[[112, 136]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...</td>\n",
       "      <td>[[35, 38]]</td>\n",
       "      <td>[[26, 31]]</td>\n",
       "      <td>['P+R']</td>\n",
       "      <td>['2*P*R']</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>468</td>\n",
       "      <td>eugene@mathcs.emory.edu Abstract Community que...</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[33, 61]]</td>\n",
       "      <td>['CQA']</td>\n",
       "      <td>['Community question answering']</td>\n",
       "      <td>[CQA]</td>\n",
       "      <td>[Community question answering, question]</td>\n",
       "      <td>[[63, 66]]</td>\n",
       "      <td>[[33, 61], [43, 51]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>469</td>\n",
       "      <td>means a type of source ? newswire (NW), broad...</td>\n",
       "      <td>[[36, 38], [57, 59], [86, 88]]</td>\n",
       "      <td>[[26, 34], [41, 55], [62, 84]]</td>\n",
       "      <td>['NW', 'BN', 'BC']</td>\n",
       "      <td>['newswire', 'broadcast news', 'broadcast conv...</td>\n",
       "      <td>[NW, BN, BC]</td>\n",
       "      <td>[a, newswire, broadcast news, broadcast conver...</td>\n",
       "      <td>[[35, 37], [56, 58], [85, 87]]</td>\n",
       "      <td>[[2, 3], [25, 33], [40, 54], [61, 83]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>470</td>\n",
       "      <td>either lexically encoded, or depends on the in...</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[104, 113]]</td>\n",
       "      <td>['VPT']</td>\n",
       "      <td>['viewpoint']</td>\n",
       "      <td>[VPT]</td>\n",
       "      <td>[a, viewpoint]</td>\n",
       "      <td>[[99, 102]]</td>\n",
       "      <td>[[12, 13], [104, 113]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>471</td>\n",
       "      <td>Abstract  This paper attempts to use an off-th...</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[55, 74]]</td>\n",
       "      <td>['AR']</td>\n",
       "      <td>['anaphora resolution']</td>\n",
       "      <td>[AR]</td>\n",
       "      <td>[anaphora resolution]</td>\n",
       "      <td>[[76, 78]]</td>\n",
       "      <td>[[55, 74]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>472</td>\n",
       "      <td>The SemEval?2007 task for extracting frame sem...</td>\n",
       "      <td>[[125, 127], [4, 11]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "      <td>['FN', 'SemEval']</td>\n",
       "      <td>['FrameNet']</td>\n",
       "      <td>[FN]</td>\n",
       "      <td>[FrameNet]</td>\n",
       "      <td>[[125, 127]]</td>\n",
       "      <td>[[115, 123]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>473</td>\n",
       "      <td>hauer, haver, haber) and the corpus does not c...</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "      <td>['PoS']</td>\n",
       "      <td>['part of speech']</td>\n",
       "      <td>[PoS]</td>\n",
       "      <td>[part of speech]</td>\n",
       "      <td>[[121, 124]]</td>\n",
       "      <td>[[105, 119]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>474</td>\n",
       "      <td>The ungrammatical distracter, e.g., are in F...</td>\n",
       "      <td>[[88, 91]]</td>\n",
       "      <td>[[72, 86]]</td>\n",
       "      <td>['POS']</td>\n",
       "      <td>['part of speech']</td>\n",
       "      <td>[POS]</td>\n",
       "      <td>[a, part of speech]</td>\n",
       "      <td>[[86, 89]]</td>\n",
       "      <td>[[8, 9], [70, 84]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>475</td>\n",
       "      <td>Words/Phrases as Themselves (WD)  Symbols/Nonl...</td>\n",
       "      <td>[[81, 83], [29, 31], [60, 62], [96, 98]]</td>\n",
       "      <td>[[65, 73], [34, 41], [86, 94]]</td>\n",
       "      <td>['PH', 'WD', 'SY', 'SP']</td>\n",
       "      <td>['Phonetic', 'Symbols', 'Spelling']</td>\n",
       "      <td>[WD, SY, PH, SP]</td>\n",
       "      <td>[Themselves, Marks, Phonetic/Sound, Spelling]</td>\n",
       "      <td>[[29, 31], [60, 62], [81, 83], [96, 98]]</td>\n",
       "      <td>[[17, 27], [53, 58], [65, 79], [86, 94]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>476</td>\n",
       "      <td>DS = Discharge Summary,  Echo = Echocardiogram...</td>\n",
       "      <td>[[109, 112], [0, 2], [25, 29], [48, 50], [76, ...</td>\n",
       "      <td>[[115, 128], [5, 22], [32, 46], [53, 73], [91,...</td>\n",
       "      <td>['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']</td>\n",
       "      <td>['Radiology and', 'Discharge Summary', 'Echoca...</td>\n",
       "      <td>[DS, ED, GI, RAD, SP]</td>\n",
       "      <td>[Discharge, Summary, Echocardiogram, Emergency...</td>\n",
       "      <td>[[0, 2], [48, 50], [76, 78], [109, 112], [130,...</td>\n",
       "      <td>[[5, 14], [15, 22], [32, 46], [53, 73], [81, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>477</td>\n",
       "      <td>mdiab@ccls.columbia.edu Abstract We analyze ov...</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[[44, 58]]</td>\n",
       "      <td>['ODPs']</td>\n",
       "      <td>['overt displays']</td>\n",
       "      <td>[ODPs]</td>\n",
       "      <td>[overt displays of power]</td>\n",
       "      <td>[[69, 73]]</td>\n",
       "      <td>[[44, 67]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>478</td>\n",
       "      <td>(PERS), organization (ORG), geo-political enti...</td>\n",
       "      <td>[[115, 118], [1, 5], [22, 25], [50, 53], [64, ...</td>\n",
       "      <td>[[105, 113], [8, 20], [28, 48], [56, 62], [70,...</td>\n",
       "      <td>['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'L...</td>\n",
       "      <td>['facility', 'organization', 'geo-political en...</td>\n",
       "      <td>[PERS, ORG, GPE, WEA, VEH, LOC, FAC]</td>\n",
       "      <td>[organization, geo-political entity, weapon, v...</td>\n",
       "      <td>[[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...</td>\n",
       "      <td>[[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>479</td>\n",
       "      <td>Abstract In this paper, we propose a new synta...</td>\n",
       "      <td>[[74, 76], [109, 111]]</td>\n",
       "      <td>[[53, 72]]</td>\n",
       "      <td>['MT', 'MT']</td>\n",
       "      <td>['machine translation']</td>\n",
       "      <td>[MT]</td>\n",
       "      <td>[a, machine translation, a]</td>\n",
       "      <td>[[74, 76]]</td>\n",
       "      <td>[[5, 6], [53, 72], [5, 6]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>480</td>\n",
       "      <td>The second one is a variant that we named  D...</td>\n",
       "      <td>[[81, 85]]</td>\n",
       "      <td>[[45, 79]]</td>\n",
       "      <td>['DLED']</td>\n",
       "      <td>['Double Levenshtein?s Edit Distance']</td>\n",
       "      <td>[DLED]</td>\n",
       "      <td>[a, Levenshtein ? s Edit Distance]</td>\n",
       "      <td>[[79, 83]]</td>\n",
       "      <td>[[18, 19], [-1, 28]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>481</td>\n",
       "      <td>by HG's (HL). In particular, we show that HL's...</td>\n",
       "      <td>[[154, 159], [3, 5], [9, 11], [42, 44], [63, 6...</td>\n",
       "      <td>[[130, 152]]</td>\n",
       "      <td>[\"MHG's\", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL...</td>\n",
       "      <td>['Modified Head Grammars']</td>\n",
       "      <td>[HL, TAL, TAG, MHG, HI.]</td>\n",
       "      <td>[HG, s, Modified Head Grammars]</td>\n",
       "      <td>[[9, 11], [63, 66], [77, 80], [154, 157], [186...</td>\n",
       "      <td>[[3, 5], [6, 7], [130, 152]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>482</td>\n",
       "      <td>TEMPLATE GENERATO R Template Generation Algori...</td>\n",
       "      <td>[[127, 132]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "      <td>[\"CSI's\"]</td>\n",
       "      <td>['concept sequence instances']</td>\n",
       "      <td>[CSI]</td>\n",
       "      <td>[concept sequence instances]</td>\n",
       "      <td>[[127, 130]]</td>\n",
       "      <td>[[99, 125]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>483</td>\n",
       "      <td>(Ramshaw and Marcus, 1995) approached chuckin...</td>\n",
       "      <td>[[87, 90]]</td>\n",
       "      <td>[[57, 86]]</td>\n",
       "      <td>['TBL']</td>\n",
       "      <td>['Transformation Based Learning']</td>\n",
       "      <td>[TBL]</td>\n",
       "      <td>[Transformation Based Learning]</td>\n",
       "      <td>[[86, 89]]</td>\n",
       "      <td>[[56, 85]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>484</td>\n",
       "      <td>demonstrate such dependencies.  The Maximum En...</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51]]</td>\n",
       "      <td>['MaxEnt']</td>\n",
       "      <td>['Maximum Entropy']</td>\n",
       "      <td>[MaxEnt]</td>\n",
       "      <td>[Maximum Entropy, a]</td>\n",
       "      <td>[[53, 59]]</td>\n",
       "      <td>[[36, 51], [8, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>485</td>\n",
       "      <td>dialogues categorized into multiple domains, w...</td>\n",
       "      <td>[[128, 133]]</td>\n",
       "      <td>[[109, 126]]</td>\n",
       "      <td>['CSHMM']</td>\n",
       "      <td>['Class Speaker HMM']</td>\n",
       "      <td>[HMM, CSHMM]</td>\n",
       "      <td>[a, hidden Markov model, Class Speaker HMM]</td>\n",
       "      <td>[[97, 100], [128, 133]]</td>\n",
       "      <td>[[2, 3], [76, 95], [109, 126]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>486</td>\n",
       "      <td>shared task, namely FLORIAN (Florian et al.,  ...</td>\n",
       "      <td>[[56, 64], [20, 27]]</td>\n",
       "      <td>[[66, 78], [29, 36]]</td>\n",
       "      <td>['CHIEU-NG', 'FLORIAN']</td>\n",
       "      <td>['Chieu and Ng', 'Florian']</td>\n",
       "      <td>[FLORIAN, CHIEU-NG]</td>\n",
       "      <td>[Chieu and Ng]</td>\n",
       "      <td>[[20, 27], [56, 64]]</td>\n",
       "      <td>[[66, 78]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>487</td>\n",
       "      <td>1 Introduction Open-domain Question Answering...</td>\n",
       "      <td>[[48, 50]]</td>\n",
       "      <td>[[28, 46]]</td>\n",
       "      <td>['QA']</td>\n",
       "      <td>['Question Answering']</td>\n",
       "      <td>[)]</td>\n",
       "      <td>[Answering]</td>\n",
       "      <td>[[49, 50]]</td>\n",
       "      <td>[[36, 45]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>488</td>\n",
       "      <td>229  Proceedings of the 2014 Conference on Emp...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "      <td>['EMNLP']</td>\n",
       "      <td>['Empirical Methods in Natural Language Proces...</td>\n",
       "      <td>[EMNLP]</td>\n",
       "      <td>[Empirical Methods in Natural Language Process...</td>\n",
       "      <td>[[93, 98]]</td>\n",
       "      <td>[[43, 91]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>489</td>\n",
       "      <td>1 Introduction Large-scale open-domain questio...</td>\n",
       "      <td>[[90, 92], [154, 156]]</td>\n",
       "      <td>[[74, 88]]</td>\n",
       "      <td>['KB', 'QA']</td>\n",
       "      <td>['Knowledge Base']</td>\n",
       "      <td>[KB]</td>\n",
       "      <td>[Knowledge Base, a]</td>\n",
       "      <td>[[90, 92]]</td>\n",
       "      <td>[[74, 88], [16, 17]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>490</td>\n",
       "      <td>th fo frture representations abstracts (AbT), ...</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73], [86, 89]]</td>\n",
       "      <td>[[29, 38], [46, 52], [61, 68], [76, 84]]</td>\n",
       "      <td>['AbT', 'ArT', 'Aut', 'Jou']</td>\n",
       "      <td>['abstracts', 'titles', 'authors', 'Journals']</td>\n",
       "      <td>[AbT, ArT, Aut]</td>\n",
       "      <td>[titles, authors, Journals]</td>\n",
       "      <td>[[40, 43], [54, 57], [70, 73]]</td>\n",
       "      <td>[[46, 52], [61, 68], [76, 84]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>491</td>\n",
       "      <td>787 After labeling the reference BINet, we tra...</td>\n",
       "      <td>[[69, 72]]</td>\n",
       "      <td>[[51, 67]]</td>\n",
       "      <td>['L2R']</td>\n",
       "      <td>['learning to rank']</td>\n",
       "      <td>[BINet, L2R]</td>\n",
       "      <td>[a learning to rank]</td>\n",
       "      <td>[[33, 38], [69, 72]]</td>\n",
       "      <td>[[49, 67]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>492</td>\n",
       "      <td>for the annotation process.  Topic models (TMs...</td>\n",
       "      <td>[[43, 46]]</td>\n",
       "      <td>[[29, 41]]</td>\n",
       "      <td>['TMs']</td>\n",
       "      <td>['Topic models']</td>\n",
       "      <td>[)]</td>\n",
       "      <td>[Topic models, a]</td>\n",
       "      <td>[[46, 47]]</td>\n",
       "      <td>[[29, 41], [8, 9]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>493</td>\n",
       "      <td>Evidence for a text?s topic and genre comes, i...</td>\n",
       "      <td>[[183, 186], [206, 209]]</td>\n",
       "      <td>[[151, 181]]</td>\n",
       "      <td>['AGC', 'AGC']</td>\n",
       "      <td>['Automatic Genre Classification']</td>\n",
       "      <td>[AGC]</td>\n",
       "      <td>[a, s, Automatic Topic Classification and Auto...</td>\n",
       "      <td>[[183, 186]]</td>\n",
       "      <td>[[13, 14], [20, 21], [116, 181]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>494</td>\n",
       "      <td>0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "      <td>['OR']</td>\n",
       "      <td>['Omission Rate']</td>\n",
       "      <td>[OR]</td>\n",
       "      <td>[Omission Rate]</td>\n",
       "      <td>[[40, 42]]</td>\n",
       "      <td>[[25, 38]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>495</td>\n",
       "      <td>The query (Figure 4a) will match adjectives (A...</td>\n",
       "      <td>[[81, 83], [45, 49], [164, 166]]</td>\n",
       "      <td>[[75, 79], [33, 43], [158, 162]]</td>\n",
       "      <td>['NN', 'ADJA', 'NE']</td>\n",
       "      <td>['noun', 'adjectives', 'name']</td>\n",
       "      <td>[ADJA, NN, NE]</td>\n",
       "      <td>[adjectives, a, noun, a, noun]</td>\n",
       "      <td>[[45, 49], [81, 83], [164, 166]]</td>\n",
       "      <td>[[33, 43], [19, 20], [75, 79], [19, 20], [75, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>the middle (Baxendale, 1958).  Sentence Positi...</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "      <td>['SPY']</td>\n",
       "      <td>['Sentence Position Yield']</td>\n",
       "      <td>[SPY]</td>\n",
       "      <td>[Sentence Position Yield]</td>\n",
       "      <td>[[56, 59]]</td>\n",
       "      <td>[[31, 54]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>By contrast, our approach operates at the l...</td>\n",
       "      <td>[[83, 86]]</td>\n",
       "      <td>[[55, 81]]</td>\n",
       "      <td>['IPS']</td>\n",
       "      <td>['inflectional property sets']</td>\n",
       "      <td>[IPS]</td>\n",
       "      <td>[inflectional property sets]</td>\n",
       "      <td>[[80, 83]]</td>\n",
       "      <td>[[52, 78]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                               text  \\\n",
       "0      1  2 Related Work The availability of emotion-ric...   \n",
       "1      2  WebDict 0.2919 Backoff 0.3282 Table 1: Mean Av...   \n",
       "2      3  Baselines are a unigram query likelihood (QL) ...   \n",
       "3      4  tion. Then, we extract expansion terms from th...   \n",
       "4      5  person, mood, voice and case, CATiB uses 6 POS...   \n",
       "5      6  (LDA), Maximum Likelihood Linear Transform (ML...   \n",
       "6      7  gold label (Section 3). Our algorithm is calle...   \n",
       "7      8  phrase markers or words. For simplicity of man...   \n",
       "8      9   When a CFG is associated with probabilistic i...   \n",
       "9     10  1] proposed a language-neutral framework for r...   \n",
       "10    11  12 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3...   \n",
       "11    12  Model 3 (Figure 3) illustrates how the source ...   \n",
       "12    13  in the V column indicates that the verb condit...   \n",
       "13    14  The big blue door.?  In this case, the GrM ask...   \n",
       "14    15  TI = terse information  INT = interrupted  TRU...   \n",
       "15    16  classes in both original text and main text co...   \n",
       "16    17  as in figure 3.  It is parsed as an adverb (AA...   \n",
       "17    18  can develop after exposure to a terrifying eve...   \n",
       "18    19  It allows for testing interaction scenarios th...   \n",
       "19    20  Feature Description lexical the words of the p...   \n",
       "20    21   The tagging has been done using a GUI-based t...   \n",
       "21    22  Och and Ney (2003) show that for larger corpor...   \n",
       "22    23  2 Relation Extraction System In this section, ...   \n",
       "23    24  target question, or zero if the target questio...   \n",
       "24    25  discussion in Ritchie(1984).  Functional unifi...   \n",
       "25    26  Bigram Perplex. ( PP) MDI Missed Samples (MS) ...   \n",
       "26    27  networks. In Proceedings of the IEEE Conferenc...   \n",
       "27    28  Therefore, identification methods like Tsuchiy...   \n",
       "28    29  domain-independent mpirical induction algorith...   \n",
       "29    30  ents) *100%  ? F1-score = 2*P*R / (P+R)  Two c...   \n",
       "..   ...                                                ...   \n",
       "467  468  eugene@mathcs.emory.edu Abstract Community que...   \n",
       "468  469   means a type of source ? newswire (NW), broad...   \n",
       "469  470  either lexically encoded, or depends on the in...   \n",
       "470  471  Abstract  This paper attempts to use an off-th...   \n",
       "471  472  The SemEval?2007 task for extracting frame sem...   \n",
       "472  473  hauer, haver, haber) and the corpus does not c...   \n",
       "473  474    The ungrammatical distracter, e.g., are in F...   \n",
       "474  475  Words/Phrases as Themselves (WD)  Symbols/Nonl...   \n",
       "475  476  DS = Discharge Summary,  Echo = Echocardiogram...   \n",
       "476  477  mdiab@ccls.columbia.edu Abstract We analyze ov...   \n",
       "477  478  (PERS), organization (ORG), geo-political enti...   \n",
       "478  479  Abstract In this paper, we propose a new synta...   \n",
       "479  480    The second one is a variant that we named  D...   \n",
       "480  481  by HG's (HL). In particular, we show that HL's...   \n",
       "481  482  TEMPLATE GENERATO R Template Generation Algori...   \n",
       "482  483   (Ramshaw and Marcus, 1995) approached chuckin...   \n",
       "483  484  demonstrate such dependencies.  The Maximum En...   \n",
       "484  485  dialogues categorized into multiple domains, w...   \n",
       "485  486  shared task, namely FLORIAN (Florian et al.,  ...   \n",
       "486  487   1 Introduction Open-domain Question Answering...   \n",
       "487  488  229  Proceedings of the 2014 Conference on Emp...   \n",
       "488  489  1 Introduction Large-scale open-domain questio...   \n",
       "489  490  th fo frture representations abstracts (AbT), ...   \n",
       "490  491  787 After labeling the reference BINet, we tra...   \n",
       "491  492  for the annotation process.  Topic models (TMs...   \n",
       "492  493  Evidence for a text?s topic and genre comes, i...   \n",
       "493  494     0.35 0.25 0.50 0.75 1.00 Omission Rate (OR) Co   \n",
       "494  495  The query (Figure 4a) will match adjectives (A...   \n",
       "495  496  the middle (Baxendale, 1958).  Sentence Positi...   \n",
       "496  497     By contrast, our approach operates at the l...   \n",
       "\n",
       "                                              acronyms  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                   [[113, 115], [42, 45], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[130, 134], [30, 35], [43, 46], [53, 56], [15...   \n",
       "5                     [[115, 118], [44, 48], [87, 91]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                  [[85, 89], [8, 11]]   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[187, 189], [18, 20], [102, 104]]   \n",
       "11                              [[127, 130], [64, 66]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13        [[71, 73], [39, 42], [117, 119], [125, 127]]   \n",
       "14              [[43, 47], [0, 2], [24, 27], [61, 66]]   \n",
       "15                              [[85, 88], [140, 143]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                          [[65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                      [[63, 65], [71, 74], [96, 98]]   \n",
       "20                                [[86, 92], [35, 38]]   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25            [[42, 44], [18, 20], [22, 25], [69, 71]]   \n",
       "26                                [[92, 96], [32, 36]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                          [[35, 38]]   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[36, 38], [57, 59], [86, 88]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                              [[125, 127], [4, 11]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[88, 91]]   \n",
       "474           [[81, 83], [29, 31], [60, 62], [96, 98]]   \n",
       "475  [[109, 112], [0, 2], [25, 29], [48, 50], [76, ...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[115, 118], [1, 5], [22, 25], [50, 53], [64, ...   \n",
       "478                             [[74, 76], [109, 111]]   \n",
       "479                                         [[81, 85]]   \n",
       "480  [[154, 159], [3, 5], [9, 11], [42, 44], [63, 6...   \n",
       "481                                       [[127, 132]]   \n",
       "482                                         [[87, 90]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                                       [[128, 133]]   \n",
       "485                               [[56, 64], [20, 27]]   \n",
       "486                                         [[48, 50]]   \n",
       "487                                         [[93, 98]]   \n",
       "488                             [[90, 92], [154, 156]]   \n",
       "489           [[40, 43], [54, 57], [70, 73], [86, 89]]   \n",
       "490                                         [[69, 72]]   \n",
       "491                                         [[43, 46]]   \n",
       "492                           [[183, 186], [206, 209]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                   [[81, 83], [45, 49], [164, 166]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                         [[83, 86]]   \n",
       "\n",
       "                                            long-forms  \\\n",
       "0                                         [[144, 160]]   \n",
       "1                                           [[39, 61]]   \n",
       "2                              [[90, 111], [132, 151]]   \n",
       "3                                           [[65, 90]]   \n",
       "4                             [[136, 142], [156, 161]]   \n",
       "5                       [[94, 113], [7, 42], [51, 85]]   \n",
       "6                                           [[52, 93]]   \n",
       "7                                         [[128, 147]]   \n",
       "8                                           [[66, 83]]   \n",
       "9                                         [[103, 126]]   \n",
       "10                     [[169, 185], [8, 16], [90, 99]]   \n",
       "11                                         [[98, 125]]   \n",
       "12                             [[91, 105], [112, 129]]   \n",
       "13                                          [[53, 69]]   \n",
       "14             [[50, 59], [5, 22], [30, 41], [69, 79]]   \n",
       "15                                          [[60, 83]]   \n",
       "16                                [[33, 42], [72, 82]]   \n",
       "17                                         [[71, 100]]   \n",
       "18                                          [[68, 98]]   \n",
       "19                                          [[45, 61]]   \n",
       "20                                          [[62, 84]]   \n",
       "21                                         [[83, 103]]   \n",
       "22                                         [[89, 108]]   \n",
       "23                                          [[67, 87]]   \n",
       "24                                          [[30, 52]]   \n",
       "25                       [[26, 40], [7, 14], [53, 67]]   \n",
       "26                                          [[51, 90]]   \n",
       "27                                          [[72, 94]]   \n",
       "28                                        [[112, 136]]   \n",
       "29                                          [[26, 31]]   \n",
       "..                                                 ...   \n",
       "467                                         [[33, 61]]   \n",
       "468                     [[26, 34], [41, 55], [62, 84]]   \n",
       "469                                       [[104, 113]]   \n",
       "470                                         [[55, 74]]   \n",
       "471                                       [[115, 123]]   \n",
       "472                                       [[105, 119]]   \n",
       "473                                         [[72, 86]]   \n",
       "474                     [[65, 73], [34, 41], [86, 94]]   \n",
       "475  [[115, 128], [5, 22], [32, 46], [53, 73], [91,...   \n",
       "476                                         [[44, 58]]   \n",
       "477  [[105, 113], [8, 20], [28, 48], [56, 62], [70,...   \n",
       "478                                         [[53, 72]]   \n",
       "479                                         [[45, 79]]   \n",
       "480                                       [[130, 152]]   \n",
       "481                                        [[99, 125]]   \n",
       "482                                         [[57, 86]]   \n",
       "483                                         [[36, 51]]   \n",
       "484                                       [[109, 126]]   \n",
       "485                               [[66, 78], [29, 36]]   \n",
       "486                                         [[28, 46]]   \n",
       "487                                         [[43, 91]]   \n",
       "488                                         [[74, 88]]   \n",
       "489           [[29, 38], [46, 52], [61, 68], [76, 84]]   \n",
       "490                                         [[51, 67]]   \n",
       "491                                         [[29, 41]]   \n",
       "492                                       [[151, 181]]   \n",
       "493                                         [[25, 38]]   \n",
       "494                   [[75, 79], [33, 43], [158, 162]]   \n",
       "495                                         [[31, 54]]   \n",
       "496                                         [[55, 81]]   \n",
       "\n",
       "                                         acronyms-text  \\\n",
       "0                                              ['TDM']   \n",
       "1                                              ['MAP']   \n",
       "2                                 ['SD', 'QL)', 'MRF']   \n",
       "3                                              ['PRF']   \n",
       "4    ['PROP', 'CATiB', 'POS', 'NOM', 'VRB', 'VRB-PA...   \n",
       "5                              ['MPE', 'MLLT', 'BMMI']   \n",
       "6                                             ['SWVP']   \n",
       "7                                              ['CNF']   \n",
       "8                                      ['PCFG', 'CFG']   \n",
       "9                                              ['LNS']   \n",
       "10                                  ['AP', 'FN', 'VP']   \n",
       "11                                       ['NLG', 'MT']   \n",
       "12                                        ['LR', 'LP']   \n",
       "13                           ['RP', 'GrM', 'UU', 'RP']   \n",
       "14                      ['TRUN', 'TI', 'INT', 'TRANS']   \n",
       "15                                      ['SVM', 'AGI']   \n",
       "16                                        ['AA', 'VG']   \n",
       "17                                            ['PTSD']   \n",
       "18                                             ['LTC']   \n",
       "19                                 ['PA', 'POS', 'PA']   \n",
       "20                                   ['DTTool', 'GUI']   \n",
       "21                                             ['AER']   \n",
       "22                                              ['RE']   \n",
       "23                                             ['MRR']   \n",
       "24                                              ['FU']   \n",
       "25                           ['MS', 'PP', 'MDI', 'MS']   \n",
       "26                                    ['CVPR', 'IEEE']   \n",
       "27                                             ['SVM']   \n",
       "28                                             ['SPE']   \n",
       "29                                             ['P+R']   \n",
       "..                                                 ...   \n",
       "467                                            ['CQA']   \n",
       "468                                 ['NW', 'BN', 'BC']   \n",
       "469                                            ['VPT']   \n",
       "470                                             ['AR']   \n",
       "471                                  ['FN', 'SemEval']   \n",
       "472                                            ['PoS']   \n",
       "473                                            ['POS']   \n",
       "474                           ['PH', 'WD', 'SY', 'SP']   \n",
       "475            ['RAD', 'DS', 'Echo', 'ED', 'GI', 'SP']   \n",
       "476                                           ['ODPs']   \n",
       "477  ['FAC', 'PERS', 'ORG', 'GPE', 'WEA', 'VEH', 'L...   \n",
       "478                                       ['MT', 'MT']   \n",
       "479                                           ['DLED']   \n",
       "480  [\"MHG's\", 'HG', 'HL', 'HL', 'TAL', 'TAG', 'MHL...   \n",
       "481                                          [\"CSI's\"]   \n",
       "482                                            ['TBL']   \n",
       "483                                         ['MaxEnt']   \n",
       "484                                          ['CSHMM']   \n",
       "485                            ['CHIEU-NG', 'FLORIAN']   \n",
       "486                                             ['QA']   \n",
       "487                                          ['EMNLP']   \n",
       "488                                       ['KB', 'QA']   \n",
       "489                       ['AbT', 'ArT', 'Aut', 'Jou']   \n",
       "490                                            ['L2R']   \n",
       "491                                            ['TMs']   \n",
       "492                                     ['AGC', 'AGC']   \n",
       "493                                             ['OR']   \n",
       "494                               ['NN', 'ADJA', 'NE']   \n",
       "495                                            ['SPY']   \n",
       "496                                            ['IPS']   \n",
       "\n",
       "                                       long-forms-text  \\\n",
       "0                                 ['Text Data Mining']   \n",
       "1                           ['Mean Average Precision']   \n",
       "2     ['sequential dependence', 'Markov random field']   \n",
       "3                        ['pseudo relevance feedback']   \n",
       "4                                  ['proper', 'verbs']   \n",
       "5    ['Minimum Phone Error', 'Maximum Likelihood Li...   \n",
       "6        ['Structured Weighted Violations Perceptron']   \n",
       "7                              ['Chomsky Normal Form']   \n",
       "8                                ['Probabilistic CFG']   \n",
       "9                          ['Language Neutral Syntax']   \n",
       "10       ['Base+Appositives', 'FrameNet', 'Verb Pair']   \n",
       "11                     ['natural language generation']   \n",
       "12             ['labeled recall', 'labeled precision']   \n",
       "13                                ['Response Planner']   \n",
       "14   ['truncated', 'terse information', 'interrupte...   \n",
       "15                         ['Support Vector Machines']   \n",
       "16                         ['an adverb', 'verb group']   \n",
       "17                   ['posttraumatic stress disorder']   \n",
       "18                  ['Language Technology Components']   \n",
       "19                                ['product attribut']   \n",
       "20                          ['Discourse Tagging Tool']   \n",
       "21                            ['Alignment Error Rate']   \n",
       "22                             ['relation extraction']   \n",
       "23                            ['Mean Reciprocal Rank']   \n",
       "24                          ['Functional unification']   \n",
       "25     ['Missed Samples', 'Perplex', 'Missed Samples']   \n",
       "26         ['Computer Vision and Pattern Recognition']   \n",
       "27                          ['Support Vector Machine']   \n",
       "28                        ['Sound Pattern of English']   \n",
       "29                                           ['2*P*R']   \n",
       "..                                                 ...   \n",
       "467                   ['Community question answering']   \n",
       "468  ['newswire', 'broadcast news', 'broadcast conv...   \n",
       "469                                      ['viewpoint']   \n",
       "470                            ['anaphora resolution']   \n",
       "471                                       ['FrameNet']   \n",
       "472                                 ['part of speech']   \n",
       "473                                 ['part of speech']   \n",
       "474                ['Phonetic', 'Symbols', 'Spelling']   \n",
       "475  ['Radiology and', 'Discharge Summary', 'Echoca...   \n",
       "476                                 ['overt displays']   \n",
       "477  ['facility', 'organization', 'geo-political en...   \n",
       "478                            ['machine translation']   \n",
       "479             ['Double Levenshtein?s Edit Distance']   \n",
       "480                         ['Modified Head Grammars']   \n",
       "481                     ['concept sequence instances']   \n",
       "482                  ['Transformation Based Learning']   \n",
       "483                                ['Maximum Entropy']   \n",
       "484                              ['Class Speaker HMM']   \n",
       "485                        ['Chieu and Ng', 'Florian']   \n",
       "486                             ['Question Answering']   \n",
       "487  ['Empirical Methods in Natural Language Proces...   \n",
       "488                                 ['Knowledge Base']   \n",
       "489     ['abstracts', 'titles', 'authors', 'Journals']   \n",
       "490                               ['learning to rank']   \n",
       "491                                   ['Topic models']   \n",
       "492                 ['Automatic Genre Classification']   \n",
       "493                                  ['Omission Rate']   \n",
       "494                     ['noun', 'adjectives', 'name']   \n",
       "495                        ['Sentence Position Yield']   \n",
       "496                     ['inflectional property sets']   \n",
       "\n",
       "                                    AN_Pred  \\\n",
       "0                                     [TDM]   \n",
       "1                                     [MAP]   \n",
       "2                             [QL, SD, MRF]   \n",
       "3                                     [PRF]   \n",
       "4    [CATiB, POS, NOM, PROP, VRB, VRB-PASS]   \n",
       "5                    [LDA, MLLT, BMMI, MPE]   \n",
       "6                                    [SWVP]   \n",
       "7                                     [CNF]   \n",
       "8                                    [PCFG]   \n",
       "9                                     [LNS]   \n",
       "10                             [FN, VP, AP]   \n",
       "11                                [MT, NLG]   \n",
       "12                                 [LR, LP]   \n",
       "13                            [GrM, RP, UU]   \n",
       "14                   [TI, INT, TRUN, TRANS]   \n",
       "15                                    [SVM]   \n",
       "16                                 [AA, VG]   \n",
       "17                            [Union, PTSD]   \n",
       "18                                    [LTC]   \n",
       "19                             [PA, ), POS]   \n",
       "20                                 [DTTool]   \n",
       "21                                    [AER]   \n",
       "22                                     [RE]   \n",
       "23                                    [MRR]   \n",
       "24                                     [FU]   \n",
       "25                             [PP, MS, MS]   \n",
       "26                             [IEEE, CVPR]   \n",
       "27                                    [SVM]   \n",
       "28                                    [SPE]   \n",
       "29                                       []   \n",
       "..                                      ...   \n",
       "467                                   [CQA]   \n",
       "468                            [NW, BN, BC]   \n",
       "469                                   [VPT]   \n",
       "470                                    [AR]   \n",
       "471                                    [FN]   \n",
       "472                                   [PoS]   \n",
       "473                                   [POS]   \n",
       "474                        [WD, SY, PH, SP]   \n",
       "475                   [DS, ED, GI, RAD, SP]   \n",
       "476                                  [ODPs]   \n",
       "477    [PERS, ORG, GPE, WEA, VEH, LOC, FAC]   \n",
       "478                                    [MT]   \n",
       "479                                  [DLED]   \n",
       "480                [HL, TAL, TAG, MHG, HI.]   \n",
       "481                                   [CSI]   \n",
       "482                                   [TBL]   \n",
       "483                                [MaxEnt]   \n",
       "484                            [HMM, CSHMM]   \n",
       "485                     [FLORIAN, CHIEU-NG]   \n",
       "486                                     [)]   \n",
       "487                                 [EMNLP]   \n",
       "488                                    [KB]   \n",
       "489                         [AbT, ArT, Aut]   \n",
       "490                            [BINet, L2R]   \n",
       "491                                     [)]   \n",
       "492                                   [AGC]   \n",
       "493                                    [OR]   \n",
       "494                          [ADJA, NN, NE]   \n",
       "495                                   [SPY]   \n",
       "496                                   [IPS]   \n",
       "\n",
       "                                               LF_Pred  \\\n",
       "0                                [a, Text Data Mining]   \n",
       "1                             [Mean Average Precision]   \n",
       "2    [a, query likelihood, a, sequential dependence...   \n",
       "3                          [pseudo relevance feedback]   \n",
       "4           [non-proper nominals, proper nouns, verbs]   \n",
       "5    [Maximum Likelihood Linear Transform, Boosted ...   \n",
       "6       [Structured Weighted Violations Perceptron, a]   \n",
       "7                                [Chomsky Normal Form]   \n",
       "8                             [a, a Probabilistic CFG]   \n",
       "9                         [a, Language Neutral Syntax]   \n",
       "10                                   [Base+Verb Pairs]   \n",
       "11                  [a natural language generation, a]   \n",
       "12                 [labeled recall, labeled precision]   \n",
       "13                                  [Response Planner]   \n",
       "14   [terse, information, interrupted, truncated, t...   \n",
       "15                           [Support Vector Machines]   \n",
       "16                              [adverb, a verb group]   \n",
       "17         [a, posttraumatic stress disorder, a, a, a]   \n",
       "18                    [Language Technology Components]   \n",
       "19                                 [product attribute]   \n",
       "20      [a, Discourse Tagging Tool, Discourse Tagging]   \n",
       "21                              [Alignment Error Rate]   \n",
       "22                               [relation extraction]   \n",
       "23                              [Mean Reciprocal Rank]   \n",
       "24                         [Functional unification, a]   \n",
       "25           [Perplex, Missed Samples, Missed Samples]   \n",
       "26        [on Computer Vision and Pattern Recognition]   \n",
       "27                           [Support Vector Machines]   \n",
       "28                          [Sound Pattern of English]   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "467           [Community question answering, question]   \n",
       "468  [a, newswire, broadcast news, broadcast conver...   \n",
       "469                                     [a, viewpoint]   \n",
       "470                              [anaphora resolution]   \n",
       "471                                         [FrameNet]   \n",
       "472                                   [part of speech]   \n",
       "473                                [a, part of speech]   \n",
       "474      [Themselves, Marks, Phonetic/Sound, Spelling]   \n",
       "475  [Discharge, Summary, Echocardiogram, Emergency...   \n",
       "476                          [overt displays of power]   \n",
       "477  [organization, geo-political entity, weapon, v...   \n",
       "478                        [a, machine translation, a]   \n",
       "479                 [a, Levenshtein ? s Edit Distance]   \n",
       "480                    [HG, s, Modified Head Grammars]   \n",
       "481                       [concept sequence instances]   \n",
       "482                    [Transformation Based Learning]   \n",
       "483                               [Maximum Entropy, a]   \n",
       "484        [a, hidden Markov model, Class Speaker HMM]   \n",
       "485                                     [Chieu and Ng]   \n",
       "486                                        [Answering]   \n",
       "487  [Empirical Methods in Natural Language Process...   \n",
       "488                                [Knowledge Base, a]   \n",
       "489                        [titles, authors, Journals]   \n",
       "490                               [a learning to rank]   \n",
       "491                                  [Topic models, a]   \n",
       "492  [a, s, Automatic Topic Classification and Auto...   \n",
       "493                                    [Omission Rate]   \n",
       "494                     [adjectives, a, noun, a, noun]   \n",
       "495                          [Sentence Position Yield]   \n",
       "496                       [inflectional property sets]   \n",
       "\n",
       "                                          AN_Pred_idxs  \\\n",
       "0                                         [[162, 165]]   \n",
       "1                                           [[63, 66]]   \n",
       "2                   [[42, 44], [113, 115], [153, 156]]   \n",
       "3                                           [[92, 95]]   \n",
       "4    [[30, 35], [43, 46], [53, 56], [130, 134], [15...   \n",
       "5             [[1, 4], [44, 48], [87, 91], [115, 118]]   \n",
       "6                                           [[95, 99]]   \n",
       "7                                         [[149, 152]]   \n",
       "8                                           [[84, 88]]   \n",
       "9                                         [[128, 131]]   \n",
       "10                  [[18, 20], [102, 104], [187, 189]]   \n",
       "11                              [[64, 66], [127, 130]]   \n",
       "12                              [[86, 88], [107, 109]]   \n",
       "13                    [[39, 42], [71, 73], [117, 119]]   \n",
       "14              [[0, 2], [24, 27], [43, 47], [61, 66]]   \n",
       "15                                          [[85, 88]]   \n",
       "16                                [[44, 46], [84, 86]]   \n",
       "17                                [[59, 64], [65, 69]]   \n",
       "18                                        [[100, 103]]   \n",
       "19                      [[63, 65], [65, 66], [71, 74]]   \n",
       "20                                          [[85, 91]]   \n",
       "21                                        [[105, 108]]   \n",
       "22                                        [[110, 112]]   \n",
       "23                                          [[89, 92]]   \n",
       "24                                          [[54, 56]]   \n",
       "25                      [[18, 20], [42, 44], [42, 44]]   \n",
       "26                                [[32, 36], [92, 96]]   \n",
       "27                                          [[96, 99]]   \n",
       "28                                        [[138, 141]]   \n",
       "29                                                  []   \n",
       "..                                                 ...   \n",
       "467                                         [[63, 66]]   \n",
       "468                     [[35, 37], [56, 58], [85, 87]]   \n",
       "469                                        [[99, 102]]   \n",
       "470                                         [[76, 78]]   \n",
       "471                                       [[125, 127]]   \n",
       "472                                       [[121, 124]]   \n",
       "473                                         [[86, 89]]   \n",
       "474           [[29, 31], [60, 62], [81, 83], [96, 98]]   \n",
       "475  [[0, 2], [48, 50], [76, 78], [109, 112], [130,...   \n",
       "476                                         [[69, 73]]   \n",
       "477  [[1, 5], [22, 25], [50, 53], [64, 67], [79, 82...   \n",
       "478                                         [[74, 76]]   \n",
       "479                                         [[79, 83]]   \n",
       "480  [[9, 11], [63, 66], [77, 80], [154, 157], [186...   \n",
       "481                                       [[127, 130]]   \n",
       "482                                         [[86, 89]]   \n",
       "483                                         [[53, 59]]   \n",
       "484                            [[97, 100], [128, 133]]   \n",
       "485                               [[20, 27], [56, 64]]   \n",
       "486                                         [[49, 50]]   \n",
       "487                                         [[93, 98]]   \n",
       "488                                         [[90, 92]]   \n",
       "489                     [[40, 43], [54, 57], [70, 73]]   \n",
       "490                               [[33, 38], [69, 72]]   \n",
       "491                                         [[46, 47]]   \n",
       "492                                       [[183, 186]]   \n",
       "493                                         [[40, 42]]   \n",
       "494                   [[45, 49], [81, 83], [164, 166]]   \n",
       "495                                         [[56, 59]]   \n",
       "496                                         [[80, 83]]   \n",
       "\n",
       "                                          LF_Pred_idxs  \n",
       "0                                 [[5, 6], [144, 160]]  \n",
       "1                                           [[39, 61]]  \n",
       "2    [[1, 2], [24, 40], [1, 2], [90, 111], [132, 151]]  \n",
       "3                                           [[65, 90]]  \n",
       "4                   [[58, 77], [136, 148], [122, 127]]  \n",
       "5                       [[7, 42], [51, 85], [94, 113]]  \n",
       "6                                   [[52, 93], [6, 7]]  \n",
       "7                                         [[128, 147]]  \n",
       "8                                   [[5, 6], [63, 82]]  \n",
       "9                               [[12, 13], [103, 126]]  \n",
       "10                                         [[85, 100]]  \n",
       "11                               [[96, 125], [26, 27]]  \n",
       "12                             [[91, 105], [112, 129]]  \n",
       "13                                          [[53, 69]]  \n",
       "14   [[5, 10], [11, 22], [30, 41], [50, 59], [69, 88]]  \n",
       "15                                          [[60, 83]]  \n",
       "16                                [[36, 42], [70, 82]]  \n",
       "17         [[1, 2], [71, 100], [1, 2], [1, 2], [1, 2]]  \n",
       "18                                          [[68, 98]]  \n",
       "19                                          [[45, 62]]  \n",
       "20                        [[5, 6], [61, 83], [61, 78]]  \n",
       "21                                         [[83, 103]]  \n",
       "22                                         [[89, 108]]  \n",
       "23                                          [[67, 87]]  \n",
       "24                                [[30, 52], [38, 39]]  \n",
       "25                       [[7, 14], [26, 40], [26, 40]]  \n",
       "26                                          [[48, 90]]  \n",
       "27                                          [[72, 95]]  \n",
       "28                                        [[112, 136]]  \n",
       "29                                                  []  \n",
       "..                                                 ...  \n",
       "467                               [[33, 61], [43, 51]]  \n",
       "468             [[2, 3], [25, 33], [40, 54], [61, 83]]  \n",
       "469                             [[12, 13], [104, 113]]  \n",
       "470                                         [[55, 74]]  \n",
       "471                                       [[115, 123]]  \n",
       "472                                       [[105, 119]]  \n",
       "473                                 [[8, 9], [70, 84]]  \n",
       "474           [[17, 27], [53, 58], [65, 79], [86, 94]]  \n",
       "475  [[5, 14], [15, 22], [32, 46], [53, 73], [81, 1...  \n",
       "476                                         [[44, 67]]  \n",
       "477  [[8, 20], [28, 48], [56, 62], [70, 77], [85, 9...  \n",
       "478                         [[5, 6], [53, 72], [5, 6]]  \n",
       "479                               [[18, 19], [-1, 28]]  \n",
       "480                       [[3, 5], [6, 7], [130, 152]]  \n",
       "481                                        [[99, 125]]  \n",
       "482                                         [[56, 85]]  \n",
       "483                                 [[36, 51], [8, 9]]  \n",
       "484                     [[2, 3], [76, 95], [109, 126]]  \n",
       "485                                         [[66, 78]]  \n",
       "486                                         [[36, 45]]  \n",
       "487                                         [[43, 91]]  \n",
       "488                               [[74, 88], [16, 17]]  \n",
       "489                     [[46, 52], [61, 68], [76, 84]]  \n",
       "490                                         [[49, 67]]  \n",
       "491                                 [[29, 41], [8, 9]]  \n",
       "492                   [[13, 14], [20, 21], [116, 181]]  \n",
       "493                                         [[25, 38]]  \n",
       "494  [[33, 43], [19, 20], [75, 79], [19, 20], [75, ...  \n",
       "495                                         [[31, 54]]  \n",
       "496                                         [[52, 78]]  \n",
       "\n",
       "[497 rows x 10 columns]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = match(df_orig, df_pred)\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.to_csv(\"../results/finetuned_models/model_eng_scientific_xb_v5/predictions_AN_LF_idxs_Inference.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.iloc[0][\"text\"].find(\"Text Data Mining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all = \"../data_tsvs/allLangDFTrain.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>92\\]. This selective approach led to significa...</td>\n",
       "      <td>[[91, 98]]</td>\n",
       "      <td>[[77, 89]]</td>\n",
       "      <td>['ATIS...']</td>\n",
       "      <td>['applications']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>We conduct a case study of dialectal language ...</td>\n",
       "      <td>[[119, 122]]</td>\n",
       "      <td>[[93, 117]]</td>\n",
       "      <td>['AAE']</td>\n",
       "      <td>['African-American English']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>An arguably better approach to representation...</td>\n",
       "      <td>[[91, 94]]</td>\n",
       "      <td>[[59, 89]]</td>\n",
       "      <td>['CCA']</td>\n",
       "      <td>['Canonical Correlation Analysis']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>23-28, 1992   Proceedings of NAACL-HLT 2015 St...</td>\n",
       "      <td>[[71, 74], [29, 38]]</td>\n",
       "      <td>[[44, 69]]</td>\n",
       "      <td>['SRW', 'NAACL-HLT']</td>\n",
       "      <td>['Student Research Workshop']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>itly mark objects of prepositions (POBJ), poss...</td>\n",
       "      <td>[[119, 121], [35, 39], [76, 81], [95, 99], [16...</td>\n",
       "      <td>[[105, 117], [10, 33], [56, 74], [84, 93], [15...</td>\n",
       "      <td>['CC', 'POBJ', 'IDAFA', 'CONJ', 'TMZ']</td>\n",
       "      <td>['conjunctions', 'objects of prepositions', 'i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               text  \\\n",
       "0   1  92\\]. This selective approach led to significa...   \n",
       "1   2  We conduct a case study of dialectal language ...   \n",
       "2   3   An arguably better approach to representation...   \n",
       "3   4  23-28, 1992   Proceedings of NAACL-HLT 2015 St...   \n",
       "4   5  itly mark objects of prepositions (POBJ), poss...   \n",
       "\n",
       "                                            acronyms  \\\n",
       "0                                         [[91, 98]]   \n",
       "1                                       [[119, 122]]   \n",
       "2                                         [[91, 94]]   \n",
       "3                               [[71, 74], [29, 38]]   \n",
       "4  [[119, 121], [35, 39], [76, 81], [95, 99], [16...   \n",
       "\n",
       "                                          long-forms  \\\n",
       "0                                         [[77, 89]]   \n",
       "1                                        [[93, 117]]   \n",
       "2                                         [[59, 89]]   \n",
       "3                                         [[44, 69]]   \n",
       "4  [[105, 117], [10, 33], [56, 74], [84, 93], [15...   \n",
       "\n",
       "                            acronyms-text  \\\n",
       "0                             ['ATIS...']   \n",
       "1                                 ['AAE']   \n",
       "2                                 ['CCA']   \n",
       "3                    ['SRW', 'NAACL-HLT']   \n",
       "4  ['CC', 'POBJ', 'IDAFA', 'CONJ', 'TMZ']   \n",
       "\n",
       "                                     long-forms-text  \n",
       "0                                   ['applications']  \n",
       "1                       ['African-American English']  \n",
       "2                 ['Canonical Correlation Analysis']  \n",
       "3                      ['Student Research Workshop']  \n",
       "4  ['conjunctions', 'objects of prepositions', 'i...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv(path_all, sep=\"\\t\")\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26947, 6)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = list(df_all['text'])\n",
    "cnts = [len(str(txt).split(\" \")) for txt in texts]\n",
    "max(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 19,\n",
       " 20,\n",
       " 19,\n",
       " 22,\n",
       " 27,\n",
       " 18,\n",
       " 20,\n",
       " 19,\n",
       " 15,\n",
       " 19,\n",
       " 27,\n",
       " 42,\n",
       " 21,\n",
       " 21,\n",
       " 29,\n",
       " 19,\n",
       " 25,\n",
       " 31,\n",
       " 15,\n",
       " 24,\n",
       " 17,\n",
       " 14,\n",
       " 23,\n",
       " 19,\n",
       " 21,\n",
       " 21,\n",
       " 18,\n",
       " 21,\n",
       " 23,\n",
       " 10,\n",
       " 23,\n",
       " 45,\n",
       " 21,\n",
       " 25,\n",
       " 16,\n",
       " 23,\n",
       " 25,\n",
       " 19,\n",
       " 16,\n",
       " 18,\n",
       " 15,\n",
       " 22,\n",
       " 17,\n",
       " 14,\n",
       " 26,\n",
       " 29,\n",
       " 19,\n",
       " 17,\n",
       " 36,\n",
       " 31,\n",
       " 25,\n",
       " 21,\n",
       " 24,\n",
       " 18,\n",
       " 19,\n",
       " 13,\n",
       " 22,\n",
       " 41,\n",
       " 17,\n",
       " 24,\n",
       " 21,\n",
       " 31,\n",
       " 26,\n",
       " 25,\n",
       " 22,\n",
       " 37,\n",
       " 20,\n",
       " 17,\n",
       " 22,\n",
       " 31,\n",
       " 21,\n",
       " 23,\n",
       " 19,\n",
       " 14,\n",
       " 30,\n",
       " 17,\n",
       " 24,\n",
       " 31,\n",
       " 33,\n",
       " 17,\n",
       " 22,\n",
       " 24,\n",
       " 17,\n",
       " 20,\n",
       " 18,\n",
       " 25,\n",
       " 36,\n",
       " 18,\n",
       " 50,\n",
       " 34,\n",
       " 131,\n",
       " 25,\n",
       " 13,\n",
       " 21,\n",
       " 13,\n",
       " 33,\n",
       " 14,\n",
       " 42,\n",
       " 34,\n",
       " 16,\n",
       " 26,\n",
       " 18,\n",
       " 23,\n",
       " 37,\n",
       " 26,\n",
       " 14,\n",
       " 17,\n",
       " 63,\n",
       " 22,\n",
       " 24,\n",
       " 18,\n",
       " 29,\n",
       " 10,\n",
       " 26,\n",
       " 30,\n",
       " 22,\n",
       " 25,\n",
       " 20,\n",
       " 21,\n",
       " 23,\n",
       " 27,\n",
       " 20,\n",
       " 18,\n",
       " 26,\n",
       " 15,\n",
       " 33,\n",
       " 18,\n",
       " 48,\n",
       " 12,\n",
       " 22,\n",
       " 28,\n",
       " 19,\n",
       " 22,\n",
       " 30,\n",
       " 26,\n",
       " 44,\n",
       " 27,\n",
       " 27,\n",
       " 20,\n",
       " 20,\n",
       " 14,\n",
       " 19,\n",
       " 19,\n",
       " 26,\n",
       " 25,\n",
       " 34,\n",
       " 23,\n",
       " 17,\n",
       " 27,\n",
       " 108,\n",
       " 19,\n",
       " 18,\n",
       " 21,\n",
       " 18,\n",
       " 40,\n",
       " 24,\n",
       " 24,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 21,\n",
       " 23,\n",
       " 19,\n",
       " 16,\n",
       " 25,\n",
       " 18,\n",
       " 26,\n",
       " 29,\n",
       " 24,\n",
       " 27,\n",
       " 24,\n",
       " 21,\n",
       " 19,\n",
       " 23,\n",
       " 27,\n",
       " 35,\n",
       " 22,\n",
       " 38,\n",
       " 21,\n",
       " 32,\n",
       " 31,\n",
       " 16,\n",
       " 19,\n",
       " 32,\n",
       " 21,\n",
       " 28,\n",
       " 23,\n",
       " 13,\n",
       " 16,\n",
       " 23,\n",
       " 16,\n",
       " 31,\n",
       " 19,\n",
       " 19,\n",
       " 10,\n",
       " 27,\n",
       " 8,\n",
       " 17,\n",
       " 28,\n",
       " 12,\n",
       " 15,\n",
       " 15,\n",
       " 29,\n",
       " 22,\n",
       " 38,\n",
       " 27,\n",
       " 21,\n",
       " 15,\n",
       " 21,\n",
       " 18,\n",
       " 21,\n",
       " 101,\n",
       " 33,\n",
       " 23,\n",
       " 28,\n",
       " 22,\n",
       " 20,\n",
       " 21,\n",
       " 49,\n",
       " 30,\n",
       " 23,\n",
       " 22,\n",
       " 18,\n",
       " 28,\n",
       " 22,\n",
       " 48,\n",
       " 20,\n",
       " 20,\n",
       " 9,\n",
       " 16,\n",
       " 19,\n",
       " 22,\n",
       " 21,\n",
       " 25,\n",
       " 22,\n",
       " 37,\n",
       " 13,\n",
       " 22,\n",
       " 20,\n",
       " 31,\n",
       " 21,\n",
       " 30,\n",
       " 17,\n",
       " 21,\n",
       " 27,\n",
       " 32,\n",
       " 24,\n",
       " 36,\n",
       " 24,\n",
       " 42,\n",
       " 31,\n",
       " 18,\n",
       " 29,\n",
       " 16,\n",
       " 22,\n",
       " 20,\n",
       " 29,\n",
       " 24,\n",
       " 27,\n",
       " 29,\n",
       " 17,\n",
       " 30,\n",
       " 26,\n",
       " 19,\n",
       " 26,\n",
       " 30,\n",
       " 26,\n",
       " 16,\n",
       " 31,\n",
       " 16,\n",
       " 17,\n",
       " 25,\n",
       " 34,\n",
       " 25,\n",
       " 40,\n",
       " 13,\n",
       " 16,\n",
       " 20,\n",
       " 23,\n",
       " 25,\n",
       " 27,\n",
       " 15,\n",
       " 21,\n",
       " 22,\n",
       " 30,\n",
       " 22,\n",
       " 23,\n",
       " 26,\n",
       " 33,\n",
       " 20,\n",
       " 25,\n",
       " 13,\n",
       " 24,\n",
       " 23,\n",
       " 16,\n",
       " 23,\n",
       " 28,\n",
       " 26,\n",
       " 29,\n",
       " 19,\n",
       " 35,\n",
       " 33,\n",
       " 18,\n",
       " 24,\n",
       " 21,\n",
       " 23,\n",
       " 20,\n",
       " 19,\n",
       " 10,\n",
       " 31,\n",
       " 21,\n",
       " 35,\n",
       " 20,\n",
       " 24,\n",
       " 11,\n",
       " 24,\n",
       " 19,\n",
       " 14,\n",
       " 16,\n",
       " 13,\n",
       " 19,\n",
       " 20,\n",
       " 19,\n",
       " 17,\n",
       " 25,\n",
       " 21,\n",
       " 22,\n",
       " 30,\n",
       " 40,\n",
       " 24,\n",
       " 22,\n",
       " 32,\n",
       " 24,\n",
       " 12,\n",
       " 27,\n",
       " 14,\n",
       " 25,\n",
       " 24,\n",
       " 23,\n",
       " 24,\n",
       " 14,\n",
       " 31,\n",
       " 25,\n",
       " 25,\n",
       " 14,\n",
       " 17,\n",
       " 28,\n",
       " 15,\n",
       " 26,\n",
       " 27,\n",
       " 15,\n",
       " 17,\n",
       " 20,\n",
       " 14,\n",
       " 24,\n",
       " 27,\n",
       " 27,\n",
       " 17,\n",
       " 24,\n",
       " 39,\n",
       " 31,\n",
       " 30,\n",
       " 57,\n",
       " 27,\n",
       " 16,\n",
       " 22,\n",
       " 16,\n",
       " 27,\n",
       " 56,\n",
       " 24,\n",
       " 19,\n",
       " 31,\n",
       " 17,\n",
       " 28,\n",
       " 15,\n",
       " 22,\n",
       " 10,\n",
       " 19,\n",
       " 10,\n",
       " 26,\n",
       " 32,\n",
       " 17,\n",
       " 18,\n",
       " 26,\n",
       " 19,\n",
       " 17,\n",
       " 24,\n",
       " 20,\n",
       " 22,\n",
       " 19,\n",
       " 16,\n",
       " 30,\n",
       " 24,\n",
       " 19,\n",
       " 23,\n",
       " 32,\n",
       " 17,\n",
       " 29,\n",
       " 20,\n",
       " 24,\n",
       " 19,\n",
       " 27,\n",
       " 9,\n",
       " 21,\n",
       " 11,\n",
       " 36,\n",
       " 23,\n",
       " 26,\n",
       " 27,\n",
       " 33,\n",
       " 77,\n",
       " 31,\n",
       " 18,\n",
       " 20,\n",
       " 34,\n",
       " 30,\n",
       " 18,\n",
       " 17,\n",
       " 19,\n",
       " 21,\n",
       " 17,\n",
       " 21,\n",
       " 29,\n",
       " 17,\n",
       " 26,\n",
       " 38,\n",
       " 20,\n",
       " 21,\n",
       " 42,\n",
       " 51,\n",
       " 19,\n",
       " 17,\n",
       " 57,\n",
       " 17,\n",
       " 12,\n",
       " 9,\n",
       " 46,\n",
       " 26,\n",
       " 45,\n",
       " 30,\n",
       " 24,\n",
       " 19,\n",
       " 27,\n",
       " 24,\n",
       " 16,\n",
       " 21,\n",
       " 15,\n",
       " 25,\n",
       " 9,\n",
       " 25,\n",
       " 9,\n",
       " 18,\n",
       " 24,\n",
       " 35,\n",
       " 18,\n",
       " 51,\n",
       " 33,\n",
       " 18,\n",
       " 19,\n",
       " 24,\n",
       " 59,\n",
       " 20,\n",
       " 32,\n",
       " 50,\n",
       " 33,\n",
       " 29,\n",
       " 31,\n",
       " 21,\n",
       " 15,\n",
       " 21,\n",
       " 17,\n",
       " 27,\n",
       " 18,\n",
       " 17,\n",
       " 21,\n",
       " 33,\n",
       " 29,\n",
       " 16,\n",
       " 21,\n",
       " 28,\n",
       " 30,\n",
       " 36,\n",
       " 12,\n",
       " 23,\n",
       " 25,\n",
       " 30,\n",
       " 32,\n",
       " 21,\n",
       " 21,\n",
       " 24,\n",
       " 23,\n",
       " 17,\n",
       " 19,\n",
       " 19,\n",
       " 29,\n",
       " 29,\n",
       " 15,\n",
       " 16,\n",
       " 26,\n",
       " 31,\n",
       " 27,\n",
       " 22,\n",
       " 31,\n",
       " 17,\n",
       " 31,\n",
       " 36,\n",
       " 19,\n",
       " 24,\n",
       " 37,\n",
       " 22,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 26,\n",
       " 10,\n",
       " 27,\n",
       " 15,\n",
       " 36,\n",
       " 23,\n",
       " 19,\n",
       " 20,\n",
       " 25,\n",
       " 19,\n",
       " 23,\n",
       " 24,\n",
       " 14,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 22,\n",
       " 14,\n",
       " 32,\n",
       " 16,\n",
       " 30,\n",
       " 17,\n",
       " 17,\n",
       " 41,\n",
       " 16,\n",
       " 12,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 21,\n",
       " 23,\n",
       " 19,\n",
       " 14,\n",
       " 21,\n",
       " 33,\n",
       " 18,\n",
       " 14,\n",
       " 29,\n",
       " 33,\n",
       " 28,\n",
       " 21,\n",
       " 21,\n",
       " 13,\n",
       " 11,\n",
       " 14,\n",
       " 21,\n",
       " 37,\n",
       " 14,\n",
       " 31,\n",
       " 19,\n",
       " 20,\n",
       " 17,\n",
       " 23,\n",
       " 23,\n",
       " 14,\n",
       " 28,\n",
       " 21,\n",
       " 41,\n",
       " 27,\n",
       " 17,\n",
       " 19,\n",
       " 13,\n",
       " 22,\n",
       " 24,\n",
       " 18,\n",
       " 20,\n",
       " 22,\n",
       " 33,\n",
       " 31,\n",
       " 37,\n",
       " 33,\n",
       " 21,\n",
       " 26,\n",
       " 24,\n",
       " 41,\n",
       " 31,\n",
       " 18,\n",
       " 31,\n",
       " 14,\n",
       " 22,\n",
       " 19,\n",
       " 48,\n",
       " 14,\n",
       " 38,\n",
       " 34,\n",
       " 23,\n",
       " 35,\n",
       " 22,\n",
       " 23,\n",
       " 26,\n",
       " 23,\n",
       " 30,\n",
       " 114,\n",
       " 19,\n",
       " 22,\n",
       " 18,\n",
       " 18,\n",
       " 25,\n",
       " 21,\n",
       " 33,\n",
       " 24,\n",
       " 24,\n",
       " 12,\n",
       " 35,\n",
       " 36,\n",
       " 94,\n",
       " 22,\n",
       " 50,\n",
       " 18,\n",
       " 21,\n",
       " 23,\n",
       " 29,\n",
       " 18,\n",
       " 16,\n",
       " 23,\n",
       " 23,\n",
       " 45,\n",
       " 12,\n",
       " 17,\n",
       " 37,\n",
       " 30,\n",
       " 16,\n",
       " 17,\n",
       " 21,\n",
       " 29,\n",
       " 29,\n",
       " 20,\n",
       " 11,\n",
       " 16,\n",
       " 14,\n",
       " 26,\n",
       " 23,\n",
       " 21,\n",
       " 18,\n",
       " 28,\n",
       " 21,\n",
       " 33,\n",
       " 22,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 17,\n",
       " 11,\n",
       " 16,\n",
       " 31,\n",
       " 28,\n",
       " 16,\n",
       " 25,\n",
       " 16,\n",
       " 14,\n",
       " 23,\n",
       " 25,\n",
       " 22,\n",
       " 28,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 31,\n",
       " 25,\n",
       " 21,\n",
       " 30,\n",
       " 22,\n",
       " 22,\n",
       " 41,\n",
       " 23,\n",
       " 25,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 20,\n",
       " 17,\n",
       " 23,\n",
       " 21,\n",
       " 34,\n",
       " 33,\n",
       " 29,\n",
       " 21,\n",
       " 32,\n",
       " 20,\n",
       " 19,\n",
       " 23,\n",
       " 35,\n",
       " 21,\n",
       " 30,\n",
       " 34,\n",
       " 35,\n",
       " 23,\n",
       " 20,\n",
       " 18,\n",
       " 30,\n",
       " 27,\n",
       " 25,\n",
       " 37,\n",
       " 18,\n",
       " 15,\n",
       " 20,\n",
       " 24,\n",
       " 19,\n",
       " 35,\n",
       " 31,\n",
       " 15,\n",
       " 24,\n",
       " 23,\n",
       " 20,\n",
       " 34,\n",
       " 11,\n",
       " 16,\n",
       " 26,\n",
       " 39,\n",
       " 36,\n",
       " 8,\n",
       " 26,\n",
       " 19,\n",
       " 10,\n",
       " 26,\n",
       " 24,\n",
       " 10,\n",
       " 24,\n",
       " 12,\n",
       " 26,\n",
       " 14,\n",
       " 20,\n",
       " 26,\n",
       " 18,\n",
       " 28,\n",
       " 25,\n",
       " 36,\n",
       " 22,\n",
       " 24,\n",
       " 23,\n",
       " 18,\n",
       " 26,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 20,\n",
       " 20,\n",
       " 18,\n",
       " 22,\n",
       " 42,\n",
       " 35,\n",
       " 31,\n",
       " 27,\n",
       " 25,\n",
       " 31,\n",
       " 25,\n",
       " 27,\n",
       " 27,\n",
       " 19,\n",
       " 23,\n",
       " 19,\n",
       " 24,\n",
       " 18,\n",
       " 22,\n",
       " 13,\n",
       " 22,\n",
       " 18,\n",
       " 16,\n",
       " 20,\n",
       " 12,\n",
       " 21,\n",
       " 24,\n",
       " 27,\n",
       " 35,\n",
       " 33,\n",
       " 22,\n",
       " 33,\n",
       " 19,\n",
       " 25,\n",
       " 17,\n",
       " 19,\n",
       " 26,\n",
       " 17,\n",
       " 24,\n",
       " 22,\n",
       " 15,\n",
       " 16,\n",
       " 23,\n",
       " 22,\n",
       " 31,\n",
       " 35,\n",
       " 23,\n",
       " 27,\n",
       " 18,\n",
       " 16,\n",
       " 40,\n",
       " 17,\n",
       " 27,\n",
       " 34,\n",
       " 33,\n",
       " 14,\n",
       " 34,\n",
       " 32,\n",
       " 32,\n",
       " 25,\n",
       " 26,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 8,\n",
       " 10,\n",
       " 18,\n",
       " 21,\n",
       " 19,\n",
       " 23,\n",
       " 19,\n",
       " 20,\n",
       " 29,\n",
       " 18,\n",
       " 14,\n",
       " 15,\n",
       " 36,\n",
       " 48,\n",
       " 30,\n",
       " 29,\n",
       " 16,\n",
       " 13,\n",
       " 27,\n",
       " 8,\n",
       " 19,\n",
       " 22,\n",
       " 18,\n",
       " 22,\n",
       " 42,\n",
       " 22,\n",
       " 23,\n",
       " 9,\n",
       " 23,\n",
       " 16,\n",
       " 18,\n",
       " 14,\n",
       " 22,\n",
       " 15,\n",
       " 46,\n",
       " 29,\n",
       " 18,\n",
       " 18,\n",
       " 26,\n",
       " 35,\n",
       " 21,\n",
       " 28,\n",
       " 20,\n",
       " 14,\n",
       " 18,\n",
       " 24,\n",
       " 17,\n",
       " 19,\n",
       " 22,\n",
       " 23,\n",
       " 14,\n",
       " 36,\n",
       " 17,\n",
       " 25,\n",
       " 32,\n",
       " 23,\n",
       " 9,\n",
       " 27,\n",
       " 22,\n",
       " 14,\n",
       " 14,\n",
       " 16,\n",
       " 28,\n",
       " 27,\n",
       " 22,\n",
       " 13,\n",
       " 25,\n",
       " 24,\n",
       " 23,\n",
       " 68,\n",
       " 16,\n",
       " 25,\n",
       " 21,\n",
       " 19,\n",
       " 26,\n",
       " 23,\n",
       " 25,\n",
       " 17,\n",
       " 20,\n",
       " 26,\n",
       " 23,\n",
       " 19,\n",
       " 30,\n",
       " 13,\n",
       " 24,\n",
       " 24,\n",
       " 14,\n",
       " 44,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 23,\n",
       " 34,\n",
       " 32,\n",
       " 19,\n",
       " 22,\n",
       " 45,\n",
       " 14,\n",
       " 28,\n",
       " 19,\n",
       " 62,\n",
       " 22,\n",
       " 12,\n",
       " 32,\n",
       " 16,\n",
       " 17,\n",
       " 30,\n",
       " 16,\n",
       " 27,\n",
       " 19,\n",
       " 28,\n",
       " 30,\n",
       " 21,\n",
       " 34,\n",
       " 17,\n",
       " 18,\n",
       " 14,\n",
       " 28,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 25,\n",
       " 21,\n",
       " 32,\n",
       " 15,\n",
       " 29,\n",
       " 19,\n",
       " 7,\n",
       " 30,\n",
       " 29,\n",
       " 14,\n",
       " 31,\n",
       " 25,\n",
       " 18,\n",
       " 16,\n",
       " 19,\n",
       " 30,\n",
       " 22,\n",
       " 18,\n",
       " 23,\n",
       " 21,\n",
       " 22,\n",
       " 20,\n",
       " 28,\n",
       " 33,\n",
       " 33,\n",
       " 27,\n",
       " 10,\n",
       " 25,\n",
       " 22,\n",
       " 35,\n",
       " 22,\n",
       " 19,\n",
       " 12,\n",
       " 26,\n",
       " 23,\n",
       " 20,\n",
       " 24,\n",
       " 17,\n",
       " 30,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 30,\n",
       " 23,\n",
       " 12,\n",
       " 16,\n",
       " 9,\n",
       " 40,\n",
       " 13,\n",
       " 24,\n",
       " 23,\n",
       " 21,\n",
       " 21,\n",
       " 18,\n",
       " 13,\n",
       " 25,\n",
       " 18,\n",
       " 18,\n",
       " 20,\n",
       " 10,\n",
       " 23,\n",
       " 22,\n",
       " 19,\n",
       " 22,\n",
       " 10,\n",
       " ...]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4317 \t 297\n",
      "4562 \t 259\n",
      "4685 \t 273\n",
      "4757 \t 388\n",
      "5139 \t 299\n",
      "5596 \t 265\n",
      "5601 \t 423\n",
      "5751 \t 271\n",
      "5991 \t 379\n",
      "6192 \t 267\n",
      "6544 \t 271\n",
      "6818 \t 337\n",
      "6831 \t 292\n",
      "7099 \t 420\n",
      "7201 \t 345\n",
      "10668 \t 383\n",
      "10738 \t 258\n",
      "10754 \t 301\n",
      "10834 \t 262\n",
      "10931 \t 265\n",
      "11348 \t 310\n",
      "11662 \t 466\n",
      "11952 \t 305\n",
      "11986 \t 259\n",
      "12195 \t 318\n",
      "12540 \t 310\n",
      "12574 \t 264\n",
      "12626 \t 261\n",
      "12635 \t 267\n",
      "12743 \t 294\n",
      "13518 \t 288\n",
      "13879 \t 543\n",
      "14180 \t 291\n",
      "14469 \t 331\n",
      "14622 \t 320\n",
      "15188 \t 304\n",
      "15235 \t 320\n",
      "15290 \t 257\n",
      "15722 \t 284\n",
      "15824 \t 268\n",
      "15830 \t 545\n",
      "16083 \t 324\n",
      "16255 \t 290\n",
      "16482 \t 531\n",
      "17038 \t 273\n",
      "17047 \t 300\n",
      "17195 \t 273\n",
      "17275 \t 339\n",
      "17331 \t 274\n",
      "17508 \t 288\n",
      "17665 \t 263\n",
      "17699 \t 309\n",
      "17715 \t 276\n",
      "17953 \t 259\n",
      "18080 \t 290\n",
      "18282 \t 294\n",
      "18579 \t 259\n",
      "18618 \t 267\n",
      "18644 \t 283\n",
      "18770 \t 293\n",
      "19276 \t 477\n",
      "19339 \t 322\n",
      "19378 \t 387\n",
      "19510 \t 326\n",
      "19573 \t 358\n",
      "19597 \t 370\n",
      "19605 \t 301\n",
      "19646 \t 269\n",
      "19703 \t 326\n",
      "19830 \t 299\n",
      "20065 \t 275\n",
      "20140 \t 268\n",
      "20426 \t 312\n",
      "20502 \t 271\n",
      "20537 \t 276\n",
      "20586 \t 262\n",
      "20868 \t 816\n",
      "21101 \t 260\n",
      "21111 \t 346\n",
      "21149 \t 539\n",
      "21303 \t 441\n",
      "21828 \t 298\n",
      "21908 \t 257\n",
      "21935 \t 737\n",
      "22025 \t 351\n",
      "22050 \t 269\n",
      "22071 \t 285\n",
      "22100 \t 1019\n",
      "22563 \t 459\n",
      "22606 \t 1051\n",
      "22717 \t 267\n",
      "22823 \t 263\n",
      "22953 \t 303\n",
      "22967 \t 770\n",
      "23038 \t 328\n",
      "23229 \t 350\n",
      "23308 \t 320\n",
      "23654 \t 455\n",
      "23922 \t 257\n",
      "24050 \t 327\n",
      "24419 \t 368\n",
      "24574 \t 342\n",
      "24594 \t 371\n",
      "24803 \t 328\n",
      "24901 \t 468\n",
      "24944 \t 265\n",
      "24952 \t 306\n",
      "25185 \t 290\n",
      "25246 \t 285\n",
      "25331 \t 281\n"
     ]
    }
   ],
   "source": [
    "cnt_outliers = []\n",
    "for i in range(len( cnts)):\n",
    "    #print(cnts[i])\n",
    "    if(cnts[i] > 256):\n",
    "        print(i,\"\\t\", cnts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"63. Les produits de l'exercice biennal dans le domaine des services d'appui administratif (budget ordinaire et fonds extrabudgÃ©taires) seront les suivants: a) Selon les besoins, reprÃ©sentation du SecrÃ©taire gÃ©nÃ©ral et du Directeur gÃ©nÃ©ral pour diverses questions administratives et financiÃ¨res aux rÃ©unions des organes intergouvernementaux de l'Organisation des Nations Unies tenues Ã  Vienne; reprÃ©sentation de l'Office des Nations Unies Ã  Vienne et de l'UNODC dans le cadre des services mixtes et communs avec l'Agence internationale de l'Ã©nergie atomique (AIEA), l'Organisation des Nations Unies pour le dÃ©veloppement industriel (ONUDI) et la Commission prÃ©paratoire de l'Organisation du TraitÃ© d'interdiction complÃ¨te des essais nuclÃ©aires, pour les questions relatives aux services communs assurÃ©s au Centre international de Vienne; reprÃ©sentation de l'ONU dans les nÃ©gociations avec le Gouvernement du pays hÃ´te concernant certains aspects de l'application de l'accord de siÃ¨ge; gestion des services d'appui administratif et suivi du dÃ©roulement de la rÃ©forme de la gestion et de l'application, par les services du SecrÃ©tariat installÃ©s Ã  Vienne, des directives et dÃ©cisions du ComitÃ© de la politique de gestion; b) Planification des programmes, budget et comptabilitÃ©: Ã©tablissement du budget-programme de l'exercice biennal 2012-2013 et des rapports sur l'exÃ©cution du budget de l'exercice biennal 2010-2011 en ce qui concerne les bureaux et les programmes de l'Office des Nations Unies Ã  Vienne et de l'UNODC; Ã©tablissement de projets d'Ã©tat des incidences sur les budgets-programmes Ã  soumettre Ã  l'approbation du SiÃ¨ge et fourniture de services connexes aux commissions techniques du Conseil Ã©conomique et social et Ã  d'autres organes; Ã©tablissement du budget consolidÃ© de l'UNODC pour les fonds extrabudgÃ©taires (Fonds du Programme des Nations Unies pour le contrÃ´le international des drogues et Fonds des Nations Unies pour la prÃ©vention du crime et la justice pÃ©nale) pour l'exercice 2012-2013; nÃ©gociation, suivi et application des accords relatifs Ã  la participation aux coÃ»ts conclus avec la Commission prÃ©paratoire de l'Organisation du TraitÃ© d'interdiction complÃ¨te des essais nuclÃ©aires, l'AIEA et l'ONUDI, ainsi que des accords de participation aux frais liÃ©s Ã  l'appui aux programmes fourni aux services du SecrÃ©tariat financÃ©s par des ressources extrabudgÃ©taires, aux entitÃ©s appliquant le rÃ©gime commun des Nations Unies et aux autres organisations utilisant les locaux du Centre international de Vienne; examen et approbation des accords de financement, et Ã©tablissement de rapports financiers Ã  l'intention des pays donateurs, conformÃ©ment aux accords; examen des principes comptables de l'Office des Nations Unies Ã  Vienne et de l'UNODC, et Ã©tablissement des comptes correspondants en vue de l'adoption des Normes comptables internationales du secteur public; comptabilitÃ©, Ã©tats de paie, ordonnancement des paiements, dÃ©caissements et gestion de la trÃ©sorerie de l'Office des Nations Unies Ã  Vienne et de l'UNODC; Ã©laboration des rÃ©ponses de l'Administration et examen des mesures prises pour donner suite aux rapports des organes de contrÃ´le externe et interne, tels que le ComitÃ© des commissaires aux comptes, le Corps commun d'inspection et le BSCI; formation et orientations relatives aux questions budgÃ©taires et financiÃ¨res pour toutes les entitÃ©s du SecrÃ©tariat sises Ã  Vienne, Ã©tablissement de procÃ©dures, suivi de l'utilisation des crÃ©dits allouÃ©s et tenue Ã  jour des statistiques concernant les vacances de postes; gestion des ressources extrabudgÃ©taires, en particulier en ce qui concerne les projets de coopÃ©ration technique, y compris l'examen des descriptifs de projet, l'ouverture de crÃ©dits, le suivi de l'exÃ©cution des dÃ©penses et l'Ã©tablissement de rapports financiers;\""
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.iloc[13879].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "543"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_l = df_all.iloc[13879].text.split()\n",
    "len(text_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['63.',\n",
       " 'Les',\n",
       " 'produits',\n",
       " 'de',\n",
       " \"l'exercice\",\n",
       " 'biennal',\n",
       " 'dans',\n",
       " 'le',\n",
       " 'domaine',\n",
       " 'des',\n",
       " 'services',\n",
       " \"d'appui\",\n",
       " 'administratif',\n",
       " '(budget',\n",
       " 'ordinaire',\n",
       " 'et',\n",
       " 'fonds',\n",
       " 'extrabudgÃ©taires)',\n",
       " 'seront',\n",
       " 'les',\n",
       " 'suivants:',\n",
       " 'a)',\n",
       " 'Selon',\n",
       " 'les',\n",
       " 'besoins,',\n",
       " 'reprÃ©sentation',\n",
       " 'du',\n",
       " 'SecrÃ©taire',\n",
       " 'gÃ©nÃ©ral',\n",
       " 'et',\n",
       " 'du',\n",
       " 'Directeur',\n",
       " 'gÃ©nÃ©ral',\n",
       " 'pour',\n",
       " 'diverses',\n",
       " 'questions',\n",
       " 'administratives',\n",
       " 'et',\n",
       " 'financiÃ¨res',\n",
       " 'aux',\n",
       " 'rÃ©unions',\n",
       " 'des',\n",
       " 'organes',\n",
       " 'intergouvernementaux',\n",
       " 'de',\n",
       " \"l'Organisation\",\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'tenues',\n",
       " 'Ã ',\n",
       " 'Vienne;',\n",
       " 'reprÃ©sentation',\n",
       " 'de',\n",
       " \"l'Office\",\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'Ã ',\n",
       " 'Vienne',\n",
       " 'et',\n",
       " 'de',\n",
       " \"l'UNODC\",\n",
       " 'dans',\n",
       " 'le',\n",
       " 'cadre',\n",
       " 'des',\n",
       " 'services',\n",
       " 'mixtes',\n",
       " 'et',\n",
       " 'communs',\n",
       " 'avec',\n",
       " \"l'Agence\",\n",
       " 'internationale',\n",
       " 'de',\n",
       " \"l'Ã©nergie\",\n",
       " 'atomique',\n",
       " '(AIEA),',\n",
       " \"l'Organisation\",\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'pour',\n",
       " 'le',\n",
       " 'dÃ©veloppement',\n",
       " 'industriel',\n",
       " '(ONUDI)',\n",
       " 'et',\n",
       " 'la',\n",
       " 'Commission',\n",
       " 'prÃ©paratoire',\n",
       " 'de',\n",
       " \"l'Organisation\",\n",
       " 'du',\n",
       " 'TraitÃ©',\n",
       " \"d'interdiction\",\n",
       " 'complÃ¨te',\n",
       " 'des',\n",
       " 'essais',\n",
       " 'nuclÃ©aires,',\n",
       " 'pour',\n",
       " 'les',\n",
       " 'questions',\n",
       " 'relatives',\n",
       " 'aux',\n",
       " 'services',\n",
       " 'communs',\n",
       " 'assurÃ©s',\n",
       " 'au',\n",
       " 'Centre',\n",
       " 'international',\n",
       " 'de',\n",
       " 'Vienne;',\n",
       " 'reprÃ©sentation',\n",
       " 'de',\n",
       " \"l'ONU\",\n",
       " 'dans',\n",
       " 'les',\n",
       " 'nÃ©gociations',\n",
       " 'avec',\n",
       " 'le',\n",
       " 'Gouvernement',\n",
       " 'du',\n",
       " 'pays',\n",
       " 'hÃ´te',\n",
       " 'concernant',\n",
       " 'certains',\n",
       " 'aspects',\n",
       " 'de',\n",
       " \"l'application\",\n",
       " 'de',\n",
       " \"l'accord\",\n",
       " 'de',\n",
       " 'siÃ¨ge;',\n",
       " 'gestion',\n",
       " 'des',\n",
       " 'services',\n",
       " \"d'appui\",\n",
       " 'administratif',\n",
       " 'et',\n",
       " 'suivi',\n",
       " 'du',\n",
       " 'dÃ©roulement',\n",
       " 'de',\n",
       " 'la',\n",
       " 'rÃ©forme',\n",
       " 'de',\n",
       " 'la',\n",
       " 'gestion',\n",
       " 'et',\n",
       " 'de',\n",
       " \"l'application,\",\n",
       " 'par',\n",
       " 'les',\n",
       " 'services',\n",
       " 'du',\n",
       " 'SecrÃ©tariat',\n",
       " 'installÃ©s',\n",
       " 'Ã ',\n",
       " 'Vienne,',\n",
       " 'des',\n",
       " 'directives',\n",
       " 'et',\n",
       " 'dÃ©cisions',\n",
       " 'du',\n",
       " 'ComitÃ©',\n",
       " 'de',\n",
       " 'la',\n",
       " 'politique',\n",
       " 'de',\n",
       " 'gestion;',\n",
       " 'b)',\n",
       " 'Planification',\n",
       " 'des',\n",
       " 'programmes,',\n",
       " 'budget',\n",
       " 'et',\n",
       " 'comptabilitÃ©:',\n",
       " 'Ã©tablissement',\n",
       " 'du',\n",
       " 'budget-programme',\n",
       " 'de',\n",
       " \"l'exercice\",\n",
       " 'biennal',\n",
       " '2012-2013',\n",
       " 'et',\n",
       " 'des',\n",
       " 'rapports',\n",
       " 'sur',\n",
       " \"l'exÃ©cution\",\n",
       " 'du',\n",
       " 'budget',\n",
       " 'de',\n",
       " \"l'exercice\",\n",
       " 'biennal',\n",
       " '2010-2011',\n",
       " 'en',\n",
       " 'ce',\n",
       " 'qui',\n",
       " 'concerne',\n",
       " 'les',\n",
       " 'bureaux',\n",
       " 'et',\n",
       " 'les',\n",
       " 'programmes',\n",
       " 'de',\n",
       " \"l'Office\",\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'Ã ',\n",
       " 'Vienne',\n",
       " 'et',\n",
       " 'de',\n",
       " \"l'UNODC;\",\n",
       " 'Ã©tablissement',\n",
       " 'de',\n",
       " 'projets',\n",
       " \"d'Ã©tat\",\n",
       " 'des',\n",
       " 'incidences',\n",
       " 'sur',\n",
       " 'les',\n",
       " 'budgets-programmes',\n",
       " 'Ã ',\n",
       " 'soumettre',\n",
       " 'Ã ',\n",
       " \"l'approbation\",\n",
       " 'du',\n",
       " 'SiÃ¨ge',\n",
       " 'et',\n",
       " 'fourniture',\n",
       " 'de',\n",
       " 'services',\n",
       " 'connexes',\n",
       " 'aux',\n",
       " 'commissions',\n",
       " 'techniques',\n",
       " 'du',\n",
       " 'Conseil',\n",
       " 'Ã©conomique',\n",
       " 'et',\n",
       " 'social',\n",
       " 'et',\n",
       " 'Ã ',\n",
       " \"d'autres\",\n",
       " 'organes;',\n",
       " 'Ã©tablissement',\n",
       " 'du',\n",
       " 'budget',\n",
       " 'consolidÃ©',\n",
       " 'de',\n",
       " \"l'UNODC\",\n",
       " 'pour',\n",
       " 'les',\n",
       " 'fonds',\n",
       " 'extrabudgÃ©taires',\n",
       " '(Fonds',\n",
       " 'du',\n",
       " 'Programme',\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'pour',\n",
       " 'le',\n",
       " 'contrÃ´le',\n",
       " 'international',\n",
       " 'des',\n",
       " 'drogues',\n",
       " 'et',\n",
       " 'Fonds',\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'pour',\n",
       " 'la',\n",
       " 'prÃ©vention',\n",
       " 'du',\n",
       " 'crime',\n",
       " 'et',\n",
       " 'la',\n",
       " 'justice',\n",
       " 'pÃ©nale)',\n",
       " 'pour',\n",
       " \"l'exercice\",\n",
       " '2012-2013;',\n",
       " 'nÃ©gociation,',\n",
       " 'suivi',\n",
       " 'et',\n",
       " 'application',\n",
       " 'des',\n",
       " 'accords',\n",
       " 'relatifs',\n",
       " 'Ã ',\n",
       " 'la',\n",
       " 'participation',\n",
       " 'aux',\n",
       " 'coÃ»ts',\n",
       " 'conclus',\n",
       " 'avec',\n",
       " 'la',\n",
       " 'Commission',\n",
       " 'prÃ©paratoire',\n",
       " 'de',\n",
       " \"l'Organisation\",\n",
       " 'du',\n",
       " 'TraitÃ©',\n",
       " \"d'interdiction\",\n",
       " 'complÃ¨te',\n",
       " 'des',\n",
       " 'essais',\n",
       " 'nuclÃ©aires,',\n",
       " \"l'AIEA\",\n",
       " 'et',\n",
       " \"l'ONUDI,\",\n",
       " 'ainsi',\n",
       " 'que',\n",
       " 'des',\n",
       " 'accords',\n",
       " 'de',\n",
       " 'participation',\n",
       " 'aux',\n",
       " 'frais',\n",
       " 'liÃ©s',\n",
       " 'Ã ',\n",
       " \"l'appui\",\n",
       " 'aux',\n",
       " 'programmes',\n",
       " 'fourni',\n",
       " 'aux',\n",
       " 'services',\n",
       " 'du',\n",
       " 'SecrÃ©tariat',\n",
       " 'financÃ©s',\n",
       " 'par',\n",
       " 'des',\n",
       " 'ressources',\n",
       " 'extrabudgÃ©taires,',\n",
       " 'aux',\n",
       " 'entitÃ©s',\n",
       " 'appliquant',\n",
       " 'le',\n",
       " 'rÃ©gime',\n",
       " 'commun',\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'et',\n",
       " 'aux',\n",
       " 'autres',\n",
       " 'organisations',\n",
       " 'utilisant',\n",
       " 'les',\n",
       " 'locaux',\n",
       " 'du',\n",
       " 'Centre',\n",
       " 'international',\n",
       " 'de',\n",
       " 'Vienne;',\n",
       " 'examen',\n",
       " 'et',\n",
       " 'approbation',\n",
       " 'des',\n",
       " 'accords',\n",
       " 'de',\n",
       " 'financement,',\n",
       " 'et',\n",
       " 'Ã©tablissement',\n",
       " 'de',\n",
       " 'rapports',\n",
       " 'financiers',\n",
       " 'Ã ',\n",
       " \"l'intention\",\n",
       " 'des',\n",
       " 'pays',\n",
       " 'donateurs,',\n",
       " 'conformÃ©ment',\n",
       " 'aux',\n",
       " 'accords;',\n",
       " 'examen',\n",
       " 'des',\n",
       " 'principes',\n",
       " 'comptables',\n",
       " 'de',\n",
       " \"l'Office\",\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'Ã ',\n",
       " 'Vienne',\n",
       " 'et',\n",
       " 'de',\n",
       " \"l'UNODC,\",\n",
       " 'et',\n",
       " 'Ã©tablissement',\n",
       " 'des',\n",
       " 'comptes',\n",
       " 'correspondants',\n",
       " 'en',\n",
       " 'vue',\n",
       " 'de',\n",
       " \"l'adoption\",\n",
       " 'des',\n",
       " 'Normes',\n",
       " 'comptables',\n",
       " 'internationales',\n",
       " 'du',\n",
       " 'secteur',\n",
       " 'public;',\n",
       " 'comptabilitÃ©,',\n",
       " 'Ã©tats',\n",
       " 'de',\n",
       " 'paie,',\n",
       " 'ordonnancement',\n",
       " 'des',\n",
       " 'paiements,',\n",
       " 'dÃ©caissements',\n",
       " 'et',\n",
       " 'gestion',\n",
       " 'de',\n",
       " 'la',\n",
       " 'trÃ©sorerie',\n",
       " 'de',\n",
       " \"l'Office\",\n",
       " 'des',\n",
       " 'Nations',\n",
       " 'Unies',\n",
       " 'Ã ',\n",
       " 'Vienne',\n",
       " 'et',\n",
       " 'de',\n",
       " \"l'UNODC;\",\n",
       " 'Ã©laboration',\n",
       " 'des',\n",
       " 'rÃ©ponses',\n",
       " 'de',\n",
       " \"l'Administration\",\n",
       " 'et',\n",
       " 'examen',\n",
       " 'des',\n",
       " 'mesures',\n",
       " 'prises',\n",
       " 'pour',\n",
       " 'donner',\n",
       " 'suite',\n",
       " 'aux',\n",
       " 'rapports',\n",
       " 'des',\n",
       " 'organes',\n",
       " 'de',\n",
       " 'contrÃ´le',\n",
       " 'externe',\n",
       " 'et',\n",
       " 'interne,',\n",
       " 'tels',\n",
       " 'que',\n",
       " 'le',\n",
       " 'ComitÃ©',\n",
       " 'des',\n",
       " 'commissaires',\n",
       " 'aux',\n",
       " 'comptes,',\n",
       " 'le',\n",
       " 'Corps',\n",
       " 'commun',\n",
       " \"d'inspection\",\n",
       " 'et',\n",
       " 'le',\n",
       " 'BSCI;',\n",
       " 'formation',\n",
       " 'et',\n",
       " 'orientations',\n",
       " 'relatives',\n",
       " 'aux',\n",
       " 'questions',\n",
       " 'budgÃ©taires',\n",
       " 'et',\n",
       " 'financiÃ¨res',\n",
       " 'pour',\n",
       " 'toutes',\n",
       " 'les',\n",
       " 'entitÃ©s',\n",
       " 'du',\n",
       " 'SecrÃ©tariat',\n",
       " 'sises',\n",
       " 'Ã ',\n",
       " 'Vienne,',\n",
       " 'Ã©tablissement',\n",
       " 'de',\n",
       " 'procÃ©dures,',\n",
       " 'suivi',\n",
       " 'de',\n",
       " \"l'utilisation\",\n",
       " 'des',\n",
       " 'crÃ©dits',\n",
       " 'allouÃ©s',\n",
       " 'et',\n",
       " 'tenue',\n",
       " 'Ã ',\n",
       " 'jour',\n",
       " 'des',\n",
       " 'statistiques',\n",
       " 'concernant',\n",
       " 'les',\n",
       " 'vacances',\n",
       " 'de',\n",
       " 'postes;',\n",
       " 'gestion',\n",
       " 'des',\n",
       " 'ressources',\n",
       " 'extrabudgÃ©taires,',\n",
       " 'en',\n",
       " 'particulier',\n",
       " 'en',\n",
       " 'ce',\n",
       " 'qui',\n",
       " 'concerne',\n",
       " 'les',\n",
       " 'projets',\n",
       " 'de',\n",
       " 'coopÃ©ration',\n",
       " 'technique,',\n",
       " 'y',\n",
       " 'compris',\n",
       " \"l'examen\",\n",
       " 'des',\n",
       " 'descriptifs',\n",
       " 'de',\n",
       " 'projet,',\n",
       " \"l'ouverture\",\n",
       " 'de',\n",
       " 'crÃ©dits,',\n",
       " 'le',\n",
       " 'suivi',\n",
       " 'de',\n",
       " \"l'exÃ©cution\",\n",
       " 'des',\n",
       " 'dÃ©penses',\n",
       " 'et',\n",
       " \"l'Ã©tablissement\",\n",
       " 'de',\n",
       " 'rapports',\n",
       " 'financiers;']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
