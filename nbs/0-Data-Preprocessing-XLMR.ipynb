{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author: Prashant K. Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io, sys\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diptesh/workspace/PR-AAAI22-SDU-ST1-AE/nbs\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"danish\", \"english\", \"french\", \"persian\", \"spanish\", \"vietnamese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON to TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "danishDFTrain = pd.read_json(\"../data/\"+langs[0]+\"/train.json\")\n",
    "danishDFDev = pd.read_json(\"../data/\"+langs[0]+\"/dev.json\")\n",
    "\n",
    "englishDFTrainLegal = pd.read_json(\"../data/\"+langs[1]+\"/legal/train.json\")\n",
    "englishDFDevLegal = pd.read_json(\"../data/\"+langs[1]+\"/legal/dev.json\")\n",
    "englishDFTrainScientific = pd.read_json(\"../data/\"+langs[1]+\"/scientific/train.json\")\n",
    "englishDFDevScientific = pd.read_json(\"../data/\"+langs[1]+\"/scientific/dev.json\")\n",
    "\n",
    "frenchDFTrain = pd.read_json(\"../data/\"+langs[2]+\"/train.json\")\n",
    "frenchDFDev = pd.read_json(\"../data/\"+langs[2]+\"/dev.json\")\n",
    "\n",
    "persianDFTrain = pd.read_json(\"../data/\"+langs[3]+\"/train.json\")\n",
    "persianDFDev = pd.read_json(\"../data/\"+langs[3]+\"/dev.json\")\n",
    "\n",
    "spanishDFTrain = pd.read_json(\"../data/\"+langs[4]+\"/train.json\")\n",
    "spanishDFDev = pd.read_json(\"../data/\"+langs[4]+\"/dev.json\")\n",
    "\n",
    "vietnameseDFTrain = pd.read_json(\"../data/\"+langs[5]+\"/train.json\")\n",
    "vietnameseDFDev = pd.read_json(\"../data/\"+langs[5]+\"/dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' An arguably better approach to representation learning is Canonical Correlation Analysis (CCA) that induces representations that are maximally cor-',\n",
       " [[91, 94]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishDFTrainScientific.text[2], englishDFTrainScientific.acronyms[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting text from the numerical indexes (Hadeel's Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHOR: Hadeel S\n",
    "\n",
    "def create_acronyms(data):\n",
    "    acs2 = []\n",
    "    longf = []\n",
    "\n",
    "    acros = data.acronyms.values\n",
    "    txt = data.text.values\n",
    "    zipped = list(zip(txt,acros))\n",
    "    for z in zipped:\n",
    "        acs = []\n",
    "        for i in range(len(z[1])):\n",
    "            acs.append(z[0][z[1][i][0]:z[1][i][1]])\n",
    "        acs2.append(acs)\n",
    "    data['acronyms-text'] =acs2\n",
    "    \n",
    "# Creating long forms from here\n",
    "\n",
    "    longform = data[\"long-forms\"].values\n",
    "    zipped2 = list(zip(txt,longform))\n",
    "    for z2 in zipped2:\n",
    "        longform_temp = []\n",
    "        for i in range(len(z2[1])):\n",
    "            longform_temp.append(z2[0][z2[1][i][0]:z2[1][i][1]])\n",
    "        longf.append(longform_temp)\n",
    "    data[\"long-forms-text\"]= longf\n",
    "    \n",
    "    return data\n",
    "#     return data.to_csv(\"datatsvs/\" + name +\".tsv\", index = False, encoding='utf8', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific = create_acronyms(englishDFTrainScientific)\n",
    "englishDFDevScientific = create_acronyms(englishDFDevScientific)\n",
    "englishDFTrainLegal = create_acronyms(englishDFTrainLegal)\n",
    "englishDFDevLegal = create_acronyms(englishDFDevLegal)\n",
    "danishDFTrain = create_acronyms(danishDFTrain)\n",
    "danishDFDev = create_acronyms(danishDFDev)\n",
    "frenchDFTrain = create_acronyms(frenchDFTrain)\n",
    "frenchDFDev = create_acronyms(frenchDFDev)\n",
    "persianDFTrain = create_acronyms(persianDFTrain)\n",
    "persianDFDev = create_acronyms(persianDFDev)\n",
    "spanishDFTrain = create_acronyms(spanishDFTrain)\n",
    "spanishDFDev = create_acronyms(spanishDFDev)\n",
    "vietnameseDFTrain = create_acronyms(vietnameseDFTrain)\n",
    "vietnameseDFDev = create_acronyms(vietnameseDFDev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating columns for proper TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific = englishDFTrainScientific[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFDevScientific = englishDFDevScientific[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFTrainLegal = englishDFTrainLegal[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFDevLegal = englishDFDevLegal[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "danishDFTrain = danishDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "danishDFDev = danishDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "frenchDFTrain = frenchDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "frenchDFDev = frenchDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "persianDFTrain = persianDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "persianDFDev = persianDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "spanishDFTrain = spanishDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "spanishDFDev = spanishDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "vietnameseDFTrain = vietnameseDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "vietnameseDFDev = vietnameseDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLangDFTrain = englishDFTrainScientific.append(englishDFTrainLegal, ignore_index=True).append(danishDFTrain, ignore_index=True).append(frenchDFTrain, ignore_index=True).append(persianDFTrain, ignore_index=True).append(spanishDFTrain, ignore_index=True).append(vietnameseDFTrain, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>1824</td>\n",
       "      <td>al., ( 2013) but replace Markov Logic Networks...</td>\n",
       "      <td>[[78, 81]]</td>\n",
       "      <td>[[52, 76]]</td>\n",
       "      <td>[PSL]</td>\n",
       "      <td>[Probabilistic Soft Logic]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4304</th>\n",
       "      <td>325</td>\n",
       "      <td>133).]  Panel discussion on \"Data for the sust...</td>\n",
       "      <td>[[73, 78]]</td>\n",
       "      <td>[[42, 71]]</td>\n",
       "      <td>[SDG's]</td>\n",
       "      <td>[sustainable development goals]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>1352</td>\n",
       "      <td>HSHW = Hazardous substances and hazardous wast...</td>\n",
       "      <td>[[73, 75], [0, 4], [48, 50]]</td>\n",
       "      <td>[[78, 101], [7, 47], [53, 72]]</td>\n",
       "      <td>[EA, HSHW, RE]</td>\n",
       "      <td>[Expected accomplishment, Hazardous substances...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23518</th>\n",
       "      <td>3774</td>\n",
       "      <td>En el sistema de las Naciones Unidas, el ACNUR...</td>\n",
       "      <td>[[370, 375], [41, 46], [274, 279]]</td>\n",
       "      <td>[[307, 368]]</td>\n",
       "      <td>[MANUD, ACNUR, ACNUR]</td>\n",
       "      <td>[Marco de Asistencia de las Naciones Unidas pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>317</td>\n",
       "      <td>features to argument classification models, bu...</td>\n",
       "      <td>[[133, 136]]</td>\n",
       "      <td>[[108, 131]]</td>\n",
       "      <td>[ILP]</td>\n",
       "      <td>[integer linear programs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7274</th>\n",
       "      <td>3295</td>\n",
       "      <td>17. Among the smaller agencies, voluntary con...</td>\n",
       "      <td>[[171, 176]]</td>\n",
       "      <td>[[143, 169]]</td>\n",
       "      <td>[UNWTO]</td>\n",
       "      <td>[World Tourism Organization]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>2331</td>\n",
       "      <td>tice to both characteristics mentioned above. ...</td>\n",
       "      <td>[[114, 116], [121, 123]]</td>\n",
       "      <td>[[98, 112]]</td>\n",
       "      <td>[CF, CF]</td>\n",
       "      <td>[context factor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19031</th>\n",
       "      <td>623</td>\n",
       "      <td>به بیان دیگر وقفه جزء تعاریف CPU می باشد . 2 -...</td>\n",
       "      <td>[[29, 32], [61, 64], [253, 256], [321, 324]]</td>\n",
       "      <td>[[47, 58]]</td>\n",
       "      <td>[CPU, ISR, PLC, CPU]</td>\n",
       "      <td>[برنامه وقفه]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10794</th>\n",
       "      <td>169</td>\n",
       "      <td>TRANS/WP.15/1999/42 (Allemagne) Documents info...</td>\n",
       "      <td>[[6, 11], [0, 5], [55, 60], [72, 78], [80, 83]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WP.15, TRANS, NF.23, INF.18, IRU]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16777</th>\n",
       "      <td>6152</td>\n",
       "      <td>FEM + Capacités 21 Total Abréviations : FEM = ...</td>\n",
       "      <td>[[145, 148], [0, 3], [40, 43], [82, 86]]</td>\n",
       "      <td>[[151, 180], [46, 80], [89, 143]]</td>\n",
       "      <td>[AST, FEM, FEM, AEPP]</td>\n",
       "      <td>[appui aux services techniques, Fonds pour l'e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26947 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text  \\\n",
       "1823   1824  al., ( 2013) but replace Markov Logic Networks...   \n",
       "4304    325  133).]  Panel discussion on \"Data for the sust...   \n",
       "5331   1352  HSHW = Hazardous substances and hazardous wast...   \n",
       "23518  3774  En el sistema de las Naciones Unidas, el ACNUR...   \n",
       "316     317  features to argument classification models, bu...   \n",
       "...     ...                                                ...   \n",
       "7274   3295   17. Among the smaller agencies, voluntary con...   \n",
       "2330   2331  tice to both characteristics mentioned above. ...   \n",
       "19031   623  به بیان دیگر وقفه جزء تعاریف CPU می باشد . 2 -...   \n",
       "10794   169  TRANS/WP.15/1999/42 (Allemagne) Documents info...   \n",
       "16777  6152  FEM + Capacités 21 Total Abréviations : FEM = ...   \n",
       "\n",
       "                                              acronyms  \\\n",
       "1823                                        [[78, 81]]   \n",
       "4304                                        [[73, 78]]   \n",
       "5331                      [[73, 75], [0, 4], [48, 50]]   \n",
       "23518               [[370, 375], [41, 46], [274, 279]]   \n",
       "316                                       [[133, 136]]   \n",
       "...                                                ...   \n",
       "7274                                      [[171, 176]]   \n",
       "2330                          [[114, 116], [121, 123]]   \n",
       "19031     [[29, 32], [61, 64], [253, 256], [321, 324]]   \n",
       "10794  [[6, 11], [0, 5], [55, 60], [72, 78], [80, 83]]   \n",
       "16777         [[145, 148], [0, 3], [40, 43], [82, 86]]   \n",
       "\n",
       "                              long-forms                       acronyms-text  \\\n",
       "1823                          [[52, 76]]                               [PSL]   \n",
       "4304                          [[42, 71]]                             [SDG's]   \n",
       "5331      [[78, 101], [7, 47], [53, 72]]                      [EA, HSHW, RE]   \n",
       "23518                       [[307, 368]]               [MANUD, ACNUR, ACNUR]   \n",
       "316                         [[108, 131]]                               [ILP]   \n",
       "...                                  ...                                 ...   \n",
       "7274                        [[143, 169]]                             [UNWTO]   \n",
       "2330                         [[98, 112]]                            [CF, CF]   \n",
       "19031                         [[47, 58]]                [CPU, ISR, PLC, CPU]   \n",
       "10794                                 []  [WP.15, TRANS, NF.23, INF.18, IRU]   \n",
       "16777  [[151, 180], [46, 80], [89, 143]]               [AST, FEM, FEM, AEPP]   \n",
       "\n",
       "                                         long-forms-text  \n",
       "1823                          [Probabilistic Soft Logic]  \n",
       "4304                     [sustainable development goals]  \n",
       "5331   [Expected accomplishment, Hazardous substances...  \n",
       "23518  [Marco de Asistencia de las Naciones Unidas pa...  \n",
       "316                            [integer linear programs]  \n",
       "...                                                  ...  \n",
       "7274                        [World Tourism Organization]  \n",
       "2330                                    [context factor]  \n",
       "19031                                      [برنامه وقفه]  \n",
       "10794                                                 []  \n",
       "16777  [appui aux services techniques, Fonds pour l'e...  \n",
       "\n",
       "[26947 rows x 6 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allLangDFTrain.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific.to_csv(\"../data_tsvs/englishDFTrainScientific.tsv\", sep='\\t', index=False)\n",
    "englishDFDevScientific.to_csv(\"../data_tsvs/englishDFDevScientific.tsv\", sep='\\t', index=False)\n",
    "englishDFTrainLegal.to_csv(\"../data_tsvs/englishDFTrainLegal.tsv\", sep='\\t', index=False)\n",
    "englishDFDevLegal.to_csv(\"../data_tsvs/englishDFDevLegal.tsv\", sep='\\t', index=False)\n",
    "danishDFTrain.to_csv(\"../data_tsvs/danishDFTrain.tsv\", sep='\\t', index=False)\n",
    "danishDFDev.to_csv(\"../data_tsvs/danishDFDev.tsv\", sep='\\t', index=False)\n",
    "frenchDFTrain.to_csv(\"../data_tsvs/frenchDFTrain.tsv\", sep='\\t', index=False)\n",
    "frenchDFDev.to_csv(\"../data_tsvs/frenchDFDev.tsv\", sep='\\t', index=False)\n",
    "persianDFTrain.to_csv(\"../data_tsvs/persianDFTrain.tsv\", sep='\\t', index=False)\n",
    "persianDFDev.to_csv(\"../data_tsvs/persianDFDev.tsv\", sep='\\t', index=False)\n",
    "spanishDFTrain.to_csv(\"../data_tsvs/spanishDFTrain.tsv\", sep='\\t', index=False)\n",
    "spanishDFDev.to_csv(\"../data_tsvs/spanishDFDev.tsv\", sep='\\t', index=False)\n",
    "vietnameseDFTrain.to_csv(\"../data_tsvs/vietnameseDFTrain.tsv\", sep='\\t', index=False)\n",
    "vietnameseDFDev.to_csv(\"../data_tsvs/vietnameseDFDev.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLangDFTrain.to_csv(\"../data_tsvs/allLangDFTrain.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSVs to CoNLL format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO(txt, an_ids, lf_ids):\n",
    "    \"\"\"\n",
    "    input: text\n",
    "    ouput: tags , bio_tags\n",
    "        \n",
    "        tags :    list (AN, FL, OT)\n",
    "        bio_tags: list (O, B-AN, I-AN, B-LF, I-LF)\n",
    "    \"\"\"\n",
    "    \n",
    "    txt_l = word_tokenize(txt)\n",
    "    \n",
    "    tags = [\"OT\"]*len(txt_l)\n",
    "    bio_tags = [\"O\"]*len(txt_l)\n",
    "    \n",
    "    #txt[an_ids[0][0]:an_ids[0][1]]\n",
    "    \n",
    "    ANs = []\n",
    "    LFs = []\n",
    "    \n",
    "    for an_id in an_ids:\n",
    "        ANs.append(txt[an_id[0]:an_id[1]])\n",
    "        \n",
    "    for lf_id in lf_ids:\n",
    "        LFs.append(txt[lf_id[0]:lf_id[1]])\n",
    "    \n",
    "    AN_start = [False]*len(ANs)\n",
    "    LF_start = [False]*len(LFs)\n",
    "    \n",
    "    \n",
    "    for tok_id in range(0,len(txt_l)):\n",
    "        tok = txt_l[tok_id]\n",
    "        \n",
    "        for AN_idx in range(0,len(ANs)):\n",
    "           \n",
    "            AN = ANs[AN_idx]\n",
    "            \n",
    "            if(tok in AN):\n",
    "                tags[tok_id] = \"AN\"\n",
    "                \n",
    "                if(AN_start[AN_idx] == False):\n",
    "                    AN_start[AN_idx] = True\n",
    "                    bio_tags[tok_id] = \"B-AN\"\n",
    "                else:\n",
    "                    bio_tags[tok_id] = \"I-AN\"\n",
    "        \n",
    "        for LF_idx in range(0,len(LFs)):\n",
    "            \n",
    "            LF = LFs[LF_idx]\n",
    "            \n",
    "            if(tok in LF):\n",
    "                tags[tok_id] = \"LF\"\n",
    "                \n",
    "                if(LF_start[LF_idx] == False):\n",
    "                    LF_start[LF_idx] = True\n",
    "                    bio_tags[tok_id] = \"B-LF\"\n",
    "                else:\n",
    "                    bio_tags[tok_id] = \"I-LF\"\n",
    "                    \n",
    "    return txt_l, tags, bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(df, idx):\n",
    "    df_i = df.iloc[idx]\n",
    "    \n",
    "    txt = str(df_i[\"text\"]).strip()\n",
    "    acrnm_ids = ast.literal_eval(df_i[\"acronyms\"])\n",
    "    longform_ids = ast.literal_eval(df_i[\"long-forms\"])\n",
    "    \n",
    "    return txt, acrnm_ids, longform_ids\n",
    "\n",
    "def prepare(df, cnt, fname=\"output.txt\"):\n",
    "    \n",
    "    file = open(fname, \"w\")\n",
    "    file.write(\"-DOCSTART- -X- O\\n\\n\")\n",
    "    for idx in tqdm(range(cnt)):\n",
    "        txt, an_ids, lf_ids = extract(df, idx)\n",
    "        \n",
    "        txt_l, tags, bio_tags = BIO(txt, an_ids, lf_ids)\n",
    "        \n",
    "        for txt_id in range(0,len(txt_l)):\n",
    "            string = txt_l[txt_id]+ \" \" + tags[txt_id] + \" \" + bio_tags[txt_id]\n",
    "            \n",
    "            file.write(string + \" \\n\")\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    \n",
    "def convert_to_BIO(fileTrain, fileDev, lang, domain):\n",
    "    df_train = pd.read_csv(fileTrain, sep=\"\\t\")\n",
    "    df_dev = pd.read_csv(fileDev, sep=\"\\t\")\n",
    "    df_combine = df_train.append(df_dev, ignore_index=True)\n",
    "    \n",
    "    cnt_train = df_train.shape[0]\n",
    "    cnt_dev = df_dev.shape[0]\n",
    "    \n",
    "    train_cnt = cnt_train - 500 ## Assuming 300 instances are enough to validate\n",
    "    valid_cnt = 500\n",
    "    test_cnt = cnt_dev\n",
    "    \n",
    "    df_train_ = df_combine.iloc[:train_cnt]\n",
    "    df_valid_ = df_combine.iloc[train_cnt: train_cnt + valid_cnt].reset_index()\n",
    "    df_test_ = df_combine.iloc[train_cnt + valid_cnt: train_cnt + valid_cnt + test_cnt].reset_index()\n",
    "    \n",
    "    # Prepare training file\n",
    "    if ( domain == \"\" ):\n",
    "        prepare(df_train_, train_cnt, \"../processed/\" + lang + \"/train.txt\") # Prepare train file\n",
    "        prepare(df_valid_, valid_cnt, \"../processed/\" + lang + \"/valid.txt\") # Prepare validation file\n",
    "        prepare(df_test_, test_cnt, \"../processed/\" + lang + \"/test.txt\") # Prepare test file\n",
    "    else:\n",
    "        prepare(df_train_, train_cnt, \"../processed/\" + lang + \"/\" + domain + \"/train.txt\") # Prepare train file\n",
    "        prepare(df_valid_, valid_cnt, \"../processed/\" + lang + \"/\" + domain +\"/valid.txt\") # Prepare validation file\n",
    "        prepare(df_test_, test_cnt, \"../processed/\" + lang+  \"/\" + domain + \"/test.txt\") # Prepare test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3064/3064 [00:01<00:00, 2152.94it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2170.15it/s]\n",
      "100%|██████████| 445/445 [00:00<00:00, 2187.93it/s]\n",
      "100%|██████████| 3480/3480 [00:01<00:00, 3149.28it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3214.38it/s]\n",
      "100%|██████████| 497/497 [00:00<00:00, 3051.39it/s]\n",
      "100%|██████████| 2582/2582 [00:01<00:00, 2132.68it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2112.48it/s]\n",
      "100%|██████████| 385/385 [00:00<00:00, 2098.66it/s]\n",
      "100%|██████████| 7283/7283 [00:03<00:00, 1911.74it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1875.05it/s]\n",
      "100%|██████████| 973/973 [00:00<00:00, 1857.16it/s]\n",
      "100%|██████████| 836/836 [00:00<00:00, 2133.70it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1706.88it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 1724.16it/s]\n",
      "100%|██████████| 5428/5428 [00:02<00:00, 1895.69it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 1903.84it/s]\n",
      "100%|██████████| 741/741 [00:00<00:00, 1910.13it/s]\n",
      "100%|██████████| 774/774 [00:00<00:00, 3259.15it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3211.68it/s]\n",
      "100%|██████████| 159/159 [00:00<00:00, 3195.48it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_BIO(\"../data_tsvs/englishDFTrainLegal.tsv\", \"../data_tsvs/englishDFDevLegal.tsv\", \"eng\", \"legal\")\n",
    "convert_to_BIO(\"../data_tsvs/englishDFTrainScientific.tsv\", \"../data_tsvs/englishDFDevScientific.tsv\", \"eng\", \"scientific\")\n",
    "convert_to_BIO(\"../data_tsvs/danishDFTrain.tsv\", \"../data_tsvs/danishDFDev.tsv\", \"dan\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/frenchDFTrain.tsv\", \"../data_tsvs/frenchDFDev.tsv\", \"fre\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/persianDFTrain.tsv\", \"../data_tsvs/persianDFDev.tsv\", \"per\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/spanishDFTrain.tsv\", \"../data_tsvs/spanishDFDev.tsv\", \"esp\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/vietnameseDFTrain.tsv\", \"../data_tsvs/vietnameseDFDev.tsv\", \"vie\", \"\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26447/26447 [00:12<00:00, 2121.08it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2933.15it/s]\n",
      "100%|██████████| 445/445 [00:00<00:00, 2022.25it/s]\n",
      "100%|██████████| 26447/26447 [00:12<00:00, 2128.56it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3126.89it/s]\n",
      "100%|██████████| 497/497 [00:00<00:00, 3165.90it/s]\n",
      "100%|██████████| 26447/26447 [00:12<00:00, 2141.74it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3108.11it/s]\n",
      "100%|██████████| 385/385 [00:00<00:00, 2120.48it/s]\n",
      "100%|██████████| 26447/26447 [00:12<00:00, 2124.54it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2531.53it/s]\n",
      "100%|██████████| 973/973 [00:00<00:00, 1862.57it/s]\n",
      "100%|██████████| 26447/26447 [00:12<00:00, 2105.10it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2566.13it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 1596.49it/s]\n",
      "100%|██████████| 26447/26447 [00:12<00:00, 2112.66it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2595.93it/s]\n",
      "100%|██████████| 741/741 [00:00<00:00, 1870.59it/s]\n",
      "100%|██████████| 26447/26447 [00:12<00:00, 2135.17it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3143.43it/s]\n",
      "100%|██████████| 159/159 [00:00<00:00, 3120.47it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/englishDFDevLegal.tsv\", \"engAll\", \"legal\")\n",
    "convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/englishDFDevScientific.tsv\", \"engAll\", \"scientific\")\n",
    "convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/danishDFDev.tsv\", \"danAll\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/frenchDFDev.tsv\", \"freAll\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/persianDFDev.tsv\", \"perAll\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/spanishDFDev.tsv\", \"espAll\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/vietnameseDFDev.tsv\", \"vieAll\", \"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
