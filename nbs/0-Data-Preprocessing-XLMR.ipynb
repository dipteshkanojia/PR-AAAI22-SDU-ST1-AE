{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author: Prashant K. Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io, sys\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diptesh/workspace/PR-AAAI22-SDU-ST1-AE/nbs\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"danish\", \"english\", \"french\", \"persian\", \"spanish\", \"vietnamese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON to TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "danishDFTrain = pd.read_json(\"../data/\"+langs[0]+\"/train.json\")\n",
    "danishDFDev = pd.read_json(\"../data/\"+langs[0]+\"/dev.json\")\n",
    "\n",
    "englishDFTrainLegal = pd.read_json(\"../data/\"+langs[1]+\"/legal/train.json\")\n",
    "englishDFDevLegal = pd.read_json(\"../data/\"+langs[1]+\"/legal/dev.json\")\n",
    "englishDFTrainScientific = pd.read_json(\"../data/\"+langs[1]+\"/scientific/train.json\")\n",
    "englishDFDevScientific = pd.read_json(\"../data/\"+langs[1]+\"/scientific/dev.json\")\n",
    "\n",
    "frenchDFTrain = pd.read_json(\"../data/\"+langs[2]+\"/train.json\")\n",
    "frenchDFDev = pd.read_json(\"../data/\"+langs[2]+\"/dev.json\")\n",
    "\n",
    "persianDFTrain = pd.read_json(\"../data/\"+langs[3]+\"/train.json\")\n",
    "persianDFDev = pd.read_json(\"../data/\"+langs[3]+\"/dev.json\")\n",
    "\n",
    "spanishDFTrain = pd.read_json(\"../data/\"+langs[4]+\"/train.json\")\n",
    "spanishDFDev = pd.read_json(\"../data/\"+langs[4]+\"/dev.json\")\n",
    "\n",
    "vietnameseDFTrain = pd.read_json(\"../data/\"+langs[5]+\"/train.json\")\n",
    "vietnameseDFDev = pd.read_json(\"../data/\"+langs[5]+\"/dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' An arguably better approach to representation learning is Canonical Correlation Analysis (CCA) that induces representations that are maximally cor-',\n",
       " [[91, 94]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishDFTrainScientific.text[2], englishDFTrainScientific.acronyms[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extracting text from the numerical indexes (Hadeel's Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHOR: Hadeel S\n",
    "\n",
    "def create_acronyms(data):\n",
    "    acs2 = []\n",
    "    longf = []\n",
    "\n",
    "    acros = data.acronyms.values\n",
    "    txt = data.text.values\n",
    "    zipped = list(zip(txt,acros))\n",
    "    for z in zipped:\n",
    "        acs = []\n",
    "        for i in range(len(z[1])):\n",
    "            acs.append(z[0][z[1][i][0]:z[1][i][1]])\n",
    "        acs2.append(acs)\n",
    "    data['acronyms-text'] =acs2\n",
    "    \n",
    "# Creating long forms from here\n",
    "\n",
    "    longform = data[\"long-forms\"].values\n",
    "    zipped2 = list(zip(txt,longform))\n",
    "    for z2 in zipped2:\n",
    "        longform_temp = []\n",
    "        for i in range(len(z2[1])):\n",
    "            longform_temp.append(z2[0][z2[1][i][0]:z2[1][i][1]])\n",
    "        longf.append(longform_temp)\n",
    "    data[\"long-forms-text\"]= longf\n",
    "    \n",
    "    return data\n",
    "#     return data.to_csv(\"datatsvs/\" + name +\".tsv\", index = False, encoding='utf8', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific = create_acronyms(englishDFTrainScientific)\n",
    "englishDFDevScientific = create_acronyms(englishDFDevScientific)\n",
    "englishDFTrainLegal = create_acronyms(englishDFTrainLegal)\n",
    "englishDFDevLegal = create_acronyms(englishDFDevLegal)\n",
    "danishDFTrain = create_acronyms(danishDFTrain)\n",
    "danishDFDev = create_acronyms(danishDFDev)\n",
    "frenchDFTrain = create_acronyms(frenchDFTrain)\n",
    "frenchDFDev = create_acronyms(frenchDFDev)\n",
    "persianDFTrain = create_acronyms(persianDFTrain)\n",
    "persianDFDev = create_acronyms(persianDFDev)\n",
    "spanishDFTrain = create_acronyms(spanishDFTrain)\n",
    "spanishDFDev = create_acronyms(spanishDFDev)\n",
    "vietnameseDFTrain = create_acronyms(vietnameseDFTrain)\n",
    "vietnameseDFDev = create_acronyms(vietnameseDFDev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating columns for proper TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific = englishDFTrainScientific[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFDevScientific = englishDFDevScientific[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFTrainLegal = englishDFTrainLegal[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFDevLegal = englishDFDevLegal[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "danishDFTrain = danishDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "danishDFDev = danishDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "frenchDFTrain = frenchDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "frenchDFDev = frenchDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "persianDFTrain = persianDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "persianDFDev = persianDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "spanishDFTrain = spanishDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "spanishDFDev = spanishDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "vietnameseDFTrain = vietnameseDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "vietnameseDFDev = vietnameseDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLangDFTrain = englishDFTrainScientific.append(englishDFTrainLegal, ignore_index=True).append(danishDFTrain, ignore_index=True).append(frenchDFTrain, ignore_index=True).append(persianDFTrain, ignore_index=True).append(spanishDFTrain, ignore_index=True).append(vietnameseDFTrain, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15267</th>\n",
       "      <td>4642</td>\n",
       "      <td>fois encore, on ne dispose pas de données vent...</td>\n",
       "      <td>[[178, 181]]</td>\n",
       "      <td>[[139, 176]]</td>\n",
       "      <td>[SCA]</td>\n",
       "      <td>[services de conseils aux agriculteurs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20071</th>\n",
       "      <td>327</td>\n",
       "      <td>1 (646) 270-9737).]  Reunión especial de alto ...</td>\n",
       "      <td>[[370, 375]]</td>\n",
       "      <td>[[314, 368]]</td>\n",
       "      <td>[PNUMA]</td>\n",
       "      <td>[Programa de las Naciones Unidas para el Medio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>2529</td>\n",
       "      <td>Geographical Information System capacity estab...</td>\n",
       "      <td>[[166, 169], [137, 139]]</td>\n",
       "      <td>[[145, 164], [121, 135]]</td>\n",
       "      <td>[VHF, HF]</td>\n",
       "      <td>[very high frequency, high frequency]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19928</th>\n",
       "      <td>184</td>\n",
       "      <td>3. Contabilidad en dos monedas para las Nacion...</td>\n",
       "      <td>[[210, 213], [115, 118]]</td>\n",
       "      <td>[[175, 208]]</td>\n",
       "      <td>[OMC, CCI]</td>\n",
       "      <td>[Organización Mundial del Comercio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>2796</td>\n",
       "      <td>Cleveland Family study dceweb1.case.edu/ serc/...</td>\n",
       "      <td>[[76, 79]]</td>\n",
       "      <td>[[85, 111]]</td>\n",
       "      <td>[CHS]</td>\n",
       "      <td>[Cardiovascular Heart Study]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20756</th>\n",
       "      <td>1012</td>\n",
       "      <td>Nueva York, 26 de noviembre de 2007 Informació...</td>\n",
       "      <td>[[233, 236], [149, 153]]</td>\n",
       "      <td>[[74, 147], [163, 231]]</td>\n",
       "      <td>[CLD, CPEX]</td>\n",
       "      <td>[primer período extraordinario de sesiones de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26653</th>\n",
       "      <td>981</td>\n",
       "      <td>Tổng quan : Locally Linear Embedding ( nguồn [...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26665</th>\n",
       "      <td>993</td>\n",
       "      <td>4 . 1 Dữ liệu . Penn Discourse TreeBank ( PDTB...</td>\n",
       "      <td>[[42, 46], [94, 98]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[PDTB, PDTB]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>1007</td>\n",
       "      <td>Jeg har derfor foreslået, at Fiskeriudvalget u...</td>\n",
       "      <td>[[303, 306], [91, 94], [178, 182]]</td>\n",
       "      <td>[[263, 301]]</td>\n",
       "      <td>[EØS, KOM, EFTA]</td>\n",
       "      <td>[Europæiske Økonomiske Samarbejdsområde]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22355</th>\n",
       "      <td>2611</td>\n",
       "      <td>En enero de 2005, la representación de la pobl...</td>\n",
       "      <td>[[158, 161]]</td>\n",
       "      <td>[[123, 156]]</td>\n",
       "      <td>[ONG]</td>\n",
       "      <td>[organizaciones no gubernamentales]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26947 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text  \\\n",
       "15267  4642  fois encore, on ne dispose pas de données vent...   \n",
       "20071   327  1 (646) 270-9737).]  Reunión especial de alto ...   \n",
       "6508   2529  Geographical Information System capacity estab...   \n",
       "19928   184  3. Contabilidad en dos monedas para las Nacion...   \n",
       "2795   2796  Cleveland Family study dceweb1.case.edu/ serc/...   \n",
       "...     ...                                                ...   \n",
       "20756  1012  Nueva York, 26 de noviembre de 2007 Informació...   \n",
       "26653   981  Tổng quan : Locally Linear Embedding ( nguồn [...   \n",
       "26665   993  4 . 1 Dữ liệu . Penn Discourse TreeBank ( PDTB...   \n",
       "8550   1007  Jeg har derfor foreslået, at Fiskeriudvalget u...   \n",
       "22355  2611  En enero de 2005, la representación de la pobl...   \n",
       "\n",
       "                                 acronyms                long-forms  \\\n",
       "15267                        [[178, 181]]              [[139, 176]]   \n",
       "20071                        [[370, 375]]              [[314, 368]]   \n",
       "6508             [[166, 169], [137, 139]]  [[145, 164], [121, 135]]   \n",
       "19928            [[210, 213], [115, 118]]              [[175, 208]]   \n",
       "2795                           [[76, 79]]               [[85, 111]]   \n",
       "...                                   ...                       ...   \n",
       "20756            [[233, 236], [149, 153]]   [[74, 147], [163, 231]]   \n",
       "26653                                  []                        []   \n",
       "26665                [[42, 46], [94, 98]]                        []   \n",
       "8550   [[303, 306], [91, 94], [178, 182]]              [[263, 301]]   \n",
       "22355                        [[158, 161]]              [[123, 156]]   \n",
       "\n",
       "          acronyms-text                                    long-forms-text  \n",
       "15267             [SCA]            [services de conseils aux agriculteurs]  \n",
       "20071           [PNUMA]  [Programa de las Naciones Unidas para el Medio...  \n",
       "6508          [VHF, HF]              [very high frequency, high frequency]  \n",
       "19928        [OMC, CCI]                [Organización Mundial del Comercio]  \n",
       "2795              [CHS]                       [Cardiovascular Heart Study]  \n",
       "...                 ...                                                ...  \n",
       "20756       [CLD, CPEX]  [primer período extraordinario de sesiones de ...  \n",
       "26653                []                                                 []  \n",
       "26665      [PDTB, PDTB]                                                 []  \n",
       "8550   [EØS, KOM, EFTA]           [Europæiske Økonomiske Samarbejdsområde]  \n",
       "22355             [ONG]                [organizaciones no gubernamentales]  \n",
       "\n",
       "[26947 rows x 6 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allLangDFTrain.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific.to_csv(\"../data_tsvs/englishDFTrainScientific.tsv\", sep='\\t', index=False)\n",
    "englishDFDevScientific.to_csv(\"../data_tsvs/englishDFDevScientific.tsv\", sep='\\t', index=False)\n",
    "englishDFTrainLegal.to_csv(\"../data_tsvs/englishDFTrainLegal.tsv\", sep='\\t', index=False)\n",
    "englishDFDevLegal.to_csv(\"../data_tsvs/englishDFDevLegal.tsv\", sep='\\t', index=False)\n",
    "danishDFTrain.to_csv(\"../data_tsvs/danishDFTrain.tsv\", sep='\\t', index=False)\n",
    "danishDFDev.to_csv(\"../data_tsvs/danishDFDev.tsv\", sep='\\t', index=False)\n",
    "frenchDFTrain.to_csv(\"../data_tsvs/frenchDFTrain.tsv\", sep='\\t', index=False)\n",
    "frenchDFDev.to_csv(\"../data_tsvs/frenchDFDev.tsv\", sep='\\t', index=False)\n",
    "persianDFTrain.to_csv(\"../data_tsvs/persianDFTrain.tsv\", sep='\\t', index=False)\n",
    "persianDFDev.to_csv(\"../data_tsvs/persianDFDev.tsv\", sep='\\t', index=False)\n",
    "spanishDFTrain.to_csv(\"../data_tsvs/spanishDFTrain.tsv\", sep='\\t', index=False)\n",
    "spanishDFDev.to_csv(\"../data_tsvs/spanishDFDev.tsv\", sep='\\t', index=False)\n",
    "vietnameseDFTrain.to_csv(\"../data_tsvs/vietnameseDFTrain.tsv\", sep='\\t', index=False)\n",
    "vietnameseDFDev.to_csv(\"../data_tsvs/vietnameseDFDev.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLangDFTrain.to_csv(\"../data_tsvs/allLangDFTrain.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSVs to CoNLL format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO(txt, an_ids, lf_ids):\n",
    "    \"\"\"\n",
    "    input: text\n",
    "    ouput: tags , bio_tags\n",
    "        \n",
    "        tags :    list (AN, FL, OT)\n",
    "        bio_tags: list (O, B-AN, I-AN, B-LF, I-LF)\n",
    "    \"\"\"\n",
    "    \n",
    "    txt_l = word_tokenize(txt)\n",
    "    \n",
    "    tags = [\"OT\"]*len(txt_l)\n",
    "    bio_tags = [\"B-O\"]*len(txt_l)\n",
    "    \n",
    "    #txt[an_ids[0][0]:an_ids[0][1]]\n",
    "    \n",
    "    ANs = []\n",
    "    LFs = []\n",
    "    \n",
    "    for an_id in an_ids:\n",
    "        ANs.append(txt[an_id[0]:an_id[1]])\n",
    "        \n",
    "    for lf_id in lf_ids:\n",
    "        LFs.append(txt[lf_id[0]:lf_id[1]])\n",
    "    \n",
    "    AN_start = [False]*len(ANs)\n",
    "    LF_start = [False]*len(LFs)\n",
    "    \n",
    "    \n",
    "    for tok_id in range(0,len(txt_l)):\n",
    "        tok = txt_l[tok_id]\n",
    "        \n",
    "        for AN_idx in range(0,len(ANs)):\n",
    "           \n",
    "            AN = ANs[AN_idx]\n",
    "            \n",
    "            if(tok in AN):\n",
    "                tags[tok_id] = \"AN\"\n",
    "                \n",
    "                if(AN_start[AN_idx] == False):\n",
    "                    AN_start[AN_idx] = True\n",
    "                    bio_tags[tok_id] = \"B-AN\"\n",
    "                else:\n",
    "                    bio_tags[tok_id] = \"I-AN\"\n",
    "        \n",
    "        for LF_idx in range(0,len(LFs)):\n",
    "            \n",
    "            LF = LFs[LF_idx]\n",
    "            \n",
    "            if(tok in LF):\n",
    "                tags[tok_id] = \"LF\"\n",
    "                \n",
    "                if(LF_start[LF_idx] == False):\n",
    "                    LF_start[LF_idx] = True\n",
    "                    bio_tags[tok_id] = \"B-LF\"\n",
    "                else:\n",
    "                    bio_tags[tok_id] = \"I-LF\"\n",
    "                    \n",
    "    return txt_l, tags, bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(df, idx):\n",
    "    df_i = df.iloc[idx]\n",
    "    \n",
    "    txt = str(df_i[\"text\"]).strip()\n",
    "    acrnm_ids = ast.literal_eval(df_i[\"acronyms\"])\n",
    "    longform_ids = ast.literal_eval(df_i[\"long-forms\"])\n",
    "    \n",
    "    return txt, acrnm_ids, longform_ids\n",
    "\n",
    "def prepare(df, cnt, fname=\"output.txt\"):\n",
    "    \n",
    "    file = open(fname, \"w\")\n",
    "    file.write(\"-DOCSTART- -X- O\\n\\n\")\n",
    "    for idx in tqdm(range(cnt)):\n",
    "        txt, an_ids, lf_ids = extract(df, idx)\n",
    "        \n",
    "        txt_l, tags, bio_tags = BIO(txt, an_ids, lf_ids)\n",
    "        \n",
    "        for txt_id in range(0,len(txt_l)):\n",
    "            string = txt_l[txt_id]+ \" \" + tags[txt_id] + \" \" + bio_tags[txt_id]\n",
    "            \n",
    "            file.write(string + \" \\n\")\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    \n",
    "def convert_to_BIO(fileTrain, fileDev, lang, domain):\n",
    "    df_train = pd.read_csv(fileTrain, sep=\"\\t\")\n",
    "    df_dev = pd.read_csv(fileDev, sep=\"\\t\")\n",
    "    df_combine = df_train.append(df_dev, ignore_index=True)\n",
    "    \n",
    "    cnt_train = df_train.shape[0]\n",
    "    cnt_dev = df_dev.shape[0]\n",
    "    \n",
    "    train_cnt = cnt_train - 1000 ## Assuming 300 instances are enough to validate\n",
    "    valid_cnt = 1000\n",
    "    test_cnt = cnt_dev\n",
    "    \n",
    "    df_train_ = df_combine.iloc[:train_cnt]\n",
    "    df_valid_ = df_combine.iloc[train_cnt: train_cnt + valid_cnt].reset_index()\n",
    "    df_test_ = df_combine.iloc[train_cnt + valid_cnt: train_cnt + valid_cnt + test_cnt].reset_index()\n",
    "    \n",
    "    # Prepare training file\n",
    "    if ( domain == \"\" ):\n",
    "        prepare(df_train_, train_cnt, \"../processed/\" + lang + \"/train.txt\") # Prepare train file\n",
    "        prepare(df_valid_, valid_cnt, \"../processed/\" + lang + \"/valid.txt\") # Prepare validation file\n",
    "        prepare(df_test_, test_cnt, \"../processed/\" + lang + \"/test.txt\") # Prepare test file\n",
    "    else:\n",
    "        prepare(df_train_, train_cnt, \"../processed/\" + lang + \"/\" + domain + \"/train.txt\") # Prepare train file\n",
    "        prepare(df_valid_, valid_cnt, \"../processed/\" + lang + \"/\" + domain +\"/valid.txt\") # Prepare validation file\n",
    "        prepare(df_test_, test_cnt, \"../processed/\" + lang+  \"/\" + domain + \"/test.txt\") # Prepare test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOSDFTrain = pd.read_csv(\"../data_tsvs/PLOS-train-data.tsv\", sep=\"\\t\", header=None)\n",
    "PLOSDFTrain = PLOSDFTrain.drop(columns=[6])\n",
    "# allLangDFTrain.append(englishDFTrainLegal, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOSDFTrain.columns = allLangDFTrain.columns\n",
    "allLangDFTrain = allLangDFTrain.append(PLOSDFTrain, ignore_index=True)\n",
    "allLangDFTrain = allLangDFTrain.sample(frac=1)\n",
    "allLangDFTrain.to_csv(\"../data_tsvs/allLangPLOSDFTrain_1000.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2564/2564 [00:01<00:00, 1833.62it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1777.24it/s]\n",
      "100%|██████████| 445/445 [00:00<00:00, 1756.99it/s]\n",
      "100%|██████████| 2980/2980 [00:01<00:00, 2624.19it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2763.54it/s]\n",
      "100%|██████████| 497/497 [00:00<00:00, 2783.53it/s]\n",
      "100%|██████████| 2082/2082 [00:01<00:00, 1780.16it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1766.14it/s]\n",
      "100%|██████████| 385/385 [00:00<00:00, 1883.51it/s]\n",
      "100%|██████████| 6783/6783 [00:04<00:00, 1631.02it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1531.30it/s]\n",
      "100%|██████████| 973/973 [00:00<00:00, 1530.05it/s]\n",
      "100%|██████████| 336/336 [00:00<00:00, 1759.84it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1626.56it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 1503.00it/s]\n",
      "100%|██████████| 4928/4928 [00:03<00:00, 1572.28it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1595.26it/s]\n",
      "100%|██████████| 741/741 [00:00<00:00, 1600.81it/s]\n",
      "100%|██████████| 274/274 [00:00<00:00, 2801.14it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2800.14it/s]\n",
      "100%|██████████| 159/159 [00:00<00:00, 2547.03it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_BIO(\"../data_tsvs/englishDFTrainLegal.tsv\", \"../data_tsvs/englishDFDevLegal.tsv\", \"eng\", \"legal\")\n",
    "convert_to_BIO(\"../data_tsvs/englishDFTrainScientific.tsv\", \"../data_tsvs/englishDFDevScientific.tsv\", \"eng\", \"scientific\")\n",
    "convert_to_BIO(\"../data_tsvs/danishDFTrain.tsv\", \"../data_tsvs/danishDFDev.tsv\", \"dan\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/frenchDFTrain.tsv\", \"../data_tsvs/frenchDFDev.tsv\", \"fre\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/persianDFTrain.tsv\", \"../data_tsvs/persianDFDev.tsv\", \"per\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/spanishDFTrain.tsv\", \"../data_tsvs/spanishDFDev.tsv\", \"esp\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/vietnameseDFTrain.tsv\", \"../data_tsvs/vietnameseDFDev.tsv\", \"vie\", \"\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/englishDFDevLegal.tsv\", \"engAll\", \"legal\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/englishDFDevScientific.tsv\", \"engAll\", \"scientific\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/danishDFDev.tsv\", \"danAll\", \"\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/frenchDFDev.tsv\", \"freAll\", \"\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/persianDFDev.tsv\", \"perAll\", \"\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/spanishDFDev.tsv\", \"espAll\", \"\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/vietnameseDFDev.tsv\", \"vieAll\", \"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40994/40994 [00:20<00:00, 1977.31it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1948.48it/s]\n",
      "100%|██████████| 497/497 [00:00<00:00, 2694.86it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_BIO(\"../data_tsvs/allLangPLOSDFTrain_1000.tsv\", \"../data_tsvs/englishDFDevScientific.tsv\", \"engAllPLOS_1000\", \"scientific\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7553fb56ab976ad5aff909b9ab0e5f0d2a23e397f8c044edf0a757925c0fad4d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
