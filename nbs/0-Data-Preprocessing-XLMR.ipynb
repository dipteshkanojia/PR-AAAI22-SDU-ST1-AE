{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author: Prashant K. Sharma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io, sys\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/diptesh/workspace/PR-AAAI22-SDU-ST1-AE/nbs\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"danish\", \"english\", \"french\", \"persian\", \"spanish\", \"vietnamese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON to TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "danishDFTrain = pd.read_json(\"../data/\"+langs[0]+\"/train.json\")\n",
    "danishDFDev = pd.read_json(\"../data/\"+langs[0]+\"/dev.json\")\n",
    "danishDFTest = pd.read_json(\"../data/\"+langs[0]+\"/test.json\")\n",
    "\n",
    "englishDFTrainLegal = pd.read_json(\"../data/\"+langs[1]+\"/legal/train.json\")\n",
    "englishDFDevLegal = pd.read_json(\"../data/\"+langs[1]+\"/legal/dev.json\")\n",
    "englishDFTestLegal = pd.read_json(\"../data/\"+langs[1]+\"/legal/test.json\")\n",
    "\n",
    "englishDFTrainScientific = pd.read_json(\"../data/\"+langs[1]+\"/scientific/train.json\")\n",
    "englishDFDevScientific = pd.read_json(\"../data/\"+langs[1]+\"/scientific/dev.json\")\n",
    "englishDFTestScientific = pd.read_json(\"../data/\"+langs[1]+\"/scientific/test.json\")\n",
    "\n",
    "frenchDFTrain = pd.read_json(\"../data/\"+langs[2]+\"/train.json\")\n",
    "frenchDFDev = pd.read_json(\"../data/\"+langs[2]+\"/dev.json\")\n",
    "frenchDFTest = pd.read_json(\"../data/\"+langs[2]+\"/test.json\")\n",
    "\n",
    "persianDFTrain = pd.read_json(\"../data/\"+langs[3]+\"/train.json\")\n",
    "persianDFDev = pd.read_json(\"../data/\"+langs[3]+\"/dev.json\")\n",
    "persianDFTest = pd.read_json(\"../data/\"+langs[3]+\"/test.json\")\n",
    "\n",
    "spanishDFTrain = pd.read_json(\"../data/\"+langs[4]+\"/train.json\")\n",
    "spanishDFDev = pd.read_json(\"../data/\"+langs[4]+\"/dev.json\")\n",
    "spanishDFTest = pd.read_json(\"../data/\"+langs[4]+\"/test.json\")\n",
    "\n",
    "vietnameseDFTrain = pd.read_json(\"../data/\"+langs[5]+\"/train.json\")\n",
    "vietnameseDFDev = pd.read_json(\"../data/\"+langs[5]+\"/dev.json\")\n",
    "vietnameseDFTest = pd.read_json(\"../data/\"+langs[5]+\"/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Safe School Committee comprises of interna...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78. JS3 noted in their joint report that auth...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In nominal pressure increments of &lt; 1 bar, thr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>procedure (WHDC)) Proposal for the development...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10. Advocating enhanced coherence and coordin...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Advice and technical support to Governments in...</td>\n",
       "      <td>442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Without a vision, any development effort is li...</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>In view of the interlinked nature of the probl...</td>\n",
       "      <td>444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>10, on children's rights in juvenile justice, ...</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Nevertheless, barriers, including the lack of ...</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>446 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text   ID\n",
       "0    The Safe School Committee comprises of interna...    1\n",
       "1     78. JS3 noted in their joint report that auth...    2\n",
       "2    In nominal pressure increments of < 1 bar, thr...    3\n",
       "3    procedure (WHDC)) Proposal for the development...    4\n",
       "4     10. Advocating enhanced coherence and coordin...    5\n",
       "..                                                 ...  ...\n",
       "441  Advice and technical support to Governments in...  442\n",
       "442  Without a vision, any development effort is li...  443\n",
       "443  In view of the interlinked nature of the probl...  444\n",
       "444  10, on children's rights in juvenile justice, ...  445\n",
       "445  Nevertheless, barriers, including the lack of ...  446\n",
       "\n",
       "[446 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishDFTestLegal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting text from the numerical indexes (Hadeel's Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHOR: Hadeel S\n",
    "\n",
    "def create_acronyms(data):\n",
    "    acs2 = []\n",
    "    longf = []\n",
    "\n",
    "    acros = data.acronyms.values\n",
    "    txt = data.text.values\n",
    "    zipped = list(zip(txt,acros))\n",
    "    for z in zipped:\n",
    "        acs = []\n",
    "        for i in range(len(z[1])):\n",
    "            acs.append(z[0][z[1][i][0]:z[1][i][1]])\n",
    "        acs2.append(acs)\n",
    "    data['acronyms-text'] =acs2\n",
    "    \n",
    "# Creating long forms from here\n",
    "\n",
    "    longform = data[\"long-forms\"].values\n",
    "    zipped2 = list(zip(txt,longform))\n",
    "    for z2 in zipped2:\n",
    "        longform_temp = []\n",
    "        for i in range(len(z2[1])):\n",
    "            longform_temp.append(z2[0][z2[1][i][0]:z2[1][i][1]])\n",
    "        longf.append(longform_temp)\n",
    "    data[\"long-forms-text\"]= longf\n",
    "    \n",
    "    return data\n",
    "#     return data.to_csv(\"datatsvs/\" + name +\".tsv\", index = False, encoding='utf8', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific = create_acronyms(englishDFTrainScientific)\n",
    "englishDFDevScientific = create_acronyms(englishDFDevScientific)\n",
    "# englishDFTestScientific = create_acronyms(englishDFTestScientific)\n",
    "\n",
    "englishDFTrainLegal = create_acronyms(englishDFTrainLegal)\n",
    "englishDFDevLegal = create_acronyms(englishDFDevLegal)\n",
    "# englishDFTestLegal = create_acronyms(englishDFTestLegal)\n",
    "\n",
    "danishDFTrain = create_acronyms(danishDFTrain)\n",
    "danishDFDev = create_acronyms(danishDFDev)\n",
    "# danishDFTest = create_acronyms(danishDFTest)\n",
    "\n",
    "frenchDFTrain = create_acronyms(frenchDFTrain)\n",
    "frenchDFDev = create_acronyms(frenchDFDev)\n",
    "# frenchDFTest = create_acronyms(frenchDFTest)\n",
    "\n",
    "persianDFTrain = create_acronyms(persianDFTrain)\n",
    "persianDFDev = create_acronyms(persianDFDev)\n",
    "# persianDFTest = create_acronyms(persianDFTest)\n",
    "\n",
    "spanishDFTrain = create_acronyms(spanishDFTrain)\n",
    "spanishDFDev = create_acronyms(spanishDFDev)\n",
    "# spanishDFTest = create_acronyms(spanishDFTest)\n",
    "\n",
    "vietnameseDFTrain = create_acronyms(vietnameseDFTrain)\n",
    "vietnameseDFDev = create_acronyms(vietnameseDFDev)\n",
    "# vietnameseDFTest = create_acronyms(vietnameseDFTest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating columns for proper TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific = englishDFTrainScientific[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFDevScientific = englishDFDevScientific[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFTrainLegal = englishDFTrainLegal[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "englishDFDevLegal = englishDFDevLegal[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "danishDFTrain = danishDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "danishDFDev = danishDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "frenchDFTrain = frenchDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "frenchDFDev = frenchDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "persianDFTrain = persianDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "persianDFDev = persianDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "spanishDFTrain = spanishDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "spanishDFDev = spanishDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "vietnameseDFTrain = vietnameseDFTrain[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]\n",
    "vietnameseDFDev = vietnameseDFDev[['ID', 'text', 'acronyms', 'long-forms', 'acronyms-text', 'long-forms-text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLangDFTrain = englishDFTrainScientific.append(englishDFTrainLegal, ignore_index=True).append(danishDFTrain, ignore_index=True).append(frenchDFTrain, ignore_index=True).append(persianDFTrain, ignore_index=True).append(spanishDFTrain, ignore_index=True).append(vietnameseDFTrain, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>acronyms</th>\n",
       "      <th>long-forms</th>\n",
       "      <th>acronyms-text</th>\n",
       "      <th>long-forms-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26745</th>\n",
       "      <td>1073</td>\n",
       "      <td>điều này , trước hết chúng ta biểu diễn logari...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19453</th>\n",
       "      <td>1045</td>\n",
       "      <td>پايانه بايد قادر به نمونه‌برداري از ورودي‌هاي ...</td>\n",
       "      <td>[[271, 274]]</td>\n",
       "      <td>[[226, 243]]</td>\n",
       "      <td>[SOE]</td>\n",
       "      <td>[ترتيب زماني حوادث]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5053</th>\n",
       "      <td>1074</td>\n",
       "      <td>Namibia Assistance was received from the EU u...</td>\n",
       "      <td>[[113, 118], [42, 44], [152, 155]]</td>\n",
       "      <td>[[55, 111], [127, 150]]</td>\n",
       "      <td>[TTIDP, EU, CBI]</td>\n",
       "      <td>[Transnational Trade and Investment Developmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11265</th>\n",
       "      <td>640</td>\n",
       "      <td>L'Institut des Nations Unies pour la formation...</td>\n",
       "      <td>[[385, 388], [337, 343], [65, 71]]</td>\n",
       "      <td>[[350, 383], [274, 335], [2, 63]]</td>\n",
       "      <td>[OMC, UNITAR, UNITAR]</td>\n",
       "      <td>[Organisation mondiale du commerce, Institut d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26531</th>\n",
       "      <td>859</td>\n",
       "      <td>HÀ NỘI , 6/2021 Đại học Bách Khoa Hà Nội</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26265</th>\n",
       "      <td>593</td>\n",
       "      <td>khác nhau của những bức ảnh khác nhau thì đó c...</td>\n",
       "      <td>[[92, 94]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[CL]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20093</th>\n",
       "      <td>349</td>\n",
       "      <td>177. Un logro importante a destacar es la sen...</td>\n",
       "      <td>[[159, 164]]</td>\n",
       "      <td>[[124, 157]]</td>\n",
       "      <td>[ONG's]</td>\n",
       "      <td>[Organizaciones no Gubernamentales]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1438</td>\n",
       "      <td>TOP (PRP ? I?) ( VP (VBP ? NEED?) (</td>\n",
       "      <td>[[17, 19], [0, 3], [5, 8]]</td>\n",
       "      <td>[[21, 24]]</td>\n",
       "      <td>[VP, TOP, PRP]</td>\n",
       "      <td>[VBP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16414</th>\n",
       "      <td>5789</td>\n",
       "      <td>7. En ce qui concerne les types d'activités, ...</td>\n",
       "      <td>[[166, 169]]</td>\n",
       "      <td>[[144, 164]]</td>\n",
       "      <td>[GES]</td>\n",
       "      <td>[gaz à effet de serre]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>2365</td>\n",
       "      <td>I dag har vi desværre forpasset en stor muligh...</td>\n",
       "      <td>[[326, 329], [135, 139]]</td>\n",
       "      <td>[[308, 324]]</td>\n",
       "      <td>[DHA, vha.]</td>\n",
       "      <td>[docosahexaensyre]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26947 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text  \\\n",
       "26745  1073  điều này , trước hết chúng ta biểu diễn logari...   \n",
       "19453  1045  پايانه بايد قادر به نمونه‌برداري از ورودي‌هاي ...   \n",
       "5053   1074   Namibia Assistance was received from the EU u...   \n",
       "11265   640  L'Institut des Nations Unies pour la formation...   \n",
       "26531   859           HÀ NỘI , 6/2021 Đại học Bách Khoa Hà Nội   \n",
       "...     ...                                                ...   \n",
       "26265   593  khác nhau của những bức ảnh khác nhau thì đó c...   \n",
       "20093   349   177. Un logro importante a destacar es la sen...   \n",
       "1437   1438                TOP (PRP ? I?) ( VP (VBP ? NEED?) (   \n",
       "16414  5789   7. En ce qui concerne les types d'activités, ...   \n",
       "9908   2365  I dag har vi desværre forpasset en stor muligh...   \n",
       "\n",
       "                                 acronyms                         long-forms  \\\n",
       "26745                                  []                                 []   \n",
       "19453                        [[271, 274]]                       [[226, 243]]   \n",
       "5053   [[113, 118], [42, 44], [152, 155]]            [[55, 111], [127, 150]]   \n",
       "11265  [[385, 388], [337, 343], [65, 71]]  [[350, 383], [274, 335], [2, 63]]   \n",
       "26531                                  []                                 []   \n",
       "...                                   ...                                ...   \n",
       "26265                          [[92, 94]]                                 []   \n",
       "20093                        [[159, 164]]                       [[124, 157]]   \n",
       "1437           [[17, 19], [0, 3], [5, 8]]                         [[21, 24]]   \n",
       "16414                        [[166, 169]]                       [[144, 164]]   \n",
       "9908             [[326, 329], [135, 139]]                       [[308, 324]]   \n",
       "\n",
       "               acronyms-text  \\\n",
       "26745                     []   \n",
       "19453                  [SOE]   \n",
       "5053        [TTIDP, EU, CBI]   \n",
       "11265  [OMC, UNITAR, UNITAR]   \n",
       "26531                     []   \n",
       "...                      ...   \n",
       "26265                   [CL]   \n",
       "20093                [ONG's]   \n",
       "1437          [VP, TOP, PRP]   \n",
       "16414                  [GES]   \n",
       "9908             [DHA, vha.]   \n",
       "\n",
       "                                         long-forms-text  \n",
       "26745                                                 []  \n",
       "19453                                [ترتيب زماني حوادث]  \n",
       "5053   [Transnational Trade and Investment Developmen...  \n",
       "11265  [Organisation mondiale du commerce, Institut d...  \n",
       "26531                                                 []  \n",
       "...                                                  ...  \n",
       "26265                                                 []  \n",
       "20093                [Organizaciones no Gubernamentales]  \n",
       "1437                                               [VBP]  \n",
       "16414                             [gaz à effet de serre]  \n",
       "9908                                  [docosahexaensyre]  \n",
       "\n",
       "[26947 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allLangDFTrain.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing TSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The label restricted means the model is restri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We have followed their methodology as best as ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adaptation scenario. Duan et al (2009) propose...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actes de la 13e Confe?rence sur le Traitement...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TGTM P=p,pk ,b   TGTM PR=pr ,  pkr ,  b r   TG...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>measured on separate grammatical and ungrammat...</td>\n",
       "      <td>494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Schu?tze reduces the dimensionality of this fe...</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>is a.t least cubic in t, ime, this fl)llows tr...</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>present specialized knowledge, since both the ...</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2014). We note the linguistic rules included i...</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text   ID\n",
       "0    The label restricted means the model is restri...    1\n",
       "1    We have followed their methodology as best as ...    2\n",
       "2    adaptation scenario. Duan et al (2009) propose...    3\n",
       "3     Actes de la 13e Confe?rence sur le Traitement...    4\n",
       "4    TGTM P=p,pk ,b   TGTM PR=pr ,  pkr ,  b r   TG...    5\n",
       "..                                                 ...  ...\n",
       "493  measured on separate grammatical and ungrammat...  494\n",
       "494  Schu?tze reduces the dimensionality of this fe...  495\n",
       "495  is a.t least cubic in t, ime, this fl)llows tr...  496\n",
       "496  present specialized knowledge, since both the ...  497\n",
       "497  2014). We note the linguistic rules included i...  498\n",
       "\n",
       "[498 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "englishDFTestScientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishDFTrainScientific.to_csv(\"../data_tsvs/englishDFTrainScientific.tsv\", sep='\\t', index=False)\n",
    "englishDFDevScientific.to_csv(\"../data_tsvs/englishDFDevScientific.tsv\", sep='\\t', index=False)\n",
    "englishDFTestScientific.to_csv(\"../data_tsvs/englishDFTestScientific.tsv\", sep='\\t', index=False)\n",
    "\n",
    "englishDFTrainLegal.to_csv(\"../data_tsvs/englishDFTrainLegal.tsv\", sep='\\t', index=False)\n",
    "englishDFDevLegal.to_csv(\"../data_tsvs/englishDFDevLegal.tsv\", sep='\\t', index=False)\n",
    "englishDFTestLegal.to_csv(\"../data_tsvs/englishDFTestLegal.tsv\", sep='\\t', index=False)\n",
    "\n",
    "danishDFTrain.to_csv(\"../data_tsvs/danishDFTrain.tsv\", sep='\\t', index=False)\n",
    "danishDFDev.to_csv(\"../data_tsvs/danishDFDev.tsv\", sep='\\t', index=False)\n",
    "danishDFTest.to_csv(\"../data_tsvs/danishDFTest.tsv\", sep='\\t', index=False)\n",
    "\n",
    "frenchDFTrain.to_csv(\"../data_tsvs/frenchDFTrain.tsv\", sep='\\t', index=False)\n",
    "frenchDFDev.to_csv(\"../data_tsvs/frenchDFDev.tsv\", sep='\\t', index=False)\n",
    "frenchDFTest.to_csv(\"../data_tsvs/frenchDFTest.tsv\", sep='\\t', index=False)\n",
    "\n",
    "persianDFTrain.to_csv(\"../data_tsvs/persianDFTrain.tsv\", sep='\\t', index=False)\n",
    "persianDFDev.to_csv(\"../data_tsvs/persianDFDev.tsv\", sep='\\t', index=False)\n",
    "persianDFTest.to_csv(\"../data_tsvs/persianDFTest.tsv\", sep='\\t', index=False)\n",
    "\n",
    "spanishDFTrain.to_csv(\"../data_tsvs/spanishDFTrain.tsv\", sep='\\t', index=False)\n",
    "spanishDFDev.to_csv(\"../data_tsvs/spanishDFDev.tsv\", sep='\\t', index=False)\n",
    "spanishDFTest.to_csv(\"../data_tsvs/spanishDFTest.tsv\", sep='\\t', index=False)\n",
    "\n",
    "vietnameseDFTrain.to_csv(\"../data_tsvs/vietnameseDFTrain.tsv\", sep='\\t', index=False)\n",
    "vietnameseDFDev.to_csv(\"../data_tsvs/vietnameseDFDev.tsv\", sep='\\t', index=False)\n",
    "vietnameseDFTest.to_csv(\"../data_tsvs/vietnameseDFTest.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "allLangDFTrain.to_csv(\"../data_tsvs/allLangDFTrain.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSVs to CoNLL format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIO(txt, an_ids, lf_ids):\n",
    "    \"\"\"\n",
    "    input: text\n",
    "    ouput: tags , bio_tags\n",
    "        \n",
    "        tags :    list (AN, FL, OT)\n",
    "        bio_tags: list (O, B-AN, I-AN, B-LF, I-LF)\n",
    "    \"\"\"\n",
    "    \n",
    "    txt_l = word_tokenize(txt)\n",
    "    \n",
    "    tags = [\"OT\"]*len(txt_l)\n",
    "    bio_tags = [\"B-O\"]*len(txt_l)\n",
    "    \n",
    "    #txt[an_ids[0][0]:an_ids[0][1]]\n",
    "    \n",
    "    ANs = []\n",
    "    LFs = []\n",
    "    \n",
    "    for an_id in an_ids:\n",
    "        ANs.append(txt[an_id[0]:an_id[1]])\n",
    "        \n",
    "    for lf_id in lf_ids:\n",
    "        LFs.append(txt[lf_id[0]:lf_id[1]])\n",
    "    \n",
    "    AN_start = [False]*len(ANs)\n",
    "    LF_start = [False]*len(LFs)\n",
    "    \n",
    "    for tok_id in range(0,len(txt_l)):\n",
    "        tok = txt_l[tok_id]\n",
    "        \n",
    "        for AN_idx in range(0,len(ANs)):\n",
    "           \n",
    "            AN = ANs[AN_idx]\n",
    "            \n",
    "            if(tok in AN):\n",
    "                tags[tok_id] = \"AN\"\n",
    "                \n",
    "                if(AN_start[AN_idx] == False):\n",
    "                    AN_start[AN_idx] = True\n",
    "                    bio_tags[tok_id] = \"B-AN\"\n",
    "                else:\n",
    "                    bio_tags[tok_id] = \"I-AN\"\n",
    "        \n",
    "        for LF_idx in range(0,len(LFs)):\n",
    "            \n",
    "            LF = LFs[LF_idx]\n",
    "            \n",
    "            if(tok in LF):\n",
    "                tags[tok_id] = \"LF\"\n",
    "                \n",
    "                if(LF_start[LF_idx] == False):\n",
    "                    LF_start[LF_idx] = True\n",
    "                    bio_tags[tok_id] = \"B-LF\"\n",
    "                else:\n",
    "                    bio_tags[tok_id] = \"I-LF\"\n",
    "                    \n",
    "    return txt_l, tags, bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(df, idx):\n",
    "    df_i = df.iloc[idx]\n",
    "    \n",
    "    txt = str(df_i[\"text\"]).strip()\n",
    "    acrnm_ids = ast.literal_eval(df_i[\"acronyms\"])\n",
    "    longform_ids = ast.literal_eval(df_i[\"long-forms\"])\n",
    "    \n",
    "    return txt, acrnm_ids, longform_ids\n",
    "\n",
    "def prepare(df, cnt, fname=\"output.txt\"):\n",
    "    \n",
    "    file = open(fname, \"w\")\n",
    "    file.write(\"-DOCSTART- -X- O\\n\\n\")\n",
    "    for idx in tqdm(range(cnt)):\n",
    "        txt, an_ids, lf_ids = extract(df, idx)\n",
    "        \n",
    "        txt_l, tags, bio_tags = BIO(txt, an_ids, lf_ids)\n",
    "        \n",
    "        for txt_id in range(0,len(txt_l)):\n",
    "            string = txt_l[txt_id]+ \" \" + tags[txt_id] + \" \" + bio_tags[txt_id]\n",
    "            \n",
    "            file.write(string + \" \\n\")\n",
    "        file.write(\"\\n\")\n",
    "    file.close()\n",
    "    \n",
    "def convert_to_BIO(fileTrain, fileDev, lang, domain):\n",
    "    df_train = pd.read_csv(fileTrain, sep=\"\\t\")\n",
    "    df_dev = pd.read_csv(fileDev, sep=\"\\t\")\n",
    "    df_combine = df_train.append(df_dev, ignore_index=True)\n",
    "    \n",
    "    cnt_train = df_train.shape[0]\n",
    "    cnt_dev = df_dev.shape[0]\n",
    "    \n",
    "    train_cnt = cnt_train - 1000 ## Assuming 300 instances are enough to validate\n",
    "    valid_cnt = 1000\n",
    "    test_cnt = cnt_dev\n",
    "    \n",
    "    df_train_ = df_combine.iloc[:train_cnt]\n",
    "    df_valid_ = df_combine.iloc[train_cnt: train_cnt + valid_cnt].reset_index()\n",
    "    df_test_ = df_combine.iloc[train_cnt + valid_cnt: train_cnt + valid_cnt + test_cnt].reset_index()\n",
    "    \n",
    "    # Prepare training file\n",
    "    if ( domain == \"\" ):\n",
    "        prepare(df_train_, train_cnt, \"../processed/\" + lang + \"/train.txt\") # Prepare train file\n",
    "        prepare(df_valid_, valid_cnt, \"../processed/\" + lang + \"/valid.txt\") # Prepare validation file\n",
    "        prepare(df_test_, test_cnt, \"../processed/\" + lang + \"/test.txt\") # Prepare test file\n",
    "    else:\n",
    "        prepare(df_train_, train_cnt, \"../processed/\" + lang + \"/\" + domain + \"/train.txt\") # Prepare train file\n",
    "        prepare(df_valid_, valid_cnt, \"../processed/\" + lang + \"/\" + domain +\"/valid.txt\") # Prepare validation file\n",
    "        prepare(df_test_, test_cnt, \"../processed/\" + lang+  \"/\" + domain + \"/test.txt\") # Prepare test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOSDFTrain = pd.read_csv(\"../data_tsvs/PLOS-train-data.tsv\", sep=\"\\t\", header=None)\n",
    "PLOSDFTrain = PLOSDFTrain.drop(columns=[6])\n",
    "# allLangDFTrain.append(englishDFTrainLegal, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOSDFTrain.columns = allLangDFTrain.columns\n",
    "allLangDFTrain = allLangDFTrain.append(PLOSDFTrain, ignore_index=True)\n",
    "allLangDFTrain = allLangDFTrain.sample(frac=1)\n",
    "allLangDFTrain.to_csv(\"../data_tsvs/allLangPLOSDFTrain_1000.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2564/2564 [00:01<00:00, 1833.62it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1777.24it/s]\n",
      "100%|██████████| 445/445 [00:00<00:00, 1756.99it/s]\n",
      "100%|██████████| 2980/2980 [00:01<00:00, 2624.19it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2763.54it/s]\n",
      "100%|██████████| 497/497 [00:00<00:00, 2783.53it/s]\n",
      "100%|██████████| 2082/2082 [00:01<00:00, 1780.16it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1766.14it/s]\n",
      "100%|██████████| 385/385 [00:00<00:00, 1883.51it/s]\n",
      "100%|██████████| 6783/6783 [00:04<00:00, 1631.02it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1531.30it/s]\n",
      "100%|██████████| 973/973 [00:00<00:00, 1530.05it/s]\n",
      "100%|██████████| 336/336 [00:00<00:00, 1759.84it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1626.56it/s]\n",
      "100%|██████████| 167/167 [00:00<00:00, 1503.00it/s]\n",
      "100%|██████████| 4928/4928 [00:03<00:00, 1572.28it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1595.26it/s]\n",
      "100%|██████████| 741/741 [00:00<00:00, 1600.81it/s]\n",
      "100%|██████████| 274/274 [00:00<00:00, 2801.14it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2800.14it/s]\n",
      "100%|██████████| 159/159 [00:00<00:00, 2547.03it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_BIO(\"../data_tsvs/englishDFTrainLegal.tsv\", \"../data_tsvs/englishDFDevLegal.tsv\", \"eng\", \"legal\")\n",
    "convert_to_BIO(\"../data_tsvs/englishDFTrainScientific.tsv\", \"../data_tsvs/englishDFDevScientific.tsv\", \"eng\", \"scientific\")\n",
    "convert_to_BIO(\"../data_tsvs/danishDFTrain.tsv\", \"../data_tsvs/danishDFDev.tsv\", \"dan\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/frenchDFTrain.tsv\", \"../data_tsvs/frenchDFDev.tsv\", \"fre\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/persianDFTrain.tsv\", \"../data_tsvs/persianDFDev.tsv\", \"per\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/spanishDFTrain.tsv\", \"../data_tsvs/spanishDFDev.tsv\", \"esp\", \"\")\n",
    "convert_to_BIO(\"../data_tsvs/vietnameseDFTrain.tsv\", \"../data_tsvs/vietnameseDFDev.tsv\", \"vie\", \"\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/englishDFDevLegal.tsv\", \"engAll\", \"legal\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/englishDFDevScientific.tsv\", \"engAll\", \"scientific\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/danishDFDev.tsv\", \"danAll\", \"\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/frenchDFDev.tsv\", \"freAll\", \"\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/persianDFDev.tsv\", \"perAll\", \"\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/spanishDFDev.tsv\", \"espAll\", \"\")\n",
    "# convert_to_BIO(\"../data_tsvs/allLangDFTrain.tsv\", \"../data_tsvs/vietnameseDFDev.tsv\", \"vieAll\", \"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40994/40994 [00:20<00:00, 1977.31it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 1948.48it/s]\n",
      "100%|██████████| 497/497 [00:00<00:00, 2694.86it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_BIO(\"../data_tsvs/allLangPLOSDFTrain_1000.tsv\", \"../data_tsvs/englishDFDevScientific.tsv\", \"engAllPLOS_1000\", \"scientific\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7553fb56ab976ad5aff909b9ab0e5f0d2a23e397f8c044edf0a757925c0fad4d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
