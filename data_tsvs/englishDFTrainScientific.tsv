ID	text	acronyms	long-forms	acronyms-text	long-forms-text
1	92\]. This selective approach led to significant  results in some restricted applications (ATIS...). 	[[91, 98]]	[[77, 89]]	['ATIS...']	['applications']
2	We conduct a case study of dialectal language in online conversational text by investigating African-American English (AAE) on Twitter.	[[119, 122]]	[[93, 117]]	['AAE']	['African-American English']
3	 An arguably better approach to representation learning is Canonical Correlation Analysis (CCA) that induces representations that are maximally cor-	[[91, 94]]	[[59, 89]]	['CCA']	['Canonical Correlation Analysis']
4	23-28, 1992   Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 110?117, Denver, Colorado, June 1, 2015.	[[71, 74], [29, 38]]	[[44, 69]]	['SRW', 'NAACL-HLT']	['Student Research Workshop']
5	itly mark objects of prepositions (POBJ), possessors in idafa construction (IDAFA), conjuncts (CONJ) and conjunctions (CC), and the accusative specifier, tamyiz (TMZ).	[[119, 121], [35, 39], [76, 81], [95, 99], [162, 165]]	[[105, 117], [10, 33], [56, 74], [84, 93], [154, 160]]	['CC', 'POBJ', 'IDAFA', 'CONJ', 'TMZ']	['conjunctions', 'objects of prepositions', 'idafa construction', 'conjuncts', 'tamyiz']
6	The metrics Precision (P), Recall (R),  F-score (F) (F=2PR/(P+R)), Recall of OOV  (ROOV) and Recall of IV (RIV) are used to  evaluate the results.	[[107, 110], [83, 87]]	[[93, 105], [12, 21], [27, 33], [40, 47], [67, 80]]	['RIV', 'ROOV']	['Recall of IV', 'Precision', 'Recall', 'F-score', 'Recall of OOV']
7	158  I. CONSTRUCT THE PROPOSED ANCHORS for Un  (a) Create set of referring expressions (RE's). 	[[88, 92]]	[[65, 86]]	"[""RE's""]"	['referring expressions']
8	Hong Kong Laws, Sinorama, Xinhua News, and English translation of Chinese Treebank), available from the Linguistic Data Consortium (LDC). To	[[132, 135]]	[[104, 130]]	['LDC']	['Linguistic Data Consortium']
9	uation test sets. Equal Error Rates (EER), where FA = FR, are given in Table 5. Results on EVAL	[[54, 56], [37, 40], [49, 51], [91, 95]]	[[18, 35]]	['FR', 'EER', 'FA', 'EVAL']	['Equal Error Rates']
10	Draw Team (DT_DT); Team ? Competition (TM_CP); Team ? City / Province / Country 	[[39, 44], [11, 16]]	[[33, 37], [0, 9]]	['TM_CP', 'DT_DT']	['tion', 'Draw Team']
11	syntactic skeleton defined in Eq. 1, namely, Subject(S), Verb(V), Direct Object(DO), Indirect Object(IO), Preposition(P) and Noun(Object) of the Preposition(N).	[[80, 82], [101, 103]]	[[66, 78], [85, 99], [45, 52], [57, 61], [106, 117]]	['DO', 'IO']	['Direct Objec', 'Indirect Objec', 'Subject', 'Verb', 'Preposition']
12	Constant 5.23 1.18 19.67 <0.000* 187.25 ADAG, n=242; HAG, n = 242; S.E = standard error; OR = Odds ratio or Exp(?); CI = confidence Interval. 	[[116, 118], [89, 91], [53, 56]]	[[121, 140], [94, 104]]	['CI', 'OR', 'HAG']	['confidence Interval', 'Odds ratio']
13	The availability of large scale data sets of manually annotated predicate?argument structures has recently favored the use of machine learning approaches to the design of automated semantic role labeling (SRL) systems. The main research in this area relates to the design choices	[[205, 208]]	[[181, 203]]	['SRL']	['semantic role labeling']
14	Table 1 FactBank annotation scheme. CT = certain; PR = probable; PS = possible; U = underspecified; + = positive; ?	[[36, 38], [50, 52], [65, 67]]	[[41, 48], [55, 63], [70, 78], [84, 98], [104, 112]]	['CT', 'PR', 'PS']	['certain', 'probable', 'possible', 'underspecified', 'positive']
15	served as the founding Editor-in-Chief of ACM  Transactions on Knowledge Discovery from Data (TKDD). He has received ACM SIGKDD In-	[[94, 98], [42, 45], [117, 120], [121, 127]]	[[47, 92]]	['TKDD', 'ACM', 'ACM', 'SIGKDD']	['Transactions on Knowledge Discovery from Data']
16	list of Frames alphabetically surrounding Compliance runs as follows: Compatibility, Competition, Complaining, Completeness, Compliance, Concessive.... Next, we attempt to catalogue the Lexical Units (LUs) associated with the frame. 	[[201, 204]]	[[186, 199]]	['LUs']	['Lexical Units']
17	{kmpark, rim}@nlp.korea.ac.kr 1 Introduction The semantic role labeling (SRL) refers to finding the semantic relation (e.g. Agent, Patient, etc.)	[[73, 76]]	[[49, 71]]	['SRL']	['semantic role labeling']
18	the current work, we used a subset of that corpus consisting of examples whose question types were PRC (procedure), RSN (reason) or ATR (atrans). 	[[99, 102], [116, 119], [132, 135]]	[[104, 113], [121, 127], [137, 143]]	['PRC', 'RSN', 'ATR']	['procedure', 'reason', 'atrans']
19	 Its data-driven approach learns a sub-word lexicon from a training corpus of words by using a Minimum Description Length (MDL) algorithm (Creutz et Lagus, 2005). It has been used with	[[123, 126]]	[[95, 121]]	['MDL']	['Minimum Description Length']
20	4 = 1, 440 items) (ADD=additive, MUL=multiplicative, PLSR=Partial Least Squares Regression, OBS=observed vectors) . 	[[92, 95], [19, 22], [33, 36], [53, 57]]	[[96, 104], [23, 31], [37, 51], [58, 90]]	['OBS', 'ADD', 'MUL', 'PLSR']	['observed', 'additive', 'multiplicative', 'Partial Least Squares Regression']
21	 The sentence-level extraction is done with the subsequence kernel (SSK) approach from (Bunescu and Mooney, 2005), which was shown to give good re-	[[68, 71]]	[[51, 66]]	['SSK']	['sequence kernel']
22	 1 Introduction Natural Language Processing (NLP) and Machine Learning (ML) are making a significant impact in	[[45, 48], [72, 74]]	[[16, 43], [54, 70]]	['NLP', 'ML']	['Natural Language Processing', 'Machine Learning']
23	 Proceedings of the lOth International Conference on  Computational Linguistics (COLING-84). Stanford 	[[81, 90]]	[[54, 79]]	['COLING-84']	['Computational Linguistics']
24	for the three sponsoring agencies. The TIPSTER  Research and Evaluation Committee (REC) was  charged with oversight responsibility of the 15 	[[83, 86]]	[[48, 81]]	['REC']	['Research and Evaluation Committee']
25	above.  Iconic Inflectional Classes (IICs) are ICs that are manually fully annotated, i.e., they have all the tem-	[[37, 41]]	[[8, 35]]	['IICs']	['Iconic Inflectional Classes']
26	probabilities. Analysis showed that the correct tag most fre-  quently missing from the lattice was the DT (determiner)  tag.	[[104, 106]]	[[108, 118]]	['DT']	['determiner']
27	ing. In Proceedings of the A CL Fifth Conference  on Applied Natural Language Processing (ANLP),  pages 139-146, Washington, DC.	[[90, 94], [29, 31], [125, 127]]	[[53, 88]]	['ANLP', 'CL', 'DC']	['Applied Natural Language Processing']
28	tion of the reference xamples takes place.  Translation Memories (TMs) are such purely  memory based MT-systems.	[[66, 69], [101, 103]]	[[44, 64]]	['TMs', 'MT']	['Translation Memories']
29	discourses presented to the human subjects.  6.1 Semantically Slanted Discourse (SSD) Methodology: The Motivation for the  First Part 	[[81, 84]]	[[49, 79]]	['SSD']	['Semantically Slanted Discourse']
30	 We integrate two sets of linguistic features into a maximum entropy (MaxEnt) model and develop aMaxEnt-based binary classifier to predict the cat-	[[70, 76], [97, 103]]	[[53, 68]]	['MaxEnt', 'MaxEnt']	['maximum entropy']
31	quences here).  1PARTMOD=participial modifier, PREP=prepositional modifier, POBJ=object of preposition.	[[47, 51], [16, 24], [76, 80]]	[[52, 65], [25, 45], [81, 102]]	['PREP', '1PARTMOD', 'POBJ']	['prepositional', 'participial modifier', 'object of preposition']
32	middle value of 6.5, was tested.  The Lexile-like measure (LX) used the same two features as the Lexile measure: mean log frequency	[[59, 61]]	[[38, 44]]	['LX']	['Lexile']
33	With respect to the EUROTRA MT system this has  important implications for the translation between the syntactic  dependency level - the EUROTRA Relational Structure (ERS)  and the semant ic  level  - the in ter face  St ructure  (IS).	[[167, 170], [20, 27], [28, 30], [231, 233]]	[[137, 165], [205, 228]]	['ERS', 'EUROTRA', 'MT', 'IS']	['EUROTRA Relational Structure', 'in ter face  St ructure']
34	  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 49?54, Boulder, Colorado, June 2009.	[[87, 92]]	[[46, 85]]	['CoNLL']	['Computational Natural Language Learning']
35	       Figure 3: Scored dependency forest 2.5 Semantic Dependency Graph (SDG) The SDG is a semantic-label word DG designed	[[81, 84], [90, 93], [119, 121]]	[[54, 79]]	['SDG', 'SDG', 'DG']	['Semantic Dependency Graph']
36	 2.4 Stanford Parser The Stanford Parser (SP) is an unlexicalized parser that rivals state-of-the-art lexical-	[[42, 44]]	[[25, 40]]	['SP']	['Stanford Parser']
37	attributes of the observation vectors and a  specific label).we can represent the input-output  pairs via joint feature map (JFM)  1	[[125, 128]]	[[106, 123]]	['JFM']	['joint feature map']
38	tence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP = copula, EXCL = exclamation mark. 	[[106, 110], [29, 32], [49, 52], [75, 78], [92, 95]]	[[113, 124], [23, 27], [35, 40], [55, 64], [81, 90], [98, 104]]	['EXCL', 'TOP', 'ADV', 'ADJ', 'COP']	['exclamation', 'noun', 'topic', 'adverbial', 'adjective', 'copula']
39	a question written in natural language is called ? Question Answering?(QA), and has gotten a lot of attention recently.	[[71, 73]]	[[51, 69]]	['QA']	['Question Answering']
40	 In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 1698?1703.	[[90, 94]]	[[55, 73]]	['LREC']	['Language Resources']
41	protein interaction as an example. In Proceedings  of the Pacific Symposium on Biocomputing (PSB),  Hawaii, USA.	[[93, 96], [108, 111]]	[[58, 91]]	['PSB', 'USA']	['Pacific Symposium on Biocomputing']
42	 3 Objective SDS (Spoken dialogue system) researchers have addressed several practical challenges of apply-	[[13, 16]]	[[18, 40]]	['SDS']	['Spoken dialogue system']
43	 tage of the OpenCCG realizer?s ability to generate from disjunctive logical forms (DLFs), i.e. packed semantic dependency graphs (White, 2004; White,	[[84, 88], [13, 20]]	[[57, 82]]	['DLFs', 'OpenCCG']	['disjunctive logical forms']
44	2http://www.statmt.org/wmt12/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (MT). 	[[121, 123]]	[[100, 119]]	['MT']	['Machine Translation']
45	2010). Alternatively, iteratively optimized embeddings such as Skip Gram (SG) model (Mikolov et al.,	[[74, 76]]	[[63, 72]]	['SG']	['Skip Gram']
46	and EN-DE) with token frequency, sense distribution and most frequent translations ordered by the corresponding senses (T = temporal, CO = concession, CT = contrast). 	[[134, 136], [151, 153], [4, 9]]	[[139, 149], [156, 164], [124, 132]]	['CO', 'CT', 'EN-DE']	['concession', 'contrast', 'temporal']
47	nications Advancement Foundation, Japan, in part by the Center for Intelligent Information Retrieval, and in part by the Defense Advanced Research Projects Agency (DARPA), USA under contract number HR0011-06-C-0023.	[[164, 169], [172, 175]]	[[121, 162]]	['DARPA', 'USA']	['Defense Advanced Research Projects Agency']
48	Rule-Based Machine Translation(MT)(Hutchins and Somers, 1992) requires large-scale knowledge to analyze both source language(SL) sentences and target language(TL) sentences.	[[125, 127], [31, 33], [159, 161]]	[[109, 123], [11, 30], [143, 158]]	['SL', 'MT', 'TL']	['source languag', 'Machine Translation', 'target language']
49	3 CLaC Methodology Preprocessing consists of tokenizing, lemmatizing, sentence splitting, and part of speech (POS) tagging. 	[[110, 113], [2, 6]]	[[94, 108]]	['POS', 'CLaC']	['part of speech']
50	performs two vital functions in the case of our Japanese  processing:  1 CN = common noun; SN = sa-inflection noun (  nominalized .verb); VB = verb; VSUF = verb suffix; CM = 	[[73, 75], [138, 140], [149, 153], [169, 171]]	[[78, 89], [143, 147], [156, 167]]	['CN', 'VB', 'VSUF', 'CM']	['common noun', 'verb', 'verb suffix']
51	 Before we discuss the significant sentence in answer mails, we classified answer mails into three types: (1) direct answer (DA) mail, (2) questioner?s reply (QR) mail, and (3) the others.	[[125, 127], [159, 161]]	[[110, 123], [139, 157]]	['DA', 'QR']	['direct answer', 'questioner?s reply']
52	This paper examines the processing predictions of the ERH on a systematic class of relative clause types, the Accessibility Hierarchy (AH) shown in figure 1.	[[135, 137], [54, 57]]	[[110, 133]]	['AH', 'ERH']	['Accessibility Hierarchy']
53	person names, organization names, location names, etc. The template element (TE) task extracts information centered around an entity, like the acronym,	[[77, 79]]	[[59, 75]]	['TE']	['template element']
54	We use multiple epochs of minibatch stochastic gradient descent and update all parameters to minimize the negative log likelihood (NLL) of our training set.	[[131, 134]]	[[106, 129]]	['NLL']	['negative log likelihood']
55	the MDL methods.  ConVote (CONVOTE) Our second dataset is taken from segments of speech from United States	[[27, 34], [4, 7]]	[[18, 25]]	['CONVOTE', 'MDL']	['ConVote']
56	We however noticed the relative degradation of quality in coordinating conjunctions (CC), determiners (DT) and personal pronouns (PRP). 	[[130, 133], [85, 87], [103, 105]]	[[111, 128], [58, 83], [90, 101]]	['PRP', 'CC', 'DT']	['personal pronouns', 'coordinating conjunctions', 'determiners']
57	 ? Unlabeled Elementary Dependencies (UED) identical to LED, except ignoring all labeling	[[38, 41], [56, 59]]	[[3, 36]]	['UED', 'LED']	['Unlabeled Elementary Dependencies']
58	Various traditional  information retrieval(IR) techniques combined  with natural language processing(NLP) tech-  niques have been re-targeted to enable efficient 	[[101, 104], [43, 45]]	[[73, 99], [21, 42]]	['NLP', 'IR']	['natural language processin', 'information retrieval']
59	 3.1 Ske le ta l  Phrase  S t ructure  Component   The role of phrase structure (PS) rules in our parser is similar to  their role in Lexical Functional Grammar \[Kaplan 83\], however they 	[[81, 83]]	[[63, 79]]	['PS']	['phrase structure']
60	 This paper proposes an approach for implicit feature detection based on SVM and Topic Model(TM). 	[[93, 95], [73, 76]]	[[81, 91]]	['TM', 'SVM']	['Topic Mode']
61	 On the other hand, the decline in performance for the composite feature vector baseline (CFV) may be attributed to the data sparseness phenomenon	[[90, 93]]	[[55, 79]]	['CFV']	['composite feature vector']
62	vised taggers. One commonly-used unsupervised tagger is the Hidden Markov model (HMM), which models the joint distribution of a word se-	[[81, 84]]	[[60, 79]]	['HMM']	['Hidden Markov model']
63	 2. Tezt routing, tezt fdter/n9, and SDI (selective dissemination of information): These terms  refer to a loose collection of text classification tasks such as managing personal electronic mail, 	[[37, 40]]	[[42, 65]]	['SDI']	['selective dissemination']
64	Estimating Proficiency  Item Response Theory (IRT)  Item Response Theory (IRT) is the basis of modern  language tests such as TOEIC, and enables Com-	[[74, 77], [46, 49], [126, 131]]	[[52, 72], [24, 44]]	['IRT', 'IRT', 'TOEIC']	['Item Response Theory', 'Item Response Theory']
65	( NN?? ) speech 1:1 ( NP ( NR ) ( NN ) ) X1 | X2 1:2 2:1 ( NP ( NR?? ) (	[[34, 36], [2, 6], [59, 61], [64, 68]]	[[22, 29]]	['NN', 'NN??', 'NP', 'NR??']	['NP ( NR']
66	Table 3 presents the total number of training examples extracted from SemCor (SC) and from the background documents (BG). As expected, by	[[117, 119], [78, 80]]	[[95, 105], [70, 76]]	['BG', 'SC']	['background', 'SemCor']
67	alignment G (both A and G can be split into two subsets AS ,AP and GS , GP , respectively representing Sure and Probable alignments) Precision (PT ), Recall (RT ), F-measure (FT ) and Alignment Error	[[144, 146], [56, 58], [60, 62], [67, 69], [72, 74], [158, 160], [175, 177]]	[[112, 131], [150, 156], [164, 173]]	['PT', 'AS', 'AP', 'GS', 'GP', 'RT', 'FT']	['Probable alignments', 'Recall', 'F-measure']
68	 4 Active Learning Active Learning (AL) is a machine learning paradigm that let the learner decide which data it	[[36, 38]]	[[19, 34]]	['AL']	['Active Learning']
69	below.  4.3.1 English Noun Compounds (ENC) Our first dataset is made up of 90 binary English	[[38, 41]]	[[14, 36]]	['ENC']	['English Noun Compounds']
70	account for these generalizations by decom-  posing the grammar rules to Immediate Dom-  inance(ID) rules and Linear Preeedence(LP)  rules.	[[128, 130], [96, 98]]	[[110, 126], [73, 95]]	['LP', 'ID']	['Linear Preeedenc', 'Immediate Dom-  inance']
71	  Abstract  Our previous work focuses on combining translation memory (TM) and statistical machine translation  (SMT) when the TM database and the SMT training set are the same.	[[71, 73], [113, 116], [127, 129], [147, 150]]	[[51, 69], [79, 110]]	['TM', 'SMT', 'TM', 'SMT']	['translation memory', 'statistical machine translation']
72	Hodellng Temporal Knowledge  Hodellng time, dasoite its olovlous importance, has  proved an elusive goal for artificial Intelligence (AI). 	[[134, 136]]	[[109, 132]]	['AI']	['artificial Intelligence']
73	? P5E3N3S3, F W A Computer Processable English (CPE) (Pulman 1996; Sukkarieh and Pulman 1999) is a controlled language that can be ?	[[48, 51], [2, 10]]	[[18, 46]]	['CPE', 'P5E3N3S3']	['Computer Processable English']
74	We also have investigated our two-step solution on two existing treebanks, the Penn Chinese Treebank (CTB) (Xue et al.,	[[102, 105]]	[[84, 100]]	['CTB']	['Chinese Treebank']
75	ual resources on pairwise comparison task (Diff. = Difficulty lexicon, CF = Crowdflower) Features	[[71, 73]]	[[76, 87]]	['CF']	['Crowdflower']
76	distribution which underlies natural language text   -- which is if not a pure Zipfian distribution at least  an LNRE (large number of rare events, cf. Baayen 	[[113, 117]]	[[119, 146]]	['LNRE']	['large number of rare events']
77	 4 Algorithms 4.1 Topic?sentence graph matching (GM) We treat a sentence and a topic as graphs.	[[49, 51]]	[[33, 47]]	['GM']	['graph matching']
78	Table 1: Number of routes, directions, and tokens for the different settings. GM = Google Maps, CI = Campus Indoor, CO = Campus Outdoor.	[[78, 80], [96, 98], [116, 118]]	[[83, 94], [101, 114], [121, 135]]	['GM', 'CI', 'CO']	['Google Maps', 'Campus Indoor', 'Campus Outdoor']
79	For each text pair on four cross levels, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (S-Ph), Phrase to Word (Ph-W) and Word to Sense (W-Se), participants are required to re-	[[119, 123]]	[[103, 117]]	['Ph-W']	['Phrase to Word']
80	mantic content Ks, sends a message m to R and R  interprets m as meaning CR. CS = cn is a neces-  sary condition for this turn of communication to 	[[77, 79], [15, 17], [73, 75]]	[[82, 87]]	['CS', 'Ks', 'CR']	['cn is']
81	istic conversational systems. In Proceedings of Intelligent User Interfaces 2001 (IUI-01), pages 1?8, Santa Fe, NM, January.	[[82, 88], [112, 114], [108, 110]]	[[48, 80]]	['IUI-01', 'NM', 'Fe']	['Intelligent User Interfaces 2001']
82	  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 53?57, Gothenburg, Sweden, April 26-30 2014.	[[70, 72], [28, 32]]	[[50, 68]]	['DM', 'EACL']	['Dialogue in Motion']
83	study on NER is mainly focused either on the proper name identification of person(PER), location(LOC), organization(ORG), time(TIM) and numeral(NUM) expressions almost in news do-	[[116, 119], [9, 12], [82, 85], [97, 100], [127, 130], [144, 147]]	[[103, 114], [75, 81], [88, 96], [122, 126], [136, 143]]	['ORG', 'NER', 'PER', 'LOC', 'TIM', 'NUM']	['organizatio', 'person', 'location', 'time', 'numeral']
84	utterance. The interface default allowing this is called Noun meaning extended (NMExt)13 NMExt: Noun(u), sem(u) is ?	[[80, 85], [89, 94]]	[[57, 78]]	['NMExt', 'NMExt']	['Noun meaning extended']
85	Table 1). Firstly, each term candidate is mapped to  a specific canonical representative (CR) by  semantically isomorphic transformations.	[[90, 92]]	[[64, 88]]	['CR']	['canonical representative']
86	tags and word.  Rich Morphological Features (Rich-MF): In addition to the elementary features we use the am-	[[45, 52]]	[[16, 43]]	['Rich-MF']	['Rich Morphological Features']
87	Figure 4: Canonical example pub from Saltzman and Munhall (1989) representing overlapping goals for tongue blade constriction degree (TBCD), lip aperture (LA), and glottis (GLO).	[[134, 138], [155, 157], [173, 176]]	[[100, 132], [141, 153], [164, 171]]	['TBCD', 'LA', 'GLO']	['tongue blade constriction degree', 'lip aperture', 'glottis']
88	Han, C-H., Han, N-R., Ko, E-S.and Palmer, M.: Development and Evaluation of a Korean  Treebank and Its Application to NLP.in Proceedings of the 3rd International Conference on  Language Resources and Evaluation (LREC).(2002)  5.	[[212, 216]]	[[177, 195]]	['LREC']	['Language Resources']
89	perceptual space between each pair. We can do this with a multidimensional scaling (MDS) algorithm. Let us call	[[84, 87]]	[[58, 82]]	['MDS']	['multidimensional scaling']
90	corresponding to the point P.  Once a n  object has been added to the geometric model by specifying values  for its GSTART, GSIZE, and ROTN (rotation), the geometric coordinates for any  location on the object may be obtained by calling the funtion EXECLOCA with the 	[[135, 139], [249, 257], [116, 122], [124, 129]]	[[141, 149]]	['ROTN', 'EXECLOCA', 'GSTART', 'GSIZE']	['rotation']
91	known as conditional random fields (CRFs) (Lafferty et al, 2001), when all variables are observed, and as hidden conditional random fields (HCRFs) (Quattoni et al, 2007), when only a subset of the variables are	[[140, 145], [36, 40]]	[[106, 138], [9, 34]]	['HCRFs', 'CRFs']	['hidden conditional random fields', 'conditional random fields']
92	Linggle: a Web-scale Linguistic Search Engine for Words in Context    Joanne Boisson+, Ting-Hui Kao*, Jian-Cheng Wu*, Tzu-His Yen*, Jason S. Chang* +Institute of Information Systems and Applications *Department of Computer Science National Tsing Hua University HsinChu, Taiwan, R.O.C. 30013 {joanne.boisson, maxis1718, wujc86, joseph.yen, jason.jschang} @gmail.com     Abstract  In this paper, we introduce a Web-scale lin-guistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and ad-ditional regular expression (RE) operators. In our approach, we incorporate inverted file in-dexing, PoS information from BNC, and se-mantic indexing based on Latent Dirichlet Al-location with Google Web 1T. The method in-volves parsing the query to transforming it in-to several keyword retrieval commands.	[[584, 587], [636, 638], [278, 283], [708, 711], [729, 732]]	[[567, 582], [616, 634]]	['PoS', 'RE', 'R.O.C', 'PoS', 'BNC']	['parts of speech', 'regular expression']
93	3.2 Graph-based Label Propagation Graph-based label propagation, a critical subclass of semi-supervised learning (SSL), has been widely used and shown to outperform other SSL meth-	[[114, 117], [171, 174]]	[[88, 112]]	['SSL', 'SSL']	['semi-supervised learning']
94	overlapping relations. In Annual Meeting of the Association for Computational Linguistics (ACL). 	[[91, 94]]	[[48, 88]]	['ACL']	['Association for Computational Linguistic']
95	guages: British sign language (BSL), Danish (DSL), French Belgian (FBSL), Flemish (FSL), Greek (GSL), and Dutch (NGT). The data for the	[[96, 99], [31, 34], [45, 48], [67, 71], [83, 86], [113, 116]]	[[89, 94], [8, 29], [37, 43], [51, 65], [74, 81], [106, 111]]	['GSL', 'BSL', 'DSL', 'FBSL', 'FSL', 'NGT']	['Greek', 'British sign language', 'Danish', 'French Belgian', 'Flemish', 'Dutch']
96	 04 (a) Same Topic (ST) 0.00 0.01 0.02 0.03 0.04 0.05 0.06	[[20, 22]]	[[8, 18]]	['ST']	['Same Topic']
97	Examples of failure of analysis  (i) JISSAI (in fact), CHOSHA-TACHI-WA (authors) {\[KORE-WO (it) TSUKATTE  (using), JUURYOKU-SOUGO-SAYOU-GA (gravitationally interacting) SHIHAI-SURU (govern-  ing)\] TENTAI-NO (astronomical) UNDOU-NI-TSUITE (about the motion), KOUSEIDO-DE (high- 	[[137, 139], [37, 43], [55, 70], [170, 181], [199, 208], [224, 239]]	[[141, 156], [183, 195], [210, 222], [241, 257]]	['GA', 'JISSAI', 'CHOSHA-TACHI-WA', 'SHIHAI-SURU', 'TENTAI-NO', 'UNDOU-NI-TSUITE']	['gravitationally', 'govern-  ing', 'astronomical', 'about the motion']
98	In Proceedings of the 19th International Conference on Computational Linguistics (COLING?02), pages 218? 	[[82, 91]]	[[55, 80]]	['COLING?02']	['Computational Linguistics']
99	"uses the mapping of concept dog to the class of alternative  expressions for named individual (such as using the name,  2 VSFL (""Very Simple Frame Language"") and SCORE CSproket  Core"") were developed at BBN Systems and Technologies by "	[[122, 126], [162, 167], [168, 176], [203, 206]]	[[129, 155]]	['VSFL', 'SCORE', 'CSproket', 'BBN']	['Very Simple Frame Language']
100	2 Complexity of GPSG Components  A generalized phrase structure grammar contains five language-  particular components - -  immediate dominance (ID) rules, meta-  rules, linear precedence (LP) statements, feature co-occurrence 	[[145, 147], [16, 20], [189, 191]]	[[124, 143], [170, 187]]	['ID', 'GPSG', 'LP']	['immediate dominance', 'linear precedence']
101	den Markov Models (HMMs), Conditional Random Fields (CRFs), Maximum Entropy Markov  Models (MEMMs), etc. 	[[92, 97], [19, 23], [53, 57]]	[[60, 90], [4, 17], [26, 51]]	['MEMMs', 'HMMs', 'CRFs']	['Maximum Entropy Markov  Models', 'Markov Models', 'Conditional Random Fields']
102	a topic model are compared. Latent Dirichlet Allocation (LDA) (Blei et al 2003) is a widely used type of topic model in which documents can be	[[57, 60]]	[[28, 55]]	['LDA']	['Latent Dirichlet Allocation']
103	are either too wordy or too ungrammatical. Table 1 shows the compression rates (CompR) for the two 8	[[80, 85]]	[[61, 78]]	['CompR']	['compression rates']
104	This paper proposes a contextdependent phrase reordering approach that uses the maximum entropy (MaxEnt) model to help the HPB decoder select appropriate re-	[[97, 103], [123, 126]]	[[80, 95]]	['MaxEnt', 'HPB']	['maximum entropy']
105	 3.2 ,6  Addi t ion  of T rans la t ion  Rules  FinMly, translation rules (TRis) are added to the set  of GLTPC, s. TRis are descriptions in which concepts 	[[75, 79], [106, 111], [116, 120]]	[[56, 73]]	['TRis', 'GLTPC', 'TRis']	['translation rules']
106	lation of term-lists, related studies are found in the  area of target word selection (for content words) in  conventional full-text machine translation (MT). 	[[154, 156]]	[[133, 152]]	['MT']	['machine translation']
107	BG 80,757 1.34 EN 94,725 2.58 Table 2: Corpus statistics: SR=Serbian, SL=Slovene, EN=English, BG=Bulgarian	[[58, 60], [70, 72], [15, 17], [0, 2], [82, 84], [94, 96]]	[[61, 68], [73, 80], [85, 92], [97, 106]]	['SR', 'SL', 'EN', 'BG', 'EN', 'BG']	['Serbian', 'Slovene', 'English', 'Bulgarian']
108	al., 2005; Vapnik, 1998) based on the Gaussian  Radial Basis kernel function (RBF). We tuned 	[[78, 81]]	[[48, 76]]	['RBF']	['Radial Basis kernel function']
109	Swedish 83.01 (82.44) 88.53 (87.36) 81.20 (81.10) 86.50 (85.86) 82.95 (82.66*) 88.29 (87.45*) 82.89 (82.44) 88.61 (87.55) Turkish 62.70 (71.27) 73.67 (78.57) 59.83 (68.31) 70.15 (75.17) 63.27* (71.63*) 73.93* (78.72*) 62.58 (70.96) 73.09 (77.95) Table 1: Parsing accuracy of the undirected planar parser with naive (UPlanarN) and label-based (UPlanarL) postprocessing in comparison to the directed planar (Planar) and the MaltParser arc-eager projective (MaltP)	[[316, 324], [343, 351], [406, 412], [455, 460]]	[[279, 314], [398, 404], [422, 432]]	['UPlanarN', 'UPlanarL', 'Planar', 'MaltP']	['undirected planar parser with naive', 'planar', 'MaltParser']
110	  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1181?1191, October 25-29, 2014, Doha, Qatar.	[[90, 95]]	[[40, 88]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
111	fidence information about each system?s hypothesis. This feature class includes the confusion network (CN) word confidence, CN slot entropy, and the number of alter-	[[103, 105], [124, 126]]	[[84, 101]]	['CN', 'CN']	['confusion network']
112	 1 Introduction Question answering (QA) from a knowledge base (KB) has a long history within natural language	[[36, 38], [63, 65]]	[[16, 34], [47, 61]]	['QA', 'KB']	['Question answering', 'knowledge base']
113	level for natural  language unders tand ing  system. In case of a  Module  Package Layer( MPL ), there are two kinds of program  packages.	[[90, 93]]	[[67, 88]]	['MPL']	['Module  Package Layer']
114	pus Linguistics 2001 Conference, pages 274?280. Lancaster University (UK). 	[[70, 72]]	[[58, 68]]	['UK']	['University']
115	859  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 217?226, Seoul, South Korea, 5-6 July 2012.	[[101, 108]]	[[51, 99]]	['SIGDIAL']	['Special Interest Group on Discourse and Dialogue']
116	tion device, a Nippon Electric Corporation DP-200, was  added to an existing natural anguage processing system,  the Natural Language Computer (NLC) (Ballard 1979,  Biermann and Ballard 1980).	[[144, 147], [43, 45]]	[[117, 142]]	['NLC', 'DP']	['Natural Language Computer']
117	Abstract Regardless of language, the standard character set for text messages (SMS) and many other social media platforms is the Roman alphabet.	[[79, 82]]	[[56, 77]]	['SMS']	['set for text messages']
118	Associative Texture Is Lost In Translation. In Proceedings  of the Workshop on Discourse in Machine Translation (DiscoMT),  pages 27?32. ACL 2013 Conference, 	[[113, 120], [137, 140]]	[[79, 111]]	['DiscoMT', 'ACL']	['Discourse in Machine Translation']
119	describing each video. We then clustered these verbs using Hierarchical Agglomerative Clustering (HAC) using the res metric from WordNet::Similarity by	[[98, 101]]	[[59, 96]]	['HAC']	['Hierarchical Agglomerative Clustering']
120	(adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and	[[102, 105], [118, 122], [14, 17], [29, 33], [46, 49], [65, 68], [139, 142], [156, 160]]	[[107, 115], [124, 136], [19, 26], [35, 43], [51, 62], [70, 99], [144, 153], [162, 173]]	['NUM', 'CONJ', 'ADV', 'PRON', 'DET', 'ADP', 'PRT', 'PUNC']	['numerals', 'conjunctions', 'adverbs', 'pronouns', 'determiners', 'prepositions or postpositions', 'particles', 'punctuation']
121	search has encore'aged the FI{UM1 ) api)roach. The  SUMMONS (SUMMarizing Online News artMes)  system (McKeown and Radev, 1999) takes tem- 	[[52, 59]]	[[61, 84]]	['SUMMONS']	['SUMMarizing Online News']
122	same time, Intelligent Computer-Assisted Language Learning (ICALL) and Intelligent Language Tutoring (ILT) systems (e.g., Heift and Schulze, 2007; Meurers, 2012) also tend to focus more on gram-	[[102, 105], [60, 65]]	[[71, 100], [11, 58]]	['ILT', 'ICALL']	['Intelligent Language Tutoring', 'Intelligent Computer-Assisted Language Learning']
123	Pivot, RHS),  generate all subconstituents  generate _rhs ( RHS ),  generate material on path to root 	[[60, 63], [7, 10]]	[[54, 57]]	['RHS', 'RHS']	['rhs']
124	fn-n1?. The annotation contains a feature structure with three features: FE (Frame element), GF (Grammatical Function), and PT	[[73, 75], [93, 95], [124, 126]]	[[77, 90], [97, 117]]	['FE', 'GF', 'PT']	['Frame element', 'Grammatical Function']
125	1. Introduction  In a Spoken Language System (SLS) we must use all avail-  able knowledge sources (KSs) to decide on the spoken sen- 	[[46, 49], [99, 102]]	[[22, 44], [80, 97]]	['SLS', 'KSs']	['Spoken Language System', 'knowledge sources']
126	{afader,soderlan,etzioni}@cs.washington.edu Abstract Open Information Extraction (IE) is the task of extracting assertions from massive corpora	[[82, 84]]	[[58, 80]]	['IE']	['Information Extraction']
127	Table 3: Results on DevTest and Test Sets compared with the Average Performance in CoNLL?07. LAS = Labelled Attachment Score, UAS = Unlabelled Attachment Score, LAcc = Label Accuracy, AV = Average score.	[[126, 129], [184, 186], [93, 96], [161, 165], [83, 88]]	[[132, 159], [189, 196], [99, 124], [168, 182]]	['UAS', 'AV', 'LAS', 'LAcc', 'CoNLL']	['Unlabelled Attachment Score', 'Average', 'Labelled Attachment Score', 'Label Accuracy']
128	 The SU detection task is conducted on two corpora: Broadcast News (BN) and Conversational Telephone Speech (CTS).	[[68, 70]]	[[52, 66]]	['BN']	['Broadcast News']
129	"local  cons t i tuents .  These i n c l u d e  Hsimplem noun phrases (NPs) and  prepositional phrases (PPs), (""simplen meaning 'up to the head noun but  not including any modifying clauses or phrases""),  and verb groups (VGs) "	[[103, 106], [70, 73], [221, 224]]	[[80, 101], [56, 68], [208, 219]]	['PPs', 'NPs', 'VGs']	['prepositional phrases', 'noun phrases', 'verb groups']
130	In Proceedings of the Fifth Mediterranean Morphology Meeting (MMM5), pages 269?290, Fr?jus.	[[62, 66]]	[[42, 60]]	['MMM5']	['Morphology Meeting']
131	 5.2 Corpus Benchmark Tool The Corpus Benchmark Tool(CBT) is one of the components in GATE which enables automatic evaluation of an	[[53, 56]]	[[31, 51]]	['CBT']	['Corpus Benchmark Too']
132	tution is the same as the cost of insertion or deletion. A normalized edit distance (NED) is calculated by dividing the total edit cost by the length of	[[85, 88]]	[[59, 83]]	['NED']	['normalized edit distance']
133	 1 Introduction Base Phrase Chunking (BPC), also known as shallow syntactic parsing, is the process by which ad-	[[38, 41]]	[[16, 36]]	['BPC']	['Base Phrase Chunking']
134	In Proceedings of the Third International Conference on Web Search and Web Data Mining (WSDM), pages 101?110, New York, NY, USA, 2010.	[[88, 92], [120, 122], [124, 127]]	[[56, 86]]	['WSDM', 'NY', 'USA']	['Web Search and Web Data Mining']
135	When these approaches are applied to normal Twitter users accuracy results significantly decrease.  Sentiment Analysis (SA) has been widely studied in the last decade in multiple domains. Most work	[[120, 122]]	[[100, 118]]	['SA']	['Sentiment Analysis']
136	ing decision can be made directly based on attention weights from the two query components or further rescored by the maximum spanning tree (MST) search algorithm.	[[141, 144]]	[[118, 139]]	['MST']	['maximum spanning tree']
137	As further evidence of the effectiveness of our framework, we have recently adapted our phrase-structure parser in Section 6 to parsing with a lexicalized grammar formalism, Combinatory Categorial Grammar (CCG), and achieved higher F-scores than the state-of-the-art C&C CCG parser (Clark and Curran 2007).	[[206, 209], [267, 270], [271, 274]]	[[174, 204]]	['CCG', 'C&C', 'CCG']	['Combinatory Categorial Grammar']
138	put language context to bias translation choices is in some sense a neural network analogy to the PSD (phrase sense disambiguation) approach for context-dependent translation probabilities of	[[98, 101]]	[[103, 130]]	['PSD']	['phrase sense disambiguation']
139	The most successful stochastic language models  have been based on finite-state descriptions such  as n-grams or hidden Markov models (HMMs)  (Jelinek et al, 1992).	[[135, 139]]	[[113, 133]]	['HMMs']	['hidden Markov models']
140	 4.1 KNN classification The basic idea of the K nearest neighbor (KNN) classification algorithm is to use already categorized	[[66, 69], [5, 8]]	[[46, 64]]	['KNN', 'KNN']	['K nearest neighbor']
141	Ney discounting and interpolation. The evaluation of stream counts is done on EP+afe+nyt (EAN) corpus, consisting of 1.1 billion words.	[[90, 93]]	[[78, 88]]	['EAN']	['EP+afe+nyt']
142	pora. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 993?1000.	[[88, 94]]	[[61, 86]]	['COLING']	['Computational Linguistics']
143	Donald, 2006). As a joint approach, we consider Reluctant Trimmer (RT), a simplification system that employs synchronous dependency grammars	[[67, 69]]	[[48, 65]]	['RT']	['Reluctant Trimmer']
144	maries C are overall better for answering questions than summaries B. Comparison between B and C (B-C) precision recall	[[98, 101]]	[[89, 96]]	['B-C']	['B and C']
145	However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009).	[[124, 127]]	[[95, 122]]	['LDA']	['Latent Dirichlet Allocation']
146	3 Framework We model the information extraction task as a markov decision process (MDP), where the model learns to utilize external sources to improve upon	[[83, 86]]	[[58, 81]]	['MDP']	['markov decision process']
147	5 Conclusion and Further Works   In this paper we have proposed a reestimation algorithm and a best-first parsing algorithm  for probabilistic dependency grammars(PDG). The reestimation algorithm is a variation of 	[[163, 166]]	[[129, 161]]	['PDG']	['probabilistic dependency grammar']
148	semantic links between NEs extracted from the answer sentence and the question focus word, which encodes the expected lexical answer type (LAT). We	[[139, 142], [23, 26]]	[[118, 137]]	['LAT', 'NEs']	['lexical answer type']
149	swer sequence tagging.  bels: B-ANSWER (beginning of answer), I-ANSWER (inside of answer), O (outside of answer).	[[30, 38], [62, 70]]	[[40, 59], [94, 111]]	['B-ANSWER', 'I-ANSWER']	['beginning of answer', 'outside of answer']
150	 Given the good results of the pilot we decided to deploy the task in Amazon Mechanical Turk (AMT) in order to crowd source the annotation task.	[[94, 97]]	[[70, 92]]	['AMT']	['Amazon Mechanical Turk']
151	Pengfei Lu Department of Computer Science Graduate Center City University of New York (CUNY) 365 Fifth Ave, New York, NY 10016 pengfei.lu@qc.cuny.edu  Matt Huenerfauth Department of Computer Science Queens College and Graduate Center City University of New York (CUNY) 65-30 Kissena Blvd, Flushing, NY 11367 matt@cs.qc.cuny.edu  Abstract  American Sign Language (ASL) synthesis software can improve the accessibility of in-formation and services for deaf individuals with low English literacy. The synthesis com-ponent of current ASL animation generation and scripting systems have limited handling of the many ASL verb signs whose movement path is inflected to indicate 3D locations in the signing space associated with discourse refer-ents.	[[363, 366], [87, 91], [118, 120], [263, 267], [299, 301], [530, 533], [611, 614]]	[[339, 361], [58, 85], [108, 116], [234, 261]]	['ASL', 'CUNY', 'NY', 'CUNY', 'NY', 'ASL', 'ASL']	['American Sign Language', 'City University of New York', 'New York', 'City University of New York']
152	 1 Introduction Language identification (LangID) is the problem of determining what natural language a document is written in.	[[41, 47]]	[[16, 39]]	['LangID']	['Language identification']
153	date translations. While the traditional Maximum A Posteriori (MAP) decision rule can be optimized as a piecewise lin-	[[63, 66]]	[[41, 61]]	['MAP']	['Maximum A Posteriori']
154	ambiguous word are included as features.  Medical Subject Headings (MeSH): The final feature is also specific to the biomedical do-	[[68, 72]]	[[42, 66]]	['MeSH']	['Medical Subject Headings']
155	given the precorrected sentence. Each Noun Phrase (NP) in  the test sentence will be pre-corrected as correc-	[[51, 53]]	[[38, 49]]	['NP']	['Noun Phrase']
156	bines the output of all the 13 base models introduced previously. We implemented the meta-classifier using Support Vector Machines (SVMs)15 with a quadratic polynomial kernel, and C = 0.01 (tuned in the development set).16 Lastly, Table 13 shows the results	[[132, 136]]	[[107, 130]]	['SVMs']	['Support Vector Machines']
157	curacy of an automatic classifier means to compare its output with the correct semantic tags on a Gold Standard (GS) dataset. Within our formal	[[113, 115]]	[[98, 111]]	['GS']	['Gold Standard']
158	CoTrain vs. TSVM(EN) 1.41E-05 0.00311 2.17E-08 CoTrain vs. TSVM(ENCN) 1.37E-08 0.0113 0.0396 CoTrain vs. SelfTrain(CN) 7.07E-18 2.79E-11 6.53E-07 CoTrain vs. SelfTrain(EN) 1.01E-07 0.0192 1.35E-07	[[115, 117], [12, 16], [17, 19], [59, 63], [64, 68], [168, 170]]	[[93, 100]]	['CN', 'TSVM', 'EN', 'TSVM', 'ENCN', 'EN']	['CoTrain']
159	ducted. The administrative body governing these decisions is the Institutional Review Board (IRB). 	[[93, 96]]	[[65, 91]]	['IRB']	['Institutional Review Board']
160	 1 Introduction A Chinese natural language processing (NLP) platform always includes lexical analysis (word	[[55, 58]]	[[26, 53]]	['NLP']	['natural language processing']
161	FA8750-09-C-0181. The first author also thanks the Vietnam Education Foundation (VEF) for its sponsorship.	[[81, 84]]	[[51, 79]]	['VEF']	['Vietnam Education Foundation']
162	els on a previously unseen test set ? the test split of part 3 of the PATB (PATB3-TEST). Table 6 shows	[[76, 86]]	[[56, 74]]	['PATB3-TEST']	['part 3 of the PATB']
163	1 Introduction Since the introduction of BLEU (Papineni et al, 2002), automatic machine translation (MT) evaluation has received a lot of research interest.	[[101, 103], [41, 45]]	[[80, 99]]	['MT', 'BLEU']	['machine translation']
164	native (see \[2\]).  Task  Manager  (TM)   In our previous experience, speech recognition systems 	[[37, 39]]	[[21, 34]]	['TM']	['Task  Manager']
165	(http://ieee.rkbexplorer.com/) repository7. The corpus of cancer research (COCR) contains 3334 domain specific abstracts of scientific publica-	[[75, 79]]	[[48, 73]]	['COCR']	['corpus of cancer research']
166	ticular, this includes a model of the grounding process (Clark, 1996) that involves recognition and construction of common ground units (CGUs) (see (Traum, 2003)). 	[[137, 141]]	[[116, 135]]	['CGUs']	['common ground units']
167	denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. 	[[92, 94], [113, 115], [79, 81]]	[[95, 111], [116, 131], [82, 90]]	['FS', 'DS', 'FL']	['Feature stacking', 'Domain stacking', 'Flagging']
168	manual mapping. In the rest of the paper, we shall first  describe the Switchboard Dialogue Act (SWBD-DA)  Corpus and its annotation scheme (i.e. SWBD-DAMSL).	[[97, 104], [146, 156]]	[[71, 95]]	['SWBD-DA', 'SWBD-DAMSL']	['Switchboard Dialogue Act']
169	- coordination (COORD) : traite les c,'~s imples de  coordination,  - statistique (STAT) : utilis6 sur des s6quences qu'il  est impossible de d6sambigui'ser A l'aide 	[[83, 87], [16, 21]]	[[70, 81], [0, 14]]	['STAT', 'COORD']	['statistique', '- coordination']
170	 In Table 3 the number of questions that get a higher and lower reciprocal rank (RR) after applying the individual lexico-semantic resources are	[[81, 83]]	[[64, 79]]	['RR']	['reciprocal rank']
171	sion with LSA and filtering according to the ET.  Further on, we apply sentiment analysis (SA)  using the approach described in Section 5.3 and 	[[91, 93], [10, 13], [45, 47]]	[[71, 89]]	['SA', 'LSA', 'ET']	['sentiment analysis']
172	search tool, but it is insufficient for domain-  specific text, such as that encountered in  the MUCs (Message Understanding Confer-  ences).	[[97, 101]]	[[103, 132]]	['MUCs']	['Message Understanding Confer-']
173	Apple Events  1 Introduction  The SlmSum (Slmulatmn of Summarizing)  system does what its name pronuses It simu- 	[[34, 40]]	[[42, 66]]	['SlmSum']	['Slmulatmn of Summarizing']
174	annotating unlabeled data, for adapting  existing CRF-based named entity recognition (NER) systems to new texts or  domains.	[[86, 89], [50, 53]]	[[60, 84]]	['NER', 'CRF']	['named entity recognition']
175	Our work is most similar to the content selection method of the multimedia conversation system RIA (Responsive Information Architect) (Zhou and Aggarwal, 2004).	[[95, 98]]	[[100, 132]]	['RIA']	['Responsive Information Architect']
176	its nondecomposability, as well as a cross between B??? and word error rate (WER) that is decomposable down to the subsentential level (in a sense to be	[[77, 80]]	[[60, 75]]	['WER']	['word error rate']
177	PROD = Predicate $SUBJ -- SUBJECT value of feature ~OL  DET = Determiner NO = Noun Phrase (B~R I)  ADJ = Adjective GD = Gender  NU '~= Number PS = Person 	[[115, 117], [0, 4], [17, 22], [52, 54], [56, 59], [73, 75], [99, 102], [128, 130], [142, 144]]	[[120, 126], [7, 16], [26, 33], [62, 72], [78, 82], [105, 114], [135, 141], [147, 153]]	['GD', 'PROD', '$SUBJ', 'OL', 'DET', 'NO', 'ADJ', 'NU', 'PS']	['Gender', 'Predicate', 'SUBJECT', 'Determiner', 'Noun', 'Adjective', 'Number', 'Person']
178	Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). The number of entities	[[116, 121]]	[[100, 114]]	['BNews']	['broadcast news']
179	"MSEAS (MS,MSEA, VP,i, OUT)  (1) Start with VP = VAR ({X1, "" "" ,X.}), MSEA = f~,  i=1, and OUT = O. When the computation is completed, MS is bound to the set of active "	[[90, 93], [0, 5], [7, 9], [10, 14], [16, 18], [22, 25], [43, 45], [48, 51], [69, 73], [134, 136]]	[[96, 119]]	['OUT', 'MSEAS', 'MS', 'MSEA', 'VP', 'OUT', 'VP', 'VAR', 'MSEA', 'MS']	['O. When the computation']
180	We implement MVM using generative model primitives drawn from Latent Dirichlet Allocation (LDA) and the Dirichlet Process (DP). |M | disparate	[[123, 125], [13, 16], [91, 94]]	[[104, 121], [62, 89]]	['DP', 'MVM', 'LDA']	['Dirichlet Process', 'Latent Dirichlet Allocation']
181	at meeting the needs of CSR research, and at serving in  a complementary ole to the corpora being collected in  the interactive Air Travel Information System (ATIS) do-  main.	[[159, 163], [24, 27]]	[[128, 157]]	['ATIS', 'CSR']	['Air Travel Information System']
182	 One example of this would be in providing support for the curation of the Gene Expression Database (GXD).4 This support could come in the form of a named entity recog-	[[101, 104]]	[[75, 99]]	['GXD']	['Gene Expression Database']
183	1 Introduction Annotated corpora are essential for most research in natural language processing (NLP). For exam-	[[97, 100]]	[[68, 95]]	['NLP']	['natural language processing']
184	contains the result for Eisner?s algorithm using no transformation (N-Proj), projectivized training data (Proj), and pseudo-projective parsing (P-Proj). The	[[144, 150], [68, 74], [106, 110]]	[[124, 142]]	['P-Proj', 'N-Proj', 'Proj']	['projective parsing']
185	The partitioning of the dataset is listed in the Table 1, where we also give the partitioning of Wall Street Journal (WSJ) (Marcus et al, 1993) used to train the English grammar.	[[118, 121]]	[[97, 116]]	['WSJ']	['Wall Street Journal']
186	conceptualizing the relation of coincidence or proximity with the whole  Landmark (LM) when it is conceived as a  point.	[[83, 85]]	[[73, 81]]	['LM']	['Landmark']
187	far and formulate new ones inspired by latent semantic analysis (LSA), which was developed within the information retrieval (IR) community to treat synonymous and polysemous terms (Deerwester et	[[125, 127], [65, 68]]	[[102, 123], [39, 63]]	['IR', 'LSA']	['information retrieval', 'latent semantic analysis']
188	 ? Reverse Gap (RG), if (i2 + 1) < i3 for OL and if (i6 + 1) < i1 for OR. (	[[16, 18]]	[[3, 14]]	['RG']	['Reverse Gap']
189	Figure 3: System I performance for each relation (CC=CAUSE-EFFECT, IA=INSTRUMENTAGENCY, PP=PRODUCT-PRODUCER, OE=ORIGIN-ENTITY, TT=THEME-TOOL,	[[50, 52], [67, 69], [88, 90], [109, 111], [127, 129]]	[[53, 65], [70, 86], [91, 107], [112, 125], [130, 140]]	['CC', 'IA', 'PP', 'OE', 'TT']	['CAUSE-EFFECT', 'INSTRUMENTAGENCY', 'PRODUCT-PRODUCER', 'ORIGIN-ENTITY', 'THEME-TOOL']
190	ZC05 (Zettlemoyer and Collins 2005) 79.3 ?  ZC07 (Zettlemoyer and Collins 2007) 86.1 ? 	[[44, 48], [0, 4]]	[[50, 78]]	['ZC07', 'ZC05']	['Zettlemoyer and Collins 2007']
191	Figure 1(a), the node @VP indicates that a binarization has been performed on the subtree VP (VBD PRT PP). All remaining rules that	[[90, 92], [23, 25]]	[[94, 101]]	['VP', 'VP']	['VBD PRT']
192	ilarity between given texts. The first approach is based on vector space models (VSMs) (Meadow, 1992).	[[81, 85]]	[[60, 79]]	['VSMs']	['vector space models']
193	1 25   2. Loca l  Word  Grouper  (LWG)  The funct ion  of th i s  b lock  is to fo rm 	[[34, 37]]	[[10, 31]]	['LWG']	['Loca l  Word  Grouper']
194	 2.3 Evaluation We use the Dutch part of EuroWordNet (DWN) (Vossen, 1998) for evaluation of our hypernym ex-	[[54, 57]]	[[27, 52]]	['DWN']	['Dutch part of EuroWordNet']
195	 1 Introduction  Natural Language Generation (NLG) systems must of  course be evaluated, like all NLP systems.	[[46, 49], [98, 101]]	[[17, 44]]	['NLG', 'NLP']	['Natural Language Generation']
196	 REF = obj123 SIZE = sizesensorreading85 SHAPE = shapesensorreading62	[[14, 18], [1, 4], [41, 46]]	[[21, 40], [49, 69]]	['SIZE', 'REF', 'SHAPE']	['sizesensorreading85', 'shapesensorreading62']
197	2008) since it also includes structural information of arguments. It is based on the Argumentation Markup Language (AML) that models argument components in a XML-based tree structure.	[[116, 119], [158, 161]]	[[85, 114]]	['AML', 'XML']	['Argumentation Markup Language']
198	Noun Variation (NV) ? Adjective Variation (AdjV) ?	[[43, 47]]	[[22, 41]]	['AdjV']	['Adjective Variation']
199	 1 Introduction Question answering (QA) has emerged as a practical research problem for pushing the boundaries	[[36, 38]]	[[16, 34]]	['QA']	['Question answering']
200	refers to the fact that either a particular lexical item or a particular grammatical construction must be present for the omission of a frame element (FE) to occur.	[[151, 153]]	[[136, 149]]	['FE']	['frame element']
201	In Proc. of the Eighth  Text Retrieval Conference (TREC-8), pages 151162.	[[51, 57]]	[[16, 49]]	['TREC-8']	['Eighth  Text Retrieval Conference']
202	are verbs.  Translation Filter (TF) handles both Predicate Mismatch and Verb?Non-Verb translation shift er-	[[32, 34]]	[[12, 30]]	['TF']	['Translation Filter']
203	started.  British National Corpus (BNC)4, American  National Corpus (ANC)5 had been referenced 	[[35, 38], [69, 72]]	[[10, 33], [42, 67]]	['BNC', 'ANC']	['British National Corpus', 'American  National Corpus']
204	characteristic, we regard the sentiment  classification as a sequence labeling problem and  use conditional random field (CRFs) model to  capture the relation between two adjacent 	[[122, 126]]	[[96, 120]]	['CRFs']	['conditional random field']
205	 3.2 Convolutional Neural Network model The Convolutional Neural Network (CNN) model using raw audio as input is shown in Figure 1.	[[74, 77]]	[[44, 72]]	['CNN']	['Convolutional Neural Network']
206	2Although independently-developed implementations of  essentially the same algorithm can be found in the source code  of The Attribute Logic Engine (ALE) version 3.2 (Carpenter  & Penn, 1999) and the SICStus Prolog term utilities library 	[[149, 152]]	[[125, 147]]	['ALE']	['Attribute Logic Engine']
207	" On the other hand, the final, ""concrete"", level  of semantic representation (SemRep) is more  like a fully-fledged logical form and it is no "	[[78, 84]]	[[53, 76]]	['SemRep']	['semantic representation']
208	Semantic Feature Performance  The semantic features include Named Entity  (NE), Noun Hypernym (NHype) and Head Verb  Synset (HVSyn).	[[95, 100], [75, 77], [125, 130]]	[[80, 93], [60, 72], [106, 123]]	['NHype', 'NE', 'HVSyn']	['Noun Hypernym', 'Named Entity', 'Head Verb  Synset']
209	al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organi-	[[42, 45], [11, 15], [82, 85]]	[[36, 40], [74, 80]]	['NAM', 'NIST', 'PER']	['name', 'person']
210	Question is defined as a Question term (QTerm).  The Answer Term (ATerm) is the Answer given by the KM corpus.	[[66, 71], [40, 45], [100, 102]]	[[53, 64], [25, 38]]	['ATerm', 'QTerm', 'KM']	['Answer Term', 'Question term']
211	2003). Another kind of verb-based multiword expression is light verb constructions (LVCs), such as the examples in (1).	[[84, 88]]	[[58, 82]]	['LVCs']	['light verb constructions']
212	results in Die event. Recent improvements of convolutional neural networks (CNNs) have been proven to be efficient for capturing syntactic and	[[76, 80]]	[[45, 74]]	['CNNs']	['convolutional neural networks']
213	    The difference between the two models was the design of the input layer. The first model (henceforth, the diagnostics model DIAG) took diagnostics as input nodes, whereas the second model (henceforth, the semantic parameters model SEMANP) took semantic parameters as input nodes, as presented in detail below.    The Diagnostics Model (DIAG): Binary ac-ceptability values of the phrases or sentences formed by the syntactic diagnostics constituted the input nodes for the network (see above for the SI diagnostics). Each syntactic diagnostic provided a binary value (either 0 or 1) to one of the input nodes.	[[340, 344], [128, 132], [235, 241]]	[[321, 338], [110, 120], [209, 234]]	['DIAG', 'DIAG', 'SEMANP']	['Diagnostics Model', 'diagnostic', 'semantic parameters model']
214	sociations to a number of affect categories including the six Ekman emotions (joy, sadness, anger, fear, disgust, and surprise).3 General Inquirer (GI) (Stone et al, 1966) has 11,788 words labeled with 182 cat-	[[148, 150]]	[[130, 146]]	['GI']	['General Inquirer']
215	An alternative to QE is to perform the expansion in the document. Document Expansion (DE) was first proposed in the speech retrieval commu-	[[86, 88], [18, 20]]	[[66, 84]]	['DE', 'QE']	['Document Expansion']
216	For the discourse  structure analysis, we suggest a statistical model  with discourse segment boundaries (DSBs)  similar to the idea of gaps suggested for a 	[[106, 110]]	[[76, 104]]	['DSBs']	['discourse segment boundaries']
217	  SVM Classification  SVM (Support Vector Machines) has attracted  much attention since it was introduced in (Boser et 	[[22, 25], [2, 5]]	[[27, 50]]	['SVM', 'SVM']	['Support Vector Machines']
218	 1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two tex-	[[37, 39]]	[[16, 35]]	['RE']	['Relation extraction']
219	representation and transformation are necessary.  (2)  Latent Semantic Indexing (LSI) without  transformation (LSI-Com): we first merge the 	[[81, 84], [111, 118]]	[[55, 79]]	['LSI', 'LSI-Com']	['Latent Semantic Indexing']
220	The processing of BADGER is not significantly different than that of the CIRCUS system used i n previous MUC evaluations [4, 5, 6] . Concept node (CN) definitions are still used to create case frame instantiation s and multiple CN definitions can apply to the same text fragment .	[[147, 149], [105, 108], [228, 230]]	[[133, 145]]	['CN', 'MUC', 'CN']	['Concept node']
221	  Secondly, as for the word order of prepositional  phrases (PP), Arabic and English are similar in  that PPs generally appear at the end of the sen-	[[61, 63], [106, 109]]	[[52, 59]]	['PP', 'PPs']	['phrases']
222	Next subsections show the results obtained by TIPSem system in each one of the TempEval-2 tasks for English (EN) and Spanish (ES). More-	[[109, 111], [46, 52], [79, 89], [126, 128]]	[[100, 107], [117, 124]]	['EN', 'TIPSem', 'TempEval-2', 'ES']	['English', 'Spanish']
223	ambiguation as a binary classification problem. Li  then uses Support Vector Machine (SVM) with  mutual information between each Chinese charac-	[[86, 89]]	[[62, 84]]	['SVM']	['Support Vector Machine']
224	inforced by the proposed method. In this method, the decision list (DL) learning algorithm (Yarowsky, 1995) is used.	[[68, 70]]	[[53, 66]]	['DL']	['decision list']
225	185  Table h Size of the corpora of HTML pages (in Mb) collected on the four patterns (1.a-d)  through AltaVista (AV) and Northern Light (NL). 	[[114, 116], [138, 140], [36, 40], [51, 53]]	[[103, 112], [122, 136]]	['AV', 'NL', 'HTML', 'Mb']	['AltaVista', 'Northern Light']
226	the second sentence provides some further description of that entity. An Entity Relation (EntRel) was annotated for such sentence pairs as below.	[[90, 96]]	[[73, 88]]	['EntRel']	['Entity Relation']
227	rank verb pairs with respect to the strength of their association with a particular discourse relation. We adapted versions of standard lexical association measures like PMI (pointwise mutual information) and their variants, as well as some measures specific to the association of a causal relation between items (Do	[[170, 173]]	[[175, 203]]	['PMI']	['pointwise mutual information']
228	spect to the ISSC. We will refer to this expanded version of the SSC as the processed SSC (PSSC). 	[[91, 95], [13, 17], [65, 68]]	[[76, 89]]	['PSSC', 'ISSC', 'SSC']	['processed SSC']
229	tried two types of expansion, one mainly using synonyms (SYN), and one mainly using hypernyms or related links (LNK). 	[[112, 115], [57, 60]]	[[105, 110], [47, 55]]	['LNK', 'SYN']	['links', 'synonyms']
230	 2003. Voice extensible markup language (VoiceXML) version 2.0.	[[41, 49]]	[[7, 39]]	['VoiceXML']	['Voice extensible markup language']
231	 5 Conclusions Multiword expressions (MWEs) are a major obstacle that hinder precise natural language processing	[[38, 42]]	[[15, 36]]	['MWEs']	['Multiword expressions']
232	 1 Introduction Natural Language Inference (NLI), i.e. the task of determining whether an NL hypothesis can be in-	[[44, 47], [90, 92]]	[[16, 42]]	['NLI', 'NL']	['Natural Language Inference']
233	SaRAD .891 .919 .905 ALICE .961 .920 .940 Chang & Sch?utze (CS) .942 .900 .921 Nadeau & Turney (NT) .954 .871 .910	[[60, 62], [0, 5], [96, 98]]	[[42, 58], [79, 94]]	['CS', 'SaRAD', 'NT']	['Chang & Sch?utze', 'Nadeau & Turney']
234	coding-related concepts that appear in the EHR.  We use General Equivalence Mappings (GEMs) between ICD-9 and ICD-10 codes (CMS, 2014)	[[86, 90], [43, 46], [100, 105], [110, 116], [124, 127]]	[[56, 84]]	['GEMs', 'EHR', 'ICD-9', 'ICD-10', 'CMS']	['General Equivalence Mappings']
235	LR = Lagrangian relaxation; DP = exhaustive dynamic programming; ILP = integer linear programming; LP = linear programming (LP does not recover an exact solution).	[[65, 68], [0, 2], [28, 30], [99, 101], [124, 126]]	[[71, 97], [5, 26], [44, 63], [104, 122]]	['ILP', 'LR', 'DP', 'LP', 'LP']	['integer linear programming', 'Lagrangian relaxation', 'dynamic programming', 'linear programming']
236	  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 928?937, October 25-29, 2014, Doha, Qatar.	[[90, 95]]	[[40, 88]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
237	the bird comes.  Table 2: Stories generated by a system that uses plots and genetic search (PlotGA), a system that uses only plots (Plot), McIntyre and Lapata (2009)?s rank-based system (Rank) and a system that randomly	[[92, 98]]	[[66, 90]]	['PlotGA']	['plots and genetic search']
238	fcn@dsic.upv.es Abstract State-of-the-art Machine Translation (MT) systems are still far from being perfect.	[[63, 65]]	[[42, 61]]	['MT']	['Machine Translation']
239	3 Results  The experiments were completed using the revised  RTE3 development set (RTE3Devmt) before the  RTE3Test results were released.	[[83, 92], [106, 114]]	[[61, 81]]	['RTE3Devmt', 'RTE3Test']	['RTE3 development set']
240	 In this paper, we propose a new, structured vector space model for word meaning (SVS) that addresses these problems.	[[82, 85]]	[[34, 57]]	['SVS']	['structured vector space']
241	 6 Conclusion In this work, we manually constructed a Phased Predicate Template Taxonomy (PPTT), which is a network of semantically coherent classes of templates and derived semantic relations including entailment	[[90, 94]]	[[54, 88]]	['PPTT']	['Phased Predicate Template Taxonomy']
242	1997). Theory retinement is mainly used (and has its  origin) in Knowledge Based Systems (KBS) (Craw  and Sleeman, 1990).	[[90, 93]]	[[65, 88]]	['KBS']	['Knowledge Based Systems']
243	User Group (WON).4 Research and development  is carried out in close collaboration with user  groups and intellectual property (IP) professionals to ensure solutions and software are delivered 	[[128, 130], [12, 15]]	[[105, 126]]	['IP', 'WON']	['intellectual property']
244	  Definition: Asymmetric nearest common  ancestor (ANCA)  The asymmetric nearest common ancestors from 	[[51, 55]]	[[14, 49]]	['ANCA']	['Asymmetric nearest common  ancestor']
245	 Disco-En-Gold consists of 349 expressions divided into training (TrainD), validation (ValD), and test data (TestD) manually assigned scores from 0	[[87, 91], [109, 114], [66, 72]]	[[75, 85], [98, 107], [43, 64]]	['ValD', 'TestD', 'TrainD']	['validation', 'test data', 'divided into training']
246	labels.5 We simulate a user?s constraints by ranking words in the training split by their information gain (IG).6 After ranking the top 200 words for each class	[[108, 110]]	[[90, 106]]	['IG']	['information gain']
247	heres to the dimensions presented in Table 2, and negation scopes are modeled using a first order linear-chain conditional random field (CRF)2, with a label set of size two indicating whether a	[[137, 140]]	[[111, 135]]	['CRF']	['conditional random field']
248	Snow follows that of (Escudero et al, 2000c).  2.4 LazyBoost ing  (LB)  The main idea of boosting algorithms is to 	[[67, 69]]	[[51, 60]]	['LB']	['LazyBoost']
249	egie Group, Inc. (CGI) of Pittsburgh, PA is to promote  and further develop automatic Text Summarization  using a Maximal Marginal Relevance (MMR) metric to  generate summaries of documents hat are directly rele- 	[[142, 145], [18, 21], [38, 40]]	[[114, 140]]	['MMR', 'CGI', 'PA']	['Maximal Marginal Relevance']
250	of units. '  i Note that unlike RST, Veins Theory (VT) is not  concerned with the type of relations which hold 	[[51, 53], [32, 35]]	[[37, 49]]	['VT', 'RST']	['Veins Theory']
251	There are two other functional tags which, unlike those listed above, can also be associated with numbered arguments in the frames files. The first one, EXT (extent), indicates that a constituent is a numerical argument on its verb, as in climbed 15%	[[153, 156]]	[[158, 164]]	['EXT']	['extent']
252	176 Figure 2: The embedding graph for the sentence Our previous results show that recombinant gp41 (aa565-647), the extracellular domain of HIV-1 transmembrane glycoprotein, stimulates interleukin-10 (IL-10) production in human monocytes.	[[201, 206], [140, 145]]	[[185, 199]]	['IL-10', 'HIV-1']	['interleukin-10']
253	Arabic Penn TreeBank POS tagset. Base Phrase (BP) Chunking is the process of creating non-recursive base phrases such	[[46, 48], [21, 24]]	[[33, 44]]	['BP', 'POS']	['Base Phrase']
254	It includes the four original partners  of the LATER project and the following new partners: University of Amsterdam (UvA) in the Netherlands, Free University of Bolzano-Bozen (FUB) 	[[118, 121], [47, 52], [177, 180]]	[[93, 116], [143, 175]]	['UvA', 'LATER', 'FUB']	['University of Amsterdam', 'Free University of Bolzano-Bozen']
255	The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF)	[[69, 75], [112, 115]]	[[52, 67], [85, 110]]	['MaxEnt', 'CRF']	['maximum entropy', 'conditional random fields']
256	ity? have been designed so far for French L1 and only one for French as a foreign language (FFL) (see Section 2).	[[92, 95], [42, 44]]	[[62, 90]]	['FFL', 'L1']	['French as a foreign language']
257	/fi:/. 2.1 Foreign Words From an information retrieval (IR) perspective, foreign words in Arabic can be classified into two gen-	[[56, 58]]	[[33, 54]]	['IR']	['information retrieval']
258	by the name of its author or the author?s place of  work.  In computational linguistic (CL) terms thi s  exercise relies on proper noun extra ction.	[[88, 90]]	[[62, 86]]	['CL']	['computational linguistic']
259	found in each row, as well as the level of granularity of analysis in each row.3 2KEY: ABS=abstract, COM=completive, CL=classifier, DEM=demonstrative, E=ergative, EV=evidential, S=singular,	[[87, 90], [101, 104], [117, 119]]	[[91, 99], [105, 115], [120, 130]]	['ABS', 'COM', 'CL']	['abstract', 'completive', 'classifier']
260	them, and finally generating and displaying them.  The Input Analyzer (IA) of the system is the  most stable end experimented component and it is 	[[71, 73]]	[[55, 69]]	['IA']	['Input Analyzer']
261	Abstract  This paper describes a reestimation method for stochastic language models uch as the  N-gram model and the Hidden Maxkov Model(HMM) from ambiguous observations. It is 	[[137, 140]]	[[117, 135]]	['HMM']	['Hidden Maxkov Mode']
262	 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al.,	[[37, 39]]	[[16, 35]]	['MT']	['Machine Translation']
263	We first describe how to generate multiple FDTs for each sentence pair in training corpus C based on the forced decoding (FD) technique, which performs via the following four steps:	[[122, 124], [43, 47]]	[[105, 120], [83, 89]]	['FD', 'FDTs']	['forced decoding', 'corpus']
264	 Figure 5 also gives a speed comparison of our method to a linear programming (LP) solver that solves the LP relaxation defined by constraints D0?	[[79, 81]]	[[59, 77]]	['LP']	['linear programming']
265	Lowe HJ, Barnett GO. ( 1994) Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches.	[[83, 87], [5, 7]]	[[57, 81]]	['MeSH', 'HJ']	['medical subject headings']
266	The RASP toolkit (Briscoe et al, 2006) is used for sentence boundary detection, tokenisation, PoS tagging and finding grammatical relations (GR) between words in the text.	[[141, 143], [4, 8], [94, 97]]	[[118, 139]]	['GR', 'RASP', 'PoS']	['grammatical relations']
267	Common error measures are the Word Error Rate (WER) and the Position Independent Word Error Rate (PER) as well as evaluation metric on the n-gram level like the BLEU and	[[98, 101], [47, 50], [161, 165]]	[[60, 96], [30, 45]]	['PER', 'WER', 'BLEU']	['Position Independent Word Error Rate', 'Word Error Rate']
268	Discovery of ambiguous and unambiguous discourse connectives via annotation projection. In Proceedings of Workshop on Annotation and Exploitation of Parallel Corpora (AEPC), pages 83?82, Tartu, Estonia.	[[167, 171]]	[[118, 165]]	['AEPC']	['Annotation and Exploitation of Parallel Corpora']
269	 2. Character Error Rate (CER): Edit distance in terms of characters between the target sentence	[[26, 29]]	[[4, 24]]	['CER']	['Character Error Rate']
270	show how to construct MWE-aware training resources for them.  The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the French Treebank (FTB) (Abeill?,	[[128, 131], [22, 31], [179, 182]]	[[111, 126], [162, 177]]	['ATB', 'MWE-aware', 'FTB']	['Arabic Treebank', 'French Treebank']
271	corpus (DUC2002) and the second is automatically extracted from related web news stories (WNS) automatically extracted.	[[90, 93], [8, 15]]	[[72, 88]]	['WNS', 'DUC2002']	['web news stories']
272	Markov Model (HHMM) word), all the corresponding TYP candidates triggered by categorized word features(CWF) should be removed.	[[103, 106], [14, 18], [49, 52]]	[[77, 101], [0, 12]]	['CWF', 'HHMM', 'TYP']	['categorized word feature', 'Markov Model']
273	concept, BLESS contains several relata,  connected to it through one relation, such as cohyponymy (COORD), hypernymy (HYPER),  meronymy (MERO) or no-relation (RANDOM-N).2 	[[118, 123]]	[[107, 116]]	['HYPER']	['hypernymy']
274	participant Japanese systems were developed in a four-  month period of time and output results comparable to the Message Understanding Conference-6 (MUC-6) \[1\]  English language systems with F-Measures between 70 - 	[[150, 155]]	[[114, 148]]	['MUC-6']	['Message Understanding Conference-6']
275	known as NP The corpus of English Wikipedia pages, known as EnWiki NP ( * NP) Hidden Markov Model (HMM) is used to solve ...	[[99, 102], [74, 76], [67, 69], [9, 11]]	[[78, 97]]	['HMM', 'NP', 'NP', 'NP']	['Hidden Markov Model']
276	forward and backward application (FA and BA), implies a  standard notion of constituency, rules like type raising (TR)  and functional composition (FC) give rise to a more  generous notion of constituency (this is what makes 'non- 	[[148, 150]]	[[124, 146]]	['FC']	['functional composition']
277	Revue des Sciences de l?Education (RSE) ? Traduction, Terminologie et R?daction (TTR) ?	[[81, 84], [35, 38]]	[[42, 79], [0, 33]]	['TTR', 'RSE']	['Traduction, Terminologie et R?daction', 'Revue des Sciences de l?Education']
278	URL?, excluded utterances with the symbols that indicate the re-posting (RT) or quoting (QT) of others?	[[89, 91], [0, 3], [73, 75]]	[[80, 87], [61, 71]]	['QT', 'URL', 'RT']	['quoting', 're-posting']
279	On the one hand, we built machine learning classifiers based on Support Vector Machines (SVMs) and Conditional Random Fields (CRFs).	[[89, 93], [126, 130]]	[[64, 87], [99, 124]]	['SVMs', 'CRFs']	['Support Vector Machines', 'Conditional Random Fields']
280	3.2 Structural model We go beyond traditional feature vectors by employing structural models (STRUCT), which encode each comment into a shallow syntactic tree.	[[94, 100]]	[[75, 85]]	['STRUCT']	['structural']
281	4.1 Syntactic template selection PERSONAGE?s input generation dictionary is made of 27 Deep Syntactic Structures (DSyntS): 9 for the recommendation claim, 12 for the comparison	[[114, 120]]	[[87, 112]]	['DSyntS']	['Deep Syntactic Structures']
282	Although later systems such as Wu and Ng (2013); Rozovskaya and Roth (2013) use Integer Linear Programming (ILP) to decode a global optimized result, the input scores	[[108, 111]]	[[80, 106]]	['ILP']	['Integer Linear Programming']
283	ture. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). 	[[91, 95], [13, 16]]	[[75, 89]]	['SAVG', 'AVG']	['stochastic AVG']
284	2 values (ARG2-4, ARGM-DIS (discourse), ARGM-LOC (locative), ARGM-MNR (manner), and ARGM-TMP (temporal)), but given the large number of degrees of free-	[[84, 92], [40, 48], [61, 69]]	[[94, 102], [50, 58], [71, 77]]	['ARGM-TMP', 'ARGM-LOC', 'ARGM-MNR']	['temporal', 'locative', 'manner']
285	 As far as discriminative models are concerned,  the Maximum Entropy (MaxEnt) model has been  applied (Bohus and Rudnicky, 2006).	[[70, 76]]	[[53, 68]]	['MaxEnt']	['Maximum Entropy']
286	2003; Shen et al, 2006; Wubben et al, 2009) The first manually collected paraphrase corpus is the Microsoft Research Paraphrase (MSRP) Corpus (Dolan et al, 2004), consisting of 5,801 sentence	[[129, 133]]	[[98, 127]]	['MSRP']	['Microsoft Research Paraphrase']
287	chanical Turk service. Two classifiers, Na??ve Bayes (NB) and a support vector machine (SVM), were applied on the tokenized and stemmed state-	[[88, 91], [54, 56]]	[[64, 86], [40, 52]]	['SVM', 'NB']	['support vector machine', 'Na??ve Bayes']
288	puterization, Technical Report 6-CICC-MT55 (1995)  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 174?182, Boulder, Colorado, June 2009.	[[136, 141], [33, 42]]	[[95, 134]]	['CoNLL', 'CICC-MT55']	['Computational Natural Language Learning']
289	2010). Domain event extraction has been advanced in particular by the BioNLP Shared Task (ST) events (Kim et al, 2011a; Kim et al, 2011b), which have	[[90, 92], [70, 76]]	[[77, 88]]	['ST', 'BioNLP']	['Shared Task']
290	The results can be seen at:  http:/ /c lwww.essex.ac.uk/w3c/.  The project was  funded by JISC (the Joint Information Systems Com-  mittee of the UK Higher Education Funding Coun- 	[[90, 94], [146, 148]]	[[100, 138]]	['JISC', 'UK']	['Joint Information Systems Com-  mittee']
291	sisting of analyses automatically created by systems participating in the recent BioNLP Shared Task (ST) 2011. In providing for the	[[101, 103], [81, 87]]	[[88, 99]]	['ST', 'BioNLP']	['Shared Task']
292	 First of all, we now first formally introduce DUDES: Definition 1 (DUDES) A DUDES is a 7-tuple (m, l, t, U,A, S,C) consisting of	[[68, 73], [77, 82]]	[[54, 66]]	['DUDES', 'DUDES']	['Definition 1']
293	ferently by (1) and (2)8.  5 The Neutral Edge Direction (NED) Measure	[[57, 60]]	[[33, 55]]	['NED']	['Neutral Edge Direction']
294	Abstract   This paper describes two algorithms which construct two differ-  ent types of generators for lexical functional grammars (LFGs). The 	[[133, 137]]	[[104, 131]]	['LFGs']	['lexical functional grammars']
295	4 GTS with the Idea of Coordination Problem Game GTS (Game Theoretic Semantics) has been developed as an alternative semantics where major se-	[[49, 52], [2, 5]]	[[54, 78]]	['GTS', 'GTS']	['Game Theoretic Semantics']
296	 As stated earlier, our unlabeled data consists of email (EMAIL) and online forum (FORUM) data.	[[58, 63], [83, 88]]	[[51, 56], [76, 81]]	['EMAIL', 'FORUM']	['email', 'forum']
297	Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task.	[[74, 77]]	[[58, 72]]	['LDA']	['let Allocation']
298	services (person or company information bureau), ht this  situation, the event aml the attitude of file inforumtion  possessor (IP) is transported to a speaker (SP);journalist. 	[[161, 163], [128, 130]]	[[152, 159], [104, 126]]	['SP', 'IP']	['speaker', 'inforumtion  possessor']
299	more, 1976; Fillmore, 1982). The classes of events are the semantic frames of the lexical units (LUs) that evoke them, and the roles associated with the	[[97, 100]]	[[82, 95]]	['LUs']	['lexical units']
300	LDEP(NEXT) + Table 1: History-based features (TOP = token on top of stack; NEXT = next token in input buffer; HEAD(w) = head of w; LDEP(w) = leftmost depen-	[[75, 79], [0, 4], [46, 49], [5, 9], [110, 117], [131, 138]]	[[82, 92], [52, 64], [120, 129], [141, 155]]	['NEXT', 'LDEP', 'TOP', 'NEXT', 'HEAD(w)', 'LDEP(w)']	['next token', 'token on top', 'head of w', 'leftmost depen']
301	 3.2 Query by Committee  Query by Committee (QBC) was introduced by  Seung, Opper, and Sompolinsky (1992).	[[45, 48]]	[[25, 43]]	['QBC']	['Query by Committee']
302	egories (left side) and with respect to the assessor?s own expertise (right side). ( Key: B=beneficial, LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE) the assessor had no idea from which condition an	[[150, 152], [104, 106]]	[[153, 172], [92, 102], [107, 124], [128, 137], [141, 148], [176, 183], [187, 190]]	['UB', 'LB']	['unlikely beneficial', 'beneficial', 'likely beneficial', 'tradeoffs', 'unknown', 'harmful', 'not']
303	Table 1: The acoustic features which are extracted from the audio clips using Praat (Boersma and Weenink, 2010).  (MFCCs)1 and Teager Energy Operator (TEO)2 (Kaiser, 1990) based features have also been con-	[[151, 154], [115, 120]]	[[127, 149]]	['TEO', 'MFCCs']	['Teager Energy Operator']
304	to facilitate understanding. We used latent Dirichlet alocation (LDA) (Blei, Ng, and Jordan, 2003) as our exploratory tool.	[[65, 68]]	[[37, 63]]	['LDA']	['latent Dirichlet alocation']
305	In this shared task we test the feasibility of eliciting parallel data for Machine Translation (MT) using Mechanical Turk (MTurk). MT poses an interesting	[[123, 128], [96, 98], [131, 133]]	[[106, 121], [75, 94]]	['MTurk', 'MT', 'MT']	['Mechanical Turk', 'Machine Translation']
306	ways available. We therefore generate the training data using a na??ve phrase aligner (NPA) instead of resorting to a real one.	[[87, 90]]	[[64, 85]]	['NPA']	['na??ve phrase aligner']
307	various respects. While CKY requires a grammar in Chomsky Normal Form (CNF), LSCP takes an ordinary PG grammar, since no equivalent of the	[[71, 74], [24, 27], [77, 81], [100, 102]]	[[50, 69]]	['CNF', 'CKY', 'LSCP', 'PG']	['Chomsky Normal Form']
308	counted through specifier phrases. A Malay example, where biji is the count classifier (CL) for fruit, is given in (1).	[[88, 90]]	[[76, 86]]	['CL']	['classifier']
309	 1 Introduction  Open domain question answering (QA), as defined  by the TREC competitions (Voorhees, 2003), 	[[49, 51], [73, 77]]	[[29, 47]]	['QA', 'TREC']	['question answering']
310	Hidden topic markov models. Artificial Intelligence and Statistics (AISTATS). 	[[68, 75]]	[[28, 66]]	['AISTATS']	['Artificial Intelligence and Statistics']
311	On the other hand, multi-lingual  ontology is very important for natural language  processing, such as machine translation (MT), web  mining (Oyama et al 2004) and cross language 	[[124, 126]]	[[103, 122]]	['MT']	['machine translation']
312	User-generated content (UGC), and specially the microblog genre, has become an interesting resource for Natural Language Processing (NLP) tools and applications.	[[133, 136], [24, 27]]	[[104, 131], [0, 22]]	['NLP', 'UGC']	['Natural Language Processing', 'User-generated content']
313	a major Department of Agriculture system, due to inadequate agency planning.  Complete sets of the Federal mfomtion processing staodards (FIPS) are now avail-  able from the National Bureau of Standards at $46.00 each.	[[138, 142]]	[[99, 136]]	['FIPS']	['Federal mfomtion processing staodards']
314	They show improvements of up to 5.3% on two real tasks: pitch accent prediction and optical character recognition (OCR). 	[[115, 118]]	[[84, 113]]	['OCR']	['optical character recognition']
315	the best among all other symmetrization heuristics. The other was a Tree Edit Distance (TED) model, popularly used in a series of NLP appli-	[[88, 91], [130, 133]]	[[68, 86]]	['TED', 'NLP']	['Tree Edit Distance']
316	In International Conference on Autonomous Agents and Multiagent Systems (AAMAS). 	[[73, 78]]	[[31, 71]]	['AAMAS']	['Autonomous Agents and Multiagent Systems']
317	features to argument classification models, but also  represent full parsing information as constraints in  integer linear programs (ILP) to resolve label inconsistencies.	[[133, 136]]	[[108, 131]]	['ILP']	['integer linear programs']
318	as defined in Section 2. For tagging, we experimented with Support Vector Machines (SVM) and Conditional Random Fields (CRF).	[[84, 87], [120, 123]]	[[59, 82], [93, 118]]	['SVM', 'CRF']	['Support Vector Machines', 'Conditional Random Fields']
319	 2. All the named entities(NE) in the question are extracted as NE set.	[[27, 29], [64, 66]]	[[12, 25]]	['NE', 'NE']	['named entitie']
320	 1 Introduction  Statistical Machine Translation (SMT) is attracting more attentions than rule-based and example-	[[50, 53]]	[[17, 48]]	['SMT']	['Statistical Machine Translation']
321	 In Proceedings of the 24th International Conference on Computational Linguistics (COLING). 	[[83, 89]]	[[56, 81]]	['COLING']	['Computational Linguistics']
322	eling the sequential nature of the output. The constraint satisfaction inference (CSInf) approach (Bosch and Canisius, 2006) improves the	[[82, 87]]	[[47, 80]]	['CSInf']	['constraint satisfaction inference']
323	The full TBCNN-pair model outperforms all existing sentence encoding-based approaches, including a 1024d gated recurrent unit (GRU)-based RNN with ?	[[127, 130], [9, 14], [138, 141]]	[[105, 125]]	['GRU', 'TBCNN', 'RNN']	['gated recurrent unit']
324	 3.2 Taboo Constraint Taboo constraint (TABOO) requires that the substitute word is a taboo word or frequently used	[[40, 45]]	[[22, 38]]	['TABOO']	['Taboo constraint']
325	Daniele Vannella, 2013). The two system types are WSI (Word Sense Induction) and WSD (Word Sense Disambiguation).	[[50, 53], [81, 84]]	[[55, 75], [86, 111]]	['WSI', 'WSD']	['Word Sense Induction', 'Word Sense Disambiguation']
326	 3.1 The Beta Process and the Bernoulli process The beta process(BP) (Thibaux and Jordan, 2007; Paisley and Carin, 2009) and the related Indian buf-	[[65, 67]]	[[52, 64]]	['BP']	['beta process']
327	 3 Keystroke Data Collection Amazon?s Mechanical Turk (MTurk) is a web service that enables crowdsourcing of tasks that are dif-	[[55, 60]]	[[38, 53]]	['MTurk']	['Mechanical Turk']
328	three modules: (1) a question processing (QP) module; (2) a passage retrieval (PR) module; and (3) an answer processing (AP) module. Questions	[[121, 123]]	[[102, 119]]	['AP']	['answer processing']
329	University of Southern California  Los Angeles, California 90089  Abstract Tiffs paper describes Kind Types (KT), a system which uses  commonsense knowledge to reason about natural anguage text.	[[109, 111]]	[[97, 107]]	['KT']	['Kind Types']
330	 Definition  Default Unification (first version) AU!B = A ~ U B, where A ~ is the maximal (i.e. most  specific) element in the subsumption ordering such that A' r- A and A ~ U B is defined.	[[49, 53]]	[[56, 63]]	['AU!B']	['A ~ U B']
331	vp?np?pp .83 .80 .83 .80 .83 .80 vp?pp?pp .75 .74 .75 .74 .75 .74 Lexical association (LAsim) Sequences PrEC PrPGR RecEC RecPGR F-SRV F-SPGR	[[87, 92]]	[[66, 85]]	['LAsim']	['Lexical association']
332	Two document collections are used in this study.  CE (Chinese Encyclopedia): This is from the electronic version of the Chinese Encyclopedia.	[[50, 52]]	[[54, 74]]	['CE']	['Chinese Encyclopedia']
333	Among the NEs we select six of them as the recognized objects, that is, personal name (PN), date or time (DT), location name (LN), team name (TN), competition title (CT) and per-	[[106, 108], [126, 128]]	[[92, 104], [111, 124]]	['DT', 'LN']	['date or time', 'location name']
334	number, fifteen predefined verbs (Paul, 2010)  are chosen as Light Verbs (LVs) for framing  the compound verbs (CompVs).  A manually 	[[112, 118], [74, 77]]	[[96, 110], [61, 72]]	['CompVs', 'LVs']	['compound verbs', 'Light Verbs']
335	Syntactic Tree  Ambiguous Semantic  Expression (EFL)  Unambiguous Semantic 	[[48, 51]]	[[36, 46]]	['EFL']	['Expression']
336	Based on these two conditions we have come up with a linear combination of two cost components, similar to Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998).	[[135, 138]]	[[107, 133]]	['MMR']	['Maximal Marginal Relevance']
337	 207  Informational Contribution (IC) function for each element  in a pair.	[[34, 36]]	[[6, 32]]	['IC']	['Informational Contribution']
338	Concerned about this inflation of the grammar constant, (DeNero et al, 2009) consider a superset of CNF called Lexical Normal Form (LNF). A rule is	[[132, 135], [100, 103]]	[[111, 130]]	['LNF', 'CNF']	['Lexical Normal Form']
339	negative has an incorrect, but plausible, stress pattern, u. We adopt a Support Vector Machine (SVM) solution to these ranking constraints as described by	[[96, 99]]	[[72, 94]]	['SVM']	['Support Vector Machine']
340	1540  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 7?14, Gothenburg, Sweden, April 27, 2014.	[[75, 80], [84, 88]]	[[41, 73]]	['HyTra', 'EACL']	['Hybrid Approaches to Translation']
341	 Other valuable categories are, for example,  pronominal adverbs (PAV) and infinitives of auxil-  iary verbs (VAINF), where the difference between 	[[66, 69], [110, 115]]	[[46, 64]]	['PAV', 'VAINF']	['pronominal adverbs']
342	well. The formal framework for analysis will be the Discourse Representation Theory (DRT). 	[[85, 88]]	[[52, 83]]	['DRT']	['Discourse Representation Theory']
343	MusicalArtist:album or Book:genre. Therefore, in addition to recognising entities with Stanford NERC, we also implement our own named entity recogniser (NER), which only recognises entity boundaries, but does not classify them.	[[153, 156], [96, 100]]	[[128, 151]]	['NER', 'NERC']	['named entity recogniser']
344	ID full papers domain (TCS) 10 BB web pages domain (BB) 2 BI abstracts domain (BS) 10 Table 1: Characteristics of BioNLP-ST 2011 main tasks.	[[79, 81], [0, 2], [23, 26], [31, 33], [52, 54], [114, 123]]	[[58, 70]]	['BS', 'ID', 'TCS', 'BB', 'BB', 'BioNLP-ST']	['BI abstracts']
345	Semantic Interpretation of Prepositions for NLP Applications Sven Hartrumpf Hermann Helbig Rainer Osswald Intelligent Information and Communication Systems (IICS) University of Hagen (FernUniversita?t in Hagen)	[[157, 161], [44, 47]]	[[106, 155]]	['IICS', 'NLP']	['Intelligent Information and Communication Systems']
346	Abbreviations POS = Part of Speech NE = Named Entity CE = Correlated Entity	[[35, 37]]	[[40, 52]]	['NE']	['Named Entity']
347	  Abstract  The Colorado Literacy Tutor (CLT) is a  technology-based literacy program, designed 	[[41, 44]]	[[16, 39]]	['CLT']	['Colorado Literacy Tutor']
348	restriction to do top-down filtering.  1986 Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood.	[[117, 120]]	[[102, 115]]	['EST']	['Establishment']
349	tions.  3.1 Simple Classification (SC) Given an expression x consisting of n words x1,	[[35, 37]]	[[12, 33]]	['SC']	['Simple Classification']
350	tributed to idiosyncrasies in the translation. For instance, Emma (EM) seems very difficult to align, which can be attributed to the use of an old transla-	[[67, 69]]	[[61, 65]]	['EM']	['Emma']
351	example, we have defined that all the subclasses of #COMMUNICATION-EVENTS (e.g.  #REPORT#,  #CONFIRM#, etc.) map their sentential complements (SENT-COMP)  to THEME, as shown below.	[[143, 152], [158, 163]]	[[119, 141]]	['SENT-COMP', 'THEME']	['sentential complements']
352	presidential nomination to seek the backing of the {{w|Libertarian Party (United States)|Libertarian Party}} (LP). 	[[110, 112]]	[[89, 107]]	['LP']	['Libertarian Party}']
353	Wiebe, 2000).  4.1.4 Strong Polar Expressions (STROPO) Instead of adding intensifiers in order to put more	[[47, 53]]	[[21, 45]]	['STROPO']	['Strong Polar Expressions']
354	To tackle this  problem, we propose to employ the  Support Vector Machines(SVM) in  determining the grammatical functions.	[[75, 78]]	[[51, 73]]	['SVM']	['Support Vector Machine']
355	obtain a bilingual database. The database is  called the ATR Dialogue Database(ADD). 	[[79, 82]]	[[57, 77]]	['ADD']	['ATR Dialogue Databas']
356	(2) works7. This algorithm is evaluated in (Jin and Tanaka-Ishii, 2006) using Peking University (PKU) 7Three segmentation criteria are given in (Jin and Tanaka-	[[97, 100]]	[[78, 95]]	['PKU']	['Peking University']
357	processing beyond keyword indexing, typically supported by Natural Language Processing (NLP)  and Information Extraction (IE) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, Li 	[[122, 124], [88, 91]]	[[98, 120], [59, 86]]	['IE', 'NLP']	['Information Extraction', 'Natural Language Processing']
358	In Table 1, we show the precision, recall, and F1 measures of our domain-aware method (DOM) and the baseline method (BL) in all three sets of experiments.	[[117, 119], [87, 90]]	[[100, 108], [66, 85]]	['BL', 'DOM']	['baseline', 'domain-aware method']
359	R6mi Zajac Inheritance and Constraint-Based Grammar Formalisms  13th International Conference on  Computational Linguistics (COLING'90). 	[[125, 134]]	[[98, 123]]	"[""COLING'90""]"	['Computational Linguistics']
360	TC 59% 59% 79% 85% BTC 86% 86% 86% 92% Table 1: Task Completion (TC) and Binary Task Completion (BTC) prediction results, using auto-	[[65, 67], [0, 2], [19, 22], [97, 100]]	[[48, 63], [73, 95]]	['TC', 'TC', 'BTC', 'BTC']	['Task Completion', 'Binary Task Completion']
361	"step, we apply the CFG IDENTIFICATION to the string  under (2) in order to ""transform"" the sequence of simple  syntactic units into so-called Segmentation Units (SU)  \[we use the following conventions: ""( )"" for facultativi- "	[[162, 164], [19, 22]]	[[142, 160]]	['SU', 'CFG']	['Segmentation Units']
362	in the preceding verb phrase (PV), the head noun in the preceding noun phrase (PN) and the head noun in the following noun phrase (FN) when available (Chodorow et al, 2007).	[[131, 133], [30, 32], [79, 81]]	[[108, 122], [17, 28], [56, 77]]	['FN', 'PV', 'PN']	['following noun', 'verb phrase', 'preceding noun phrase']
363	 In unvocalised text, the standard written form of Modern Standard Arabic (MSA), it may happen that the stem and the root of a word form are one and the	[[75, 78]]	[[51, 73]]	['MSA']	['Modern Standard Arabic']
364	 The test set (Table 13) consisted of the beginnings of three short stories by  Ernest  Hemingway,  15 three articles f rom the New York Times (NYT), 16 the first three chapters of  a novel  by  Uwe JohnsonS the first two chapters of a short story by Heiner Mfiller, TM 	[[144, 147], [267, 269]]	[[128, 142]]	['NYT', 'TM']	['New York Times']
365	1 3 Note that there is only one column for recall, which is unaffected by the choice o f Matched/Missing (M/M) versus All Templates (AT) . 	[[133, 135], [106, 109]]	[[118, 131], [89, 104]]	['AT', 'M/M']	['All Templates', 'Matched/Missing']
366	associated with each. The next column indicates the percentage of the majority class (MAJ.) and count	[[86, 90]]	[[70, 78]]	['MAJ.']	['majority']
367	4.2 Results Table 3 shows the results of our compression models by compression rate (CompR), dependencybased F1 (F1-Dep), and SRL-based F1 (F1-SRL).	[[85, 90], [126, 129], [140, 147]]	[[67, 83]]	['CompR', 'SRL', 'F1-SRL)']	['compression rate']
368	Orlando, Florida 32810  AFIPS CONSTITUENT SOCIETIES  Instrument Society of America (ISA)  Purpose 	[[84, 87], [24, 29]]	[[53, 82]]	['ISA', 'AFIPS']	['Instrument Society of America']
369	This paper describes the UNITOR system participating in both tasks of the *SEM 2013 shared task on Semantic Textual Similarity (STS), described in (Agirre et al 2013):	[[128, 131], [25, 31], [75, 78]]	[[99, 126]]	['STS', 'UNITOR', 'SEM']	['Semantic Textual Similarity']
370	 This representation treats clitics as separate tokens and abstracts the orthographic rewrites they undergo when cliticized. See the handling of the l/PREP+Al/DET in word #6 in Table 5.  This representation is used by the LDC in the Penn Arabic Treebank (PATB) (Maamouri  et al., 2004) and tools such as MADAMIRA (Pasha et al.,	[[255, 259], [222, 225], [149, 155], [156, 162], [304, 312]]	[[233, 253]]	['PATB', 'LDC', 'l/PREP', 'Al/DET', 'MADAMIRA']	['Penn Arabic Treebank']
371	Na?ve Bayes (De Sitter and Daelemans, 2003).  Maximum Entropy (ME) conditional models like  ME Markov models (McCallum et al, 2000) and 	[[63, 65], [92, 94]]	[[46, 61]]	['ME', 'ME']	['Maximum Entropy']
372	1 Scope and Prior Work We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems.	[[120, 123]]	[[88, 118]]	['GND']	['geographic name disambiguation']
373	the curve (AUC) from the trained detector on the heldout test set. Results are reported in terms of the mean and standard deviation (SD) of the AUC across all splits. 	[[133, 135], [11, 14], [144, 147]]	[[113, 131]]	['SD', 'AUC', 'AUC']	['standard deviation']
374	TIGER treebank. In Proceedings of the 1st Workshop on Treebanks and Linguistic Theories (TLT), pages 24?42. 	[[89, 92], [0, 5]]	[[54, 87]]	['TLT', 'TIGER']	['Treebanks and Linguistic Theories']
375	Since it is costly to compute the partition function over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model (Mnih and	[[118, 121]]	[[87, 116]]	['NCE']	['noise constrastive estimation']
376	Linear Program relaxation based on single-commodity flow. LP(M): Linear Program relaxation based on multi-commodity flow.	[[58, 60]]	[[65, 79]]	['LP']	['Linear Program']
377	  Figure 2.  Words Correct (WC) scores from Teachers  and Machine scoring at the reader level (n = 87).	[[28, 30]]	[[13, 26]]	['WC']	['Words Correct']
378	+ raising verb - non-raising verb KEYAGR (key agreement) ?	[[34, 40]]	[[42, 55]]	['KEYAGR']	['key agreement']
379	Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Transducer. 	[[125, 127], [35, 38], [57, 60], [86, 90]]	[[128, 143], [61, 84], [91, 123]]	['TT', 'SMT', 'FST', 'SCFG']	['Tree Transducer', 'Finite State Transducer', 'Synchronous Context-Free Grammar']
380	(section 24). Legend of models: ST=Split Tags; EC=enhanced connectivity. 	[[47, 49]]	[[50, 71]]	['EC']	['enhanced connectivity']
381	Because of a scarcity of such corpora, most work has used the International Corpus of Learner English (ICLEv2) (Granger et al 2009) for training and evaluation	[[103, 109]]	[[62, 101]]	['ICLEv2']	['International Corpus of Learner English']
382	teams employed vector-based linear classifiers of different types: Hacioglu et al (2004) and Park et al (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al (2004) used Voted Percep-	[[141, 144]]	[[116, 139]]	['SVM']	['Support Vector Machines']
383	 1 Introduction As Machine Translation (MT) systems become widely adopted both for gisting purposes and to	[[40, 42]]	[[19, 38]]	['MT']	['Machine Translation']
384	 ITSPOKE-WOZ is a semi-automatic version of ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), which is a speech-enhanced ver-	[[44, 51], [1, 12]]	[[53, 80]]	['ITSPOKE', 'ITSPOKE-WOZ']	['Intelligent Tutoring SPOKEn']
385	twelve NE tags. The NE tagged corpus has been  used to develop Named Entity Recognition (NER)  system in Bengali using pattern directed shallow 	[[89, 92], [7, 9], [20, 22]]	[[63, 87]]	['NER', 'NE', 'NE']	['Named Entity Recognition']
386	vided within scientific articles. In addition, image Regions of Interest (ROIs) are commonly referred to within the image caption.	[[74, 78]]	[[53, 72]]	['ROIs']	['Regions of Interest']
387	C5 Compound Analyser for Robust Morphological Analysis]. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.	[[85, 90]]	[[57, 83]]	['CTexT']	['Centre for Text Technology']
388	The majority of dependency-based features are constructed using the properties of edges and vertices along the shortest path (SP) of an entity pair. 	[[126, 128]]	[[111, 124]]	['SP']	['shortest path']
389	to the recent evaluations of domain-independent Q/A systems organized in the context of the Text REtrieval Conference (TREC)1. The TREC	[[119, 123], [48, 51], [131, 135]]	[[92, 117]]	['TREC', 'Q/A', 'TREC']	['Text REtrieval Conference']
390	not observable from the sentence.  The dynamic nature of named entities (NEs) makes it difficult to enumerate all of their evolv-	[[73, 76]]	[[57, 71]]	['NEs']	['named entities']
391	Table 4: Accuracies on a balanced test set (random baseline: 0.5). NB = NaiveBayes. JRip?2 = Jripper with ?	[[67, 69], [84, 88]]	[[72, 82], [93, 100]]	['NB', 'JRip']	['NaiveBayes', 'Jripper']
392	{csjxu, csluqin}@comp.polyu.edu.hk Abstract The Semantic Textual Similarity (STS) task aims to exam the degree of semantic	[[77, 80]]	[[48, 75]]	['STS']	['Semantic Textual Similarity']
393	3.1 Feature Structure To implement the twin model, we adopt the log linear or maximum entropy (MaxEnt) model (Berger et al, 1996) for its flexibility of combining diverse sources of	[[95, 101]]	[[78, 93]]	['MaxEnt']	['maximum entropy']
394	conditional models are computed directly from data.  In this study, we use a Maximum Entropy (MaxEnt) classifier to combine the decision characteristic fea-	[[94, 100]]	[[77, 92]]	['MaxEnt']	['Maximum Entropy']
395	 won out on F-measure while giza++ syllabized attained better alignment error rate (AER). Refer to Table 3 for	[[84, 87]]	[[62, 82]]	['AER']	['alignment error rate']
396	A Linear Support Vector Machine text classifier (Joachims, 1999) was trained on Web pages from the Open Directory Project (ODP)6. These pages ef-	[[123, 126]]	[[99, 121]]	['ODP']	['Open Directory Project']
397	Our objective is to study the effect of using bigram features against co?occurrences in first (PB) and second (SC) order context vectors while using relatively small amounts of training data per word.	[[111, 113], [95, 97]]	[[103, 109]]	['SC', 'PB']	['second']
398	ticPhrase, Predicate, Argument, ? the MILE Data Categories (MDC) which constitute the attributes and values to adorn	[[60, 63]]	[[38, 58]]	['MDC']	['MILE Data Categories']
399	From raw text extract the temporal entities (events and timexes), identify the pairs of temporal entities that have a temporal link (TLINK) and classify the temporal relation between them.	[[133, 138]]	[[118, 131]]	['TLINK']	['temporal link']
400	six different domains: Newswire (NW), Broadcast News (BN), Broadcast Conversations (BC), Webblog (WL), Usenet (UN), and Conversational Telephone Speech (CTS).	[[111, 113], [33, 35], [54, 56], [84, 86], [98, 100], [153, 156]]	[[103, 109], [23, 31], [38, 52], [59, 82], [89, 96], [120, 151]]	['UN', 'NW', 'BN', 'BC', 'WL', 'CTS']	['Usenet', 'Newswire', 'Broadcast News', 'Broadcast Conversations', 'Webblog', 'Conversational Telephone Speech']
401	 Within this framework, in this paper we describe our attempt to bridge Semantic Role Labeling (SRL) and CE by modeling proposition-level semantics for	[[96, 99], [105, 107]]	[[72, 94]]	['SRL', 'CE']	['Semantic Role Labeling']
402	perimented with three classifiers available in R?  logistic regression (LogR), decision tree (DTree) and support vector machines (SVM).	[[72, 76], [94, 99], [130, 133]]	[[51, 70], [79, 92], [105, 127]]	['LogR', 'DTree', 'SVM']	['logistic regression', 'decision tree', 'support vector machine']
403	5.1 Candidate Generation We generate a list of candidate antecedents by first extracting all VPs and ADJPs (and all contiguous combinations of their constituents) from the current	[[101, 106], [93, 96]]	[[108, 126]]	['ADJPs', 'VPs']	['and all contiguous']
404	 ? Minimum Bayes Risk (MBR). We rescore the	[[23, 26]]	[[3, 21]]	['MBR']	['Minimum Bayes Risk']
405	  2.1. The behavior of manner adverbs (MA) and sentence  adverbs (SA) in the sentence is widely different: 	[[39, 41], [66, 68]]	[[23, 37], [47, 64]]	['MA', 'SA']	['manner adverbs', 'sentence  adverbs']
406	Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1?47.	[[74, 78], [51, 54]]	[[55, 72]]	['CSUR', 'ACM']	['computing surveys']
407	We tested two differing algorithms on text from the Wall Street  Journal (WSJ). Using BBN's part of speech tagger (POST), tagged  text was parsed using the full unification grammar of Delphi to fred 	[[115, 119], [74, 77]]	[[92, 113], [52, 72]]	['POST', 'WSJ']	['part of speech tagger', 'Wall Street  Journal']
408	 The headline  generation system we present uses  Singular Value Decomposition (SVD) to  guide the generation of a headline 	[[80, 83]]	[[50, 78]]	['SVD']	['Singular Value Decomposition']
409	a different word, such as job, in a given context) by defining a one-class learning algorithm based on support vector machines (SVM). They train a one-	[[128, 131]]	[[103, 126]]	['SVM']	['support vector machines']
410	Huang and Chiang, 2005), which is used to represent all derivations (i.e. parse trees) for a given  sentence under a context free grammar (CFG). A 	[[139, 142]]	[[117, 137]]	['CFG']	['context free grammar']
411	 At least for case schemata we have as first al-   ternative the choice between the realization types :CLAUSE  and :NG (noun group). For semantic structures from titles we 	[[116, 118]]	[[120, 130]]	['NG']	['noun group']
412	                                                           8  Feature type: CB=Clause boundary based feature type,  PT=Parse tree based feature type  9A ?	[[116, 118], [76, 78]]	[[119, 129], [79, 94]]	['PT', 'CB']	['Parse tree', 'Clause boundary']
413	tion of precise probability scores for partial hypotheses contain-  ing islands, in the context of  a Stochastic-Context-Free-Grammar  (SCFG) for Language Modeling (LM). The second issue is the 	[[165, 167], [136, 140]]	[[146, 163], [102, 133]]	['LM', 'SCFG']	['Language Modeling', 'Stochastic-Context-Free-Grammar']
414	be expressed via correspondences. We will define a  variant of SSTC called synchronous SSTC (S-SSTC).  	[[93, 99], [63, 67]]	[[75, 91]]	['S-SSTC', 'SSTC']	['synchronous SSTC']
415	1 Automatic Discovery of Phone(me)s Statistical models learnt from data are extensively used in modern automatic speech recognition (ASR) systems.	[[133, 136]]	[[103, 131]]	['ASR']	['automatic speech recognition']
416	subcategorisation information in the form of a Y in col-  umn 8, then we can assign the label for wh-pronou,  head (HWH). An exception list is used to map to the 	[[116, 119]]	[[110, 114]]	['HWH']	['head']
417	structures. 4 Such an algorithm has to define for every LFG a relation  F?(~,s)  (s is generable from (I)) between directed acyclic graphs (DAGs)  and terminal strings.	[[140, 144], [56, 59], [72, 74]]	[[115, 138]]	['DAGs', 'LFG', 'F?']	['directed acyclic graphs']
418	Based on RenCECps (an annotated emotion corpus) and MaxEnt (Maximum entropy) model, several contextual features and their com-	[[52, 58], [9, 17]]	[[60, 75]]	['MaxEnt', 'RenCECps']	['Maximum entropy']
419	In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 75? 	[[92, 95]]	[[49, 89]]	['ACL']	['Association for Computational Linguistic']
420	 1 Introduction The approach to factoid question answering (QA) that we adopt was first described in (Whittaker et	[[60, 62]]	[[40, 58]]	['QA']	['question answering']
421	sentence length. This gives us five metrics of complexity: sentence length (SL), tree depth (TD), branching factor (BF), normalized tree depth	[[76, 78], [93, 95], [116, 118]]	[[59, 74], [81, 91], [98, 114]]	['SL', 'TD', 'BF']	['sentence length', 'tree depth', 'branching factor']
422	.  Longest TM Candidate Indicator (LTC):  Which indicates whether the given  is the 	[[35, 38]]	[[3, 33]]	['LTC']	['Longest TM Candidate Indicator']
423	the number of sentences in each review.  Latent Dirichlet Allocation (LDA) We also apply Latent Dirichlet Allocation (Blei et al.,	[[70, 73]]	[[41, 68]]	['LDA']	['Latent Dirichlet Allocation']
424	While named entity recognition (NER) and relation or event extraction are regarded as standard tasks of information extraction (IE), coreference resolution (Ng, 2010; Bejan and Harabagiu, 2010) is more	[[128, 130]]	[[104, 126]]	['IE']	['information extraction']
425	Multiword Expressions (MWEs) are one of the stumbling blocks for more precise Natural Language Processing (NLP) systems.	[[107, 110], [23, 27]]	[[78, 105], [0, 21]]	['NLP', 'MWEs']	['Natural Language Processing', 'Multiword Expressions']
426	ing different linguistic aspects and an effective way to combine such information is made available by Support Vector (SV) regression, described in (Smola and Scho?lkopf, 2004).	[[119, 121]]	[[103, 117]]	['SV']	['Support Vector']
427	NLP-Based  Index ing  in  In fo rmat ion   Ret r ieva l   In information retrieval (IR), a typical task is to  fetch relevant documents from a large archive in 	[[84, 86], [0, 3]]	[[61, 82]]	['IR', 'NLP']	['information retrieval']
428	occurrence restrictions (FCRs), feature specification  defaults (FSDs), linear precedence (LP) statements,  and universal feature instantiation principles (UIPs). 	[[156, 160], [25, 29], [65, 69], [91, 93]]	[[112, 154], [32, 63], [72, 89]]	['UIPs', 'FCRs', 'FSDs', 'LP']	['universal feature instantiation principles', 'feature specification  defaults', 'linear precedence']
429	automatic speech recognition (ASR), machine translation (MT), text-to-speech synthesis (TTS), as well as technology for language resource and fundamental tool de-	[[88, 91], [30, 33], [57, 59]]	[[62, 86], [0, 28], [36, 55]]	['TTS', 'ASR', 'MT']	['text-to-speech synthesis', 'automatic speech recognition', 'machine translation']
430	where cLen is the length of a pattern; cMatch is  the number of matched tags; cPtn is the number of  protein name tag (PTN) skipped by the alignment  in the sentence;  cVb is the number of skipped 	[[119, 122], [168, 171], [78, 82], [6, 10], [39, 45]]	[[101, 113], [18, 24], [64, 71]]	['PTN', 'cVb', 'cPtn', 'cLen', 'cMatch']	['protein name', 'length', 'matched']
431	Results for the Mention-Pair Model 1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.5 2 Base+YAGO Types (YT) 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.8 3 Base+YAGO Means (YM) 56.7 70.0 62.7 55.3 66.5 60.4 50.6 57.0 53.6 49.3 54.9 51.9	[[121, 123], [204, 206]]	[[104, 119], [192, 202]]	['YT', 'YM']	['Base+YAGO Types', 'YAGO Means']
432	The English supervised NE tagger correctly identifies Asian as a named entity of type MISC (miscellaneous). The word-alignments sug-	[[86, 90], [23, 25]]	[[92, 105]]	['MISC', 'NE']	['miscellaneous']
433	 1 Introduction Information extraction (IE), defined as the task of extracting structured information (e.g., events, bi-	[[40, 42]]	[[16, 38]]	['IE']	['Information extraction']
434	A S O W B E  - as + Object of be  A S S E R T I O N ~ S ~ ~ ~ ~ ~ ~  + Tense + V e r b  Object  ASTG = Adjective String  C l  SHOULD -- Subjunctive foYm of ASSERTllgZQ 	[[96, 100]]	[[103, 119]]	['ASTG']	['Adjective String']
435	case. In this paper, we investigate using Amazon?s Mechanical Turk (MTurk) to make MT test sets cheaply.	[[68, 73]]	[[51, 66]]	['MTurk']	['Mechanical Turk']
436	 5 for SK with the use of POS information (SK-POS). 	[[43, 49]]	[[7, 29]]	['SK-POS']	['SK with the use of POS']
437	qn  e.g., ? MP (Member of Parliament)? 	[[12, 14]]	[[16, 36]]	['MP']	['Member of Parliament']
438	AS for word  order,  ther{{ ar<~ basieal\]}Z two  phrase  'types i n  German:  noun-dependent   phrases,  l i ke  no(:~n phrase  ( NP ) and  prepos i t iona l  phrase  ( !:'	[[131, 133]]	[[114, 127]]	['NP']	['no(:~n phrase']
439	For the time being,  the object of the testing is the generative component (GC) of this  description enumerating semantic representations (SR's) of sentences. 	[[139, 143], [76, 78]]	[[113, 137], [54, 74]]	"[""SR's"", 'GC']"	['semantic representations', 'generative component']
440	We use two ways to measure contribution in terms of graphemes: contseq(w, b) is the length of the longest prefix/suffix of word w which blend b begins/ends with, and contlcs(w, b) is the longest common subsequence (LCS) of w and b. This yields four features:	[[215, 218]]	[[187, 213]]	['LCS']	['longest common subsequence']
441	Stanford Dependencies (SDC), as described by de Marneffe et al (2006), were generated by converting Penn Treebank (PTB)-style (Marcus et al, 1993) output using the Stanford CoreNLP Tools2 into the	[[115, 118], [23, 26], [177, 180]]	[[100, 113], [0, 21]]	['PTB', 'SDC', 'NLP']	['Penn Treebank', 'Stanford Dependencies']
442	 TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding	[[58, 60], [36, 39], [1, 5]]	[[45, 56], [15, 34]]	['HS', 'MTG', 'TCGs']	['Hearthstone', 'Magic the Gathering']
443	entire web corpus. We use the API provided by the Measures of Semantic Relatedness (MSR)4 engine for this purpose.	[[84, 87], [30, 33]]	[[50, 82]]	['MSR', 'API']	['Measures of Semantic Relatedness']
444	Abstract We experiment with using different sources of distant supervision to guide unsupervised and semi-supervised adaptation of part-of-speech (POS) and named entity taggers (NER) to Twitter. 	[[178, 181], [147, 150]]	[[156, 176], [131, 145]]	['NER', 'POS']	['named entity taggers', 'part-of-speech']
445	CoTrain vs. SVM(EN) 2.15E-18 1.1E-17 3.46E-13 CoTrain vs. SVM(ENCN) 2.08E-13 8.13E-12 1.05E-12 CoTrain vs. TSVM(CN) 1.2E-19 1.07E-06 0.0624 CoTrain vs. TSVM(EN) 1.41E-05 0.00311 2.17E-08	[[112, 114], [12, 15], [16, 18], [58, 61], [62, 66], [152, 156], [157, 159], [107, 111]]	[[95, 102]]	['CN', 'SVM', 'EN', 'SVM', 'ENCN', 'TSVM', 'EN', 'TSVM']	['CoTrain']
446	guistics to guide the solution of locality of arguments. In particular, Maximal Projection (MP) which dominates	[[92, 94]]	[[72, 90]]	['MP']	['Maximal Projection']
447	contributing to the irregularity of hangul orthography is the differences in spelling between South Korea (S.K.) and North Korea (N.K.). The	[[107, 111], [130, 134]]	[[94, 105], [117, 128]]	['S.K.', 'N.K.']	['South Korea', 'North Korea']
448	 1 Introduction Stanford Dependencies (SD) provide a functional characterization of the grammatical relations in	[[39, 41]]	[[16, 37]]	['SD']	['Stanford Dependencies']
449	NG = ngrams, E = emoticon replacement, IR = informal register replacement, TL = tweet length, RT = retweet count, SVO = subject-verbobject structures. %	[[94, 96], [0, 2], [39, 41], [75, 77], [114, 117]]	[[99, 106], [5, 11], [17, 25], [44, 61], [80, 92], [120, 138]]	['RT', 'NG', 'IR', 'TL', 'SVO']	['retweet', 'ngrams', 'emoticon', 'informal register', 'tweet length', 'subject-verbobject']
450	 Assi, S. (1997). Farsi linguistic database (FLDB). 	[[45, 49]]	[[18, 43]]	['FLDB']	['Farsi linguistic database']
451	9http://disi.unitn.it/moschitti/Tree-Kernel.htm 10for STS-2012 we also report the results for a concatenation of all five test sets (ALL) provement with the Mean = 0.7416 and Pearson	[[133, 136]]	[[113, 116]]	['ALL']	['all']
452	1995. A bi-directional Russian-toEnglish machine translation system (ETAP-3). In	[[69, 75]]	[[33, 67]]	['ETAP-3']	['English machine translation system']
453	 Twice it is labeled as a noun phrase (NP) and once as a prepositional phrase (PP). 	[[79, 81]]	[[57, 77]]	['PP']	['prepositional phrase']
454	reranking step works on top of the generative model.   ZC05 (Zettlemoyer and Collins 2005) learns a discriminative log-linear model over CCG derivations.	[[56, 60], [138, 141]]	[[62, 90]]	['ZC05', 'CCG']	['Zettlemoyer and Collins 2005']
455	In the  past a few year, we have put forward methods for Kazakh morphological analysis, which includes  stem extraction, part of speech(POS) tagging, spelling check, etc. Recently, we are working on syntax 	[[136, 139]]	[[121, 134]]	['POS']	['part of speec']
456	Engineering neighborhood of bank  bank  Subject Code EG = Engineering  river wall flood thick 	[[53, 55]]	[[58, 69]]	['EG']	['Engineering']
457	HowNet and Its Computation of Meaning. In Actes de COLING-2010, Beijing, 4 p. Francopoulo, G., Bel, N., George, M., Calzolari, N., Monachini, M., Pet, M. and Soria, C. (2009). Multilingual resources for NLP in the lexical markup framework (LMF). In journal de Language Resources and Evaluation, March 2009, Volume 43, pp.	[[240, 243]]	[[214, 238]]	['LMF']	['lexical markup framework']
458	KODIAK (Keystone to Overall Design for Inte-  gration and Application of Knowledge) is an implemen-  tation of CRT (Cognitive Representation Theory), an  approach to knowledge representation that bears simi- 	[[111, 114], [0, 6]]	[[116, 147], [8, 82]]	['CRT', 'KODIAK']	['Cognitive Representation Theory', 'Keystone to Overall Design for Inte-  gration and Application of Knowledge']
459	 3 BUDS dialogue manager The Bayesian Update of Dialogue State (BUDS) dialogue manager is a POMDP-based dialogue	[[64, 68], [3, 7], [92, 97]]	[[29, 62]]	['BUDS', 'BUDS', 'POMDP']	['Bayesian Update of Dialogue State']
460	Theorem LG is NP-complete.  Bin Packing (BP) which is NP-complete is  polynomial-t ime Karp reducible to LG.	[[41, 43], [8, 10], [14, 16], [105, 107], [54, 56]]	[[28, 39]]	['BP', 'LG', 'NP', 'LG', 'NP']	['Bin Packing']
461	 Each verb-noun pair was presented to turkers via Amazon Mechanical Turk (AMT) and they were asked to describe (by text) the changes of	[[74, 77]]	[[50, 72]]	['AMT']	['Amazon Mechanical Turk']
462	The s t ruc ture  11ke a combinat ion of  a verb (V) and  an adverb ia l  par t tc le  (ADVPART) tn th ts  sequence  wtth or w i thout  a pronoun (PRON) tn between tn  Engltsh t swr t t ten  as fo l lows .	[[147, 151], [88, 95]]	[[138, 145], [44, 48], [61, 85]]	['PRON', 'ADVPART']	['pronoun', 'verb', 'adverb ia l  par t tc le']
463	Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005).	[[126, 128]]	[[102, 124]]	['CE']	['Contrastive Estimation']
464	tence is assigned a Sense_Weight_Score (SWS)  for each emotion tag which is calculated by dividing the total Sense_Tag_Weight (STW)of all  occurrences of an emotion tag in the sentence by 	[[127, 130], [40, 43]]	[[109, 125], [20, 38]]	['STW', 'SWS']	['Sense_Tag_Weight', 'Sense_Weight_Score']
465	In order to test the classification rules for the extraction of part?whole relations, we selected two different text collections: the LA Times news articles from TREC 9 and the Wall Street Journal (WSJ) articles from Treebank2.10 From each collection we randomly selected 10,000 sentences that formed two distinct test corpora.	[[198, 201], [162, 166], [134, 136]]	[[177, 196]]	['WSJ', 'TREC', 'LA']	['Wall Street Journal']
466	Sematnic data models are systems for  constructing precise descriptions of protions of  the real world - semantic data description (SDD)-  using terms that come from the real world rather 	[[132, 135]]	[[105, 130]]	['SDD']	['semantic data description']
467	2 BP Neural Network  At the moment, there are about more than 30 kinds of  artificial neural network (ANN) in the domain of  research and application.	[[102, 105], [2, 4]]	[[75, 100]]	['ANN', 'BP']	['artificial neural network']
468	for unsupervised learning. In this paper, we are interested in partitioning words into several clusters without any label priori using unsupervised LP (Un-LP) algorithm. Firstly we randomly select K (K ?	[[152, 157]]	[[135, 150]]	['Un-LP']	['unsupervised LP']
469	morpheme omitted.   We adopt Support Vector Machines(SVM) as  the device by which a given adnoun clause is 	[[53, 56]]	[[29, 51]]	['SVM']	['Support Vector Machine']
470	evaluating the attribute subsets. Their evaluation is based on consistency (CBF) and correlation (CFS). 	[[76, 79], [98, 101]]	[[63, 74], [85, 96]]	['CBF', 'CFS']	['consistency', 'correlation']
471	words words  Fig. 1 Structure of Bunruigoihy6 (BGH)  This paper focuses on classifying only nouns in terms of 	[[47, 50]]	[[33, 45]]	['BGH']	['Bunruigoihy6']
472	Statistical machine translation based on LDA.  In Universal Communication Symposium (IUCS), 2010 4th International, pages 286?290.	[[85, 89], [41, 44]]	[[47, 83]]	['IUCS', 'LDA']	['In Universal Communication Symposium']
473	navigation instruction to guide the IF to a convenient location at which she can then use a simple referring expression (RE). That is, there is an inter-	[[121, 123], [36, 38]]	[[99, 119]]	['RE', 'IF']	['referring expression']
474	   then  Cast_in_Chain(Antecedent, Anaphor)    then  Cast_in_Chain(Antecedent, Anaphor) RULE-1-Filter-1-Pronoun (R1F1Pron) RULE-1-Filter-1-Nominal (R1F1Nom)	[[113, 121], [148, 155]]	[[88, 111], [123, 146]]	['R1F1Pron', 'R1F1Nom']	['RULE-1-Filter-1-Pronoun', 'RULE-1-Filter-1-Nominal']
475	few labels. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on, pages 192?199.	[[64, 70]]	[[15, 62]]	['ASONAM']	['Advances in Social Networks Analysis and Mining']
476	et al, 2000b). A more complex application targets the Aircraft Maintenance Manual (AMM) of the Airbus A320 (Rinaldi et al, 2002b).	[[83, 86]]	[[54, 81]]	['AMM']	['Aircraft Maintenance Manual']
477	In this section, we compare the running time5 of our sampling algorithm (FAST) and our algorithm with the refined bucket (RB) against the unfactored Gibbs sampler (NAI?VE) and examine the effect of sorting.	[[122, 124], [73, 77], [164, 170]]	[[106, 120]]	['RB', 'FAST', 'NAI?VE']	['refined bucket']
478	The lack of a  fully comprehensive bilingual dictionary including  the entries for all named entities (NEs) renders the  task of transliteration necessary for certain natural 	[[103, 106]]	[[87, 101]]	['NEs']	['named entities']
479	 3 In this work, we use HL (Hu and Liu, 2004), MPQA (Wilson et al.,	[[24, 26], [47, 51]]	[[28, 38]]	['HL', 'MPQA']	['Hu and Liu']
480	increasingly more important.  In ordinary phrase structure grammars (PSG's),  the only mechanism for capturing the kinds of merg- 	[[69, 74]]	[[42, 67]]	"[""PSG's""]"	['phrase structure grammars']
481	specifier phrase with one of a set of (usually numeric) specifiers; the specifier phrase typically occurs in apposition or as a genitive modifier (GEN) to the head noun.	[[147, 150]]	[[128, 136]]	['GEN']	['genitive']
482	Given an OOV word a and its IV version b we have extracted character transformation rules from a to b using the longest common substring (LCS) algorithm (See Table 5).	[[138, 141], [9, 12], [28, 30]]	[[112, 136]]	['LCS', 'OOV', 'IV']	['longest common substring']
483	Table 2: Key details of semantic orientation (SO) lexicons. ASL = affix seeds lexicon, GI = General Inquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN = SentiWordNet, TLL = Turney?Littman lexicon.	[[110, 114], [157, 160], [46, 48], [60, 63], [87, 89], [190, 193], [210, 213]]	[[117, 155], [163, 188], [24, 44], [66, 85], [92, 108], [196, 208], [216, 238]]	['MSOL', 'PSL', 'SO', 'ASL', 'GI', 'SWN', 'TLL']	['Macquarie semantic orientation lexicon', 'Pitt subjectivity lexicon', 'semantic orientation', 'affix seeds lexicon', 'General Inquirer', 'SentiWordNet', 'Turney?Littman lexicon']
484	PoS tagF5. Lemma (L)F6. Inflection (INFL)F7. Main verb of main clause (MV)F8.	[[36, 40], [0, 3], [71, 73]]	[[24, 34], [11, 16], [45, 54]]	['INFL', 'PoS', 'MV']	['Inflection', 'Lemma', 'Main verb']
485	Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning tech-	[[110, 112]]	[[89, 108]]	['MT']	['machine translation']
486	The scores returned by the similarity measures are used as features in a Maximum Entropy (ME) classifier (Jaynes, 1957; Good, 1963), which learns to sepa-	[[90, 92]]	[[73, 88]]	['ME']	['Maximum Entropy']
487	From our point of view, it  is very interesting to compare the results of Czech  stochastic POS (SPOS) tagger and a modified RB-  POS tagger for Czech.	[[97, 101]]	[[81, 95]]	['SPOS']	['stochastic POS']
488	In support to the on going project of Multilingual  Machine Translation Sytem for Asian Language organized by  Center for International Cooperaliou inComputerization (CICC)-  Japan and other Asian cotmtries (China.	[[167, 171]]	[[111, 165]]	['CICC']	['Center for International Cooperaliou inComputerization']
489	  Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 143?147, October 25, 2014, Doha, Qatar.	[[80, 84], [21, 26]]	[[44, 78]]	['ANLP', 'EMNLP']	['Arabic Natural Langauge Processing']
490	the procedures that manipulate an  intermediate representat ion of the query,  cal led the Meta Query Language (MQL). 	[[112, 115]]	[[91, 110]]	['MQL']	['Meta Query Language']
491	5.1 Experimental Settings To evaluate our algorithm?s performance, we designed a Mechanical Turk (MTurk) experiment in which human annotators assess the quality of the	[[98, 103]]	[[81, 96]]	['MTurk']	['Mechanical Turk']
492	formation from non-expert bilingual speakers. The  Translation Correction Tool (TCTool) is a userfriendly online tool that allows users to add, delete 	[[80, 86]]	[[51, 78]]	['TCTool']	['Translation Correction Tool']
493	 3. Construct a character list (CH)3, in which the characters are top 20 frequency in training	[[32, 34]]	[[16, 25]]	['CH']	['character']
494	ging. We ran this set of experiments on the part of Chinese Treebank 4 (CTB-4)6. Ninety percent of the	[[72, 77]]	[[52, 70]]	['CTB-4']	['Chinese Treebank 4']
495	verbs. But, the second Light Verb (LV) may be  a part of another Complex Predicate (CP). This 	[[84, 86], [35, 37]]	[[65, 82], [23, 33]]	['CP', 'LV']	['Complex Predicate', 'Light Verb']
496	Table 1: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for decipherment of letter	[[118, 123], [29, 32], [56, 59], [75, 77], [153, 157]]	[[111, 116], [9, 26], [35, 53], [66, 72]]	['ORDER', 'SER', 'MER', 'RT', 'BEAM']	['order', 'Symbol error rate', 'Mapping error rate', 'untime']
497	We tested the Arabic sentiment system on two existing Arabic datasets (Mourad and Darwish (2013) (MD) and Refaee and Rieser (2014a) (RR)) and two newly sentiment-annotated Arabic datasets (BBN	[[98, 100]]	[[117, 123]]	['MD']	['Rieser']
498	 ? Generat ionCoverage(GC).Thepercentageofcoml) lete  and correct iuterlingna expressions R}r which the gener- 	[[23, 25]]	[[3, 21]]	['GC']	['Generat ionCoverag']
499	3.3 Participants and Task Eighty participants were recruited from Amazon?s Mechanical Turk2 (MTurk) for this between2http://www.mturk.com	[[93, 98]]	[[75, 91]]	['MTurk']	['Mechanical Turk2']
500	pound verb (CompV) is framed with these two  verbs. But, the second Light Verb (LV) may be  a part of another Complex Predicate (CP).	[[80, 82]]	[[68, 78]]	['LV']	['Light Verb']
501	In this paper we propose a system which uses  hybrid methods that combine both rule-based  and machine learning (ML)-based approaches  to solve GENIA Event Extraction of BioNLP 	[[113, 115], [144, 149], [170, 176]]	[[95, 111]]	['ML', 'GENIA', 'BioNLP']	['machine learning']
502	61 Table 2: Three kinds of preprocessing of a sentence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP	[[78, 81], [98, 101], [124, 127], [141, 144]]	[[84, 89], [72, 76], [130, 139], [104, 113]]	['TOP', 'ADV', 'ADJ', 'COP']	['topic', 'noun', 'adjective', 'adverbial']
503	 3.1 Sentence Splitting (SS) Sentence Splitting (SS) is the rewriting of a sentence by breaking it into two or more sentences,	[[49, 51], [25, 27]]	[[29, 47], [5, 23]]	['SS', 'SS']	['Sentence Splitting', 'Sentence Splitting']
504	End the tutoring problem? Cause another round of dialogue/essay revision ITSpoke (Intelligent Tutoring SPOKEn dialogue system) 17.08.08 |  Computer Science Department | Ubiquitous Knowledge Processing Lab  |  156/206	[[73, 80]]	[[82, 109]]	['ITSpoke']	['Intelligent Tutoring SPOKEn']
505	the preferred word attachments.  EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents	[[49, 52]]	[[33, 47]]	['ECB']	['EventCorefBank']
506	Studies such as Cinque (2006) and Rizzi  (1999) propose detailed functional phrases such  as TopP (Topic Phrase) in order to fully describe  the syntactic structures of a language.	[[93, 97]]	[[99, 111]]	['TopP']	['Topic Phrase']
507	2.4 Longest Common Substring  Given two strings, T of length n and H of length m,  the Longest Common Sub-string (LCS) problem  (Dan, 1999) will find the longest string that is a 	[[114, 117]]	[[87, 105]]	['LCS']	['Longest Common Sub']
508	MTCL = Machine Translation and Computational  Linguistics (1965-1968)  AJCL = AmericanJournal of Computational  Linguistics (1974-present) 	[[71, 75], [0, 4]]	[[78, 110], [7, 57]]	['AJCL', 'MTCL']	['AmericanJournal of Computational', 'Machine Translation and Computational  Linguistics']
509	corporates agreement features is superior to CB models that treat morphosyntax as statesplits (SP), and that the RR model benefits more from inflectional features.	[[95, 97], [45, 47], [113, 115]]	[[87, 93]]	['SP', 'CB', 'RR']	['splits']
510	these areas. Specically, our goals when entering MUC-7 were to:  Increase the accuracy in the Template Element (TE) task and the Template Relation (TR) task suciently for operational use, i.e., F-Measures of 85% and 80% respectively,	[[114, 116], [150, 152], [50, 55]]	[[96, 112], [131, 148]]	['TE', 'TR', 'MUC-7']	['Template Element', 'Template Relation']
511	2115  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 63?67, Gothenburg, Sweden, April 26-30 2014.	[[74, 76], [32, 36]]	[[54, 72]]	['DM', 'EACL']	['Dialogue in Motion']
512	  1. RecallCorrectTransliteration  (RTrans)  The recall is going to be computed using the 	[[36, 42]]	[[5, 33]]	['RTrans']	['RecallCorrectTransliteration']
513	 1 Introduction The Uniform Information Density (UID) hypothesis holds that speakers tend to maintain a relatively	[[49, 52]]	[[20, 47]]	['UID']	['Uniform Information Density']
514	tf iD j  is the inverse document frequency(IDF)  of the j-th word calculated as below: 	[[43, 46]]	[[16, 41]]	['IDF']	['inverse document frequenc']
515	tion results in a better AER. Running ALP with deletion (TD) templates followed by multi-word (TMW ) templates results in a lower AER than running ALP	[[57, 59], [25, 28], [38, 41], [130, 133], [147, 150], [95, 98]]	[[47, 55], [61, 93]]	['TD', 'AER', 'ALP', 'AER', 'ALP', 'TMW']	['deletion', 'templates followed by multi-word']
516	in the clustering.  Cumulative Micro Precision (CMP). As pointed	[[48, 51]]	[[20, 46]]	['CMP']	['Cumulative Micro Precision']
517	For each sense s i of the target words, we place a Hierarchical Dirichlet process (HDP) prior on the mixture proportion to latent concepts shown as follows:	[[83, 86]]	[[51, 81]]	['HDP']	['Hierarchical Dirichlet process']
518	 1 Introduction Recurrent Neural Network (RNN)-based conditional language models (LM) have been shown to	[[42, 45], [82, 84]]	[[16, 40], [65, 80]]	['RNN', 'LM']	['Recurrent Neural Network', 'language models']
519	lines are published results by Hall and Nivre 2008 (HN08), Maier et al 2012 (M12), van Cranenburgh 2012 (C12), Kallmeyer and Maier 2013 (KM13), van Cranenburgh and Bod 2013 (CB13), and Versley 2014a, 2014b (V14a, V14b).	[[137, 141], [52, 56], [77, 80], [105, 108], [174, 178], [207, 217]]	[[111, 135], [31, 50], [60, 75], [87, 103], [148, 172], [185, 205]]	['KM13', 'HN08', 'M12', 'C12', 'CB13', 'V14a, V14b']	['Kallmeyer and Maier 2013', 'Hall and Nivre 2008', 'aier et al 2012', 'Cranenburgh 2012', 'Cranenburgh and Bod 2013', 'Versley 2014a, 2014b']
520	248   Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1284?1295, October 25-29, 2014, Doha, Qatar.	[[94, 99]]	[[44, 92]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
521	 3.2 Duluth Systems Evaluation The FScore (F-10), F-Measure (F-SC), and Jaccard Coefficient result in a comparable and consistent	[[61, 65]]	[[35, 41]]	['F-SC']	['FScore']
522	in sentence level are not combined. That is why  most of the verb phrases(VP) are inside match  (53.28%).	[[74, 76]]	[[61, 72]]	['VP']	['verb phrase']
523	derivations obtained from all stop states in the chart.  3.6 Minimum Risk Parse (MRP) MPD and MPP aim at obtaining the structure of a	[[81, 84]]	[[61, 79]]	['MRP']	['Minimum Risk Parse']
524	to any text, so we shall comment on them.  Frequent Candidates (FC) ? this is a boosting score	[[64, 66]]	[[43, 62]]	['FC']	['Frequent Candidates']
525	traction) consists of two steps: keyphrase candidate extraction and the genetic programming of keyphrase scoring measures (KSMs).1 3.1 Step 1: Keyphrase candidate extraction	[[123, 127]]	[[95, 121]]	['KSMs']	['keyphrase scoring measures']
526	As shown in Table 3, using the transformation of dependency trees, the Dep2Str model implemented in Moses (D2S) is comparable with the standard implementation	[[107, 110]]	[[71, 78]]	['D2S']	['Dep2Str']
527	10?16). Results for per-predication (PR) and per-whole-graph (GRPH) tagging percentage accuracies are listed. (	[[62, 66], [37, 39]]	[[55, 60], [24, 35]]	['GRPH', 'PR']	['graph', 'predication']
528	Domains  In this section we compare some characteristics of the  English Travel Domain (ETD) and the English Spon-  taneous Scheduling Task (ESST).	[[88, 91], [141, 145]]	[[65, 86], [101, 139]]	['ETD', 'ESST']	['English Travel Domain', 'English Spon-  taneous Scheduling Task']
529	 A more flexible approach is to do it in two steps: complementation, forming a VP (verb phrase) from the verb and the object, and predication	[[79, 81]]	[[83, 94]]	['VP']	['verb phrase']
530	measure specifically their performance, a selection of them is also given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the	[[127, 132], [87, 90], [104, 107], [152, 157]]	[[110, 125], [75, 85], [93, 102], [135, 150]]	['PSubj', 'WHS', 'WHO', 'CSubj']	['passive Subject', 'WH-Subject', 'WH-Object', 'control Subject']
531	machine learning tasks. We used character n-grams, word n-grams, Parts of Speech (POS) tag n-grams, and perplexity of character trigrams as features.	[[82, 85]]	[[65, 80]]	['POS']	['Parts of Speech']
532	NLP track report. In Proceedings of the 5th  Text REtrieval Conference (TREC-5). 	[[72, 78], [0, 3]]	[[40, 70]]	['TREC-5', 'NLP']	['5th  Text REtrieval Conference']
533	Table 1: Purity (Pu), collocation (Co), and F1 scores of our model and the syntactic baseline in percent. Performance on arguments (Argn), adjuncts (ArgM), and overall results (Arg*) are shown separately. 	[[149, 153], [35, 37], [177, 180], [132, 136], [17, 19]]	[[121, 130], [22, 33], [9, 15]]	['ArgM', 'Co', 'Arg', 'Argn', 'Pu']	['arguments', 'collocation', 'Purity']
534	Table 4: Human expert evaluated accuracy (Acc.)  and full cluster accuracy (FAcc.) of models on	[[76, 81], [42, 45]]	[[53, 74], [32, 40]]	['FAcc.', 'Acc']	['full cluster accuracy', 'accuracy']
535	 Two sorts of recursion can be distinguished: 1)  middle field (MF) recursion, where the embedded  base clause is framed by the left and right verb parts 	[[64, 66]]	[[50, 62]]	['MF']	['middle field']
536	therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). 	[[129, 132]]	[[92, 127]]	['HRL']	['Hierarchical Reinforcement Learning']
537	JOPER (enclitic personal) DEMON (demonstrative) 1 SING (singular) INTG (interogative) 2 PLUR (plural) CREFX (common reflexive) 3	[[66, 70], [88, 92], [0, 5], [26, 31], [50, 54], [102, 107]]	[[72, 84], [94, 100], [33, 46], [7, 24], [56, 64], [109, 125]]	['INTG', 'PLUR', 'JOPER', 'DEMON', 'SING', 'CREFX']	['interogative', 'plural', 'demonstrative', 'enclitic personal', 'singular', 'common reflexive']
538	Table 1). CC = coordinating conjunction; CD = cardinal number; JJ = adjective; MD = modal; NN = singular noun; NNP = proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB =	[[41, 43], [79, 81], [10, 12], [63, 65], [91, 93], [111, 114], [130, 134], [189, 191], [198, 200], [176, 178], [157, 160]]	[[46, 54], [84, 89], [15, 39], [68, 77], [105, 109], [117, 128], [137, 155], [163, 174], [181, 187]]	['CD', 'MD', 'CC', 'JJ', 'NN', 'NNP', 'NNPS', 'TO', 'VB', 'RB', 'NNS']	['cardinal', 'modal', 'coordinating conjunction', 'adjective', 'noun', 'proper noun', 'plural proper noun', 'plural noun', 'adverb']
539	Hyderabad, India Abstract Named Entity Recognition(NER) is the task of identifying and classifying tokens in a	[[51, 54]]	[[26, 49]]	['NER']	['Named Entity Recognitio']
540	http://www.cs.ualberta.ca/?yx2/research.html 59 class-based hidden Markov model (HMM) model (Zhang et al, 2003).	[[81, 84]]	[[60, 79]]	['HMM']	['hidden Markov model']
541	 2 Related Work Sentiment analysis (SA) and related topics have been extensively studied in recent years.	[[36, 38]]	[[16, 34]]	['SA']	['Sentiment analysis']
542	Results on final test sets. LAS = labeled attachment score. UAS = unlabeled attachment score. 	[[60, 63], [28, 31]]	[[66, 92], [34, 58]]	['UAS', 'LAS']	['unlabeled attachment score', 'labeled attachment score']
543	 1 Introduction Named entity recognition (NER) is the most studied information extraction (IE) task.	[[42, 45], [91, 93]]	[[16, 40], [67, 89]]	['NER', 'IE']	['Named entity recognition', 'information extraction']
544	recognized cue phrase. Five systems followed a pure token classification approach (TC) for cue detection while others used sequential labeling tech-	[[83, 85]]	[[52, 81]]	['TC']	['token classification approach']
545	described in Section 3. We then trained linear SVMs (Support Vector Machine) using the LIBLINEAR software (Fan et al, 2008), using L1 loss	[[47, 51], [87, 96]]	[[53, 75]]	['SVMs', 'LIBLINEAR']	['Support Vector Machine']
546	 The constructed ontology is evaluated using  cluster purity (CP), instances knowledge (IK),  and relation concept (RC).	[[62, 64], [88, 90], [116, 118]]	[[46, 60], [67, 86], [98, 114]]	['CP', 'IK', 'RC']	['cluster purity', 'instances knowledge', 'relation concept']
547	 3.2.2 Optimization We use stochastic gradient descent (SGD) to maximize the simplified objectives.	[[56, 59]]	[[27, 54]]	['SGD']	['stochastic gradient descent']
548	 2 Graphical Model Framework A Graphical Model (GM) represents a factorization of a family of joint probability distributions over a	[[48, 50]]	[[31, 46]]	['GM']	['Graphical Model']
549	Using the observation that LL is correlated with MRR on the same data set, we expect that optimizing LL on a development set (LLdev) will also improve MRR on an evaluation set (MRReval).	[[126, 131], [27, 29], [49, 52], [151, 154], [177, 184]]	[[101, 120]]	['LLdev', 'LL', 'MRR', 'MRR', 'MRReval']	['LL on a development']
550	majority instances of all the clusters.   Mutual information (MI) is more theoretically  well-founded than purity.	[[62, 64]]	[[42, 60]]	['MI']	['Mutual information']
551	(MBF).  4 Multilingual PRF (MultiPRF) The schematic of the MultiPRF approach is shown	[[28, 36], [1, 4], [59, 67]]	[[10, 26]]	['MultiPRF', 'MBF', 'MultiPRF']	['Multilingual PRF']
552	I first  describe the language used to characterize the semantics of  lexical items, SEL (for Simple Episodic Logic), then the  syntax and interpretation f logical forms.	[[85, 88]]	[[94, 115]]	['SEL']	['Simple Episodic Logic']
553	during the last two years. In this first of four installerments, the  Association of Data Processing Service Organizations, Inc. (ADAPSO) is  considered with respect to its membership, charter, organization and 	[[130, 136]]	[[70, 122]]	['ADAPSO']	['Association of Data Processing Service Organizations']
554	" 3 The Discourse Model Informally, a DiscourseModel (DM)may be described as the set of entities ""specified"" in a discourse, linked together by the relations they participate in."	[[53, 55]]	[[37, 51]]	['DM']	['DiscourseModel']
555	ghar.  Parse: The modern town of [NP (np Mumbai)  (punct ,) (advP about 50 km south of Navi 	[[34, 36], [61, 65]]	[[38, 40]]	['NP', 'advP']	['np']
556	mance. The query-based selection model utilizes Support Vector Regression (SVR) models to predict the mean average precision (MAP) of each query	[[75, 78], [126, 129]]	[[48, 73], [102, 124]]	['SVR', 'MAP']	['Support Vector Regression', 'mean average precision']
557	Headline Generation. In the Proceedings of the  Document Understanding Conference (DUC). 	[[83, 86]]	[[48, 81]]	['DUC']	['Document Understanding Conference']
558	 ? Subordinate clause reordering (SubCR) which involve moving postnominal relative	[[34, 39]]	[[3, 32]]	['SubCR']	['Subordinate clause reordering']
559	 To solve this problem, we introduce  Document oriented Preference Sets(DoPS). The 	[[72, 76]]	[[38, 70]]	['DoPS']	['Document oriented Preference Set']
560	173 Figure 3: Frequency of word classes in the three corpora (BN = Broadcast News, Est = Press, Euro = Europarl).	[[62, 64], [96, 100]]	[[67, 81], [103, 111]]	['BN', 'Euro']	['Broadcast News', 'Europarl']
561	wqK ,   and the word sequence of the web page,                    A=WA=wA1, wA2, ?, wAL,  	[[68, 70]]	[[71, 74]]	['WA']	['wA1']
562	(Associativity)  \[A\[BC\]\] = \[\[AB\]C\]  (A(BC)) = ((AB)C)  (L, -singleton bidirectionality) 	[[56, 60]]	[[45, 49]]	['AB)C']	['A(BC']
563	More  technically, for each syntactic feature {sf1, sf2, ...,  sfn} of the set SF (Syntactic Features) represented  in the lexical typology, we define the goal of our 	[[79, 81]]	[[83, 101]]	['SF']	['Syntactic Features']
564	Frustration Frustrated (F), Neutral (N), Correctness Correct (C), Incorrect (I) Partially Correct (PC) Percent Correct 50-100% (High), 0-50% (Low)	[[99, 101]]	[[80, 97], [12, 22], [28, 35], [41, 60], [66, 75]]	['PC']	['Partially Correct', 'Frustrated', 'Neutral', 'Correctness Correct', 'Incorrect']
565	In LREC 2006, Genoa Yes?im Aksan and Mustafa Aksan 2012. Construction of the Turkish National Corpus (TNC). In LREC 2012,	[[102, 105], [3, 7]]	[[77, 100]]	['TNC', 'LREC']	['Turkish National Corpus']
566	mance on the NER task.  Maximum entropy classification (MaxEnt): The MaxEnt approach, or logistic regression, is	[[56, 62], [13, 16], [69, 75]]	[[24, 54]]	['MaxEnt', 'NER', 'MaxEnt']	['Maximum entropy classification']
567	many-to-one word-level alignments produced by the IBM series models (Brown et al, 1993) or the Hidden Markov Model (HMM) (Vogel et al, 1996).	[[116, 119], [50, 53]]	[[95, 114]]	['HMM', 'IBM']	['Hidden Markov Model']
568	model in their study is the relative height by range model, where (in our notation): (13) relative height by range (RH-R): ? 	[[116, 120]]	[[90, 114]]	['RH-R']	['relative height by range']
569	3. Formal descriptions about specific games which are classified as formal texts (FoT) 4.	[[82, 85]]	[[68, 80]]	['FoT']	['formal texts']
570	06 33.3 125 33.1 73 Average: 35.4 216 36.1 125 Table 3: Directed dependency accuracies (DDA) and iteration counts for the 10 (of 23) train/test splits affected by	[[88, 91]]	[[56, 86]]	['DDA']	['Directed dependency accuracies']
571	 2.3 Graphical User Interface The graphical user interface (GUI) is an important feature that has been recently added to Clairlib	[[60, 63]]	[[34, 58]]	['GUI']	['graphical user interface']
572	We focus on predicting an absolute indication of quality rather than only ranking the sentences by quality; this is why we use the Mean Absolute Error (MAE) as the main evaluation measure rather than Spearman?s correlation or DeltaAvg (Callison-Burch et al.,	[[152, 155], [226, 234]]	[[131, 150]]	['MAE', 'DeltaAvg']	['Mean Absolute Error']
573	ogy (GO), Cell Type Ontology (CTO), BRENDA Tissue Ontology (BTO), Foundational Model of Anatomy (FMA), Cell Cycle Ontology (CCO), and Sequence Ontology (SO)?and a small number of	[[124, 127], [5, 7], [30, 33], [60, 63], [97, 100], [153, 155]]	[[103, 122], [10, 28], [36, 58], [66, 95], [134, 151]]	['CCO', 'GO', 'CTO', 'BTO', 'FMA', 'SO']	['Cell Cycle Ontology', 'Cell Type Ontology', 'BRENDA Tissue Ontology', 'Foundational Model of Anatomy', 'Sequence Ontology']
574	edge associated with it.  Definition 3 (Informative Feature Extraction (IF)) We define the Informative-Features(IF ) feature	[[72, 74], [112, 114]]	[[40, 59], [91, 110]]	['IF', 'IF']	['Informative Feature', 'Informative-Feature']
575	This  paper focuses on this problem in the context of  Information Extraction (IE). 2 Many extraction 	[[79, 81]]	[[55, 77]]	['IE']	['Information Extraction']
576	 ) are based on the pairwise mutual information (PMI) between two phrases.	[[49, 52]]	[[20, 47]]	['PMI']	['pairwise mutual information']
577	day(x, fri) ? during(x, night) Here the logical form (LF) is a lambda-calculus expression defining a set of entities that are flights	[[54, 56]]	[[40, 52]]	['LF']	['logical form']
578	generative models which are respectively estimated  on their corresponding named entity lists using  maximum likelihood estimation (MLE), together  with smoothing methods4.	[[132, 135]]	[[101, 130]]	['MLE']	['maximum likelihood estimation']
579	518 ? SS = Stanford parser style:5 the first conjunct is the head and the remaining conjuncts (as	[[6, 8]]	[[11, 32]]	['SS']	['Stanford parser style']
580	Model (LEM) We propose an unsupervised latent variable model, called the Latent Event Model (LEM), to extract events from tweets.	[[93, 96], [7, 10]]	[[73, 91]]	['LEM', 'LEM']	['Latent Event Model']
581	tical finite-state transducer (SFST) as a generative model and a support vector machine (SVM) and conditional random fields (CRF) as discriminative models.	[[125, 128], [31, 35], [89, 92]]	[[98, 123], [0, 29], [65, 87]]	['CRF', 'SFST', 'SVM']	['conditional random fields', 'tical finite-state transducer', 'support vector machine']
582	derived from these MDPs: (1) Diff?s: the number of states whose policy differs from the Baseline 2 policy, (2) Percent Policy change (P.C.): the weighted amount of change between the two policies (100%	[[134, 138], [19, 23]]	[[119, 132]]	['P.C.', 'MDPs']	['Policy change']
583	ment, we compared the following three methods for word  similarity measure:  * the Bunruigoihyo thesaurus (BGH): the similarity  between case fillers is measured by a function be- 	[[107, 110]]	[[83, 105]]	['BGH']	['Bunruigoihyo thesaurus']
584	the features N, V, or 0 which indicate a nominal parent (NP or PP), a verbal parent (VP, S), or anything else, respectively. In case of adverbial phrases (AVP), the label is propagated to the head child.	[[155, 158], [57, 59], [63, 65], [85, 87]]	[[136, 153], [70, 83], [41, 55]]	['AVP', 'NP', 'PP', 'VP']	['adverbial phrases', 'verbal parent', 'nominal parent']
585	In all other tests, including all closed  tests, City University of Hong Kong (CityU)  open test and Microsoft Research (MSR) open  test, we trained our system using the relevant 	[[121, 124], [79, 84]]	[[101, 119], [49, 64]]	['MSR', 'CityU']	['Microsoft Research', 'City University']
586	mantic feature called the latent topic feature, which is extracted by exploiting the Latent Dirichlet Allocation (LDA) algorithm. Unlike syntactic fea-	[[114, 117]]	[[85, 112]]	['LDA']	['Latent Dirichlet Allocation']
587	Translation Equivalents and Semantic  Relations   Note that two translation equivalents (TE)  in a pair of languages stand in a lexical semantic 	[[89, 91]]	[[64, 87]]	['TE']	['translation equivalents']
588	morphological types and variables.  The Encyclopedia Specialist (ES) is able to access the Encyclo-  pedia for extracting semantic information and world knowledge.	[[65, 67]]	[[40, 63]]	['ES']	['Encyclopedia Specialist']
589	to systems that rely on brittle features is that many texts are not well-formed. One  such class of texts are those that are the output of optical character recognition (OCR);  typically these texts contain many extraneous or incorrect characters.	[[170, 173]]	[[139, 168]]	['OCR']	['optical character recognition']
590	Recall all the methods rely on the wrapped classifier. We select two classic but very different classifiers: the Maximum Entropy model (MaxEnt) and the Decision Tree C4.5 (Tree). We implement these	[[136, 142]]	[[113, 128]]	['MaxEnt']	['Maximum Entropy']
591	corpora (section 2.2).  The workflow for named entity (NE) and  terminology extraction and mapping from 	[[55, 57]]	[[41, 53]]	['NE']	['named entity']
592	  Abstract  Over the last few years, machine translation (MT) has transformed from an academic research  platform into a productivity or gisting tool adopted by several end users.	[[58, 60]]	[[37, 56]]	['MT']	['machine translation']
593	sources Tony Mullen and Nigel Collier National Institute of Informatics (NII) Hitotsubashi 2-1-2, Chiyoda-ku	[[73, 76]]	[[38, 71]]	['NII']	['National Institute of Informatics']
594	to this tree as the Hidden Factors Tree (HFT). We use Minimum Discriminative Information (MDI) algorithm (Zitouni, 2007) to build the tree.	[[90, 93], [41, 44]]	[[54, 88], [20, 39]]	['MDI', 'HFT']	['Minimum Discriminative Information', 'Hidden Factors Tree']
595	The thesis will examine two main areas: modelling cohesive devices within sentences and modelling discourse relations (DRs) across sentences.	[[119, 122]]	[[98, 117]]	['DRs']	['discourse relations']
596	"923 DOs, Active: ""AGENT STRING AUX active-verb-element DETERMINER * POSTMOD""DOs, Passive: ""DETERMINER * AUX active-verb-element element""TVs, Active: ""AGENT STRING AUX * DETERMINER active-noun- element POSTMOD""TVs, Passive:""DET active-noun-element AUX * POSTMOD"" Figure 4: Query patterns for retrieving direct objects (DOs) and transitive verbs (TVs) in the Hypothesize step. "	[[318, 321], [345, 348], [31, 34], [104, 107], [163, 166], [4, 7], [76, 79], [247, 250], [136, 139], [209, 212]]	[[302, 316], [327, 343]]	['DOs', 'TVs', 'AUX', 'AUX', 'AUX', 'DOs', 'DOs', 'AUX', 'TVs', 'TVs']	['direct objects', 'transitive verbs']
597	tive two-step model. We compare models based on the Akaike Information Criterion (AIC). 	[[82, 85]]	[[52, 80]]	['AIC']	['Akaike Information Criterion']
598	ordinating conjunction; JJ = adjective; JJR = comparative adjective; NN = singular or mass noun; NNS = plural noun; POS = possessive clitic; RB = adverb; RBR = comparative adverb; RP = particle; UH = interjection; VB =	[[116, 119], [24, 26], [40, 43], [69, 71], [97, 100], [141, 143], [154, 157], [180, 182], [195, 197], [214, 216]]	[[122, 132], [29, 38], [46, 67], [74, 95], [103, 114], [146, 152], [160, 178], [185, 193], [200, 212]]	['POS', 'JJ', 'JJR', 'NN', 'NNS', 'RB', 'RBR', 'RP', 'UH', 'VB']	['possessive', 'adjective', 'comparative adjective', 'singular or mass noun', 'plural noun', 'adverb', 'comparative adverb', 'particle', 'interjection']
599	1 Introduction  Turkish Discourse Bank (TDB) is the first discourse-annotated corpus of Turkish, which follows the  principles of Penn Discourse Tree Bank (PDTB) (Prasad et al., 2008) and includes annotations for dis-	[[156, 160], [40, 43]]	[[130, 154], [16, 38]]	['PDTB', 'TDB']	['Penn Discourse Tree Bank', 'Turkish Discourse Bank']
600	that first contains TOOV and given by the search  engine. Average_Rank (A_Rank) is the average position of TOOV in the returned snippets.	[[72, 78], [20, 24], [107, 111]]	[[58, 70]]	['A_Rank', 'TOOV', 'TOOV']	['Average_Rank']
601	3.5 A Novel Lattice Statistical Language Model Representation Our final statistical language model is a novel latent-variable statistical language model, called a Partial Lattice MRF (PL-MRF), with rich latent structure, shown in Figure 3. The	[[184, 190]]	[[163, 182]]	['PL-MRF']	['Partial Lattice MRF']
602	data as described in Niehues and Vogel (2008).  The phrase table (PT) is built using the Moses toolkit (Koehn et al.,	[[66, 68]]	[[52, 64]]	['PT']	['phrase table']
603	(1987) presents in his Case grid.  Case Analysis (CA) extracts the acts and Case  constellations around them from the structure 	[[50, 52]]	[[35, 48]]	['CA']	['Case Analysis']
604	The IXM2 is the first massively parallel associative  processor that clearly demonstrates the computing  power of a large Associative Memory (AM). The AM 	[[142, 144], [4, 8], [151, 153]]	[[122, 140]]	['AM', 'IXM2', 'AM']	['Associative Memory']
605	Improving part-of-speech tagging for context-free parsing. In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP), pages 1260?1268, Chiang Mai, Thailand.	[[144, 150]]	[[81, 142]]	['IJCNLP']	['International Joint Conference on Natural Language Processing']
606	The current state of the art within the comment summarisation field is to cluster comments using Latent Dirichlet Allocation (LDA) topic modelling (Khabiri et al, 2011; Ma et al, 2012;	[[126, 129]]	[[97, 124]]	['LDA']	['Latent Dirichlet Allocation']
607	Certain schemes have been aimed at abstracts, e.g., (McKnight  &  Srinivasan, 2003; Ruch et al, 2007; Hirohata et al, 2008; Bj?rne et al, 2009). The work of Hirohata et al (2009) has been integrated with the MEDIE service5 (Miyao et al, 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al, 1999; Teufel  &  Moens, 2002; Teufel et al, 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers.	[[387, 389], [494, 496]]	[[365, 385]]	['AZ', 'AZ']	['argumentative zoning']
608	get, with results deteriorating as we move to information retrieval (IR), multi-document summarization (SUM), and information extraction (IE). 	[[138, 140]]	[[114, 136]]	['IE']	['information extraction']
609	construct such a treebank from scratch. Fortunately, RST Discourse Treebank (RST-DT)  (Carlson et al, 2001) is an available resource to 	[[77, 83]]	[[53, 75]]	['RST-DT']	['RST Discourse Treebank']
610	Holes, 2004). Most tools and resources developed for natural language processing (NLP) of Arabic are designed for MSA.	[[82, 85], [114, 117]]	[[53, 80]]	['NLP', 'MSA']	['natural language processing']
611	nese personal naming system. Therefore, we hold  Chinese personal name disambiguation (CPND) to  explore those problems.	[[87, 91]]	[[49, 85]]	['CPND']	['Chinese personal name disambiguation']
612	 Like most of the successful AQUAINT QA systems,  LCC?s system uses an answer type (AT) ontology for  the classification of AT categories.	[[84, 86], [29, 36], [37, 39], [50, 53], [124, 126]]	[[71, 82]]	['AT', 'AQUAINT', 'QA', 'LCC', 'AT']	['answer type']
613	Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In IEEE International Conference on Computer Vision (ICCV), December.	[[169, 173], [119, 123]]	[[124, 167]]	['ICCV', 'IEEE']	['International Conference on Computer Vision']
614	U, x /y /z ,  T, V ~ VYw\[Y /X\ ]   (14)  5Here the 'full' version of (VR) is being used, incorpo-  rating a change of bound variable.	[[71, 73]]	[[59, 66]]	['VR']	['version']
615	  Although we use the same techniques to derive global features (assessor variety (AV) feature with 2~6 grams) from both training and test-	[[83, 85]]	[[65, 81]]	['AV']	['assessor variety']
616	Marton, Habash, and Rambow Arabic Parsing with Lexical and Inflectional Features Table 8 Models with functional features: GENDER, NUMBER, rationality (RAT). FN* = functional	[[151, 154], [157, 159]]	[[138, 149]]	['RAT', 'FN']	['rationality']
617	Deep-syntactic structures (DSyntSs);  ? Surface syntactic structures (SSyntSs);  61 	[[70, 77]]	[[40, 68]]	['SSyntSs']	['Surface syntactic structures']
618	Linguistic expressions can vanish and appear in translation. For example, the preposition (PP) in the source rule does not show up in any of   Machine Translation Based on Constraint-Based Synchronous Grammar 615 	[[91, 93]]	[[78, 89]]	['PP']	['preposition']
619	The general architecture of D2S is represented in Figure 1. It consists of two modules, the Language Gen-  eration Module (LGM), and the Speech Generation Module (SGM). The LGM takes data as input .and 	[[163, 166], [28, 31], [123, 126], [173, 176]]	[[137, 161], [92, 121]]	['SGM', 'D2S', 'LGM', 'LGM']	['Speech Generation Module', 'Language Gen-  eration Module']
620	2011) created a corpus of posts from several online forums about breast cancer, which later was used to extract potential adverse reactions from the most commonly used drugs to treat this disease: tamoxifen, anastrozole, letrozole and axemestane. The authors collected a lexicon of lay medical terms from websites and databases about drugs and adverse events. The lexicon was extended with the Consumer Health Vocabulary (CHV)5, a vocabulary closer to the lay terms, which patients usually use to describe their medical experiences. Then, pairs of terms co-occurring within a window of 20 tokens were considered.	[[422, 425]]	[[394, 420]]	['CHV']	['Consumer Health Vocabulary']
621	approach and extract features from the names.  They use Maximum Entropy (MaxEnt) model and a number of features based on n-grams,	[[73, 79]]	[[56, 71]]	['MaxEnt']	['Maximum Entropy']
622	is the DIAGRAM grammar \ [9 \ ] .   It is un for tunate ly  the very power .of APSGs (and ATNs)  that  makes it  d i f f i cu l t  to capture l inguist ic  general izat ions 	[[79, 84], [90, 94]]	[]	['APSGs', 'ATNs']	[]
623	 1 Introduction Interactive question answering (QA) has been identified as one of the important directions in QA	[[48, 50], [110, 112]]	[[28, 46]]	['QA', 'QA']	['question answering']
624	5.3 Evaluation Metrics For YA, we used the standard implementations for P@1 and mean reciprocal rank (MRR) (Manning et al, 2008).	[[102, 105], [27, 29]]	[[80, 100]]	['MRR', 'YA']	['mean reciprocal rank']
625	erences to the instructors in the posts etc.  3.3 Linear Chain Markov Model (LCMM) The logistic regression model is good at exploit-	[[77, 81]]	[[50, 75]]	['LCMM']	['Linear Chain Markov Model']
626	The bacteria track consists of two tasks, BB and BI.  2.4.1 Bacteria biotope task (BB) The aim of the BB task (Bossy et al, 2011) is to ex-	[[83, 85], [102, 104], [42, 44], [49, 51]]	[[60, 76]]	['BB', 'BB', 'BB', 'BI']	['Bacteria biotope']
627	Abstract We present the first provably optimal polynomial time dynamic programming (DP) algorithm for best-first shift-reduce parsing, which	[[84, 86]]	[[63, 82]]	['DP']	['dynamic programming']
628	1408 then apply our model to the well-established sequence labeling task: noun phrase (NP) chunking. 	[[87, 89]]	[[74, 85]]	['NP']	['noun phrase']
629	data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong Kong City University (CityU) corpus. We first split	[[118, 123], [35, 37], [71, 74]]	[[101, 116], [18, 33]]	['CityU', 'AS', 'PKU']	['City University', 'Academia Sinica']
630	 4.3 The Limited-Memory BFGS Algorithm The limited memory BFGS (L-BFGS) algorithm is a general purpose numerical optimization algorithm (Nocedal and Wright 1999).	[[64, 70], [24, 28]]	[[43, 62]]	['L-BFGS', 'BFGS']	['limited memory BFGS']
631	In the leftmost column SSE-TRA and SSE-HDC refer to the trained and hand-coded SSE versions; the column NS indicates the number of sessions; the columns PSucc (perceived success), PAtt (perceived attention recognition), PUnd (perceived understanding), PNat (perceived naturalness), and POv (perceived overall performance) give average	[[153, 158], [180, 184], [23, 30], [35, 42], [79, 82], [104, 106], [220, 224], [252, 256]]	[[160, 177], [186, 205], [226, 249], [258, 279], [291, 320], [286, 289]]	['PSucc', 'PAtt', 'SSE-TRA', 'SSE-HDC', 'SSE', 'NS', 'PUnd', 'PNat']	['perceived success', 'perceived attention', 'perceived understanding', 'perceived naturalness', 'perceived overall performance', 'POv']
632	Evaluation (LREC?08), Marrakech, Morocco, may.  European Language Resources Association (ELRA). 	[[89, 93], [12, 16]]	[[48, 87]]	['ELRA', 'LREC']	['European Language Resources Association']
633	search excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-o?konomischer Exzellenz (LOEWE) as part of the research center Digital Humanities.	[[101, 106]]	[[26, 99]]	['LOEWE']	['Landes-Offensive zur Entwicklung Wissenschaftlich-o?konomischer Exzellenz']
634	The Text Encoding Initiative (TEI) is a cooperative undertaking of the Association  for Computers and the Humanities (ACH), the Association for Computational Lin-  guistics (ACL), and the Association for Literary and Linguistic Computing (ALLC). 	[[239, 243], [30, 33], [118, 121], [174, 177]]	[[188, 237], [4, 28], [71, 116], [128, 172]]	['ALLC', 'TEI', 'ACH', 'ACL']	['Association for Literary and Linguistic Computing', 'Text Encoding Initiative', 'Association  for Computers and the Humanities', 'Association for Computational Lin-  guistics']
635	If the polarity of expressed statement is not  neutral and reinforcement is negative, then the  polarity of the statement (PP) is reversed and  score is intensified: 	[[123, 125]]	[[96, 104]]	['PP']	['polarity']
636	 1 Introduction Amazon?s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in	[[41, 44]]	[[16, 39]]	['AMT']	['Amazon?s MechanicalTurk']
637	tween arguments. We thus propose to model the reranking phase (RR) as a HMM sequence labeling task.	[[63, 65], [72, 75]]	[[46, 55]]	['RR', 'HMM']	['reranking']
638	six basic emotion tags to the Bengali blog sentences. Conditional Random Field (CRF)  based word level emotion classifier classifies 	[[80, 83]]	[[54, 78]]	['CRF']	['Conditional Random Field']
639	of two kinds of data: One is the hand built seg-  mentation dictionary (HBSD)  and the other is the  simple noun dictionary for segmentation (SND). 	[[142, 145], [72, 76]]	[[101, 123], [33, 70]]	['SND', 'HBSD']	['simple noun dictionary', 'hand built seg-  mentation dictionary']
640	Using either method of uncertainty sampling,  the computational cost of picking an example from  T candidates is: O(TD) where D is the number of  model parameters.	[[116, 118]]	[[97, 109]]	['TD']	['T candidates']
641	 Recently, there have been increasing interests for dialogue act (DA) recognition in spoken and written conversations, which include meetings,	[[66, 68]]	[[52, 64]]	['DA']	['dialogue act']
642	labeled DGs.  (C3) Projectivity constraint(PJC): No arc crosses another arc5	[[43, 46], [8, 11]]	[[19, 41]]	['PJC', 'DGs']	['Projectivity constrain']
643	integer linear programming (ILP) conditional random field (CRF) support vector machine (SVM) latent semantic analysis (LSA)	[[88, 91], [28, 31], [59, 62], [119, 122]]	[[64, 86], [0, 26], [33, 57], [93, 117]]	['SVM', 'ILP', 'CRF', 'LSA']	['support vector machine', 'integer linear programming', 'conditional random field', 'latent semantic analysis']
644	2https://www.mturk.com/mturk/. 3http://tartarus.org/martin/PorterStemmer/ 4Reviews with NDr = NTr are regarded as incorrectly classified by TopicSpam.	[[94, 97], [88, 91]]	[[98, 125]]	['NTr', 'NDr']	['are regarded as incorrectly']
645	 The objective of this study is to illustrate a  word support model (WSM) that is able to improve our WP-identifier by achieving better 	[[69, 72], [102, 104]]	[[49, 67]]	['WSM', 'WP']	['word support model']
646	(I) it performs a translation between  intermediate languages constructed  over source language (SL) and target  language (TL) respectively (called 	[[97, 99], [123, 125]]	[[80, 95], [105, 121]]	['SL', 'TL']	['source language', 'target  language']
647	press first-order logic. This requirement motivates our use of Inductive Logic Programming (ILP), a learning algorithm capable of inferring logic pro-	[[92, 95]]	[[63, 90]]	['ILP']	['Inductive Logic Programming']
648	   (2)  LSA-based (Latent Semantic Analysis based)  trigger word similarity: LSA (Deerwester et 	[[8, 17], [77, 80]]	[[19, 43]]	['LSA-based', 'LSA']	['Latent Semantic Analysis']
649	 250 Support Vector Machines (SVMs) construct a hyperplane in a multi-dimensional space which yields a good separation between positive and negative training examples, represented as data points.	[[30, 34]]	[[5, 28]]	['SVMs']	['Support Vector Machines']
650	recursive noun phrases (NPs), main verb groups (MVs), and a common annotation for adjectival and adverbial phrases (APs). Example (3) be-	[[116, 119], [24, 27], [48, 51]]	[[97, 114], [10, 22], [30, 46]]	['APs', 'NPs', 'MVs']	['adverbial phrases', 'noun phrases', 'main verb groups']
651	He washed it. With Kamp's  discourse representation theory (DRT) (Kamp 1981; Kamp and Reyle 1993) a discourse  representation structure (DRS) in which the reference to the pronoun he is constrained 	[[60, 63], [137, 140]]	[[27, 58], [100, 135]]	['DRT', 'DRS']	['discourse representation theory', 'discourse  representation structure']
652	  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 33?37, Gothenburg, Sweden, April 26-30 2014.	[[70, 72], [28, 32]]	[[50, 68]]	['DM', 'EACL']	['Dialogue in Motion']
653	Long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the	[[119, 122]]	[[93, 117]]	['RNN']	['recurrent neural network']
654	 We evaluate performance using 3 measures:  exact match (EM), head match (HM), and partial  match (PM), similar to Choi et al (2006).	[[57, 59], [74, 76], [99, 101]]	[[44, 55], [62, 72], [83, 97]]	['EM', 'HM', 'PM']	['exact match', 'head match', 'partial  match']
655	Prevalence and count of conditions by temporal  category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department, 	[[75, 77]]	[[80, 97]]	['DS']	['Discharge Summary']
656	 1 Introduction Predicate argument structure (PAS) analysis is a shallow semantic parsing task that identifies ba-	[[46, 49]]	[[16, 44]]	['PAS']	['Predicate argument structure']
657	110  ehange(CHA) communication(COMM)  cognition(COG) competition(COMP)  contact(CeNT) motion(MOT) 	[[48, 51], [65, 69]]	[[38, 46], [53, 63]]	['COG', 'COMP']	['cognitio', 'competitio']
658	(SBAR-TMP (IN after) (S (NP (DT the) (NN sale)) (VP (AUX is) (VP (VBN completed))) ))))))))	[[66, 69], [1, 9], [25, 27], [29, 31], [38, 40], [49, 51], [53, 56], [62, 64]]	[]	['VBN', 'SBAR-TMP', 'NP', 'DT', 'NN', 'VP', 'AUX', 'VP']	[]
659	mark concept are sent to the kernel-based location belief tracker, while all other concepts are sent to a Dynamic Probabilistic Ontology Trees (DPOT) semantic belief tracker, whose structure is shown in	[[144, 148]]	[[106, 142]]	['DPOT']	['Dynamic Probabilistic Ontology Trees']
660	 Since this is a binary classification task, we have 5 different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Beginning an	[[71, 74], [107, 110], [140, 143]]	[[76, 98], [112, 131], [145, 157]]	['B-L', 'I-L', 'B-I']	['Beginning of a literal', 'Inside of a literal', 'Beginning an']
661	In Proceedings of the 15th International Conference on  Computational Linguistics (COLING'94),  Kyoto, Japan, August.	[[83, 92]]	[[56, 81]]	"[""COLING'94""]"	['Computational Linguistics']
662	 0.7  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random NLPBA	[[60, 62], [83, 86], [107, 110], [118, 123]]	[[39, 58], [63, 69]]	['ID', 'FIR', 'SVE', 'NLPBA']	['information density', 'Fisher']
663	vv@iiit.ac.in Abstract Recognition of Named Entities (NEs) is a difficult process in Indian languages like Hindi,	[[54, 57]]	[[38, 52]]	['NEs']	['Named Entities']
664	ing in the non-realizable case. In Advances in Neural Information Processing Systems (NIPS), 2010.	[[86, 90]]	[[47, 84]]	['NIPS']	['Neural Information Processing Systems']
665	unitary operator. Therefore, the primary problem  of building a quantum classifier (QC) is to find  the correct or optimal unitary operator.	[[84, 86]]	[[64, 82]]	['QC']	['quantum classifier']
666	categories, as shown in Figure 1. Messages may  involve a request (REQ), provide information  (INF), or fall into the category of interpersonal 	[[67, 70], [95, 98]]	[[58, 65], [81, 92]]	['REQ', 'INF']	['request', 'information']
667	Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and	[[81, 84]]	[[53, 79]]	['MDL']	['minimum description length']
668	? P5E3N4S3, F W I International Language of Service and Maintenance (ILSAM) (Pym 1990) is an influential language similar to Caterpillar Fundamental English, from which it was derived	[[69, 74], [2, 10]]	[[18, 67]]	['ILSAM', 'P5E3N4S3']	['International Language of Service and Maintenance']
669	4.2 Cognate based features Dictionaries mostly fail to return translation entries for named entities (NEs) or specialized terminology.	[[102, 105]]	[[86, 100]]	['NEs']	['named entities']
670	 In the medical domain, Castan?o et al (2002) used UMLS (Unified Medical Language System)7 as their knowledge source.	[[51, 55]]	[[57, 90]]	['UMLS']	['Unified Medical Language System)7']
671	as the World Wide Web. Various traditional  information retrieval(IR) techniques combined  with natural language processing(NLP) tech- 	[[66, 68], [124, 127]]	[[44, 64], [96, 123]]	['IR', 'NLP']	['information retrieva', 'natural language processing']
672	BUS (business) Belgium Labour Federation 9 4440 11.0 NOB (nobility) Albert II 6 4179 15.1 COM (comics) Suske and Wiske 3 4000 10.5 MUS (music) Sandra Kim, Urbanus 3 1296 14.6	[[90, 93], [0, 3], [53, 56], [131, 134]]	[[95, 101], [5, 13], [58, 66], [136, 141]]	['COM', 'BUS', 'NOB', 'MUS']	['comics', 'business', 'nobility', 'music']
673	ayman,R.Gaizauskas@dcs.shef.ac.uk Abstract Disambiguating named entities (NE) in running text to their correct interpretations in a specific knowledge base (KB) is an important problem in NLP.	[[74, 76], [157, 159], [188, 191]]	[[58, 72], [141, 155]]	['NE', 'KB', 'NLP']	['named entities', 'knowledge base']
674	single weight matrixW . In contrast, the CVG uses a syntactically untied RNN (SU-RNN) which has a set of such weights.	[[78, 84]]	[[52, 76]]	['SU-RNN']	['syntactically untied RNN']
675	integrate Chinese word segmentation and NE  identification into a unified framework using a  class-based language model (LM). &ODVVEDVHG/0 IRU1(,GHQWLILFDWLRQ The n-gram LM is a stochastic model which 	[[121, 123], [40, 42], [175, 177]]	[[105, 119]]	['LM', 'NE', 'LM']	['language model']
676	New in three aspects. First, the basic units of their model are elementary discourse units (EDUs) from Rhetorical Structure Theory (RST) (Mann	[[92, 96], [132, 135]]	[[64, 90], [103, 130]]	['EDUs', 'RST']	['elementary discourse units', 'Rhetorical Structure Theory']
677	P (di|h(d1,..., di?1)) Using a neural network architecture called Simple Synchrony Networks (SSNs), the history representation h(d1,..., di?1) is incrementally computed from	[[93, 97]]	[[73, 91]]	['SSNs']	['Synchrony Networks']
678	2 As a matter of fact, Figure 1 only shows 8 columns, although  the CoNLL-X format includes two additional columns for the  projective head (PHEAD) and projective dependency relation  (PDEPREL), which have not been used in our work.	[[141, 146], [68, 75], [185, 192]]	[[124, 139], [152, 182]]	['PHEAD', 'CoNLL-X', 'PDEPREL']	['projective head', 'projective dependency relation']
679	collection; however, to facilitate comparisons with prior work (e.g., McCarthy et al 2004a), all our experiments use the British National Corpus (BNC). In	[[146, 149]]	[[121, 144]]	['BNC']	['British National Corpus']
680	MUC-6, 1995; Agirre, 2007). By contrast, gold  standard Named Entity (NE) annotations are easy  to produce; indeed, there are many NE annotated 	[[70, 72]]	[[56, 68]]	['NE']	['Named Entity']
681	 For each combination, we measure the attachment score (AS) and the exact match (EM). A signif-	[[81, 83], [56, 58]]	[[68, 79], [38, 54]]	['EM', 'AS']	['exact match', 'attachment score']
682	treebank. In: Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT), pages 24? 	[[86, 89]]	[[51, 84]]	['TLT']	['Treebanks and Linguistic Theories']
683	/(cs + |N |?)  P (GR = gri|SCF = s) = (csgri +?) /(cs + |G|?)	[[18, 20]]	[[23, 30]]	['GR']	['gri|SCF']
684	The underlying learning algorithm has been successfully applied to some other Natural Language Processing (NLP) tasks. 	[[107, 110]]	[[78, 105]]	['NLP']	['Natural Language Processing']
685	of-the-art in more detail.  The field of Information Extraction (IE) has been heavily influenced by the Information Retrieval (IR)	[[65, 67]]	[[41, 63]]	['IE']	['Information Extraction']
686	we maximize the log likelihood J(?) using a simple optimization technique called stochastic gradient descent (SGD). N,W	[[110, 113]]	[[81, 108]]	['SGD']	['stochastic gradient descent']
687	The classification step currently supports three machine learning algorithms from the Python scikit-learn4 package: Na??ve Bayes (NB), Maximum Entropy (MaxEnt), and Support Vector	[[130, 132], [152, 158]]	[[116, 128], [135, 150]]	['NB', 'MaxEnt']	['Na??ve Bayes', 'Maximum Entropy']
688	2.1 Collection Tasks To collect our data, we used two different types of human intelligence tasks (HITs). In type 1, the	[[99, 103]]	[[73, 97]]	['HITs']	['human intelligence tasks']
689	Since this is a binary classification task, we have 5 different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Beginning an Idiomatic chunk), I-I (Inside an Idiomatic chunk),	[[106, 109], [139, 142], [70, 73], [175, 178]]	[[111, 130], [144, 166], [75, 97], [180, 199]]	['I-L', 'B-I', 'B-L', 'I-I']	['Inside of a literal', 'Beginning an Idiomatic', 'Beginning of a literal', 'Inside an Idiomatic']
690	However, to the best of our knowledge only the model of Blunsom & Cohn (2006), which is based on a Conditional Random Field (CRF) (Lafferty et al, 2001), can compute word indices pairs?	[[125, 128]]	[[99, 123]]	['CRF']	['Conditional Random Field']
691	the Natural Language Generation (NLG) component to create the textual form of the output and last, the Text To Speech (TTS) component to convert the text to spoken output.	[[119, 122], [33, 36]]	[[103, 117], [4, 31]]	['TTS', 'NLG']	['Text To Speech', 'Natural Language Generation']
692	formation retrieval resulted in creation of reusable test collections in large-scale evaluations such as the Text REtrieval Conference (TREC)1. Researchers in	[[136, 140]]	[[109, 134]]	['TREC']	['Text REtrieval Conference']
693	P2E5N5S1, C W D I FAA Air Traffic Control Phraseology (FAA 2010) is a controlled language defined by the Federal Aviation Administration (FAA) and used for the communication in air traffic 135	[[138, 141], [0, 8], [55, 58], [18, 21]]	[[105, 136]]	['FAA', 'P2E5N5S1', 'FAA', 'FAA']	['Federal Aviation Administration']
694	tions. This model has three main steps including Local Term Weighting (LTW), Global Term Weighting (GTM), and Fuzzy Clustering (Algo-	[[71, 74]]	[[49, 69]]	['LTW']	['Local Term Weighting']
695	to) Peet a friend?  Figure 10: An argument post particle phrase (PP) (upper) and an adjunct PP (lower).	[[65, 67], [92, 94]]	[[48, 63]]	['PP', 'PP']	['particle phrase']
696	are contained in a XML file and each query  consists of following elements: Topic  Number(NUM),Topic Title(TITLE),Topic  question(DESC),Topic Narrative(NARR) and 	[[90, 93], [19, 22], [107, 112], [130, 134], [152, 156]]	[[83, 88], [101, 106], [142, 151]]	['NUM', 'XML', 'TITLE', 'DESC', 'NARR']	['Numbe', 'Title', 'Narrative']
697	ber of hours of domain-specific spontaneous speech used for AM adaptation, the number of titles used to construct the language model (LM), the type of LM, the type of grammar rules in the Phoenix book	[[134, 136], [151, 153], [60, 62]]	[[118, 132]]	['LM', 'LM', 'AM']	['language model']
698	 1 Introduction  Generation of referring expression (GRE) is an  important task in the field of Natural Language 	[[53, 56]]	[[17, 51]]	['GRE']	['Generation of referring expression']
699	from standard formats to OpenNLP specific ones. We represented standard formats with EMF (an Ecore model for each one) and we created specific transformations using Java Emitter Templates (JET) 16	[[189, 192], [29, 32], [85, 88]]	[[165, 187]]	['JET', 'NLP', 'EMF']	['Java Emitter Templates']
700	structural and behavioral parts have to be fully specified at this level.  2.3 Eclipse Modeling Framework (EMF) We chose to adopt EMF as the underlying modeling framework and tooling for our model-driven ap-	[[107, 110], [130, 133]]	[[79, 105]]	['EMF', 'EMF']	['Eclipse Modeling Framework']
701	As suggested from the tables, the accuracy values of the component classifiers (Ccn and Cen) in CoTrain are almost always higher than those of the corresponding TSVM(CN) and TSVM(EN), based on any machine translation service.	[[166, 168], [161, 165], [174, 178], [179, 181], [80, 83], [88, 91]]	[]	['CN', 'TSVM', 'TSVM', 'EN', 'Ccn', 'Cen']	[]
702	a set of features in the Sentence Scoring phase.  The Maximal Marginal Relevance (MMR) algorithm is then used in the Sentence Re-ordering	[[82, 85]]	[[54, 80]]	['MMR']	['Maximal Marginal Relevance']
703	CP = Relative Pronoun  +  IP  PP = Preposition  + NP AdjP = Adjective + NP	[[30, 32]]	[[35, 46]]	['PP']	['Preposition']
704	#and (a probabilistic AND) is used. Otherwise, the  probabilistic passage operator #UWn (unordered window)  is used.	[[84, 87], [22, 25]]	[[89, 105]]	['UWn', 'AND']	['unordered window']
705	Ill the last two  experimeuts a memory with the analysis of tile  most frequent word-forms (MFW) in Basque  was used, so that only word-forms not found 	[[92, 95]]	[[66, 84]]	['MFW']	['most frequent word']
706	strated via an algorithm for joint unsupervised learning of the topology and parameters of a hidden Markov model (HMM); states and short state-sequences through this HMM cor-	[[114, 117], [166, 169]]	[[93, 112]]	['HMM', 'HMM']	['hidden Markov model']
707	interchange of LRs. It also demonstrate how MLI  can be applied to Asian Language Resource (ALR)  through making the results of collaborative en-	[[92, 95], [15, 18], [44, 47]]	[[67, 90]]	['ALR', 'LRs', 'MLI']	['Asian Language Resource']
708	reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).  Entity coherence, which is based on the way the referents of noun phrases (NPs) relate subsequent clauses in the text, is an important aspect of textual organization.	[[134, 137]]	[[120, 132]]	['NPs']	['noun phrases']
709	tor. Together, these modifications reduce the BLEU score by 1.49 BLEU points (BP)9 at the largest training size.	[[78, 80], [46, 50]]	[[65, 76]]	['BP', 'BLEU']	['BLEU points']
710	 The best method ASSVM outperforms other methods most clearly on METH (Method) category. Al-	[[65, 69], [17, 22]]	[[71, 77]]	['METH', 'ASSVM']	['Method']
711	 3.3 Bootstrapped Voting Experts The Bootstrapped Voting Experts (BVE) algorithm (Hewlett and Cohen, 2009) is an extension to VE.	[[66, 69], [126, 128]]	[[37, 64]]	['BVE', 'VE']	['Bootstrapped Voting Experts']
712	  This approach has been employed in Augmentative  and Alternative Communication (AAC), in the  form of multimodal vocabularies in assistive de-	[[82, 85]]	[[37, 80]]	['AAC']	['Augmentative  and Alternative Communication']
713	1Note: LM(language model); ME(maximum entropy).  Brand Name(BRA), Product Type(TYP), Product Name(PRO), and BRA and TYP are often embed-	[[60, 63], [7, 9], [27, 29], [79, 82], [98, 101], [108, 111], [116, 119]]	[[49, 54], [10, 24], [30, 45], [66, 78], [85, 92]]	['BRA', 'LM', 'ME', 'TYP', 'PRO', 'BRA', 'TYP']	['Brand', 'language model', 'maximum entropy', 'Product Type', 'Product']
714	 Rosario and Hearst (2001) and Rosario, Hearst, and Fillmore (2002) classify nounmodifier relations in the medical domain, using Medical Subject Headings (MeSH) and Unified Medical Language System (UMLS) as lexical resources for representing each	[[155, 159], [198, 202]]	[[129, 153], [165, 196]]	['MeSH', 'UMLS']	['Medical Subject Headings', 'Unified Medical Language System']
715	and in part by the TerraSwarm Research Center, one of six centers supported by the STARnet phase of the Focus Center Research Program (FCRP) a Semiconductor Research Corporation program sponsored by	[[135, 139], [83, 90]]	[[104, 133]]	['FCRP', 'STARnet']	['Focus Center Research Program']
716	Name Discrimination by Clustering Similar Contexts, Proceedings of the World Wide Web Conference (WWW). 	[[98, 101]]	[[71, 85]]	['WWW']	['World Wide Web']
717	comparability between documents that are  chosen for exploitation in the current research,  are Mutual Infromation (MI) and Normalized  Mutual Infroamtion (NMI).	[[116, 118], [156, 159]]	[[96, 114], [124, 154]]	['MI', 'NMI']	['Mutual Infromation', 'Normalized  Mutual Infroamtion']
718	HG-ALN 0.266 0.359 Table 1: The Pk and WindowDiff scores of uniform segmentation (UNI), TextTiling (TT), baseline alignment (B-ALN), and alignment with hier-	[[100, 102], [82, 85], [0, 6], [125, 130]]	[[88, 98], [60, 67], [105, 123]]	['TT', 'UNI', 'HG-ALN', 'B-ALN']	['TextTiling', 'uniform', 'baseline alignment']
719	not vacillate, vacillate is, line vacillate?  English Context(EC): shake/vacillate  Putting on Search Engine and getting counts:  	[[62, 64]]	[[46, 60]]	['EC']	['English Contex']
720	represented in Table 3 based on the place (bilabial  (BL), lab-dental (LD), dental (DE), alveopalatal  (AP), velar (VL), uvular (UV) and glottal (GT))  and manner of articulation (stops (ST), fricatives 	[[146, 148], [54, 56], [71, 73], [84, 86], [104, 106], [116, 118], [129, 131], [187, 189]]	[[137, 144], [43, 51], [59, 69], [76, 82], [89, 101], [109, 114], [121, 127], [180, 185]]	['GT', 'BL', 'LD', 'DE', 'AP', 'VL', 'UV', 'ST']	['glottal', 'bilabial', 'lab-dental', 'dental', 'alveopalatal', 'velar', 'uvular', 'stops']
721	pendency and constituency parsing.  2.4.1 On Dependency Parsing (DP) ?	[[65, 67]]	[[45, 63]]	['DP']	['Dependency Parsing']
722	cjlin/libsvm/ System P R F1 Schwartz & Hearst (SH) .978 .940 .959 SaRAD .891 .919 .905	[[47, 49]]	[[28, 45]]	['SH']	['Schwartz & Hearst']
723	targeted text. This type of question has been studied extensively in the Text Retrieval Conference Question Answering (QA) Track (Dang, Kelly, and Lin 2007). Using the	[[119, 121]]	[[99, 117]]	['QA']	['Question Answering']
724	Table 5 shows the results when experimenting with various tree structures (see columns 2-5): (i) the basic tree (BT), (ii) the basic tree augmented with part-of-speech information (BTP), (iii) shallow syntactic tree (ShT), and (iv) syntactic tree (ST). We	[[217, 220], [113, 115], [181, 184], [248, 250]]	[[193, 215], [101, 111], [127, 179], [232, 246]]	['ShT', 'BT', 'BTP', 'ST']	['shallow syntactic tree', 'basic tree', 'basic tree augmented with part-of-speech information', 'syntactic tree']
725	"as a statistical model of natural anguage, t and weak-  eus Jelinek et al's contention that ""in an ambiguous  but appropriately chosen probabilistic CFG (PCFG),  correct parses are Ifigh probability parses"" (p. 2)."	[[154, 158]]	[[135, 152]]	['PCFG']	['probabilistic CFG']
726	each other.  Normalized common neighbors (NCN). Nor-	[[42, 45]]	[[13, 40]]	['NCN']	['Normalized common neighbors']
727	Figure 3: A Graphical Representation of the Infinite Tree Model than a simple Dirichlet process (DP)2 (Ferguson, 1973) is that we have to introduce coupling across	[[97, 99]]	[[78, 95]]	['DP']	['Dirichlet process']
728	LOC(at. IN) The ACT (Actor) can be any noun in the subjective case (the abbreviation n), the PAT (Patient)	[[16, 19], [0, 3], [93, 96]]	[[21, 26], [98, 105]]	['ACT', 'LOC', 'PAT']	['Actor', 'Patient']
729	ing the following measures:   1. PrecisionCorrectTransliterations(PTrans)  2.	[[66, 72]]	[[33, 64]]	['PTrans']	['PrecisionCorrectTransliteration']
730	Other formats have been suggested for dictionary sharing,  notably those developed under the Text Encoding Initiative  using SGML (Standard Generalized Markup Language). We 	[[125, 129]]	[[131, 167]]	['SGML']	['Standard Generalized Markup Language']
731	the global normalization of random field models,  and avoid the label bias problem that exists in  maximum entropy Markov models (MEMMs). 	[[130, 135]]	[[99, 128]]	['MEMMs']	['maximum entropy Markov models']
732	= li), ? Hierarchical loss (H-Loss) function is defined as:	[[28, 34]]	[[9, 26]]	['H-Loss']	['Hierarchical loss']
733	UniProt the protein sequence database managed by the Swiss Institute of Bioinformatics (SIB), the European Bioinformatics Institute (EBI) and the Protein Information Resource (PIR)	[[133, 136], [88, 91], [176, 179]]	[[98, 131], [53, 85], [146, 174]]	['EBI', 'SIB', 'PIR']	['European Bioinformatics Institute', 'Swiss Institute of Bioinformatic', 'Protein Information Resource']
734	In ? Proceedings of  the Eighth Text REtrieval Conference (TREC-9)?, 	[[59, 65]]	[[25, 57]]	['TREC-9']	['Eighth Text REtrieval Conference']
735	Hence, any numerical optimization strategy may be used and practical solutions include limited memory BFGS (L-BFGS) (Liu and Nocedal, 1989), which is used in the popu-	[[108, 114]]	[[87, 106]]	['L-BFGS']	['limited memory BFGS']
736	In Proceedings of the Eighteenth International Conference on Machine Learning (ICML), pages 282?289. 	[[79, 83]]	[[33, 77]]	['ICML']	['International Conference on Machine Learning']
737	input format. For instance, if the input is annotated with word and PoS (WP), so must be the translation model.	[[73, 75]]	[[59, 71]]	['WP']	['word and PoS']
738	in their absence. If both people are aware of the event, we call it Interaction (INR): for example, one person is telling the other a story.	[[81, 84]]	[[68, 79]]	['INR']	['Interaction']
739	2,5 y1,5 Figure 1: The Partial Lattice MRF (PL-MRF) Model for a 5-word sentence and a 4-layer lattice.	[[44, 50]]	[[23, 42]]	['PL-MRF']	['Partial Lattice MRF']
740	several systems have been developed such as BITS (Bilingual Internet Test Search) (Ma and Liberman, 1999), PTMiner (Parallel Text Miner) (Chen and Nie, 2000), and STRAND (Structural Translation	[[107, 114], [44, 48], [163, 169]]	[[116, 135], [50, 80], [171, 193]]	['PTMiner', 'BITS', 'STRAND']	['Parallel Text Miner', 'Bilingual Internet Test Search', 'Structural Translation']
741	583  Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 1?7, New York City, New York, June 2006.	[[68, 72], [86, 95]]	[[24, 66]]	['ACTS', 'HLT-NAACL']	['Analyzing Conversations in Text and Speech']
742	Secondly, there is the group of speech particles which are not part of the core sentence construction,  yet pragmatically cannot stand on their own. These 'sentence external' (SE) elements can be subclassified into two classes.	[[176, 178]]	[[156, 173]]	['SE']	['sentence external']
743	various evidential features are proposed and  integrated effectively and efficiently through a  Hidden Markov Model (HMM). In addition, a 	[[117, 120]]	[[96, 115]]	['HMM']	['Hidden Markov Model']
744	thematic structure, and defined well formedness conditions on the thematic structure and on the relation between thematic structure (TH) and syntactic dominance (ID) structure.	[[133, 135], [162, 164]]	[[113, 121], [0, 8]]	['TH', 'ID']	['thematic', 'thematic']
745	A set of the hyponym candidates extracted from a single itemization or list is called a hyponym candidate set (HCS). For the itemization	[[111, 114]]	[[88, 109]]	['HCS']	['hyponym candidate set']
746	 1 Introduction Question answering (QA) systems have received a great deal of attention because they provide both	[[36, 38]]	[[16, 34]]	['QA']	['Question answering']
747	Table 2: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer, McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary	[[138, 140], [73, 75], [94, 97], [182, 188], [205, 210], [245, 247], [280, 284]]	[[141, 157], [76, 92], [98, 125], [172, 180], [189, 195], [211, 236], [248, 267], [285, 295]]	['ML', 'BI', 'NLP', 'Porter', 'McCCJ', 'SD', 'Dict']	['Machine Learning', 'Bioinformatician', 'Natural Language Processing', 'Linguist', 'Porter', 'McClosky-Charniak-Johnson', 'Stanford Dependency', 'Dictionary']
748	 2.6 Adverb  Group (AdvG)   Adverb group (AdvG) is used in the realization  of several circumstantial functions given in Sec- 	[[42, 46], [20, 24]]	[[28, 40]]	['AdvG', 'AdvG']	['Adverb group']
749	The divergence of the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-) divergence (Cover and Thomas 1991): Fixednesssyn (v, n)	[[136, 139]]	[[118, 134]]	['KL-']	['Kullback Leibler']
750	changes and there is a complement particle between complement constituents(COP)  and verbs. So in the information dictionary, the characteristics of {V(CHA), VP+COP}  should be described.	[[152, 155], [75, 78], [161, 164], [158, 160]]	[[130, 145], [51, 73]]	['CHA', 'COP', 'COP', 'VP']	['characteristics', 'complement constituent']
751	A List of POS-tags ADJ (adjectives), ADV (adverbs), CJ (conjunctions), CL (clitics), CN (common nouns), DA (definite articles), DEM (demonstratives),	[[71, 73], [85, 87], [10, 13], [19, 22], [37, 40], [52, 54], [104, 106], [128, 131]]	[[75, 82], [89, 101], [24, 34], [42, 49], [56, 68], [108, 125], [133, 147]]	['CL', 'CN', 'POS', 'ADJ', 'ADV', 'CJ', 'DA', 'DEM']	['clitics', 'common nouns', 'adjectives', 'adverbs', 'conjunctions', 'definite articles', 'demonstratives']
752	3.2 Machine Learning Framework SVM-Light (Joachims, 1999), an implementation of Support Vector Machines (SVM), is used for the classification task.	[[105, 108], [31, 40]]	[[80, 103]]	['SVM', 'SVM-Light']	['Support Vector Machines']
753	social media. In Proceedings of the ACM International Conference on Web Search and Data Mining (WSDM), pages 291?300.	[[96, 100]]	[[68, 94]]	['WSDM']	['Web Search and Data Mining']
754	of Excellence and the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.	[[133, 137], [65, 70]]	[[102, 131], [22, 63]]	['AFRL', 'DARPA']	['Air Force Research Laboratory', 'Defense Advanced Research Projects Agency']
755	particular components - -  immediate dominance (ID) rules, meta-  rules, linear precedence (LP) statements, feature co-occurrence  restrictions (FCRs), and feature specification defaults (FSDs)  - -  and four universal components - - a theory of syntactic fea- 	[[188, 192], [48, 50], [92, 94], [145, 149]]	[[156, 186], [27, 46], [73, 90], [108, 143]]	['FSDs', 'ID', 'LP', 'FCRs']	['feature specification defaults', 'immediate dominance', 'linear precedence', 'feature co-occurrence  restrictions']
756	we describe SCANMail, a system that employs automatic speech recognition (ASR), information retrieval (IR), information extraction (IE), and human computer interaction (HCI) technology to permit users to browse and search their voicemail messages by content	[[169, 172], [12, 20], [74, 77], [103, 105], [132, 134]]	[[141, 167], [44, 72], [80, 101], [108, 130]]	['HCI', 'SCANMail', 'ASR', 'IR', 'IE']	['human computer interaction', 'automatic speech recognition', 'information retrieval', 'information extraction']
757	characterize the AAV functions mediating this effect, cloned AAV type 2 wild-type or mutant genomes were transfected into simian virus 40 (SV40)-transformed hamster cells together with the six HSV replication genes	[[139, 143], [17, 20], [61, 64], [193, 196]]	[[122, 137]]	['SV40', 'AAV', 'AAV', 'HSV']	['simian virus 40']
758	grammatical features from a diachronic corpus of academic English, and visualise our extraction results with Structured Parallel Coordinates (SPC), a tool for the visualisation of structured multidi-	[[142, 145]]	[[109, 140]]	['SPC']	['Structured Parallel Coordinates']
759	Abstract This paper reports on the participation of the TALP Research Center of the UPC (Universitat Polit?cnica de Catalunya) to the ACL WMT 2008 evaluation	[[84, 87], [56, 60], [134, 137], [138, 141]]	[[89, 112]]	['UPC', 'TALP', 'ACL', 'WMT']	['Universitat Polit?cnica']
760	lar, at the level of additional information (CR), we observed some differences in judgement in particular between restrictions (AR) and warnings (AA), and a few others between CSFH and CSFC whose	[[146, 148], [45, 47], [128, 130], [176, 180], [185, 189]]	[]	['AA', 'CR', 'AR', 'CSFH', 'CSFC']	[]
761	"onomy using a Japanese-English bilingual dictionary as  a ""bridge"", in order to support semantic processing in a  knowledge-based machine translation (MT) system. "	[[151, 153]]	[[130, 149]]	['MT']	['machine translation']
762	tion string in discourse: PER, GPE, ORG, LOC,  FAC, NPER (NOMINAL PER), NGPE, NORG,  NLOC, NFAC, PPER (PROUNOUN PER),  PGPE, PORG, PLOC, and PFAC.	[[97, 101], [26, 29], [31, 34], [36, 39], [41, 44], [47, 50], [52, 56], [72, 76], [78, 82], [85, 89], [91, 95], [119, 123], [125, 129], [131, 135], [141, 145]]	[[103, 115], [58, 69]]	['PPER', 'PER', 'GPE', 'ORG', 'LOC', 'FAC', 'NPER', 'NGPE', 'NORG', 'NLOC', 'NFAC', 'PGPE', 'PORG', 'PLOC', 'PFAC']	['PROUNOUN PER', 'NOMINAL PER']
763	The unspecified role filler is not 'bound'  to a complement (i.e. any item on the SUBCAT list)  but is existentially quantified (EX-Q). The ergative 	[[129, 133], [82, 88]]	[[103, 127]]	['EX-Q', 'SUBCAT']	['existentially quantified']
764	Non-MonoClausal Verbs (NMCV), Passives  (Pass) and Auxiliary Construction (AC) that  are identified as compound verbs (CompVs). 	[[119, 125], [23, 27], [41, 45], [75, 77]]	[[103, 117], [0, 21], [30, 38], [51, 73]]	['CompVs', 'NMCV', 'Pass', 'AC']	['compound verbs', 'Non-MonoClausal Verbs', 'Passives', 'Auxiliary Construction']
765	3.1 The Information Extraction Pipeline  The extraction task we are addressing is that of the  Automatic Content Extraction (ACE)1 evaluations. 	[[125, 128]]	[[95, 123]]	['ACE']	['Automatic Content Extraction']
766	tions (Abe et al, 1996).  1.1 Question answering (QA) Unlike IR systems which return a list of documents	[[50, 52]]	[[30, 48]]	['QA']	['Question answering']
767	helpful in identifying the embedded words. However  some ambiguous segmentation strings(ASSs) and  unregistered words (i.e. the word that is not registered 	[[88, 92]]	[[57, 87]]	['ASSs']	['ambiguous segmentation strings']
768	 2.2 Hierarchical Softmax Hierarchical Softmax (HSM) organizes the output vocabulary into a tree where the leaves are	[[48, 51]]	[[26, 46]]	['HSM']	['Hierarchical Softmax']
769	In the next experiments, we run experiments across three different locales in Places domain: United Kingdom (GB), Australia (AU), and India (IN).	[[125, 127], [109, 111], [141, 143]]	[[114, 123], [93, 107], [134, 139]]	['AU', 'GB', 'IN']	['Australia', 'United Kingdom', 'India']
770	ing and understanding convolutional networks. In European Conference on Computer Vision (ECCV). 	[[89, 93]]	[[49, 87]]	['ECCV']	['European Conference on Computer Vision']
771	tion (IE). Considerable volume of location data was imported in a knowledge base (KB) with entities of general importance used for seman-	[[82, 84], [6, 8]]	[[66, 80]]	['KB', 'IE']	['knowledge base']
772	multilingual MT system developed at the Laboratoire d'Analyse et de Technologie du Langage (LATL), University of Geneva. 	[[92, 96], [13, 15]]	[[40, 90]]	['LATL', 'MT']	"[""Laboratoire d'Analyse et de Technologie du Langage""]"
773	assignment. We use a generative model based on a Dirichlet Process (DP) defined over composed rules.	[[68, 70]]	[[49, 66]]	['DP']	['Dirichlet Process']
774	 Because of data sparseness, we cannot reliably use a  maximum likelihood estimator (MLE) for bigram prob-  abilities.	[[85, 88]]	[[55, 83]]	['MLE']	['maximum likelihood estimator']
775	Frustratingly easy domain adaptation.  In Association for Computational Linguistics (ACL). 	[[85, 88]]	[[42, 83]]	['ACL']	['Association for Computational Linguistics']
776	and the speech. The SU detection task is evaluated on both the reference human transcriptions (REF) and speech recognition outputs (STT).	[[95, 98], [20, 22], [132, 135]]	[[63, 72], [104, 130]]	['REF', 'SU', 'STT']	['reference', 'speech recognition outputs']
777	Each corpus uses a different set of entity labels.  MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numeri-	[[73, 76], [94, 97], [52, 55], [119, 122]]	[[62, 71], [79, 92], [103, 111]]	['LOC', 'ORG', 'MUC', 'PER']	['locations', 'organisations', 'personal']
778	Baselines and Evaluation We compare prediction results with a set of single-task baselines: a Support Vector Machine (SVM) using an RBF kernel with hyperparameters optimised via cross-	[[118, 121], [132, 135]]	[[94, 116]]	['SVM', 'RBF']	['Support Vector Machine']
779	perform rather well because the combined recency  bias representation worked well on its own and be-  cause the restricted memory (RM) bias essentially  discards features that are distant from the relative 	[[131, 133]]	[[112, 129]]	['RM']	['restricted memory']
780	that is neither terminal nor lexical. An interior node is  said to meet he foot condition (FC) iffeach foot feature  that it contains appears also on at least one daughter 	[[91, 93]]	[[75, 89]]	['FC']	['foot condition']
781	However, a study regarding maximal recall shows that we do not remove too many true positives (TPs) (more details in Section 4.1).	[[95, 98]]	[[79, 93]]	['TPs']	['true positives']
782	frequency pairs, Erk et al(2010) drew (R, t)  pairs from each of five frequency bands in the  entire British National Corpus (BNC):  50-100  occurrences; 101-200; 201-500; 500-1000; and 	[[126, 129]]	[[101, 124]]	['BNC']	['British National Corpus']
783	details of this collection.  AECMA Simplified English (AECMA-SE) (AECMA 1986) was the predecessor of ASD Simplified Technical English.	[[55, 63], [66, 71], [101, 104]]	[[29, 53]]	['AECMA-SE', 'AECMA', 'ASD']	['AECMA Simplified English']
784	an efficient bottom-up algorithm. The asymptotic time complexity of this search is O(SR) where S is the number of source nodes andR is the number	[[85, 87]]	[[73, 79]]	['SR']	['search']
785	  2. PrecisionCorrectTransliteration  (PTrans)  The precision is going to be computed using the 	[[39, 45]]	[[5, 36]]	['PTrans']	['PrecisionCorrectTransliteration']
786	The parameters ? are estimated through the optimization of a Maximum Likelihood (ML) criterion using the Expectation-Maximization (EM) al-	[[81, 83], [131, 133]]	[[61, 79], [105, 129]]	['ML', 'EM']	['Maximum Likelihood', 'Expectation-Maximization']
787	2 We use a new related measure, which we call the overall percentage error reduction (OPER), that uses the entire area under the curves given by	[[86, 90]]	[[50, 84]]	['OPER']	['overall percentage error reduction']
788	 1 Introduction Massive Open Online Courses (MOOCs), run by organizations such as Coursera, have been among	[[45, 50]]	[[16, 43]]	['MOOCs']	['Massive Open Online Courses']
789	of a multi-class document categorization. We introduce PRBEP (precision recall break even point) as a measure which is popular in the area of infor-	[[55, 60]]	[[62, 95]]	['PRBEP']	['precision recall break even point']
790	erature for further details.  Semantic Role Labeling (SRL) Our first task is that of identifying arguments of verbs in a sen-	[[54, 57]]	[[30, 52]]	['SRL']	['Semantic Role Labeling']
791	\[Harman, 1996\] D. Harman. Overview of the Forth  Text RetrievalConference (TREC-4). In Proceedings 	[[77, 83]]	[[44, 75]]	['TREC-4']	['Forth  Text RetrievalConference']
792	languages.  Mutual Information (MI)  For the purpose of this experiment we use a 	[[32, 34]]	[[12, 30]]	['MI']	['Mutual Information']
793	Schafer and Graham, 2002) discussed several approaches such as case deletion, mean substitution, and recommended maximum likelihood (ML) and Bayesian multiple imputation (MI).	[[133, 135], [171, 173]]	[[113, 131], [150, 169]]	['ML', 'MI']	['maximum likelihood', 'multiple imputation']
794	Figure 3: Dialogue system architecture    The Dialogue Manager (DM) uses sceneobject attributes, such as type, angle or interval 	[[64, 66]]	[[46, 62]]	['DM']	['Dialogue Manager']
795	of the first studies to investigate such constancy is Genzel and Charniak (2002), in which the authors proposed the Constant Entropy Rate (CER) hypothesis: in written text, the entropy per sig-	[[139, 142]]	[[116, 137]]	['CER']	['Constant Entropy Rate']
796	 In this definition, WHEN has two formal parameters x and y; each of them refers to a  situation that occurs at the same point in time (PTIM). Any occurrence of the relation 	[[136, 140]]	[[121, 134]]	['PTIM']	['point in time']
797	Central to the predictive dialogue is the topic representation for each scenario, which enables the population of a Predictive Dialogue Network (PDN). 	[[145, 148]]	[[116, 143]]	['PDN']	['Predictive Dialogue Network']
798	88 lated based on a co-occurrence relationship between i and w. Next, the semantic orientation (SO) of the phrase i is obtained by calculating the difference be-	[[96, 98]]	[[74, 94]]	['SO']	['semantic orientation']
799	These  probabilities could be estimated from training cases  with Maximum Likelihood Estimation (MLE). Let l 	[[97, 100]]	[[66, 95]]	['MLE']	['Maximum Likelihood Estimation']
800	 1 Introduction Automatic evaluation of machine translation (MT) quality is essential to developing high-quality ma-	[[61, 63]]	[[40, 59]]	['MT']	['machine translation']
801	"Semitic languages (in which vowels are not written), etc.  The problem of word sense disambiguation (WSD) has been described as ""AI-complete,""  that is, a problem which can be solved only by first resolving all the difficult problems "	[[101, 104]]	[[74, 99]]	['WSD']	['word sense disambiguation']
802	ajaynagesh@cse.iitb.ac.in Abstract Information Extraction (IE) has become an indispensable tool in our quest to handle the data	[[59, 61]]	[[35, 57]]	['IE']	['Information Extraction']
803	Because LSA is closely related to principle component analysis (PCA), extensions of PCA such as canonical correlation analysis (CCA) and oriented principle component analysis (OPCA) can leverage	[[128, 131], [8, 11], [64, 67], [84, 87], [176, 180]]	[[96, 126], [34, 62], [137, 174]]	['CCA', 'LSA', 'PCA', 'PCA', 'OPCA']	['canonical correlation analysis', 'principle component analysis', 'oriented principle component analysis']
804	Perhaps not surprisingly, therefore, few natural language understanding (NLU) systems use graphical presentational features to aid interpretation, and few natural language generation (NLG) systems attempt to render the output texts in a principled way.	[[184, 187], [73, 76]]	[[155, 182], [41, 71]]	['NLG', 'NLU']	['natural language generation', 'natural language understanding']
805	The implementations of the oracles described in the first part of this work (sections 3 and 4) use the common formalism of finite state acceptors (FSA) over different semirings and are implemented us-	[[147, 150]]	[[123, 145]]	['FSA']	['finite state acceptors']
806	 NB-B uses full Bayesian inference and NB-M uses Maximum a posteriori (MAP). 	[[71, 74]]	[[49, 69]]	['MAP']	['Maximum a posteriori']
807	The general idea of the main algorithm is to align phrase synsets from the Patty taxonomy with verb synsets in WordNet. To this end, we first construct a directed candidate alignment graph (CAG). Section	[[190, 193]]	[[163, 188]]	['CAG']	['candidate alignment graph']
808	rently, a pure statistical MT system based on Pharaoh is developed by BPPT and National News  Agency (ANTARA) using 500K sentences pair,  expected to have better accuracy and robustness 	[[102, 108], [27, 29], [70, 74]]	[[79, 100]]	['ANTARA', 'MT', 'BPPT']	['National News  Agency']
809	NP and selection of the head NP by the relative 1The following abbreviations are used in glosses: NOM = nominative, ACC = accusative, PRES = non-past and POT = potential. ( )	[[116, 119], [29, 31], [0, 2], [98, 101], [134, 138], [154, 157]]	[[122, 132], [104, 114], [160, 169]]	['ACC', 'NP', 'NP', 'NOM', 'PRES', 'POT']	['accusative', 'nominative', 'potential']
810	Entropy Guided Transformation Learning (ETL) is a new machine learning strategy that combines the advantages of Decision Trees (DT) and Transformation-Based Learning (TBL) (dos Santos	[[128, 130], [40, 43], [167, 170]]	[[112, 126], [0, 38], [136, 165]]	['DT', 'ETL', 'TBL']	['Decision Trees', 'Entropy Guided Transformation Learning', 'Transformation-Based Learning']
811	The TRIANGLE application  (TRIANGLE) and a new Windows 95  Accessible Graphing Calculator (ACG) both  developed by the SAP uses tone plots to 	[[91, 94], [27, 35], [119, 122]]	[[59, 78], [4, 12]]	['ACG', 'TRIANGLE', 'SAP']	['Accessible Graphing', 'TRIANGLE']
812	 Bidirectional CLSTM Graves and Schmidhuber (2005) proposed a Bidirectional LSTM (B-LSTM) model, which utilizes additional backward informa-	[[82, 88], [15, 20]]	[[62, 80]]	['B-LSTM', 'CLSTM']	['Bidirectional LSTM']
813	Run 3 100% 18 (9.0%) 38 (19.0%)  Table 7.  Effect of Translation (E-C)   	[[66, 69]]	[[43, 49]]	['E-C']	['Effect']
814	UMichigan non-Tipster UNK X USouthern California non-Tipster SNAP X USussex (UK) non-Tipster SUSSEX X Table 1 .	[[77, 79], [22, 25], [61, 65], [93, 99]]	[[68, 75]]	['UK', 'UNK', 'SNAP', 'SUSSEX']	['USussex']
815	Adverb Variation (AdvV) ? Modifier Variation (ModV) ?	[[46, 50]]	[[26, 44]]	['ModV']	['Modifier Variation']
816	 ? WHNP_NN_IN  Syntactic Parse Trees (PT)  72	[[38, 40], [3, 13]]	[[25, 36]]	['PT', 'WHNP_NN_IN']	['Parse Trees']
817	2. TREE ADJO IN ING GRAMMARS- -TAG's   We now introduce tree adjoining grammars (TAG's). TAG's 	[[81, 86], [31, 36], [89, 94]]	[[56, 79]]	"[""TAG's"", ""TAG's"", ""TAG's""]"	['tree adjoining grammars']
818	proach to automatically recognize predicate  heads of Chinese sentences based on a preprocessing step for maximal noun phrases 1(MNPs). 	[[129, 133]]	[[106, 126]]	['MNPs']	['maximal noun phrases']
819	entire search process.  (b) EPN for the first optimum solution (EPN-F): The number of the expanded problems when	[[64, 69]]	[[28, 45]]	['EPN-F']	['EPN for the first']
820	an ASR system. The main idea was to design a language model (LM) to combine the trigram language model probability with the translation	[[61, 63], [3, 6]]	[[45, 59]]	['LM', 'ASR']	['language model']
821	summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) features. 	[[103, 106]]	[[89, 101]]	['BOW']	['bag of words']
822	city. City and state can then be used to select a local area language model (LM) for recognizing listing names.	[[77, 79]]	[[61, 75]]	['LM']	['language model']
823	We trained the UKP machine learning classifier originally developed for the Semantic Textual Similarity (STS) task at SemEval-2012 (B?r et al 2012) on the averaged binary and senary judge-	[[105, 108], [15, 18], [118, 130]]	[[76, 103]]	['STS', 'UKP', 'SemEval-2012']	['Semantic Textual Similarity']
824	4.3 Classifiers All evaluation tests were performed using two classifiers, Maximum Entropy (MaxEnt) and Support Vector Machines (SVM).	[[92, 98], [129, 132]]	[[75, 90], [104, 127]]	['MaxEnt', 'SVM']	['Maximum Entropy', 'Support Vector Machines']
825	375 that...). Extreme case formulations (ECF) are lexical patterns emphasizing extremeness (e.g., This is	[[41, 44]]	[[14, 39]]	['ECF']	['Extreme case formulations']
826	 NPHIL = Stray NP: Volume I: Syntax,  SUBJ = Subject: H__~e reads., 	[[38, 42]]	[[45, 52]]	['SUBJ']	['Subject']
827	In this paper, we describe an initial  implementation of a general spoken language interface, the  Carnegie Mellon Spoken Language Shell (CM-SLS) which  provides voice interface services to a variable number of applica- 	[[138, 144]]	[[99, 136]]	['CM-SLS']	['Carnegie Mellon Spoken Language Shell']
828	implicit (assuming a good enough coverage of the marker resource). The Annodis corpus lists rhetorical relations between elementary discourse units (EDUs), typically clauses, and complex discourse units (sets of EDUs) ; as a simplification we only consider EDUs, since the question of what is a main verb of	[[149, 153], [212, 216], [257, 261]]	[[121, 147]]	['EDUs', 'EDUs', 'EDUs']	['elementary discourse units']
829	among other ways). We refer to cases in which the subject and object of the relation are contained within a phrase headed by a Noun as Relational NP?s (RNP). 	[[152, 155]]	[[135, 150]]	['RNP']	['Relational NP?s']
830	been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents.	[[160, 163]]	[[125, 158]]	['ISS']	['iterative semantic specialization']
831	 ? Left Reveal (LRev) : Pop the top two nodes in the stack (left, right).	[[16, 20]]	[[3, 14]]	['LRev']	['Left Reveal']
832	 ? MEA+LexPageRank (MEALR) : This method applies the proposed mixture-event-aspect model to	[[20, 25]]	[[3, 18]]	['MEALR']	['MEA+LexPageRank']
833	To experiment on MAYA, we compute the  performance score as the Reciprocal Answer  Rank (RAR) of the first correct answer given by  each question.	[[89, 92], [17, 21]]	[[83, 87]]	['RAR', 'MAYA']	['Rank']
834	  ? PT (parse tree)  	[[4, 6]]	[[8, 18]]	['PT']	['parse tree']
835	gle word maze); B-M (beginning of multi-word 72 maze); I-M (in multi-word maze); and E-M (end of multi-word maze).	[[55, 58], [16, 19], [85, 88]]	[[60, 68], [21, 39], [90, 103]]	['I-M', 'B-M', 'E-M']	['in multi', 'beginning of multi', 'end of multi-']
836	(wine) is localized to exterior locus (barrel) and crosses the intermediate locus IME(LOC) to be localized to the interior INT(LOC) (the_bottle). 	[[127, 130], [86, 89], [82, 85], [123, 126]]	[[97, 106], [76, 81]]	['LOC', 'LOC', 'IME', 'INT']	['localized', 'locus']
837	 2 System Overview Our system, named PML Tree Query (PML-TQ), consists of three main components (discussed fur-	[[53, 59]]	[[37, 51]]	['PML-TQ']	['PML Tree Query']
838	Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al (2006) have shown that a shift-reduce parser can give	[[72, 75]]	[[54, 70]]	['CTB']	['Chinese Treebank']
839	NEARING A PROJECT PROPOSAL TO OTTA BOARD  As the  Urns,,. C6ngress Office of Techdblogy Assessment (OTA) planning  sttidy on te lecomunica t ions ,  computers and information p o l i c i e s  approaches 	[[100, 103], [30, 34]]	[[67, 98]]	['OTA', 'OTTA']	['Office of Techdblogy Assessment']
840	1 Introduction In this paper, we describe our submission to the Genia Event (GE) information extraction subtask of the BioNLP Shared Task.	[[77, 79], [119, 125]]	[[64, 75]]	['GE', 'BioNLP']	['Genia Event']
841	"1 N 79.4 80.6 1.89 46.2 74.5 2 Y 80.8 81.4 1.70 44.3 80.4 Table 1: Results for parsing section 0 (   40 words) of the WSJ Penn Treebank: OP = overparsing, LP/LR = labelled precision/recall."	[[137, 139], [118, 121], [155, 160]]	[[142, 153], [163, 188]]	['OP', 'WSJ', 'LP/LR']	['overparsing', 'labelled precision/recall']
842	guished in Boxer: 1. Discourse Representation Structures (DRSs) 2.	[[58, 62]]	[[21, 56]]	['DRSs']	['Discourse Representation Structures']
843	 3.3 Linear-chain CRF for Extraction The alignment CRF (AlignCRF) model described in Section 3.1 is able to predict labels for a text	[[56, 64], [18, 21]]	[[41, 54]]	['AlignCRF', 'CRF']	['alignment CRF']
844	Table 2: The evaluation for each combination of agents. LB = ListenerBot; DB = DialogBot. 	[[56, 58], [74, 76]]	[[61, 72], [79, 88]]	['LB', 'DB']	['ListenerBot', 'DialogBot']
845	 3.1 Annotating message-level data Amazon?s Mechanical Turk (MTurk) was used to annotate the 5,000 random English (American)	[[61, 66]]	[[44, 59]]	['MTurk']	['Mechanical Turk']
846	Abstract This paper introduces a tensor-based approach to semantic role labeling (SRL). The motiva-	[[82, 85]]	[[58, 80]]	['SRL']	['semantic role labeling']
847	1. INTRODUCTION  Hidden Markov Models (HMMs) have been used suc-  cessfully in a wide variety of recognition tasks ranging 	[[39, 43]]	[[17, 37]]	['HMMs']	['Hidden Markov Models']
848	course structure. In Proceedings of the International Conference in Computational Linguistics (COLING), pages 43?49. 	[[95, 101]]	[[68, 93]]	['COLING']	['Computational Linguistics']
849	namely, the set {? ( D)d |ad = a}, where ad is the author of document d. An alternative approach is LDA-S (LDA with a single document per author), where each author?s documents are concatenated into a single document in a preprocessing step, LDA is run	[[100, 105], [242, 245]]	[[107, 124]]	['LDA-S', 'LDA']	['LDA with a single']
850	It has been shown in (Ando and Zhang, 2005a) that the optimization problem (3) has a simple solution using singular value decomposition (SVD) when we choose square regularization: r(f	[[137, 140]]	[[107, 135]]	['SVD']	['singular value decomposition']
851	complementary features ( Section 3.3).  3.2 Mining Labeled Sequential Patterns ( LSP ) Labeled Sequential Patterns (LSP).	[[81, 84], [115, 119]]	[[51, 78], [87, 114]]	['LSP', '(LSP']	['Labeled Sequential Patterns', 'Labeled Sequential Patterns']
852	"class of mildly context sensitive grsmmars which we  230  call ""Ranked Node Rewriting Grammaxs"" (RNRG's). "	[[97, 103]]	[[64, 94]]	"[""RNRG's""]"	['Ranked Node Rewriting Grammaxs']
853	we often decompose the global probability of sequences using a directed graphical model (e.g., an HMM (Brants, 2000) or a conditional Markov model (CMM) (Ratnaparkhi, 1996)).	[[148, 151], [98, 101]]	[[122, 146]]	['CMM', 'HMM']	['conditional Markov model']
854	To this end, we employ the  27 Adaptive Hierarchical Density Histograms (AHDH) as visual feature vectors, due to the fact that they  have shown discriminative power between binary complex drawings (Sidiropoulos et al.,	[[73, 77]]	[[31, 71]]	['AHDH']	['Adaptive Hierarchical Density Histograms']
855	actual dependency annotations cheaply. We use the Graph Fragment Language (GFL), which was created with the goal of making annotations eas-	[[75, 78]]	[[50, 73]]	['GFL']	['Graph Fragment Language']
856	with parts of the annotated logical form.   ZC07 (Zettlemoyer and Collins 2007) extends ZC05 with extra (disharmonic) combinators to increase the expressive power of the model.	[[45, 49], [89, 93]]	[[51, 79]]	['ZC07', 'ZC05']	['Zettlemoyer and Collins 2007']
857	 1 Introduction Referring Expression Generation (REG) is a keytask in NLG, and the topic of the REG 2008 Chal-	[[49, 52], [70, 73], [96, 99]]	[[16, 47]]	['REG', 'NLG', 'REG']	['Referring Expression Generation']
858	Our data comes from English (ENG), Chinese (CHI), Portuguese (POR), and Kinyarwanda (KIN). 	[[85, 88], [29, 32], [44, 47], [62, 65]]	[[72, 83], [20, 27], [35, 42], [50, 60]]	['KIN', 'ENG', 'CHI', 'POR']	['Kinyarwanda', 'English', 'Chinese', 'Portuguese']
859	Beijing University of Posts and Telecommunications (BUPT) ? ?  Beijing Institute of Technology (BIT) ?  	[[96, 99], [52, 56]]	[[63, 94], [0, 49]]	['BIT', 'BUPT']	['Beijing Institute of Technology', 'Beijing University of Posts and Telecommunication']
860	the traditional k-nearest neighbor (kNN) algorithm.  Maximum a posteriori (MAP) principle is used to determine which emotion set is related to the giv-	[[75, 78], [36, 39]]	[[53, 73], [16, 34]]	['MAP', 'kNN']	['Maximum a posteriori', 'k-nearest neighbor']
861	 2.2 Recognizing subwords An automatic speech recognition (ASR) system (Jelinek, 1998) serves to recognize both queries	[[59, 62]]	[[29, 57]]	['ASR']	['automatic speech recognition']
862	In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL), pages 440?447, Hong Kong, October.	[[86, 89]]	[[43, 84]]	['ACL']	['Association for Computational Linguistics']
863	ductive transfer learning. In Proceedings of the IEEE International Conference on Data Mining (ICDM) 2007 Workshop on Mining and Management of Bio-	[[95, 99], [49, 53]]	[[54, 93]]	['ICDM', 'IEEE']	['International Conference on Data Mining']
864	"srlrr(r** TRANSPORMATIONS IC*S**  SCAN CALLED AT 1 I  ANTEST CALLED F O R  5""SYLLAB "" (AACC) ,SD= 6. RES= 11."	[[87, 91], [94, 96], [101, 104]]	[[54, 67]]	['AACC', 'SD', 'RES']	['ANTEST CALLED']
865	 ? Self-training Segmenters (STS): two variant models were defined by the approach re-	[[29, 32]]	[[17, 27]]	['STS']	['Segmenters']
866	Again, even the Table 5 Precision (PD), recall (RD), and F-measure (FD) for malapropism detection with five measures of semantic relatedness, varying the scope of the search for related words to 1, 3, or 5 paragraphs	[[35, 37], [48, 50], [68, 70]]	[[24, 33], [40, 46], [57, 66]]	['PD', 'RD', 'FD']	['Precision', 'recall', 'F-measure']
867	 2.2 Human Intelligence Tasks A Human Intelligence Task (HIT) is a short paid task on MTurk.	[[57, 60]]	[[32, 55]]	['HIT']	['Human Intelligence Task']
868	A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proc. Conference on Empirical Methods in Natural Language Processing 2009, (EMNLP-09). David Yarowsky.	[[158, 166], [82, 86]]	[[102, 155]]	['EMNLP-09', 'Proc']	['Empirical Methods in Natural Language Processing 2009']
869	on terrorism in a matter of weeks. Since the terrorism  domain knowledge bases (KB's) were developed for English  for MUC-4 already, and since the KB's can be shared across 	[[80, 84], [118, 123], [147, 151]]	[[63, 78]]	"[""KB's"", 'MUC-4', ""KB's""]"	['knowledge bases']
870	augmented by a set of PRIDES-specific Common  Gateway Interfaces (CGIs), communicates with the  client via Hypertext Transport Protocol (HTTP). A 	[[137, 141], [22, 37], [66, 70]]	[[107, 135], [38, 64]]	['HTTP', 'PRIDES-specific', 'CGIs']	['Hypertext Transport Protocol', 'Common  Gateway Interfaces']
871	In Proc. of Seventh Text REtrieval Conference (TREC-7). 	[[47, 53]]	[[12, 45]]	['TREC-7']	['Seventh Text REtrieval Conference']
872	 For this reason, NIST assessors not only marked  the segments shared between system units (SU)  and model units (MU), they also indicated the 	[[92, 94], [18, 22], [114, 116]]	[[78, 90], [101, 112]]	['SU', 'NIST', 'MU']	['system units', 'model units']
873	 The implementation of our approach is a system called LetSum (Legal text Summarizer), which has been developed in Java and Perl.	[[55, 61]]	[[63, 84]]	['LetSum']	['Legal text Summarizer']
874	model for sequence classification. In Proceedings of International Conference on Data Mining (ICDM). 	[[94, 98]]	[[53, 92]]	['ICDM']	['International Conference on Data Mining']
875	In Proceedings of the 2nd International Conference on Language Resources and Evaluation (LREC). 	[[89, 93]]	[[54, 72]]	['LREC']	['Language Resources']
876	References TREC (Text REtrieval Conference) : http://trec.nist.gov/ NTCIR (NII-NACSIS Test Collection for IR Systems) project: http://research.nii.ac.jp/ntcir/index-en.html	[[75, 85], [11, 15], [68, 73], [106, 108]]	[[17, 42]]	['NII-NACSIS', 'TREC', 'NTCIR', 'IR']	['Text REtrieval Conference']
877	As first step we try to map the linguistic triple  into an ontology triple, by using an adaptation of  Aqualog?s Relation Similarity Service (RSS).  	[[142, 145]]	[[113, 140]]	['RSS']	['Relation Similarity Service']
878	baseline (BAS) system which is close to the system described in (Hu et al 2009), and three variants of our novel divide and conquer (DAC) system. Fea-	[[133, 136], [10, 13]]	[[113, 131], [0, 8]]	['DAC', 'BAS']	['divide and conquer', 'baseline']
879	LS (List item marker) LS  MD (Modal) MD  NN (Noun, singular or mass) N  NNS (Noun, plural) N 	[[41, 43], [0, 2], [22, 24], [26, 28], [37, 39], [72, 75]]	[[45, 49], [30, 35], [77, 81]]	['NN', 'LS', 'LS', 'MD', 'MD', 'NNS']	['Noun', 'Modal', 'Noun']
880	the toolkits. Sizes are given for the resulting transducers (VM = Verbmobil). 	[[61, 63]]	[[66, 75]]	['VM']	['Verbmobil']
881	tools for the Indian Languages. For Telugu, though a Part Of Speech(POS) Tagger for Telugu, is available, the accuracy is less when compared to English	[[68, 71]]	[[53, 66]]	['POS']	['Part Of Speec']
882	determines the nature of such space.  For example, Syntactic Tree Kernel (STK) are used to model complete context free rules as in (Collins	[[74, 77]]	[[51, 72]]	['STK']	['Syntactic Tree Kernel']
883	summarization. During these intervening decades,  progress in Natural Language Processing (NLP), coupled  with great increases of computer memory and speed, 	[[91, 94]]	[[62, 89]]	['NLP']	['Natural Language Processing']
884	c?2008 Association for Computational Linguistics Dialect Classification for online podcasts fusing Acoustic and Language based Structural and Semantic Information   Rahul Chitturi, John. H.L. Hansen1 Center for Robust Speech Systems(CRSS) Erik Jonsson School of Engineering and Computer Science University of Texas at Dallas Richardson, Texas 75080, U.S.A {rahul.ch@student, john.hansen@}utdallas.edu Abstract  The variation in speech due to dialect is a factor which significantly impacts speech system per-formance.	[[233, 237], [350, 355]]	[[200, 231]]	['CRSS', 'U.S.A']	['Center for Robust Speech System']
885	Sotelo, 2007).  4.3 Polarity Lexicon (LEX) We have built a polarity lexicon with both Positive	[[38, 41]]	[[29, 36]]	['LEX']	['Lexicon']
886	tend to conflate the distances between all possible sense pairs. Latent semantic analysis (LSA) (Landauer et al, 1998) has also been used to measure dis-	[[91, 94]]	[[65, 89]]	['LSA']	['Latent semantic analysis']
887	cal patterns and the impacts of different  constraints that are used to identify the  Complex Predicates (CPs). System 	[[106, 109]]	[[86, 104]]	['CPs']	['Complex Predicates']
888	semantically interpreted.  We apply conditional random fields (CRFs) to the task of SRL proposed by the CoNLL shared	[[63, 67]]	[[36, 61]]	['CRFs']	['conditional random fields']
889	category and report genre. DS = Discharge Summary,  Echo = Echocardiogram, ED = Emergency Department,  GI = Operative Gastrointestinal, RAD = Radiology and 	[[75, 77], [27, 29], [52, 56], [103, 105], [136, 139]]	[[80, 100], [32, 49], [59, 73], [118, 134], [142, 151]]	['ED', 'DS', 'Echo', 'GI', 'RAD']	['Emergency Department', 'Discharge Summary', 'Echocardiogram', 'Gastrointestinal', 'Radiology']
890	lected randomly from some reference corpus.  Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of	[[62, 64]]	[[45, 60]]	['AL']	['Active Learning']
891	The described research is undertaken in the course of the development of human computer interfaces for natural interaction in Virtual Reality (VR). Conducting empirical inves-	[[143, 145]]	[[126, 141]]	['VR']	['Virtual Reality']
892	In Proceedings of Workshop on Setting Standards for Searching Electronically Stored Information In Discovery Proceedings (DESI-4). 	[[122, 128]]	[[99, 120]]	['DESI-4']	['Discovery Proceedings']
893	given threshold, this binary feature fires.  4.3 Variant Feature (VAR) In the variant cipher, the plaintext is written into	[[66, 69]]	[[49, 64]]	['VAR']	['Variant Feature']
894	The second and last step to generate qwn-ppv(s) consists of propagating over a WordNet graph to obtain a Personalized PageRanking Vector (PPV), one for each polarity.	[[138, 141], [37, 44]]	[[105, 136]]	['PPV', 'qwn-ppv']	['Personalized PageRanking Vector']
895	This gave rise to a relatively new research area within the emerging field of Textto-Text Generation (TTG) called Multiple-Choice Test Item Generation (MCTIG).1	[[102, 105], [152, 157]]	[[78, 100], [114, 150]]	['TTG', 'MCTIG']	['Textto-Text Generation', 'Multiple-Choice Test Item Generation']
896	We extend the SPARSELDA (Yao et al, 2009) inference scheme for latent Dirichlet alocation (LDA) to tree-based topic models.	[[91, 94], [14, 23]]	[[63, 89]]	['LDA', 'SPARSELDA']	['latent Dirichlet alocation']
897	For each IDR triple  all the object grammar triples are generated whose CF-PS rules  conform with the linear precedence(LP) rules, the fourth rule set  of the metagrammar.	[[120, 122], [9, 12], [72, 77]]	[[102, 118]]	['LP', 'IDR', 'CF-PS']	['linear precedenc']
898	SVO = Subject-Verb-Object GE = General Event PE = Predefined Event Grammar Module	[[45, 47], [0, 3], [26, 28]]	[[50, 66], [6, 25], [31, 44]]	['PE', 'SVO', 'GE']	['Predefined Event', 'Subject-Verb-Object', 'General Event']
899	 For all NER experiments, we use a sequential firstorder conditional random field (CRF) with a unit variance Normal prior, trained with L-BFGS until	[[83, 86], [9, 12], [136, 142]]	[[57, 81]]	['CRF', 'NER', 'L-BFGS']	['conditional random field']
900	the titles of the entity articles titles(e) to represent the entities in the query and two ranking functions, Recursive TFISF (R-TFISF) and LC, 3	[[127, 134], [140, 142]]	[[110, 125]]	['R-TFISF', 'LC']	['Recursive TFISF']
901	Text5 holiday(0.432) humor(0.23) 0.099 blues(0.15) Table 2: Percent Agreement(PA) to manually extracted index terms	[[78, 80]]	[[60, 76]]	['PA']	['Percent Agreemen']
902	The score of an abstract based on extracted PICO elements, SPICO, is broken into individual components according to the following formula: SPICO = Sproblem + Spopulation + Sintervention + Soutcome (4) The first component in the equation, Sproblem, reflects a match between the primary	[[139, 144], [44, 48], [59, 64]]	[[147, 196]]	['SPICO', 'PICO', 'SPICO']	['Sproblem + Spopulation + Sintervention + Soutcome']
903	 3.3.1 Determinantal Point Processes Determinantal point processes (DPPs) are distributions over subsets that jointly prefer quality of	[[68, 72]]	[[37, 66]]	['DPPs']	['Determinantal point processes']
904	 They use Only Word-Seg (OWS), Whole Layer Weight (WLW), SC (SC) and FeedBack mechanism (FB) separately.	[[51, 54], [25, 28], [57, 59], [61, 63], [89, 91]]	[[43, 49], [10, 23], [69, 77]]	['WLW', 'OWS', 'SC', 'SC', 'FB']	['Weight', 'Only Word-Seg', 'FeedBack']
905	In Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI-89), volume 2, pages 1511?1517.	[[86, 94]]	[[27, 84]]	['IJCAI-89']	['International Joint Conference on Artificial Intelligence']
906	Similarity function We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008).	[[121, 125]]	[[101, 119]]	['BInc']	['Balanced Inclusion']
907	wsj_1286)) In addition to the semantic roles described in the rolesets, verbs can take any of a set of general, adjunct-like arguments (ArgMs), distinguished by one of the function tags shown in Table 1.	[[136, 141]]	[[125, 134]]	['ArgMs']	['arguments']
908	"Under the former, they include  personal pronouns, sentential "" i t ,""  and null-comple-  ment anaphora, and under the latter, verb phrase (VP)  ellipsis, sluicing, gapping, and stripping."	[[140, 142]]	[[127, 138]]	['VP']	['verb phrase']
909	4 Crowd-sourcing Multiple-choice Questions from Tables We use Amazon?s Mechanical Turk (MTurk) service to generate MCQs by imposing constraints	[[88, 93], [115, 119]]	[[71, 86]]	['MTurk', 'MCQs']	['Mechanical Turk']
910	a directed acyclic graph, and a set of conditionnal probablities, each node being represented as a Random Variable (RV). Parametrizing the BN	[[116, 118], [139, 141]]	[[99, 114]]	['RV', 'BN']	['Random Variable']
911	                  Threshold is a function of Length(LR) and text  size. The basic idea is larger amount of length(LR)  or text size matches larger amount of Threshold.	[[114, 116]]	[[90, 96]]	['LR']	['larger']
912	mid method in DUC 2005. Proceedings of the 5th Document Understanding Conference (DUC). Van-	[[82, 85], [14, 17]]	[[47, 80]]	['DUC', 'DUC']	['Document Understanding Conference']
913	on the guidance of domain experts, who can devise pedagogically valuable reading lists that order docAutomatic Speech Recognition (ASR) with HMMs Noisy Channel Model Viterbi Decoding for ASR	[[131, 134], [187, 190]]	[[101, 129]]	['ASR', 'ASR']	['Automatic Speech Recognition']
914	ing (NLP) applications. In Information Retrieval (IR) and Question Answering (QA) it is typically termed query/question expansion (Moldovan and	[[78, 80], [5, 8], [50, 52]]	[[58, 76], [27, 48]]	['QA', 'NLP', 'IR']	['Question Answering', 'Information Retrieval']
915	In sum, the text-to-text similarity measure combined with our sentence-level sen-timent analysis algorithm helps us identify the representative rationales of diverse opinions in an online deliberation. An overview of our meth-od is shown in Figure 1.  We applied our method in analyzing Wikipe-dia Article for Deletion (AfD) deliberation con-tent. Next we discuss how this method is used to analyze the content.	[[320, 323]]	[[298, 318]]	['AfD']	['Article for Deletion']
916	computational devices for natural language processing.  called active production networks (APNs), and explore  how certain kinds of movement are handled.	[[91, 95]]	[[63, 89]]	['APNs']	['active production networks']
917	Figure 1: Top-k Accuracy Level Configuration MRR 0 Baseline (BL) 0.6559 1	[[61, 63], [45, 48]]	[[51, 59]]	['BL', 'MRR']	['Baseline']
918	mental state labels that are highly similar to the context of the scene in a latent, conceptual vector space; and an information retrieval (IR) model that identifies labels commonly appearing in sentences	[[140, 142]]	[[117, 138]]	['IR']	['information retrieval']
919	sentences (Sent.), the number of tokens (Tokens) and the unlabeled attachment score (UAS) of MST. 	[[85, 88], [11, 15], [41, 47], [93, 96]]	[[57, 83], [0, 9], [33, 39]]	['UAS', 'Sent', 'Tokens', 'MST']	['unlabeled attachment score', 'sentences', 'tokens']
920	 We have three versions of reference summaries based on summarization ratio(SR): 10%, 15% and 20% respectively.	[[76, 78]]	[[56, 75]]	['SR']	['summarization ratio']
921	particularly helpful in parsing where the sequence  of words forming the MWE is treated as a single  word with a single part of speech (POS) tag. MWE 	[[136, 139], [73, 76], [146, 149]]	[[120, 134]]	['POS', 'MWE', 'MWE']	['part of speech']
922	 1 Introduction Machine translation (MT) systems have different strengths and weaknesses which can be exploited	[[37, 39]]	[[16, 35]]	['MT']	['Machine translation']
923	To tackle this challenge, we incorporate multiple graphs probabilistic factorization with two alternatively designed combination strategies into collaborative topic regression (CTR). Experimental results on real dataset demonstrate	[[177, 180]]	[[145, 175]]	['CTR']	['collaborative topic regression']
924	operating system.  For production deployment we  used Message Driven Beans (MDBs)using IBM  Websphere Application Server? (	[[76, 80], [87, 90]]	[[54, 74]]	['MDBs', 'IBM']	['Message Driven Beans']
925	But, it is observed that the identification of  lexical scopes of compound verbs (CompVs)  and conjunct verbs (ConjVs) from long sequence of successive Complex Predicates 	[[111, 117], [82, 88]]	[[95, 109], [66, 80]]	['ConjVs', 'CompVs']	['conjunct verbs', 'compound verbs']
926	lowing rules are used in the detailed example.  if tense of el and of e2 is simple past (SP)  with perfective AP tben there is justification for 	[[89, 91], [110, 112]]	[[76, 87]]	['SP', 'AP']	['simple past']
927	3  Chinese NER Using CRFs Model Integrating Multiple Features  Besides the text feature(TXT), simplified part-ofspeech (POS) feature, and small-vocabulary-	[[88, 91], [11, 14], [21, 25], [120, 123]]	[[75, 87], [105, 118]]	['TXT', 'NER', 'CRFs', 'POS']	['text feature', 'part-ofspeech']
928	In a second step, unmatched words are converted into stems or synonyms and then matched. A method that uses the concept of maximum matching string (MMS) is presented by Turian, Shen, and Melamed (2003).	[[148, 151]]	[[123, 146]]	['MMS']	['maximum matching string']
929	LMs by perplexity (PPL). We use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB). 	[[88, 91], [0, 3], [19, 22], [57, 60]]	[[73, 86], [7, 17], [36, 55]]	['PTB', 'LMs', 'PPL', 'WSJ']	['Penn Treebank', 'perplexity', 'Wall Street Journal']
930	We evaluate the following two search algorithms: ? beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000)	[[74, 76]]	[[51, 62]]	['BS']	['beam search']
931	In Proceedings of the 23rd International Conference on Computational Linguistics (COLING?10), pages 617? 	[[82, 91]]	[[55, 80]]	['COLING?10']	['Computational Linguistics']
932	Translation retrieval (TR) is a description of this process of selecting from the TM a set of translation records (TRecs) of maximum L1 similarity to a given input.	[[115, 120], [23, 25], [82, 84]]	[[94, 113], [0, 21]]	['TRecs', 'TR', 'TM']	['translation records', 'Translation retrieval']
933	 2 Abstract Syntax Trees We describe abstract syntax trees (ASTs) using an example from CFR Section 610.11:	[[60, 64], [88, 91]]	[[37, 58]]	['ASTs', 'CFR']	['abstract syntax trees']
934	 1.1 Language Modeling Formally, a language model (LM) is a probability distribution over strings of a language:	[[51, 53]]	[[35, 49]]	['LM']	['language model']
935	   First Paragraph (FPAR): We examined several  hundred pages, and observed that a human could 	[[20, 24]]	[[3, 18]]	['FPAR']	['First Paragraph']
936	We used 1300 texts (DEV) as our training set, 200 texts (TST1+TST2) for tuning, and 200 texts (TST3+TST4) as a test set. All 1700 docu-	[[95, 104]]	[[88, 93]]	['TST3+TST4']	['texts']
937	tion task which involves acquiring patterns in the distribution of opinion-bearing words and targets using machine learning (ML) techniques. In partic-	[[125, 127]]	[[107, 123]]	['ML']	['machine learning']
938	Given these restrictions the DAR problem becomes to find that value da of DA that maximises P ( DA = da | f1 = v1, . . . , fn = vn,	[[96, 98], [29, 32], [74, 76]]	[[101, 103]]	['DA', 'DAR', 'DA']	['da']
939	 1 Introduction Question Answering (QA) from structured data, such as DBPedia (Auer et al.,	[[36, 38], [70, 77]]	[[16, 34]]	['QA', 'DBPedia']	['Question Answering']
940	They found that using a combination of all the features in a Support Vector Machine (SVM), they can obtain an accuracy of 80% in the classification of 5 differ-	[[85, 88]]	[[61, 83]]	['SVM']	['Support Vector Machine']
941	 One application area of increasing interest is  information extraction (IE) (see, e.g., Cowie and  Lehnert (1996)).	[[73, 75]]	[[49, 71]]	['IE']	['information extraction']
942	 ? Reduce(RE): Pop the stack. 	[[10, 12]]	[[3, 9]]	['RE']	['Reduce']
943	PP Fu?r diese Behauptung has the grammatical function OAMOD, which indicates that it is a modifier (MOD) of a direct object (OA) elsewhere in the structure (in this case keinen	[[100, 103], [125, 127], [0, 2], [54, 59]]	[[90, 98], [105, 109]]	['MOD', 'OA', 'PP', 'OAMOD']	['modifier', 'of a']
944	Pierre PN Note that a preposition (PR) with lemma de and a determiner (DT) with lemma le and the same gender and number as the common noun have been	[[71, 73], [7, 9], [35, 37]]	[[59, 69], [22, 33]]	['DT', 'PN', 'PR']	['determiner', 'preposition']
945	AT&T Labs-Research Abstract Statistical Machine Translation (SMT) systems are heavily dependent on the qual-	[[61, 64], [0, 4]]	[[28, 59]]	['SMT', 'AT&T']	['Statistical Machine Translation']
946	template includes, for each sentence, syntactic Infor-  mation that Is represented in a tree whose nodes are  syntacti~ categories such as S (sentence), CL (clause),  SUBJECT or VERB.	[[153, 155]]	[[157, 163], [142, 150]]	['CL']	['clause', 'sentence']
947	Ph i lade lph ia ,  PA  191C4  .ABSTRACT  Tree Adjoining Grammar (TAG) is a formalism for natural  language grammars.	[[66, 69], [20, 22]]	[[42, 64], [0, 16]]	['TAG', 'PA']	['Tree Adjoining Grammar', 'Ph i lade lph ia']
948	LL; it is calculated from contingency table information as follows: LO = log (O11 + 0.5)(O22 + 0.5)	[[68, 70]]	[[73, 76]]	['LO']	['log']
949	Hyderabad, India Abstract Named Entity Recognition (NER) is the task of identifying and classifying all proper	[[52, 55]]	[[26, 50]]	['NER']	['Named Entity Recognition']
950	 ADEPT tags documents in a uniform fashion, using  Standard Generalized Markup (SGML) according to  OIR standards.	[[80, 84], [1, 6], [100, 103]]	[[51, 78]]	['SGML', 'ADEPT', 'OIR']	['Standard Generalized Markup']
951	with one object sentences (IMP_VNP), V_NP_PP  agentless passive sentences (PAS_VNPP), V_NP bypassives (BYPAS_VN), and N_PP clauses (NPP) and  these are all decisions that happen in the realiser, 	[[132, 135], [27, 34], [37, 44], [75, 83], [103, 111]]	[[118, 122], [86, 101], [46, 73]]	['NPP', 'IMP_VNP', 'V_NP_PP', 'PAS_VNPP', 'BYPAS_VN']	['N_PP', 'V_NP bypassives', 'agentless passive sentences']
952	of spoken dialogue systems is database retrieval.  Some IVR (interactive voice response) systems using the speech recognition technology are being put	[[56, 59]]	[[61, 87]]	['IVR']	['interactive voice response']
953	the left hand side (LHS) of the rule, and ? the right hand side (RHS) of the rule.	[[65, 68], [20, 23]]	[[48, 63], [4, 18]]	['RHS', 'LHS']	['right hand side', 'left hand side']
954	provement against Naive Bayes was reported. Our proposed model, tag guided RNN (TG-RNN), is designed to use the syntactic tag of the parent	[[80, 86]]	[[64, 78]]	['TG-RNN']	['tag guided RNN']
955	  ME Classification  ME (Maximum Entropy) classification is used here  to directly estimate the posterior probability for 	[[21, 23], [2, 4]]	[[25, 40]]	['ME', 'ME']	['Maximum Entropy']
956	mentary and United Nations parallel corpora. The semantic phrase table (SPT) was extracted from the same corpora annotated with FreeLing (Carreras et	[[72, 75]]	[[49, 70]]	['SPT']	['semantic phrase table']
957	In particular, we use transition probability and emission probability in Hidden Markov Model (HMM) (Leek, 1997) to capture this dependency.	[[94, 97]]	[[73, 92]]	['HMM']	['Hidden Markov Model']
958	 What we describe is part of a theory of language (knowledge and processing)  called Word Grammar (WG) (Hudson 1984; 1990). Section 2 introduces the knowledge 	[[99, 101]]	[[85, 97]]	['WG']	['Word Grammar']
959	er positions less than the penalty of reordering the first ranks.  iii):  Weighted Normalized Discounted Cumulative Gain (WNDCG): NDCG (Moffat and Zobel,  2008) normally compares the rankings of two lists.	[[122, 127], [130, 134]]	[[74, 120]]	['WNDCG', 'NDCG']	['Weighted Normalized Discounted Cumulative Gain']
960	consider in (14) five salient phases of the passage: at the beginning of the process, the parts of river are localized to the exterior EXT(LOC), then to the boundary FRO(LOC), they arrive in	[[139, 142], [135, 138], [166, 169], [170, 173]]	[[109, 118]]	['LOC', 'EXT', 'FRO', 'LOC']	['localized']
961	and normalizing it with a view to discriminate the importance of words across documents and then approximating it using singular value decomposition(SVD) in R dimensions (Bellegarda, 2000).	[[149, 152]]	[[120, 147]]	['SVD']	['singular value decompositio']
962	2.85 0.001 8 29. ( VP (VBP ? NEED?) (	[[19, 21]]	[[23, 26]]	['VP']	['VBP']
963	To estimate the weights ? i in formula (1), we  use Minimum Error Rate Training (MERT) algorithm, which is widely used for phrase-based 	[[81, 85]]	[[52, 79]]	['MERT']	['Minimum Error Rate Training']
964	al., 2013), containing essays written by students at the National University of Singapore (NUS) which have been manually corrected by English instruc-	[[91, 94]]	[[57, 89]]	['NUS']	['National University of Singapore']
965	3.4 The  NATO Research  Study  Group  on  Speech  Process ing   The North Atlantic Treaty Organization (NATO) Re-  search Study Group on Speech Processing (RSG10) \[87\], 	[[104, 108], [9, 13], [156, 161]]	[[68, 102], [110, 154]]	['NATO', 'NATO', 'RSG10']	['North Atlantic Treaty Organization', 'Re-  search Study Group on Speech Processing']
966	Abstract One of the most neglected areas of biomedical Text Mining (TM) is the development of systems based on carefully assessed user	[[68, 70]]	[[55, 66]]	['TM']	['Text Mining']
967	question answering (QA). However, few models in ad hoc information retrieval (IR) use paths for document ranking due to	[[78, 80], [20, 22]]	[[55, 76], [0, 18]]	['IR', 'QA']	['information retrieval', 'question answering']
968	 ? Adjuncts (AM-): General arguments that any verb may take optionally.	[[13, 16]]	[[3, 11]]	['AM-']	['Adjuncts']
969	ing is how to identify a temporal relation between a pair of temporal entities such as events (EVENT) and time expressions (TIMEX) in a narrative. Af-	[[124, 129]]	[[106, 122]]	['TIMEX']	['time expressions']
970	 The obtained Japanese scores as compared to the  scores from the initial English experiment (E-E-E)  are shown in Figure 6.	[[94, 99]]	[[74, 92]]	['E-E-E']	['English experiment']
971	word if it exceeds a certain threshold.  4.2 LDA Graph Method (LDA-GM) The LDA-GM algorithm creates a similarity graph	[[63, 69], [75, 81]]	[[45, 61]]	['LDA-GM', 'LDA-GM']	['LDA Graph Method']
972	English using the frequency dictionary of Arabic (Buckwalter and Parkinson, 2011) and the Corpus of Contemporary American English (COCA) top 5,000 words (Davies, 2010).	[[131, 135]]	[[100, 121]]	['COCA']	['Contemporary American']
973	 1 Introduction Grammatical Framework (GF) (Ranta, 2004) is a grammar formalism designed in particular to serve	[[39, 41]]	[[16, 37]]	['GF']	['Grammatical Framework']
974	Sentence) and McDonald (2006) on a sentence-by-sentence basis. Table 1 shows the compression rates (CompR) for the three systems and evaluates the quality of their output using grammatical relations F1.	[[100, 105]]	[[81, 98]]	['CompR']	['compression rates']
975	 A statistical classification technique based  on the use of Hidden Markov Models (HMM) was  used as a language discriminator.	[[83, 86]]	[[61, 81]]	['HMM']	['Hidden Markov Models']
976	and so on. Recently, sequential learning methods, such as hidden Markov models (HMMs) and conditional random fields (CRFs), have been used suc-	[[80, 84]]	[[58, 78]]	['HMMs']	['hidden Markov models']
977	In this section, we define pomsets as a model for describing concurrency. A labelled partial order (LPO) is a 4 tuple (V, ?,	[[100, 103]]	[[76, 98]]	['LPO']	['labelled partial order']
978	Since the bilingual corpus is only aligned at the document level, we performed sentence alignment using the Champollion Tool Kit (CTK).4 After removing sentences with no aligned sentence, a total	[[130, 133]]	[[108, 128]]	['CTK']	['Champollion Tool Kit']
979	toolkit that contains a suit of modules for generic tasks in Natural Language Processing (NLP), Information Retrieval (IR), and Network Analysis (NA). 	[[146, 148]]	[[128, 144]]	['NA']	['Network Analysis']
980	based chunking; 3. MEMM-based word segmenter with Support Vector Machines (SVM)-based chunking.	[[75, 78], [19, 23]]	[[50, 73]]	['SVM', 'MEMM']	['Support Vector Machines']
981	2004). Accordingly, we define in-NE probability  to help delete and create named entities (NE). 	[[91, 93], [30, 35]]	[[75, 89]]	['NE', 'in-NE']	['named entities']
982	interpreters. In International Conference on Learning Representations (ICLR). 	[[71, 75]]	[[17, 69]]	['ICLR']	['International Conference on Learning Representations']
983	Just as in Figure 2, the initial model is denoted with a bold symbol in the left part of the plot. Also for reference the relevant Lexicon 1 accuracy (LEX 1) is denoted with a ? at the far right.	[[151, 156]]	[[131, 140]]	['LEX 1']	['Lexicon 1']
984	 (e.g., hand, heart, blood, DNA) Diseases and Symptoms (DisSym): Diseases and symptoms.	[[56, 62], [28, 31]]	[[33, 54]]	['DisSym', 'DNA']	['Diseases and Symptoms']
985	ning of ORG (B-O). Many of them are mislabeled as O and beginning of location (B-L), resulting low recall and low precision for ORG.	[[79, 82], [8, 11], [13, 16], [128, 131]]	[[56, 77]]	['B-L', 'ORG', 'B-O', 'ORG']	['beginning of location']
986	  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87?95, Gothenburg, Sweden, April 27, 2014.	[[71, 76], [80, 84]]	[[37, 69]]	['HyTra', 'EACL']	['Hybrid Approaches to Translation']
987	roles they typically enter: ACT (Actor), PAT (Patient), ADDR (Addressee), ORIG (Origin) and EFF (Effect). Syntactic criteria are used to identify	[[92, 95], [28, 31], [41, 44], [56, 60], [74, 78]]	[[97, 103], [33, 38], [46, 53], [62, 71], [80, 86]]	['EFF', 'ACT', 'PAT', 'ADDR', 'ORIG']	['Effect', 'Actor', 'Patient', 'Addressee', 'Origin']
988	other models, such as vector space model (VSM), Okapi model (Robertson et al, 1994) or language model (LM). The pipeline meth-	[[103, 105], [42, 45]]	[[87, 101], [22, 40]]	['LM', 'VSM']	['language model', 'vector space model']
989	the identified reading level. Text plans define  rules on Noun Phrase (NP) density and lexical  choice.	[[71, 73]]	[[58, 69]]	['NP']	['Noun Phrase']
990	Not Available?.  OmegaWiki (OW) is a freely editable online dictionary like WKT.	[[28, 30], [76, 79]]	[[17, 26]]	['OW', 'WKT']	['OmegaWiki']
991	We also include a simple baseline that selects the first document sentence as a caption and show the average caption length (AvgLen) for each model.	[[125, 131]]	[[101, 123]]	['AvgLen']	['average caption length']
992	For instance, a problem-tagged entity is represented as a first word tagged B-P (begin problem) and other 59	[[76, 79]]	[[81, 94]]	['B-P']	['begin problem']
993	All experiments carried out in this study are for the English (EN) - French (FR) language pair. 	[[77, 79], [63, 65]]	[[69, 75], [54, 61]]	['FR', 'EN']	['French', 'English']
994	pirical study for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).	[[123, 125], [155, 159]]	[[111, 121], [131, 154]]	['NB', 'SVMs']	['Naive Baye', 'Support Vector Machines']
995	James Clark. 1999. XSL transformations (XSLT). W3C Recommendation, 16 November.	[[40, 44]]	[[19, 38]]	['XSLT']	['XSL transformations']
996	class. Among these are: L (Larsen, 1999), D (Van Dongen, 2000), misclassification index (MI) (Zeng et al, 2002), H (Meila, 2001), clustering F-measure	[[89, 91]]	[[64, 87], [27, 33]]	['MI']	['misclassification index', 'Larsen']
997	 UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston and Burnard, 1998).	[[44, 46], [58, 60], [1, 3], [13, 15], [28, 30]]	[[49, 56], [63, 79], [6, 11], [18, 26], [33, 42]]	['WN', 'BN', 'UW', 'GW', 'WP']	['WordNet', 'British National', 'ukWaC', 'Gigaword', 'Wikipedia']
998	 1 Introduction Mental State Verbs (MSVs), such as think, know, and want, are very frequent in child-directed lan-	[[36, 40]]	[[16, 34]]	['MSVs']	['Mental State Verbs']
999	Acknowledgment This work is supported by the 6th Framework Research Program of the European Union (EU), LUNA Project, IST contract no 33549,www.ist-luna.eu	[[99, 101], [104, 108], [118, 121]]	[[83, 97]]	['EU', 'LUNA', 'IST']	['European Union']
1000	2010. The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results.	[[55, 62]]	[[17, 53]]	['VOC2010']	['Visual Object Classes Challenge 2010']
1001	This examination can be held by plotting values of recall, precision and F-measure during each step  of merging process. Figure 5 shows the fluctuation of positive recall(PR), positive preclsion(AP),  averaged recall(AR), averaged precision and F-measure (FM).	[[171, 173]]	[[155, 169]]	['PR']	['positive recal']
1002	(6) health issues (HI) trend other (7) personal issues (PI) trend decreasing (8) lectures attended (LA) trend other (9) revision (R) trend decreasing	[[100, 102], [19, 21], [56, 58]]	[[81, 98], [4, 16], [39, 53], [120, 128]]	['LA', 'HI', 'PI']	['lectures attended', 'health issue', 'personal issue', 'revision']
1003	X at Y (verb phr:~se, noun phrase)  X a.m. (corot?rand word)  After step (I) is finished, steps (II)-(IV) are repeated  recursively.	[[97, 99]]	[[77, 88]]	['II']	['is finished']
1004	some freedom in the ordering of major phrasal categories like  NPs  and adverbial phrases - for example, in the linear order of  subject (SUB J), direct object (DOBJ), and indirect object (lOB J)  with respect o one another.	[[138, 143], [63, 66], [161, 165], [189, 195]]	[[129, 136], [146, 159], [172, 187]]	['SUB J', 'NPs', 'DOBJ', 'lOB J)']	['subject', 'direct object', 'indirect object']
1005	1979). Most tools and resources developed for natural language processing (NLP) of Arabic are designed for MSA.	[[75, 78], [107, 110]]	[[46, 73]]	['NLP', 'MSA']	['natural language processing']
1006	means of |P (G)|. This guarantees that neither trees of maximum height (MHT) nor of maximum degree (MDT), i.e. trees which trivially	[[72, 75], [100, 103]]	[[56, 70], [84, 98]]	['MHT', 'MDT']	['maximum height', 'maximum degree']
1007	 Single-tokenization of compound verbs  and named entities (NE) provides significant gains over the baseline PB-SMT 	[[60, 62], [109, 115]]	[[44, 58]]	['NE', 'PB-SMT']	['named entities']
1008	 1 Introduction Information Extraction (IE) is a natural language processing task in which text documents are ana-	[[40, 42]]	[[16, 38]]	['IE']	['Information Extraction']
1009	call this set of features Feat-IV.  Table 3 demonstrates the word error rate(WER) improvement enabled by our binary subsampling	[[77, 80]]	[[61, 75]]	['WER']	['word error rat']
1010	tistical machine translation. In Proceedings of the Machine Translation Summit (MT-Summit). 	[[80, 89]]	[[52, 78]]	['MT-Summit']	['Machine Translation Summit']
1011	Table 5: Sample Hindi complex predicates   5 Corpus and pre-processing  Basic Travel Expressions Corpus (BTEC)  containing travel conversations is used for 	[[105, 109]]	[[72, 103]]	['BTEC']	['Basic Travel Expressions Corpus']
1012	Finally, it is clear from Figure 6 that certain relations are particularly difficult for both parsers. For example, indirect object (IObj) dependents are low scoring nodes: this is because they are often attached to the correct head but are	[[133, 137]]	[[116, 131]]	['IObj']	['indirect object']
1013	we have established direct contact with the south korean delegation through tribal elders .  Figure 2: Random sample of 5 items from study in Section 4: original Google translation (GT), results of targeted paraphrasing translation process (TP), and a human reference translation.	[[182, 184], [241, 243]]	[[162, 180], [220, 239]]	['GT', 'TP']	['Google translation', 'translation process']
1014	 2 Changes to the AZ Scheme Argumentative Zoning II (AZ-II) is a new annotation scheme, which is an elaboration of the orig-	[[53, 58], [18, 20]]	[[28, 51]]	['AZ-II', 'AZ']	['Argumentative Zoning II']
1015	its title  read{ARG0, ARG1}  Figure 2: A sentence parse tree with two predicative tree structures (PAST s) which is equal 1 if the target fi is rooted at node n	[[99, 105], [16, 20], [22, 26]]	[[70, 97]]	['PAST s', 'ARG0', 'ARG1']	['predicative tree structures']
1016	We train a source-totarget PBMT system (SYS_ST) and a target-tosource PBMT system (SYS_TS) on the parallel  corpus.	[[83, 89], [40, 46], [27, 31]]	[[63, 81], [32, 38]]	['SYS_TS', 'SYS_ST', 'PBMT']	['source PBMT system', 'system']
1017	Narrative Summarization. Journal Traitement automatique des langues (TAL): Special  issue  on Context:  Automatic Text  Summariza-	[[69, 72]]	[[33, 67]]	['TAL']	['Traitement automatique des langues']
1018	This article describes the collaborative work on applying  the newly proposed ISO standard for dialogue act  annotation to the Switchboard Dialogue Act (SWBD-DA)  Corpus, as part of our on-going effort to promote 	[[153, 160], [78, 81]]	[[127, 151]]	['SWBD-DA', 'ISO']	['Switchboard Dialogue Act']
1019	Note: in genera\], the resultant segments,  such as Declarative Sentence(SDEC),  Noun Phrase(NP), Inf init ive  Phrase(INF), and Verb Phrase(VP), 	[[93, 95], [73, 77], [119, 122], [141, 143]]	[[81, 91], [52, 72], [98, 110], [129, 140]]	['NP', 'SDEC', 'INF', 'VP']	['Noun Phras', 'Declarative Sentence', 'Inf init ive', 'Verb Phrase']
1020	Reject? validations for reliably deliberate (Rel) and unreliable (URel) subsets of the metaphor production data, given that the	[[66, 70], [45, 48]]	[[54, 64], [24, 32]]	['URel', 'Rel']	['unreliable', 'reliably']
1021	 In integrating this approach into a dialog system, we see that the dialog manager (DM) no longer determines surface strings to send to the TTS system, as is often the case in current dialog systems.	[[84, 86], [140, 143]]	[[68, 82]]	['DM', 'TTS']	['dialog manager']
1022	document is zero.  Agglomerative Hierarchical Clustering (AHC)  AHC is a bottom-up hierarchical clustering 	[[58, 61], [64, 67]]	[[19, 56]]	['AHC', 'AHC']	['Agglomerative Hierarchical Clustering']
1023	Since we are going to be  concerned with definability, we first translate CFGs  into CFTs (Context Free Theories). The 	[[85, 89], [74, 78]]	[[91, 112]]	['CFTs', 'CFGs']	['Context Free Theories']
1024	described as operating within the same three-stage framework as STRAND.  Parallel Text Miner (PTMiner) (Chen and Nie 2000) exploits already-existing Web search engines to locate pages by querying for pages in a given language that contain	[[94, 101], [64, 70]]	[[73, 92]]	['PTMiner', 'STRAND']	['Parallel Text Miner']
1025	proaches, we also report on supplying them with different reordering rule sets: a set that was learned on manually aligned data (MAN), and a set learned on the same data but with automatic	[[129, 132]]	[[106, 114]]	['MAN']	['manually']
1026	VNMT 32.25 34.50++ 33.78++ 36.72?++ 30.92?++ 24.41?++ 32.07 Table 1: BLEU scores on the NIST Chinese-English translation task. AVG = average BLEU scores on test sets. We	[[141, 145], [0, 4], [69, 73], [88, 92], [127, 130]]	[[133, 140]]	['BLEU', 'VNMT', 'BLEU', 'NIST', 'AVG']	['average']
1027	301 3. False Attributes (FA) - these are situations where a value for an attribute has been spec-	[[25, 27]]	[[7, 23]]	['FA']	['False Attributes']
1028	 Performance has been measured with both the question followed by an extension (Q+E), as well as the question followed by the target and then	[[80, 83]]	[[45, 78]]	['Q+E']	['question followed by an extension']
1029	 As the noun or adjective occur in the first slot  of conjunct verbs (ConjVs) construction, the  search starts from the point of noun or adjec-	[[70, 76]]	[[54, 68]]	['ConjVs']	['conjunct verbs']
1030	Taking into account the above strategies, we propose three concrete DS to CS conversions: Flat conversion with H auxiliary symbol (FlatH). 	[[131, 136], [68, 70], [74, 76]]	[[90, 112]]	['FlatH', 'DS', 'CS']	['Flat conversion with H']
1031	Moreover, in (Hahn et al, 2008a) two more models are applied to SLU: a Maximum Entropy (EM) model and a model coming from the Statistical Machine Translation (SMT) commu-	[[88, 90], [64, 67], [159, 162]]	[[79, 86], [126, 157]]	['EM', 'SLU', 'SMT']	['Entropy', 'Statistical Machine Translation']
1032	5.2.1 Query Focused Rewards We have proposed an extension to both reward functions to allow for query focused (QF) summarization.	[[111, 113]]	[[96, 109]]	['QF']	['query focused']
1033	Many different algorithms  have been used for this task, including some  machine learning (ML) algorithms, such as  Na?ve Bayesian model, decision trees, and 	[[91, 93]]	[[73, 89]]	['ML']	['machine learning']
1034	tence pairs were selected from the WMT Giga corpus if the perplexity of their French part with respect to a language model (LM) trained on French news data was below a given threshold.	[[124, 126], [35, 38]]	[[108, 122]]	['LM', 'WMT']	['language model']
1035	POS Entropy (PosH) 0.55 0.02 0.21 -0.20 0.18 -0.11 0.22 1.00 0.16 -0.11 Derivation steps (Step) -0.05 0.11 0.38 -0.35 0.31 -0.16 0.32 0.16 1.00 -0.24 Word Length (WLen) 0.18 -0.29 -0.03 0.64 -0.72 0.56 0.02 -0.11 -0.24 1.00 Table 1: Correlations between (mean-centered) predictors.	[[163, 167], [13, 17], [90, 94]]	[[150, 161], [0, 11], [83, 88]]	['WLen', 'PosH', 'Step']	['Word Length', 'POS Entropy', 'steps']
1036	As expected, the poor performance observed on the sniper text is mainly due to two reasons: the presence of out of vocabulary (OOV) words and the incorrect translations of terminological	[[127, 130]]	[[108, 125]]	['OOV']	['out of vocabulary']
1037	to pour) means that the entity (wine) is localized to exterior locus (barrel) and crosses the intermediate locus IME(LOC) to be localized to the interior INT(LOC) (the_bottle).	[[117, 120]]	[[107, 112]]	['LOC']	['locus']
1038	the set of required domains. Various classification systems were considered, including the Dewey Decimal Classification (DDC) and Universal Decimal Classification (UDC). These schemes, however, are	[[164, 167], [121, 124]]	[[130, 162], [91, 119]]	['UDC', 'DDC']	['Universal Decimal Classification', 'Dewey Decimal Classification']
1039	these paths. This corresponds to the Viterbi approximation i speech recognition or in  other related areas for which hidden Markov models (HMM's) are used. In all such 	[[139, 144]]	[[117, 137]]	"[""HMM's""]"	['hidden Markov models']
1040	2.4 Dictionaries of  Bahasa Nusantara, Indonesian Linguistics Association (MLI)   Masyarakat Linguistik Indonesia (MLI) is a group  of institutions, organizations and corporation, 	[[115, 118], [75, 78]]	[[82, 113]]	['MLI', 'MLI']	['Masyarakat Linguistik Indonesia']
1041	The Multi-Perspective Question-Answering (MPQA) newswire corpus (Wilson and Wiebe, 2005) and the J. D. Power & Associates (JDPA) automotive review blog post (Kessler et al, 2010)	[[123, 127], [42, 46]]	[[97, 121], [4, 40]]	['JDPA', 'MPQA']	['J. D. Power & Associates', 'Multi-Perspective Question-Answering']
1042	Each word is considered as an  instance. Maximum Entropy (MaxEnt) is used in  this paper.	[[58, 64]]	[[41, 56]]	['MaxEnt']	['Maximum Entropy']
1043	gradient method. In The Conference on Advances in Neural Information Processing Systems (NIPS). 	[[89, 93]]	[[50, 87]]	['NIPS']	['Neural Information Processing Systems']
1044	When using the classifiers to predict the class of a test example, there are four possible outcomes; true positive (TP), true negative (TN), false positive (FP), and false nega-	[[116, 118], [136, 138], [157, 159]]	[[101, 114], [121, 134], [141, 155]]	['TP', 'TN', 'FP']	['true positive', 'true negative', 'false positive']
1045	nym, Instance Hypernym, Part Holonym, Member Holonym, Substance Meronym, Entailment Table 1: Similarity features using WordNet (WN). 	[[128, 130]]	[[119, 126]]	['WN']	['WordNet']
1046	former networks for image recognition. Bulletin of the International Statistical Institute (ISI). 	[[92, 95]]	[[55, 90]]	['ISI']	['International Statistical Institute']
1047	e-mail: lynet te@goldilocks.lcs.mit.edu  ABSTRACT  The Air Travel Information System (ATIS) domain serves as  the common task for DARPA spoken language system re- 	[[86, 90], [130, 135]]	[[55, 84]]	['ATIS', 'DARPA']	['Air Travel Information System']
1048	Finance and economics (FAE) 100  Education (EDU) 100  Entertainment (ENT) 100  Computer (COM) 100 	[[69, 72], [23, 26], [44, 47], [89, 92]]	[[54, 67], [0, 20], [33, 42], [79, 87]]	['ENT', 'FAE', 'EDU', 'COM']	['Entertainment', 'Finance and economic', 'Education', 'Computer']
1049	ferent corpora, Academia Sinica (AS), City  University of Hong Kong (HK), Peking University (PK), and Microsoft Research Asia (MSR),  each of which has its own definition of a word.	[[127, 130], [33, 35], [69, 71], [93, 95]]	[[102, 120], [16, 31], [58, 67], [74, 91]]	['MSR', 'AS', 'HK', 'PK']	['Microsoft Research', 'Academia Sinica', 'Hong Kong', 'Peking University']
1050	1434  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 82?86, Gothenburg, Sweden, April 27, 2014.	[[75, 80], [84, 88]]	[[41, 73]]	['HyTra', 'EACL']	['Hybrid Approaches to Translation']
1051	F is a frame name, E a frame element name, and t and s are sequences of word indices (t is for the target (FEE)) Using this measure of partial agreement, we now	[[107, 110]]	[[91, 105], [7, 17], [29, 41]]	['FEE']	['for the target', 'frame name', 'element name']
1052	Note that the autoPS heuristic for ranking senses is a more precise estimator than the WordNet most?frequent?sense (MFS). 	[[116, 119], [14, 20]]	[[95, 114]]	['MFS', 'autoPS']	['most?frequent?sense']
1053	 of the International Joint Conference on Artificial Intelligence (IJCAI). 	[[67, 72]]	[[8, 65]]	['IJCAI']	['International Joint Conference on Artificial Intelligence']
1054	We use the two representative corpora mentioned above, Penn Chinese Treebank (CTB) and PKU?s People?s Daily (PPD) in our experiments.	[[109, 112], [78, 81], [87, 92]]	[[93, 107], [60, 76]]	['PPD', 'CTB', 'PKU?s']	['People?s Daily', 'Chinese Treebank']
1055	a new father node. The following simple rule forms a  noun phrase (NP). 	[[67, 69]]	[[54, 65]]	['NP']	['noun phrase']
1056	 2 Data Kinyarwanda (KIN) and Malagasy (MLG) are lowresource, KIN is morphologically rich, and English	[[21, 24], [40, 43], [62, 65]]	[[8, 19], [30, 38]]	['KIN', 'MLG', 'KIN']	['Kinyarwanda', 'Malagasy']
1057	only limited discontinuities in each tree.  Generalized Multitext Grammar (GMTG) offers a way to synchronize Mildly Context-Sensitive	[[75, 79]]	[[44, 73]]	['GMTG']	['Generalized Multitext Grammar']
1058	"To allow for portability, the SABA parser translates its natural  language input into an ~mtermediate"" s mantic network formalism  called SF (for ""Sentence Formalism'), presented in details in (Binot,  1984, 1985)."	[[138, 140], [30, 34]]	[[147, 165]]	['SF', 'SABA']	['Sentence Formalism']
1059	ENTITY_A appos Figure 2: Dependency tree (DT) for the entity blinded sentence ?	[[42, 44]]	[[25, 40]]	['DT']	['Dependency tree']
1060	 Classifier models. We used a first-order linear chain conditional random fields (CRF) model as a sequence labeler and a Maximum Entropy (Maxent) classifier model as a	[[82, 85], [138, 144]]	[[55, 80], [121, 136]]	['CRF', 'Maxent']	['conditional random fields', 'Maximum Entropy']
1061	 A dialogue act is defined as a pair consisting of a communicative function (CF) and a semantic content (SC): a =< CF,SC >.	[[77, 79], [105, 107], [115, 117], [118, 120]]	[[53, 75], [87, 103]]	['CF', 'SC', 'CF', 'SC']	['communicative function', 'semantic content']
1062	The most  important forms of discourse of interest o  Natural Language Processing (NLP) are text  and dialogue.	[[83, 86]]	[[54, 81]]	['NLP']	['Natural Language Processing']
1063	17 classes set in Sun et al(2008).  We used the spectral clustering (SPEC) method and settings as in Sun and Korhonen (2009) but	[[69, 73]]	[[48, 67]]	['SPEC']	['spectral clustering']
1064	 MRS constraints have three kinds of literals, two kinds of elementary predications (EPs) in the first two lines and handle constraints in the third line:	[[85, 88], [1, 4]]	[[60, 83]]	['EPs', 'MRS']	['elementary predications']
1065	media, where evidences for both actions and ties are available. We begin by necessary description of preliminaries and notations, we then present the mutual latent random graphs (MLRGs) model, upon which both sources of evidence could be exploited simultaneously to capture their mutual influence.	[[179, 184]]	[[150, 177]]	['MLRGs']	['mutual latent random graphs']
1066	= Task Defined RAW SCORI~  ((! OMM(~OST x number of messages)  - (INFCOST x nnmber  o f  in fe rences)  	[[31, 34]]	[[36, 60]]	['OMM']	['OST x number of messages']
1067	  ? suffixes (SUF), such as verb endings, nominal  cases, the nominal feminine ending -at, etc.;	[[14, 17]]	[[4, 12]]	['SUF']	['suffixes']
1068	tors of a lower-dimensional space. LSI, which is based on Singular Value Decomposition (SVD) of matrices, has showed to have the ability to ex-	[[88, 91], [35, 38]]	[[58, 86]]	['SVD', 'LSI']	['Singular Value Decomposition']
1069	Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM). More recent	[[109, 112], [65, 68]]	[[88, 107]]	['HMM', 'MLE']	['hidden Markov model']
1070	ing instance is created as during training, and then presented to the decision tree, which returns a confidence value (CF)2 indicating the likelihood that NPi is coreferential to NPj .	[[119, 121], [155, 158], [179, 182]]	[[101, 111]]	['CF', 'NPi', 'NPj']	['confidence']
1071	otherwise be very limited annotated data. The resource, the Online Database of INterlinear text (ODIN), makes this data available and provides additional annotation	[[97, 101]]	[[60, 90]]	['ODIN']	['Online Database of INterlinear']
1072	this corpus is labelled with a set of seven classes: beneficial (BNF), direction (DIR), extent (EXT), location (LOC), manner (MNR), purpose (PRP), and temporal (TMP).	[[112, 115], [141, 144], [65, 68], [82, 85], [96, 99], [126, 129], [161, 164]]	[[102, 110], [132, 139], [53, 63], [71, 80], [88, 94], [118, 124], [151, 159]]	['LOC', 'PRP', 'BNF', 'DIR', 'EXT', 'MNR', 'TMP']	['location', 'purpose', 'beneficial', 'direction', 'extent', 'manner', 'temporal']
1073	sider this sentence is correctly tagged.  Test data set 1 (TDS 1): contains about 10%  of the sentences from the complete emotion-	[[59, 64]]	[[42, 57]]	['TDS 1']	['Test data set 1']
1074	Syntactic features from SLA research (SLASYN) ? Mean length of clause (MLC) ?	[[71, 74], [24, 27], [38, 44]]	[[48, 69]]	['MLC', 'SLA', 'SLASYN']	['Mean length of clause']
1075	factoid ones - as well as new elements ? such as  expected polarity type (EPT). However, opi-	[[74, 77]]	[[50, 72]]	['EPT']	['expected polarity type']
1076	 2.5 Noise Contrastive Estimation Noise contrastive estimation (NCE) is another sampling-based technique (Hyv?arinen, 2010;	[[64, 67]]	[[34, 62]]	['NCE']	['Noise contrastive estimation']
1077	tions for this sentence.  Figure 5: The LF (left) and MRS (right) representations for the sentence ?	[[40, 42], [54, 57]]	[[44, 48]]	['LF', 'MRS']	['left']
1078	cessing information from a structured database ? a natural language interface to a database (NLIDB) (Kapetanios et al, 2010).	[[93, 98]]	[[51, 91]]	['NLIDB']	['natural language interface to a database']
1079	in cooperation between HU Berlin, U Frankfurt and U Jena, conducted in the wider context of the Deutsch Diachron Digital (DDD) initiative. The	[[122, 125], [23, 25]]	[[96, 120]]	['DDD', 'HU']	['Deutsch Diachron Digital']
1080	These interfaces stand to play a critical role in the  ongoing migration of interaction fi'oln the desktop  to wireless portable computing devices (PI)As, next-  generation phones) that offer limited screen real es- 	[[148, 150]]	[[120, 146]]	['PI']	['portable computing devices']
1081	As noted by Melamed (2000), the problem of finding the best set of links is the maximum-weighted bipartite matching (MWBM) problem: Given a weighted bipartite graph G = (V1 ? V2, E) with	[[117, 121]]	[[107, 115], [159, 164]]	['MWBM']	['matching', 'graph']
1082	He et al (2010) measure the similarity between hypothesis and reference translation in terms of the Lexical Functional Grammar (LFG) representation.	[[128, 131]]	[[100, 126]]	['LFG']	['Lexical Functional Grammar']
1083	Tutor's Priming Ratio aggregated by task set (TS = Task Set)     Figure 3. Student's Priming Ratio aggregated by task set (TS = Task Set)  Several significant relationships emerged within the models. We discuss a subset of these here.	[[123, 125], [46, 48]]	[[128, 136], [51, 59]]	['TS', 'TS']	['Task Set', 'Task Set']
1084	 3.1 Classification Our evaluation was performed using the Maximum Entropy (MaxEnt) and Support Vector Machine (SVM) classifiers.	[[76, 82], [112, 115]]	[[59, 74], [88, 110]]	['MaxEnt', 'SVM']	['Maximum Entropy', 'Support Vector Machine']
1085	 Symptom name recognition rate (RRdet),  recognition error rate (RERdet) and recognition  F-Measure (RFMdet): these metrics are designed 	[[65, 71], [32, 37], [101, 107]]	[[41, 63], [14, 30], [77, 99]]	['RERdet', 'RRdet', 'RFMdet']	['recognition error rate', 'recognition rate', 'recognition  F-Measure']
1086	 Dictionary-based methods rely on some dictionary or lexical knowledge base (LKB) such as WordNet (Fellbaum and Miller, 1998) that con-	[[77, 80]]	[[53, 75]]	['LKB']	['lexical knowledge base']
1087	 The corpora used in our experiments are the Penn Arabic Treebank (ATB) (Maamouri et al2004) and the French Treebank (FTB) (Abeill?, Cl?ment, and Kinyon	[[118, 121], [67, 70]]	[[101, 116], [45, 65]]	['FTB', 'ATB']	['French Treebank', 'Penn Arabic Treebank']
1088	between interdependent ? E steps (as might arise for an  assumption such as ((ANB)?C)). It is straightforward 	[[78, 84]]	[[57, 72]]	['ANB)?C']	['assumption such']
1089	Table 3: Example of retrieved Wikipedia pages from the four different methods tested in this paper.  Results of diverse merging (DivM) appear to cover more topics relevant to the conversation fragment than other methods.	[[129, 133]]	[[112, 127]]	['DivM']	['diverse merging']
1090	line models: ? Character Segmenter (CS): this model simply divides Chinese sentences into sequences	[[36, 38]]	[[15, 34]]	['CS']	['Character Segmenter']
1091	parser on merged development PTB/PRBK data (section 24). Legend of models: ST=Split Tags; EC=enhanced connectivity.	[[75, 77], [29, 37], [90, 92]]	[[78, 88], [93, 114]]	['ST', 'PTB/PRBK', 'EC']	['Split Tags', 'enhanced connectivity']
1092	Participants in this study included 39 children with typical development (TD) and 21 children with autism spectrum disorder (ASD). ASD was di-	[[125, 128], [74, 76], [131, 134]]	[[99, 123], [53, 72]]	['ASD', 'TD', 'ASD']	['autism spectrum disorder', 'typical development']
1093	5.5.3 Corpus Statistics. For training of the question detection module, we used the manually annotated set of about 200,000 SWITCHBOARD speech acts14 (SAs);15 for training of the answer detection component, we used the eight English CALLHOME dia-	[[151, 154]]	[[136, 149]]	['SAs']	['speech acts14']
1094	 IV. RETROSPECTIVE SSL (R-SSL). After	[[24, 29]]	[[5, 22]]	['R-SSL']	['RETROSPECTIVE SSL']
1095	"October-2001 w/out 2000 SPA 30.88   95.68   0.08   Table 2: Results for Identifying Speech-Act DATE tags in the October-2001 Communicator Corpus, (Dim = Dimension of Date used for output classification (SPA = Speech Act, Maj. Cl."	[[203, 206], [24, 27], [95, 99], [147, 150], [226, 228]]	[[209, 219], [153, 162]]	['SPA', 'SPA', 'DATE', 'Dim', 'Cl']	['Speech Act', 'Dimension']
1096	We also used two Korean speech recognizers: a speech recognizer made by LG-Elite (LG Electronics Institute of Technology) and a Korean commercial speech recog-	[[72, 80]]	[[82, 106]]	['LG-Elite']	['LG Electronics Institute']
1097	 2001. Linguistic Inquiry and Word Count (LIWC):  LIWC2001.	[[42, 46]]	[[7, 40]]	['LIWC']	['Linguistic Inquiry and Word Count']
1098	ory (Mann and Thompson, 1988), or RST, represents text by labeled hierarchical structures called Discourse Trees (DTs), which can incorporate several layers of other linguistic information, e.g.,	[[114, 117], [34, 37]]	[[97, 112]]	['DTs', 'RST']	['Discourse Trees']
1099	the Internet (Colbath, 2012). Social media poses three major computational challenges, dubbed by Gartner the 3Vs of big data: volume, velocity, and variety1. Natural Language Processing (NLP) methods, in particular, face further difficulties arising from the short, noisy, and strongly contextualised nature of social media. In order to address the 3Vs of social media, new language technologies have emerged, such as the identification and definition of users' language varieties and the translation to a different language, than the source.	[[187, 190]]	[[158, 185]]	['NLP']	['Natural Language Processing']
1100	the feature structures which are associated with each node, which prohibit certain compositions, are not shown. Note also that this is not a lexicalised TAG (LTAG). This is somewhat unusual; we intend, as	[[158, 162]]	[[141, 156]]	['LTAG']	['lexicalised TAG']
1101	would also be possible.  Personalized PageRank similarity (PPR) (Agirre and Soroa, 2009) measures the semantic relatedness between two word senses s	[[59, 62]]	[[25, 57]]	['PPR']	['Personalized PageRank similarity']
1102	 5 Clustering Methods Spectral clustering (SPEC) has proved promising in previous verb clustering experiments (Brew	[[43, 47]]	[[22, 41]]	['SPEC']	['Spectral clustering']
1103	  TGTM P=p,pk ,b   TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l  	[[24, 26], [2, 6], [19, 23], [46, 50], [51, 53]]	[[27, 29], [55, 58]]	['PR', 'TGTM', 'TGTM', 'TGTM', 'PL']	['pr', 'p l']
1104	S# for the count in non-speculative ones)  3 Methods  Conditional random fields (CRF) model was  firstly introduced by Lafferty et al (2001).	[[81, 84]]	[[54, 79]]	['CRF']	['Conditional random fields']
1105	 (5) Rank Value:  i. Top Rank (T-Rank): The rank of snippet  that first contains the candidate.	[[31, 37]]	[[21, 29]]	['T-Rank']	['Top Rank']
1106	TH + DR + EG + LC 56.51 TH + EG + LC 56.50 Character Type (CT) 51.96 Word Familiarity (WF) 51.50	[[59, 61], [0, 2], [5, 7], [10, 12], [15, 17], [24, 26], [29, 31], [34, 36], [87, 89]]	[[43, 57], [69, 85]]	['CT', 'TH', 'DR', 'EG', 'LC', 'TH', 'EG', 'LC', 'WF']	['Character Type', 'Word Familiarity']
1107	 5 Conclusions The spoken language understanding (SLU) system discussed in this paper is entirely statistically based.	[[50, 53]]	[[19, 48]]	['SLU']	['spoken language understanding']
1108	Langman dictionary.  Maximum Entropy (MaxEnt) principle has been successfully applied in many classification and tagging tasks (Rat-	[[38, 44]]	[[21, 36]]	['MaxEnt']	['Maximum Entropy']
1109	generation of textual descriptions from visual data, robot navigation tasks, giving directional instructions, and geographical information systems (GIS). 	[[148, 151]]	[[114, 146]]	['GIS']	['geographical information systems']
1110	 3.2 Formal basis: Lexical Resource Semantics Lexical Resource Semantics (LRS) (Richter and Sailer, 2003) is an underspecified semantic formal-	[[74, 77]]	[[46, 72]]	['LRS']	['Lexical Resource Semantics']
1111	Intersection (I), Union (U), (Koehn et al, 2003) Grow Diagonal Final (GDF), (Och and Ney, 2003) H refined heuristics and Power Mean (PMn) alignment sets where n = 5.	[[133, 136], [70, 73]]	[[121, 131], [0, 12], [18, 23], [49, 68]]	['PMn', 'GDF']	['Power Mean', 'Intersection', 'Union', 'Grow Diagonal Final']
1112	{wangruibo,gaoyahui}@sxu.edu.cn Abstract In this paper, semantic role labeling(SRL) on Chinese FrameNet is divided into the	[[79, 82]]	[[56, 78]]	['SRL']	['semantic role labeling']
1113	at a supermarket 2 1 1 0 able unable 1 0 0 1 Table 1: Word-based Levenshtein distance (LD) feature and separated edit operations (D = deletions, I	[[87, 89]]	[[65, 85]]	['LD']	['Levenshtein distance']
1114	We use four groups of datasets. The first group comes from the English Web Treebank (EWT),4 also used in the Parsing the Web shared task	[[85, 88]]	[[63, 83]]	['EWT']	['English Web Treebank']
1115	Abstract  Most machine transliteration systems  transliterate out of vocabulary (OOV)  words through intermediate phonemic 	[[81, 84]]	[[62, 79]]	['OOV']	['out of vocabulary']
1116	We propose a method to improve the accuracy of parsing bilingual texts (bitexts) with the help of statistical machine translation (SMT) systems.	[[131, 134]]	[[98, 129]]	['SMT']	['statistical machine translation']
1117	leaves=29). The first branching, which corresponds to no AP (Absolute Position) and no C (Colour), assigns n to as many as 1571 instances	[[57, 59]]	[[61, 78], [90, 96]]	['AP']	['Absolute Position', 'Colour']
1118	 1 Introduct ion  Many of the Natural Language Generation (NLG)  systems that produce flexible output, i.e. sentences 	[[59, 62]]	[[30, 57]]	['NLG']	['Natural Language Generation']
1119	We then build three pairwise comparison matrices: one comparing pairs of typically developing (TD) children; one comparing pairs of children with ASD; and a third com-	[[95, 97], [146, 149]]	[[73, 93]]	['TD', 'ASD']	['typically developing']
1120	 6 Over Segmentation  For wrongly spelled or OOV (out of vocabulary)  Urdu words, the system may forcibly break the 	[[45, 48]]	[[50, 67]]	['OOV']	['out of vocabulary']
1121	Verbs can also involve non-core modifier arguments, such as ArgMTMP (time), ArgM-LOC (location), ArgM-CAU (cause), etc.	[[81, 84], [60, 67], [97, 105]]	[[86, 94], [69, 73], [107, 112]]	['LOC', 'ArgMTMP', 'ArgM-CAU']	['location', 'time', 'cause']
1122	 SYSTEM ARCHITECTURE The TIA used for MUC-3 was developed from the AD-TIA (Alternate Domain TIA) . This system, shown	[[67, 73], [38, 43], [25, 28]]	[[75, 95]]	['AD-TIA', 'MUC-3', 'TIA']	['Alternate Domain TIA']
1123	 In this paper, we introduce a novel method called Random Manhattan Indexing (RMI). RMI	[[78, 81], [84, 87]]	[[51, 76]]	['RMI', 'RMI']	['Random Manhattan Indexing']
1124	Abstract Crowd-sourcing approaches such as Amazon?s Mechanical Turk (MTurk) make it possible to annotate or collect large amounts of	[[69, 74]]	[[52, 67]]	['MTurk']	['Mechanical Turk']
1125	vestigate four factors: text length (TL), sentence length (SL), average number of words per sentence (WS), and average number of characters per word (CW). Since	[[150, 152], [37, 39], [59, 61], [102, 104]]	[[129, 148], [24, 35], [42, 57], [82, 100]]	['CW', 'TL', 'SL', 'WS']	['characters per word', 'text length', 'sentence length', 'words per sentence']
1126	Our initial experiment includes language model (LM), word posterior probability (WPP), confusion network (CN), and word lexicon (WL) features for a total of 11	[[106, 108], [48, 50], [81, 84], [129, 131]]	[[87, 104], [32, 46], [53, 79], [115, 127]]	['CN', 'LM', 'WPP', 'WL']	['confusion network', 'language model', 'word posterior probability', 'word lexicon']
1127	Figure 5: Example GMM fitting 2. Gaussian mixture model (GMM)-based POI probability (prior) calculation	[[57, 60], [18, 21], [68, 71]]	[[33, 55]]	['GMM', 'GMM', 'POI']	['Gaussian mixture model']
1128	other synthesis techniques.  The TD-PSOLA (Time Domain Pitch Synchronous Overlap-add) developed by CNET is a very simple  but ingenious method which assures high voice quality, the only disadvantage is that it is based on a time: 	[[33, 41], [99, 103]]	[[43, 84]]	['TD-PSOLA', 'CNET']	['Time Domain Pitch Synchronous Overlap-add']
1129	direct analogy to Sidner's \[26\] potential local foci,  and assumes only one temporal referent in the  temporal focus (TF). 	[[120, 122]]	[[104, 118]]	['TF']	['temporal focus']
1130	 ? Modifier substitution (M-Sub) :  t2 is a substitution of t~ if and only if : 	[[26, 31]]	[[3, 24]]	['M-Sub']	['Modifier substitution']
1131	the Tomita parser, which handles Japanese and Spanish as well as English . The parser output is grammatical structures called Functionally Labelled Templates (FLTs) which are built using a linguistic formalism that modifies and extends the f-structure of Lexical-Functional Grammar (LFG) .	[[159, 163], [283, 286]]	[[126, 157], [255, 281]]	['FLTs', 'LFG']	['Functionally Labelled Templates', 'Lexical-Functional Grammar']
1132	MT is one of the oldest and most important areas of Natural Language Processing (NLP) / Computational Linguistics (CL).2 From its beginnings we have witnessed some changes in the	[[115, 117], [0, 2], [81, 84]]	[[88, 113], [52, 79]]	['CL', 'MT', 'NLP']	['Computational Linguistics', 'Natural Language Processing']
1133	and round corners for processes. Lexical access is applied to the input string to produce  (nondeterministically) the extended lexical item (ELI) of each word. Its output is split 	[[141, 144]]	[[118, 139]]	['ELI']	['extended lexical item']
1134	 1 Introduction Question answering(QA) system aims at finding exact answers to a natural language question.	[[35, 37]]	[[16, 33]]	['QA']	['Question answerin']
1135	All words are labeled as basic or not basic according to Ogden?s Basic English 850 (BE850) list (Ogden, 1930).3 In order to measure the lexical complexity	[[84, 89]]	[[65, 82]]	['BE850']	['Basic English 850']
1136	  For all the reasons listed above, a dictionary of  Turkish Language Association (TLA) is used in  this study.	[[83, 86]]	[[53, 81]]	['TLA']	['Turkish Language Association']
1137	the classification of various verb groups (generic verbs versus specific verbs) based on the semantic distance with Latent Semantic Analysis (LSA) and Cluster Analysis.	[[142, 145]]	[[116, 140]]	['LSA']	['Latent Semantic Analysis']
1138	Recently, McDonald et al (2005b) formalized dependency parsing as a maximum spanning tree (MST) problem, which can be solved in quadratic time relative	[[91, 94]]	[[68, 89]]	['MST']	['maximum spanning tree']
1139	SC is mainly used in  mainland China while TC is mainly used in Taiwan  and Hong Kong (HK). In this experiment, we further 	[[87, 89], [0, 2], [43, 45]]	[[76, 85]]	['HK', 'SC', 'TC']	['Hong Kong']
1140	1 Introduction 1.1 Background Back in 2004, ETV (Eenadu Television), Hyderabad, felt a need for a text editor to prepare news	[[44, 47]]	[[49, 66]]	['ETV']	['Eenadu Television']
1141	imated conversational characters from recordings of human performance. ACM Transactions on Graphics (TOG), 23(3):506?513.	[[101, 104], [71, 74]]	[[75, 99]]	['TOG', 'ACM']	['Transactions on Graphics']
1142	TEXT), DECL (Declarative), HON (Honorific), IMPER (Imperative), NOM (Nominative), ORTH (ORTHOGRAPHY), PST (Past), SYN (SYNTAX), SEM (SEMANTICS), RELS (RELATIONS), and POS (part of speech).	[[102, 105], [7, 11], [27, 30], [44, 49], [64, 67], [82, 86], [114, 117], [128, 131], [145, 149], [167, 170]]	[[107, 111], [13, 24], [32, 41], [51, 61], [69, 79], [88, 99], [119, 125], [133, 142], [151, 160], [172, 186]]	['PST', 'DECL', 'HON', 'IMPER', 'NOM', 'ORTH', 'SYN', 'SEM', 'RELS', 'POS']	['Past', 'Declarative', 'Honorific', 'Imperative', 'Nominative', 'ORTHOGRAPHY', 'SYNTAX', 'SEMANTICS', 'RELATIONS', 'part of speech']
1143	that provides a good compression rate of the text.  3.2 Byte Pair Encoding (BPE) Byte Pair Encoding (BPE) (Gage, 1994) is a sim-	[[76, 79], [101, 104]]	[[56, 74], [81, 99]]	['BPE', 'BPE']	['Byte Pair Encoding', 'Byte Pair Encoding']
1144	 2.2 Thread-level analysis Next, we perform named entity recognition (NER) over each thread to identify entities such as package	[[70, 73]]	[[44, 68]]	['NER']	['named entity recognition']
1145	and tfidf of unigrams, bigrams, and trigrams.  DAL (Dictionary of Affect in Language) is a psycholinguistic resource to measure the emo-	[[47, 50]]	[[52, 84]]	['DAL']	['Dictionary of Affect in Language']
1146	nuqaqa llakiy qhachqa p?achakunata churakurqani.  Abbreviations: AMLQ = Academia Mayor de la Lengua Quechua en Cusco, norm = normalized, span = Spanish orthography, boliv = (old) Bolivian orthography Table 1: Different Orthographies with Corresponding Standardized Version	[[65, 69]]	[[72, 107]]	['AMLQ']	['Academia Mayor de la Lengua Quechua']
1147	collected as follows: Positive Diccn: 3,730 Chinese positive terms (e.g., /good-looking, / lucky) were collected from the Chinese Vocabulary for Sentiment Analysis (VSA)20 released by HOWNET.	[[165, 168], [184, 190]]	[[130, 163]]	['VSA', 'HOWNET']	['Vocabulary for Sentiment Analysis']
1148	 The system includes four main stages: topic classification, named entity recognition (NER), disease/location detection, and visualization.	[[87, 90]]	[[61, 85]]	['NER']	['named entity recognition']
1149	based classifier is used to select the most informative examples for training an another type of  classifier based on multinomial na?ve Bayes (NB)  model (McCallum and Nigam, 1998b).	[[143, 145]]	[[130, 141]]	['NB']	['na?ve Bayes']
1150	bic Diacritics. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and the 44th Annual Meeting of the the Association	[[98, 104]]	[[71, 96]]	['COLING']	['Computational Linguistics']
1151	any hypotheses between frames and events.  (2) SameFrame (SF) is the second baseline system, which applies H1 over the results from AN-	[[58, 60]]	[[47, 56]]	['SF']	['SameFrame']
1152	In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL). 	[[92, 95]]	[[49, 90]]	['ACL']	['Association for Computational Linguistics']
1153	? similar count? score (SC) is calculated as the number of characters that match between two	[[24, 26]]	[[17, 22]]	['SC']	['score']
1154	constituent boundary prediction algorithm, the  followiug measures were used:  1) The cost time(CT) of the kernal  functions(CPU: Celeron TM 366, RAM: 64M).	[[96, 98], [125, 128], [138, 140], [146, 149]]	[[86, 94]]	['CT', 'CPU', 'TM', 'RAM']	['cost tim']
1155	   Simple Segmentation Algorithm(SSA):  1.	[[33, 36]]	[[3, 31]]	['SSA']	['Simple Segmentation Algorith']
1156	  For example, the s t ruc ture  tn whtch  ad jec t ives  (ADJ) repeat  a rb i t ra ry  ttmes and a noun  (N) fo l lows  them tn Engl lsh ts expressed as 	[[59, 62]]	[[43, 56], [100, 104]]	['ADJ']	['ad jec t ives', 'noun']
1157	ALO ( in to )   RO (OST LOC)  PO (PRE)  ON (VO)) 	[[34, 37], [0, 3], [16, 18], [20, 23], [24, 27], [44, 46]]	[[30, 32]]	['PRE', 'ALO', 'RO', 'OST', 'LOC', 'VO']	['PO']
1158	be intuitively characterized as a way of trigger-  ing semantically related concepts which define for  each role the projective conclusion space (PCS). 	[[146, 149]]	[[117, 144]]	['PCS']	['projective conclusion space']
1159	September/NNP \] \[O ./. \]  we can extract following chunk patterns:  NP=NULL 90 PRP 99 VBZ  VP=PRP 99 VBZ 99 DT 	[[74, 78], [10, 13], [71, 73], [82, 85], [89, 92], [94, 96], [97, 100], [104, 107], [111, 113]]	[]	['NULL', 'NNP', 'NP', 'PRP', 'VBZ', 'VP', 'PRP', 'VBZ', 'DT']	[]
1160	term t appears in position i around the entity.  Bigram Context (BCON): The bigram-based  context model was built in a similar way to UCON, 	[[65, 69], [134, 138]]	[[49, 63]]	['BCON', 'UCON']	['Bigram Context']
1161	In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST), pages 25?32, Rochester, NY.	[[70, 74], [3, 8], [101, 103]]	[[45, 68]]	['SSST', 'NAACL', 'NY']	['Statistical Translation']
1162	search engine, e.g., Google, Bing, etc. and (ii) a database (DB) system for accessing previously resolved crossword puz-	[[61, 63]]	[[51, 59]]	['DB']	['database']
1163	extraction (RE) in a bootstrapping framework are regarded as very effective methods for building information extraction (IE) systems and for adapting them to new domains (e. g., (Riloff,	[[121, 123], [12, 14]]	[[97, 119]]	['IE', 'RE']	['information extraction']
1164	ISREM 1 iff the candidate occurs 2 or more sentences before the anaphor POSITION 1 iff the antecedent occurs before anaphor SEMANTIC ROLE LABELLING (SR) IVERB 1 iff the governing verb of the given candidate is an issue verb	[[149, 151], [0, 5], [72, 80], [153, 158]]	[[124, 137]]	['SR', 'ISREM', 'POSITION', 'IVERB']	['SEMANTIC ROLE']
1165	using the written and spoken language corpora.  Occurrence probabilities (OPs) of expressions in the written and spoken language corpora can be used to dis-	[[74, 77]]	[[48, 72]]	['OPs']	['Occurrence probabilities']
1166	We experiment with multiple ways to select a snippet: the first 50 words of the summary (START), the last 50 words (END) and 50 words starting at a randomly chosen sentence	[[89, 94], [116, 119]]	[[80, 87]]	['START', 'END']	['summary']
1167	Pattern Pattern Patterns composed of high frequency words (HFWs) 4	[[59, 63]]	[[37, 57]]	['HFWs']	['high frequency words']
1168	that combining additional knowledge sources, including lexical features (LX1) and the non-verbal features, prosody (PROS), motion (MOT), and context (CTXT), yields a further improvement (of 8.8%	[[116, 120], [131, 134], [73, 75], [150, 154]]	[[107, 114], [123, 129], [55, 70], [141, 148]]	['PROS', 'MOT', 'LX', 'CTXT']	['prosody', 'motion', 'lexical feature', 'context']
1169	(ADV), cause (CAU), direction (DIR), extent (EXT), location (LOC), manner (MNR), and time (TMP), modal verbs (MOD), negative markers (NEG), and discourse connectives (DIS). 	[[134, 137], [1, 4], [14, 17], [31, 34], [45, 48], [61, 64], [75, 78], [91, 94], [110, 113], [167, 170]]	[[116, 124], [7, 12], [20, 29], [37, 43], [51, 59], [67, 73], [85, 89], [97, 102], [144, 153]]	['NEG', 'ADV', 'CAU', 'DIR', 'EXT', 'LOC', 'MNR', 'TMP', 'MOD', 'DIS']	['negative', 'cause', 'direction', 'extent', 'location', 'manner', 'time', 'modal', 'discourse']
1170	than three thousand five hundred days of imprisonment .  Figure 4: Example translation using the back-off and the continuous space language model (CSLM). 	[[147, 151]]	[[114, 145]]	['CSLM']	['continuous space language model']
1171	Inspired by the work of web search (Gao et al2010) and question retrieval in community question answer (Q&A) (Zhou et al2011), we assume the following generative	[[104, 107]]	[[87, 102]]	['Q&A']	['question answer']
1172	As an extension, Zhang et al (2008a) proposed two more categories: Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR).	[[95, 98], [132, 135]]	[[67, 93], [104, 130]]	['SRR', 'DPR']	['Structure Reordering Rules', 'Discontiguous Phrase Rules']
1173	the MACH-III expert system, we should begin with  a brief description. Functional hierarchy (FH) is a  new paradigm for organizing expert system 	[[93, 95], [4, 12]]	[[71, 91]]	['FH', 'MACH-III']	['Functional hierarchy']
1174	In this paper we investigate the relation between positive and negative pairs in Textual Entailment (TE), in order to highlight the role of contradiction in TE	[[101, 103], [157, 159]]	[[81, 99]]	['TE', 'TE']	['Textual Entailment']
1175	typically expressed inTopic Statements.  The Subject Field Coder (SFCoder) uses an  establishet~ semantic oding scheme from the machine- 	[[66, 73]]	[[45, 64]]	['SFCoder']	['Subject Field Coder']
1176	abbreviations. ACM Transactions on Information Systems (TOIS), 24(3):380?404. 	[[56, 60], [15, 18]]	[[19, 54]]	['TOIS', 'ACM']	['Transactions on Information Systems']
1177	The evaluation strategy follows the global standard as  Text Retrieval Conference (TREC)8 metrics. It 	[[83, 87]]	[[56, 81]]	['TREC']	['Text Retrieval Conference']
1178	method for unambiguous bilingual segmentation where tokens are defined as minimal phrases, called minimal translation units (MTUs). Figure 1	[[125, 129]]	[[98, 123]]	['MTUs']	['minimal translation units']
1179	for PTB III data evaluated by label accuracy system test additional resources JESS-CM (CRF/HMM) 95.15 1G-word unlabeled data 94.67 15M-word unlabeled data	[[87, 94], [4, 7], [78, 85], [102, 109]]	[]	['CRF/HMM', 'PTB', 'JESS-CM', '1G-word']	[]
1180	 We used an online-large margin algorithm, MIRA (McDonald and Pereira, 2006; Crammer et al, 2005), for updating the weights.	[[43, 47]]	[[49, 69]]	['MIRA']	['McDonald and Pereira']
1181	notation for real-world applications. In Machine Translation (MT) Quality Estimation (QE), for instance, using human-	[[62, 64], [86, 88]]	[[41, 60], [66, 84]]	['MT', 'QE']	['Machine Translation', 'Quality Estimation']
1182	2011a.  Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011.	[[45, 47], [57, 63]]	[[24, 43]]	['ID', 'BioNLP']	['Infectious Diseases']
1183	 ? Term Base eXchange (TBX): XML Terminology Exchange Standard.	[[23, 26]]	[[3, 21]]	['TBX']	['Term Base eXchange']
1184	There were four data sources used in the  training set: the Wall Street Journal, Associated  Press, Federal Register (FR), and Department of  215 	[[118, 120]]	[[100, 116]]	['FR']	['Federal Register']
1185	resolution pipeline consisted primarily of the C&C parser and Boxer (Curran et al, 2007), which produce Discourse Representation Structures (DRSs). 	[[141, 145], [47, 50]]	[[104, 139]]	['DRSs', 'C&C']	['Discourse Representation Structures']
1186	" B E B 	E B  B FE B AE EA B  B ) B , B 	AE	 EA B E) B FE+$EB AAF	H0 B  B$ B  EF B ,FIJE"" B /66B0& B( B 	 B E B 	 		)E B 	 B  B  B E	EAB 	AF	AEBB	BEABB$FB$BE	)BF	BEBEDEEB	B	B EF&B(EBFAEBA	B BFEBB	BB	'EBB	FFBB$BE&B(EEE""B$AEBB	B EBEEF EBBEB"	[[300, 307]]	[[309, 326]]	['EF\x07\x02\x14&B']	['EB\x04\x14\x06\x08F\x06\x04AEBA\x06\x03\x06\x07']
1187	(Schubert 1987; Maxwell & Schubert 1989), and fig-  ure 1 shows the dependency trees for this example,  cross-coded for translation units (TUs). Each ellipse 	[[139, 142]]	[[120, 137]]	['TUs']	['translation units']
1188	edges. The NEs includes personal name(PRE), location name(LOC) and organization name(ORG). 	[[85, 88], [11, 14], [38, 41], [58, 61]]	[[67, 79], [24, 32], [44, 52]]	['ORG', 'NEs', 'PRE', 'LOC']	['organization', 'personal', 'location']
1189	We  describe our approach towards building a Wordnet for Tunisian dialect (TD). We proceed, first-	[[75, 77]]	[[57, 73]]	['TD']	['Tunisian dialect']
1190	Note that for the purposes of Figure 1: Example Babytalk Input Data: Sensors HR = Heart Rate; TcPO2 = blood O2 level; TCPCO2 = blood CO2 level; SaO2 = oxygen saturation; T1 = chest	[[77, 79], [94, 99], [118, 124], [144, 148], [170, 172]]	[[82, 92], [102, 110], [127, 136], [151, 168]]	['HR', 'TcPO2', 'TCPCO2', 'SaO2', 'T1']	['Heart Rate', 'blood O2', 'blood CO2', 'oxygen saturation']
1191	3.1 Data and preprocessing All embeddings are trained on 22 million tokens from the the North American News Text (NANT) corpus (Graff, 1995).	[[114, 118]]	[[88, 112]]	['NANT']	['North American News Text']
1192	Extraction Abbreviations NE = Named Entity CE = Correlated Entity	[[25, 27], [43, 45]]	[[30, 42], [48, 65]]	['NE', 'CE']	['Named Entity', 'Correlated Entity']
1193	"internal structure. This type of expression is an example  of what will be called a ""comp\]ex basic expression"" (CBE). "	[[113, 116]]	[[85, 110]]	['CBE']	['comp\\]ex basic expression']
1194	bath?). The builder can choose to represent binaries as either relational noun phrases (RELNP) or generalized transitive verbs (VP/NP).	[[88, 93], [128, 133]]	[[63, 86], [121, 126]]	['RELNP', 'VP/NP']	['relational noun phrases', 'verbs']
1195	the vertex of the Abox which it expresses. Our current model uses the Tree Adjoining Grammar (TAG) formalism, see Joshi (1987), and the Atree acts as a	[[94, 97]]	[[70, 92]]	['TAG']	['Tree Adjoining Grammar']
1196	The generator  uses as its linguistic resource a lexicon encoded in a version  of Categorial Grammar (CG), the extension of which with  rules of function composition gives rise to a problem of 	[[102, 104]]	[[82, 100]]	['CG']	['Categorial Grammar']
1197	spectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews).	[[80, 85], [128, 133]]	[[70, 78], [112, 126]]	['NWire', 'BNews']	['newswire', 'broadcast news']
1198	25 Techniques like self-training (SELF) and system combinations (COMBO) can further improve parsing accuracies, but are also orthogonal to our work.	[[65, 70], [34, 38]]	[[51, 63], [19, 32]]	['COMBO', 'SELF']	['combinations', 'self-training']
1199	 ? Domain communicat ion knowledge (DCK). This is knowledge about how to communi- 	[[36, 39]]	[[3, 34]]	['DCK']	['Domain communicat ion knowledge']
1200	the first optimum solution is obtained.  (c) EPN for the last optimum solution (EPN-L): The number of the expanded problems when	[[80, 85]]	[[45, 78]]	['EPN-L']	['EPN for the last optimum solution']
1201	build a bridge between UNL and one of the  internal representations of ETAP, namely  Normalized Syntactic Structure (NormSS), and in  this way link UNL with all other levels of text 	[[117, 123], [23, 26], [71, 75], [148, 151]]	[[85, 115]]	['NormSS', 'UNL', 'ETAP', 'UNL']	['Normalized Syntactic Structure']
1202	by ranking the set of utterances by our confidence that they contain the query word, a task known as Ranked Utterance Retrieval (RUR). In particular,	[[129, 132]]	[[101, 127]]	['RUR']	['Ranked Utterance Retrieval']
1203	non relevant texts has the lower expectation. Figure 1 describes the probability density function (PDF ) for domain frequency scores of the SPORT domain	[[99, 102], [140, 145]]	[[69, 97]]	['PDF', 'SPORT']	['probability density function']
1204	 These features of Latin influenced the choice  of Dependency Grammars (DG)2 as the most  suitable grammar framework for building Latin 	[[72, 74]]	[[51, 70]]	['DG']	['Dependency Grammars']
1205	PER (PN), only PER candidates beginning with  the family name is considered. For PER (FN), a  candidate is generated only if all its composing 	[[86, 88], [0, 3], [5, 7], [15, 18]]	[[77, 84]]	['FN', 'PER', 'PN', 'PER']	['For PER']
1206	(BN) dev04f, rt03 and rt04 task using the stateof-the-art acoustic models trained on the English Broadcast News (BN) corpus (430 hours of audio) provided to us by IBM (Chen et al, 2009).	[[113, 115], [1, 3], [163, 166]]	[[97, 111]]	['BN', 'BN', 'IBM']	['Broadcast News']
1207	on a 1,000 TU, EN-IT test set. B=Basic, LI=language identification, QE=quality estimation, WE=word embedding. 	[[68, 70], [91, 93], [11, 13], [15, 20], [33, 38], [40, 42]]	[[71, 89], [94, 108], [43, 66]]	['QE', 'WE', 'TU', 'EN-IT', 'Basic', 'LI']	['quality estimation', 'word embedding', 'language identification']
1208	605  NP- - - -NP:NP NP=S:N I '  VP~-VP:VP  S~---S:S  NP=NI ' :PP  NP~-PP :NP  VP=VP:NP  S=S:N I '   NP  ~.-~NP :VP  PP -~-PP :PP  VP=VP:P I  ) S~S:  	[[56, 58], [5, 7], [14, 16], [17, 19], [20, 22], [32, 38], [39, 41], [53, 55], [62, 64], [66, 72], [74, 76], [78, 80], [100, 102], [112, 114], [116, 118], [126, 128], [130, 132]]	[[59, 62], [81, 86], [90, 93], [133, 137]]	['NI', 'NP', 'NP', 'NP', 'NP', 'VP~-VP', 'VP', 'NP', 'PP', 'NP~-PP', 'NP', 'VP', 'NP', 'VP', 'PP', 'PP', 'VP']	"[""' :"", 'VP:NP', 'S:N', 'VP:P']"
1209	follows. NLC(:A): the analysis of concepts that play  a role in natural anguage; (NL)CA: the lattice the-  Karaphuis and Sarbo 205 Natural Language Concept Analysis 	[[82, 84], [9, 12], [85, 87]]	[[64, 79]]	['NL', 'NLC', 'CA']	['natural anguage']
1210	matical relations on the arcs).  Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corre-	[[55, 57]]	[[33, 53]]	['GR']	['Grammatical Relation']
1211	the interpolated and adapted models are compared.  For the Estonian task, letter error rate (LER) is also reported, since it tends to be a more indicative	[[93, 96]]	[[74, 91]]	['LER']	['letter error rate']
1212	NE translation.   Although NE translation is less sophisticated than machine translation (MT) in general, to some extent, the issues in NE translation are similar to those in MT.	[[90, 92], [0, 2], [27, 29], [136, 138], [175, 177]]	[[69, 88]]	['MT', 'NE', 'NE', 'NE', 'MT']	['machine translation']
1213	OWL DL is a logical language that combines the expressivity of OWL2 with the favourable computational properties of Description Logics (DL), notably decidability and monotonicity (Baader et al, 2003).	[[136, 138], [0, 3], [4, 6], [63, 67]]	[[116, 134]]	['DL', 'OWL', 'DL', 'OWL2']	['Description Logics']
1214	 2 Named Entity Extraction  Named Entity Recognition (NER) is useful in  NLP applications such as question answering, 	[[54, 57], [73, 76]]	[[28, 52]]	['NER', 'NLP']	['Named Entity Recognition']
1215	 Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996),	[[57, 59]]	[[37, 55]]	['MD']	['minimum divergence']
1216	Finally, Lampert, Dale, and Paris (2006) describe a statistical classifier trained on text-based features for automatically predicting eight different speech acts derived from a taxonomy called Verbal Response Modes (VRM). The experiments are conducted	[[217, 220]]	[[194, 215]]	['VRM']	['Verbal Response Modes']
1217	  4.1 Speech recognition   The automatic speech recognition module (ASR)  is based on the Sphinx 4 system (Lamere et al, 	[[68, 71]]	[[31, 59]]	['ASR']	['automatic speech recognition']
1218	534 3.1 Cross Validation on the Training Queries Random Walk with Restart (RWR) (also called personalized PageRank (Haveliwala, 2002)) is a	[[75, 78]]	[[49, 73]]	['RWR']	['Random Walk with Restart']
1219	transcription is carried out by using dynamic  programming alignment on the recognizer?s  hypothesis (HYP) and the non-literal transcription  that is used as reference (REF).	[[102, 105], [169, 172]]	[[90, 100], [158, 167]]	['HYP', 'REF']	['hypothesis', 'reference']
1220	 rio deal with unknown words is a big problem in  natural language processing(NLP) too. To recognize 	[[78, 81]]	[[50, 76]]	['NLP']	['natural language processin']
1221	 3.4.1 Arabic Named Entity Recognition Named Entity Recognition (NER) is a subtask of information extraction, where each proper name in	[[65, 68]]	[[39, 63]]	['NER']	['Named Entity Recognition']
1222	University of Massachusetts-Boston  Abstract  Word sense disambiguation (WSD) is one of  the main challenges in Computational 	[[73, 76]]	[[46, 71]]	['WSD']	['Word sense disambiguation']
1223	To standardize the measures to have fixed bounds, (Strehl and Ghosh, 2003) defined the normalized Mutual Information (NMI) as: NMI(Cr,Cg) =	[[118, 121], [127, 130]]	[[87, 116]]	['NMI', 'NMI']	['normalized Mutual Information']
1224	These models, variously known as vector spaces, semantic spaces, word spaces, corpus-based semantic models, or, using the term we will adopt, distributional semantic models (DSMs), all rely on some version of the distributional hypothesis (Harris 1954; Miller and Charles 1991), stating that the degree	[[174, 178]]	[[142, 172]]	['DSMs']	['distributional semantic models']
1225	with the overall metric of error per response fill  (ERR), overgeneration (OVG) does not correlate  with it, and substitution (SUB) correlates with it  only to a limited extent.	[[127, 130], [53, 56], [75, 78]]	[[113, 125], [59, 73]]	['SUB', 'ERR', 'OVG']	['substitution', 'overgeneration']
1226	Hajdinjak and Mihelic? The PARADISE Evaluation Framework Number of help messages (NHM) and help-message ratio (HMR), i.e., the number and the ratio of system?s help messages;	[[82, 85], [111, 114]]	[[57, 80], [91, 109]]	['NHM', 'HMR']	['Number of help messages', 'help-message ratio']
1227	for topic models. In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI), pages 27?34.	[[99, 102]]	[[59, 97]]	['UAI']	['Uncertainty in Artificial Intelligence']
1228	the predicted margin. Dredze and Crammer (2008a) showed how Confidence Weighted (CW) learning could be used to generate a more informative mea-	[[81, 83]]	[[60, 79]]	['CW']	['Confidence Weighted']
1229	In Proceedings of the 15th International Conference on Computational Linguistics (COLING?94), pages 1145?1150, Kyoto, Japan.	[[82, 91]]	[[55, 80]]	['COLING?94']	['Computational Linguistics']
1230	 1 Introduction In statistical machine translation (SMT) reordering (also called distortion) refers to the order in	[[52, 55]]	[[19, 50]]	['SMT']	['statistical machine translation']
1231	use the two cluster class features. The other selected features and the chosen algorithms (CL) are displayed in Table 1.	[[91, 93]]	[[72, 89]]	['CL']	['chosen algorithms']
1232	Section 2  introduces some relevant work in IR and  question answering (QA). Section 3 talks about 	[[72, 74], [44, 46]]	[[52, 70]]	['QA', 'IR']	['question answering']
1233	lar expressions. So the detection of factoid words  can be achieved by Finite State Automaton(FSA). 	[[94, 97]]	[[71, 92]]	['FSA']	['Finite State Automato']
1234	Following are the two broad types of social events that were annotated: Interaction event (INR): When both entities participating in an event are aware of each other and of	[[91, 94]]	[[72, 83]]	['INR']	['Interaction']
1235	67  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 189?195, October 25-29, 2014, Doha, Qatar.	[[92, 97]]	[[42, 90]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
1236	lead to over-fitting. Therefore, we propose another method, Probabilistic Soft Logic (PSL) (Broecheler et al, 2010).	[[86, 89]]	[[60, 84]]	['PSL']	['Probabilistic Soft Logic']
1237	joining grammars. In Proceedings of the 12 th International  Conference on Computational Linguistics (COLING'88),  Budapest, Hungary, August 1988.	[[102, 111]]	[[61, 100]]	"[""COLING'88""]"	['Conference on Computational Linguistics']
1238	usually similar with that in word sense disambiguation (WSD), including bag of word lemmas  in the sentence, n-grams and parts of speech (POS)  in a window, etc.	[[138, 141], [56, 59]]	[[121, 136], [29, 54]]	['POS', 'WSD']	['parts of speech', 'word sense disambiguation']
1239	coverage of the language's grammar rules.  This paper introduces a hidden Markov model (HMM) which  has been developed for Japanese word segmentation.	[[88, 91]]	[[67, 86]]	['HMM']	['hidden Markov model']
1240	semantic tree setups are depicted as follows:  TP2TP1 (a) Bag Of Features(BOF) ENT	[[74, 77], [47, 53], [79, 82]]	[[58, 72]]	['BOF', 'TP2TP1', 'ENT']	['Bag Of Feature']
1241	  1. RecallCorrectTransliteration  (RTrans)  The recall was computed using the sample as 	[[36, 42]]	[[5, 33]]	['RTrans']	['RecallCorrectTransliteration']
1242	108   Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, page 1, Gothenburg, Sweden, April 27, 2014.	[[75, 80], [84, 88]]	[[41, 73]]	['HyTra', 'EACL']	['Hybrid Approaches to Translation']
1243	Table 1: Probabilities computed for each type of linguistic information. Error codes correspond to the five error types in the CoNLL 2013 shared task: ArtOrDet (article or determiner), Nn (noun number), Prep (prepositions), SVA (subject-verb agreement) and Vform (verb form).	[[151, 159], [224, 227], [127, 132], [185, 187]]	[[161, 182], [229, 251], [189, 200]]	['ArtOrDet', 'SVA', 'CoNLL', 'Nn']	['article or determiner', 'subject-verb agreement', 'noun number']
1244	We applied 3 MCMC algorithms: Gibbs sampling (GS), MCSAT and Simulated Tempering (ST) for inference and the comparative NER results are shown in Table 1.	[[82, 84]]	[[61, 80]]	['ST']	['Simulated Tempering']
1245	(reduced dimensions). The general idea behind the  Pseudo Relevance Feedback (PRF) (Croft &  Harper, 1979) or its more recent variation called 	[[78, 81]]	[[51, 76]]	['PRF']	['Pseudo Relevance Feedback']
1246	Sparse Lexicalised features and Topic Adaptation for SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 268?275.	[[143, 148], [53, 56]]	[[88, 141]]	['IWSLT', 'SMT']	['International Workshop on Spoken Language Translation']
1247	"is Tws, for ""Translator's Workstation."" We also used the  C-based X11 toolkit called MOTIF (Motif, 1991) and its Com-  monLisp interface called CLM (Babatz et."	[[85, 90]]	[[92, 97]]	['MOTIF']	['Motif']
1248	subsumption hierarchy of Patty is very sparse. It contains only 8,000 hypernymy links between phrases, and the entire taxonomy is kind of fragmented into a many-rooted DAG (directed acyclic graph). More-	[[168, 171]]	[[173, 195]]	['DAG']	['directed acyclic graph']
1249	REF = obj123 SIZE = sizesensorreading85 SHAPE = shapesensorreading62 COLOUR = coloursensorreadning78	[[40, 45], [0, 3], [13, 17], [69, 75]]	[[48, 68], [20, 39], [78, 100], [6, 12]]	['SHAPE', 'REF', 'SIZE', 'COLOUR']	['shapesensorreading62', 'sizesensorreading85', 'coloursensorreadning78', 'obj123']
1250	Proc. ACM Multimedia (MM), ACM, Florence, Italy. pp.	[[22, 24]]	[[15, 20]]	['MM']	['media']
1251	validity of the large margin method is guaranteed by the theorems of Structural Risk Minimization (SRM) under Probably Approximately Correct (PAC) framework2; test error is related to training data error, number of training	[[142, 145], [99, 102]]	[[110, 140], [69, 97]]	['PAC', 'SRM']	['Probably Approximately Correct', 'Structural Risk Minimization']
1252	In addition, adding the soft joint-inference formula results in further gain, and our full system (FULL) attained an F1 of 55.5. 	[[99, 103]]	[[86, 90]]	['FULL']	['full']
1253	possession (PO)  process (PR)  quantity (QU)  relation (RE) 	[[41, 43], [12, 14], [26, 28], [56, 58]]	[[31, 39], [0, 10], [17, 24], [46, 54]]	['QU', 'PO', 'PR', 'RE']	['quantity', 'possession', 'process', 'relation']
1254	In contrast to standard 357 multi-class Word Sense Disambiguation (WSD), it uses a coarse-grained sense inventory that allows to	[[67, 70]]	[[40, 65]]	['WSD']	['Word Sense Disambiguation']
1255	using the distributional similarity metric described by Lin (1998). We use WordNet (WN) as our sense inventory.	[[84, 86]]	[[75, 82]]	['WN']	['WordNet']
1256	impulses are possibly found. It is realized with very  simple space management transition networks (SMTNs),  in which $EXP, a distinguished symbol on an arc, 	[[100, 105]]	[[62, 98]]	['SMTNs']	['space management transition networks']
1257	our system; where we achieved 0.627 top1 accuracy for Japanese transliterated to Japanese Kanji(JJ), 0.713 for English-toChinese(E2C) and 0.510 for English-to-	[[96, 98], [129, 132]]	[[81, 94], [111, 128]]	['JJ', 'E2C']	['Japanese Kanj', 'English-toChinese']
1258	6 6   Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 79?83, Seoul, South Korea, 5-6 July 2012.	[[102, 109]]	[[52, 100]]	['SIGDIAL']	['Special Interest Group on Discourse and Dialogue']
1259	After applying the linguistic phenomena  resolution algorithm we obtain a new slot  structure (SS) that will store both the anaphora  and their antecedents.	[[95, 97]]	[[84, 93]]	['SS']	['structure']
1260	They refer to  syntactical features of a constituent such as number  (NUM), gender (GEN) etc. and to grammatical functions 	[[84, 87]]	[[76, 82]]	['GEN']	['gender']
1261	 1 Introduction Semantic Role Labeling (SRL), independently of the approach adopted, comprehends two steps be-	[[40, 43]]	[[16, 38]]	['SRL']	['Semantic Role Labeling']
1262	3http://www.cjk.org 4https://translit.i2r.a-star.edu.sg/news2009/evaluation/ 5The six metrics are Word Accuracy in Top-1 (ACC), Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank	[[122, 125]]	[[103, 111]]	['ACC']	['Accuracy']
1263	assumption of which is the stratificational  approach to sentence analysis pursued by  Functional Sentence Perspective (FSP), a  linguistic theory developed by Jan Firbas in the 	[[120, 123]]	[[87, 118]]	['FSP']	['Functional Sentence Perspective']
1264	side were installed from 2003 to 2005.?  3.4 Sentence Reordering (RE) Some of the transformation operations results in	[[66, 68]]	[[54, 64]]	['RE']	['Reordering']
1265	Two categories of neural networks have been used for language modeling: 1) recurrent neural networks (RNN), and 2) feedfoward network (FFN):	[[102, 105], [135, 138]]	[[75, 100], [115, 133]]	['RNN', 'FFN']	['recurrent neural networks', 'feedfoward network']
1266	CCG, as well as others.  A combinatory categorial grammar (CCG) is a categorial grammar whose rule system consists of	[[59, 62], [0, 3]]	[[27, 57]]	['CCG', 'CCG']	['combinatory categorial grammar']
1267	 3.3 Question Classification We look next at question classification (QC). 	[[70, 72]]	[[45, 68]]	['QC']	['question classification']
1268	based on such formalisms include Generalized Phrase  Structure Grammar (GPSG) \[Gazdar et al 1985\],  Lexical Functional Grammar (LFG) \[Bresnan 1982\],  Functional Unification Grammar (bUG) \[Kay 1984\], 	[[130, 133], [72, 76], [186, 189]]	[[102, 128], [33, 70], [165, 184]]	['LFG', 'GPSG', 'bUG']	['Lexical Functional Grammar', 'Generalized Phrase  Structure Grammar', 'Unification Grammar']
1269	Computational Linguistics Volume 40, Number 2 Because of their prevalence and irregularity, MWEs must be stored in lexicons of natural language processing (NLP) applications. Awareness of MWEs was proven	[[156, 159], [92, 96], [188, 192]]	[[127, 154]]	['NLP', 'MWEs', 'MWEs']	['natural language processing']
1270	sets for Chinese and English. For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted 2For replicability, a complete description of all features can	[[82, 86]]	[[60, 80]]	['CTB5']	['Chinese Treebank 5.1']
1271	This convexity given the n4 Discounted cumulative gain (DCG) is widely used in information retrieval learning-to-rank settings.	[[56, 59]]	[[28, 54]]	['DCG']	['Discounted cumulative gain']
1272	 The focus of this paper is a discussion of various methods used to create a set of acoustic models for  characterizing the PLU's used in large vocabulary recognition (LVR). The set of context independent 	[[168, 171], [124, 129]]	[[138, 166]]	"['LVR', ""PLU's""]"	['large vocabulary recognition']
1273	 In Proc. Rich Text 2004 Fall Workshop (RT-04F). 	[[40, 46]]	[[10, 29]]	['RT-04F']	['Rich Text 2004 Fall']
1274	In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics(ACL). 	[[91, 94]]	[[49, 89]]	['ACL']	['Association for Computational Linguistic']
1275	of Machine Translation and present an implemetation of a morphological analyser for Amharic using Xerox Finite State Tools (XFST). The different	[[124, 128]]	[[98, 122]]	['XFST']	['Xerox Finite State Tools']
1276	Abstract  This paper presents a new bootstrapping  approach to named entity (NE)  classification.	[[77, 79]]	[[63, 75]]	['NE']	['named entity']
1277	supervised labels to train our user model.  We used Amazon Mechanical Turk (MTurk) to collect data.	[[76, 81]]	[[59, 74]]	['MTurk']	['Mechanical Turk']
1278	are the formal-language theoretic foundation for n-gram models (Garcia et al, 1990), which are widely used in natural language processing (NLP) in part because such distributions can be estimated	[[139, 142]]	[[110, 137]]	['NLP']	['natural language processing']
1279	functions(CPU: Celeron TM 366, RAM: 64M).  2) Prediction precision(PP) =  number of words with correct BPs(CortBP) 	[[67, 69], [10, 13], [23, 25], [103, 106], [107, 113], [31, 34]]	[[46, 66]]	['PP', 'CPU', 'TM', 'BPs', 'CortBP', 'RAM']	['Prediction precision']
1280	columbia, edu  Abstract  Concept To Speech (CTS) systems are  closely related to two other types of 	[[44, 47]]	[[25, 42]]	['CTS']	['Concept To Speech']
1281	 The attribute of a node is one of part of speech (POS), lexical value (LEX), or dependency label (DEP), as for instance LEX(QUEUE0)	[[72, 75], [51, 54], [99, 102], [121, 124], [125, 131]]	[[57, 64], [35, 49], [81, 91]]	['LEX', 'POS', 'DEP', 'LEX', 'QUEUE0']	['lexical', 'part of speech', 'dependency']
1282	for every language. All results in percent. LAS = labeled attachment score, UAS = unlabeled attachment score.	[[44, 47], [76, 79]]	[[50, 74], [82, 108]]	['LAS', 'UAS']	['labeled attachment score', 'unlabeled attachment score']
1283	PROJECT GOALS  This project involves the integration of speech and natural-  language processing for spoken language systems (SLS). The 	[[126, 129]]	[[101, 124]]	['SLS']	['spoken language systems']
1284	tracting sentence plan construction rules from the only publicly available corpus of discourse trees, the RST Discourse Treebank (RST-DT) (Carlson et al, 2002).	[[130, 136]]	[[106, 128]]	['RST-DT']	['RST Discourse Treebank']
1285	Thus, we  name our system for generating compressions the  Adjustable Rate Compressor (ARC).   	[[87, 90]]	[[59, 85]]	['ARC']	['Adjustable Rate Compressor']
1286	machine learning algorithm is used to discover the morph set of the language in question, using minimum description length (MDL) as an optimization criterion.	[[124, 127]]	[[96, 122]]	['MDL']	['minimum description length']
1287	slot-def SN subslot-of SN1 . . . SNn (SN v SN1) . . . ( SN v SNn)	[[33, 36], [9, 11]]	[[38, 46]]	['SNn', 'SN']	['SN v SN1']
1288	descriptions 3800 4247 Table 2: Properties of the annotated two subcorpora, genetics (GEN) and computational linguistics (CL)	[[86, 89], [122, 124]]	[[76, 84], [95, 120]]	['GEN', 'CL']	['genetics', 'computational linguistics']
1289	state%1:03:00; ? AB = abstraction%1:03:00 \ (event%1:03:00 ? state%1:03:00).	[[17, 19]]	[[22, 41]]	['AB']	['abstraction%1:03:00']
1290	We require that the language I has an available Wordnet linked to the Princeton Wordnet (PWN) (Fellbaum, 1998). 	[[89, 92]]	[[70, 87]]	['PWN']	['Princeton Wordnet']
1291	that together with the BOW it yields higher accuracy. Their results show a significant 1 The reciprocal rank (RR) for a question is 1 divided by the rank ordinal of the highest ranked relevant answer.	[[110, 112], [23, 26]]	[[93, 108]]	['RR', 'BOW']	['reciprocal rank']
1292	University of Brighton There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG.	[[154, 157], [284, 287]]	[[125, 152]]	['NLG', 'NLG']	['Natural Language Generation']
1293	I .  INTRODUCTION  Preliminary research on machine translat ion (MT) started soon af ter  computers  became avai lable.	[[65, 67]]	[[43, 59]]	['MT']	['machine translat']
1294	Proc. of the IEEE International Conference on Data Mining (ICDM). 	[[59, 63]]	[[54, 57]]	['ICDM']	['ing']
1295	(AvgToRecipients) in emails sent by p, the percentage of emails p received in which he/she was in the To list (InToList%), boolean features denoting whether p added or removed people when	[[111, 120], [1, 16]]	[[95, 109], [43, 53]]	['InToList%', 'AvgToRecipients']	['in the To list', 'percentage']
1296	regression model. For our regression task we use a Generalised Linear Model (GLM) via penalized maximum likelihood (Friedman et al, 2010).	[[77, 80]]	[[51, 75]]	['GLM']	['Generalised Linear Model']
1297	 org/wiki/California?. It is distinct from named entity extraction (NEE) in that it identifies not the occurrence of names but their reference.	[[68, 71]]	[[43, 66]]	['NEE']	['named entity extraction']
1298	take scope over another.  Those natural language processing (NLP) systems that have managed to provide some sort of account of quantifier scope preferences have done so by using a separate	[[61, 64]]	[[32, 59]]	['NLP']	['natural language processing']
1299	sual scenes. In their seminal work Dale and Reiter (1995) present the Incremental Algorithm (IA) for GRE.	[[93, 95], [101, 104]]	[[70, 91]]	['IA', 'GRE']	['Incremental Algorithm']
1300	e i Algorithm 1 Sparse projection (SP) Require: v // Vocabulary: vector of n words	[[35, 37]]	[[16, 33]]	['SP']	['Sparse projection']
1301	three statistical models: Conditional Random  Fields(CRF), Maximum Entropy(ME), and  Support Vector Machine(SVM), which have  good performance and used widely in the 	[[108, 111], [53, 56], [75, 77]]	[[85, 106], [26, 52], [59, 74]]	['SVM', 'CRF', 'ME']	['Support Vector Machin', 'Conditional Random  Fields', 'Maximum Entropy']
1302	First, we rewrite equation 1 in a more detailed fashion as: A?R = argmax A	[[60, 63]]	[[66, 72]]	['A?R']	['argmax']
1303	outer: the perceived concrete or abstract source, goal, or  location of the action,event, or state  Correspondent (CAR):  inner: the entity perceived as being in correspondence with 	[[115, 118]]	[[100, 113]]	['CAR']	['Correspondent']
1304	! JIM: { Person67 / Person83 / Name18 / (TYPE=&Person, SEX=Male, NAME=NamelS) }  Finally, the association between grammatical functions and 	[[65, 69]]	[[70, 76]]	['NAME']	['NamelS']
1305	(Pierce and Cardie, 2001). A related idea is to use Expectation Maximization (EM) to impute labels.	[[78, 80]]	[[52, 76]]	['EM']	['Expectation Maximization']
1306	prove sentiment classification. In Annual Meeting of the Association for Computational Linguistics (ACL). 	[[100, 103]]	[[57, 98]]	['ACL']	['Association for Computational Linguistics']
1307	Deployment management, enabling rapid deployment of locally tested charac-ters to highly available web servers as well as review and data warehousing functions for both analytic and refinement purposes. The information model is implemented in a re-lational database that fully specifies, relates and allows inquiry and validation of authored infor-mation. Additionally, a complete web application programming interface (API) powers the Roundtable application, providing a transactional framework for data operations as well as user privilege enforcement, but which also allows application expansion. The information model also serves to decouple the authoring representation from the data struc-tures necessary to drive dialogue behavior at runtime.	[[420, 423]]	[[385, 418]]	['API']	['application programming interface']
1308	Approach for Arabic-English Named Entity Translation, Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages (ACL),  University of Michigan, Ann Arbor	[[136, 139], [73, 76]]	[[103, 134]]	['ACL', 'ACL']	['Approaches to Semitic Languages']
1309	include other language skills such as listening and reading. These constitute the integrated (INT) items.	[[94, 97]]	[[82, 92]]	['INT']	['integrated']
1310	[NP : [XNOUNS : MERINO'S (NOUN) HOME (NOUN)] ] [NP : [XNOUNS : MERINO'S (NOUN)] ] [VP : [VERB_GROUP : HOME (VERB)] ] [XPPS : [PP : IN (PREPOSITION )	[[102, 106], [48, 50], [83, 85], [126, 128], [7, 13], [54, 60], [118, 122]]	[[89, 99], [1, 3]]	['HOME', 'NP', 'VP', 'PP', 'XNOUNS', 'XNOUNS', 'XPPS']	['VERB_GROUP', 'NP']
1311	appear in the labeled training data. In this paper, we call their method the latent variable method (LVM). 	[[101, 104]]	[[77, 99]]	['LVM']	['latent variable method']
1312	                                                                  Barcelona, July 2004                                               Association for Computations Linguistics                        ACL Special Interest Group on Computational Phonology (SIGPHON)                                                     Proceedings of the Workshop of the	[[252, 259], [197, 200]]	[[201, 250]]	['SIGPHON', 'ACL']	['Special Interest Group on Computational Phonology']
1313	However, Turkers are not trained to provide reliable annotations for natural language processing (NLP) tasks, and some Turkers attempt to game the system by submitting	[[98, 101]]	[[69, 96]]	['NLP']	['natural language processing']
1314	{xiaoluo,hraghav,vittorio,smaskey,raduf}@us.ibm.com Abstract In natural language question answering (QA) systems, questions often contain terms and	[[101, 103]]	[[81, 99]]	['QA']	['question answering']
1315	We used four machine learning algorithms implemented in Mallet (McCallum, 2002): decision tree, Naive Bayes, maximum entropy (MaxEnt), and conditional random field (CRF).5 Table 4 shows the	[[126, 132], [165, 168]]	[[109, 124], [139, 163]]	['MaxEnt', 'CRF']	['maximum entropy', 'conditional random field']
1316	adv Adverbial words(RB, RBR, RBS)  adj Adjunct word(JJ,JJR,JJS)  advP Adverb phrase(ADVP)  punct Punctuation(,) 	[[84, 88]]	[[70, 82]]	['ADVP']	['Adverb phras']
1317	composition process.  4.1 Tag Guided RNN (TG-RNN) We propose Tag Guided RNN (TG-RNN) to re-	[[42, 48], [77, 83]]	[[26, 40], [61, 75]]	['TG-RNN', 'TG-RNN']	['Tag Guided RNN', 'Tag Guided RNN']
1318	S = Spanish  JV = Joint Venture ?????????? ME = Microelectronics  Brief History of the Message Understanding Conferences	[[43, 45], [13, 15]]	[[48, 64], [4, 11], [18, 31]]	['ME', 'JV']	['Microelectronics', 'Spanish', 'Joint Venture']
1319	Learning dependency-based compositional semantics.  In Association for Computational Linguistics (ACL). 	[[98, 101]]	[[55, 96]]	['ACL']	['Association for Computational Linguistics']
1320	In STS, we encoded only similarity feature between the two sentences. Thus, we used two classes of kernels: (1) the syntactic/semantic class (SS) with the final kernel defined as K(p 1	[[142, 144], [3, 6]]	[[126, 140]]	['SS', 'STS']	['semantic class']
1321	to prevent this class of mistakes. To put it another way, we hoped to exploit the correlation between named-entities and noun phrase (NP) boundaries. A	[[134, 136]]	[[121, 132]]	['NP']	['noun phrase']
1322	 5 Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context in which it occurs. { 4,5}	[[29, 32]]	[[3, 27]]	['WSD']	['Word Sense Disambiguatio']
1323	 Conf. on Language Resources and Evaluation (LREC), pages 147?152, Las Palmas, Spain, May.	[[45, 49]]	[[10, 28]]	['LREC']	['Language Resources']
1324	Table 4: Reranking results (%BLEU on TEST).  Discriminative Word/Tag LMs (DISC): For each language pair, we generated 10,000-best lists for	[[74, 78], [69, 72], [37, 41], [29, 33]]	[[45, 59]]	['DISC', 'LMs', 'TEST', 'BLEU']	['Discriminative']
1325	274  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1192?1202, October 25-29, 2014, Doha, Qatar.	[[93, 98]]	[[43, 91]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
1326	(Suchanek et al 2007) have been playing a pivotal role in many AI applications, such as relation extraction(RE), question answering(Q&A), etc. 	[[132, 135], [108, 110], [63, 65]]	[[113, 130], [88, 107]]	['Q&A', 'RE', 'AI']	['question answerin', 'relation extraction']
1327	1093  Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 1?9, Gothenburg, Sweden, April 26-30 2014.	[[91, 96], [25, 29]]	[[47, 89]]	['TTNLS', 'EACL']	['Type Theory and Natural Language Semantics']
1328	 The machine receives natural language input (text)  with referring expressions (RE), and possibly other  input (e.g. mouse clicks on a screen) with pseudo- 	[[81, 83], [46, 50]]	[[58, 79], [39, 44]]	['RE', 'text']	['referring expressions', 'input']
1329	 4 . i .1  Bagging (BAG)  From a training set of n examples, severaI sam- 	[[20, 23]]	[[11, 18]]	['BAG']	['Bagging']
1330	 In table 1, we present the accuracy of the model trained on the output of the joint inference (JOINT) against that of the self-training baseline (SELF).	[[96, 101], [147, 151]]	[[79, 84], [123, 136]]	['JOINT', 'SELF']	['joint', 'self-training']
1331	the earliest in the passage is returned. We used the selective gain computation (SGC) algorithm (Zhou et al, 2003) to select features and estimate	[[81, 84]]	[[53, 79]]	['SGC']	['selective gain computation']
1332	Vector Machines (SVM) with radial basis kernel, Na??ve Bayes (NB), J48 Decision Trees (DT), and Neural Networks (NN) with back propagation. In	[[113, 115], [17, 20], [62, 64], [87, 89]]	[[96, 111], [0, 15], [48, 60], [71, 85]]	['NN', 'SVM', 'NB', 'DT']	['Neural Networks', 'Vector Machines', 'Na??ve Bayes', 'Decision Trees']
1333	Computing Center ,  Academy of Sc iences ,  Hosoow, USSR  1.  Personal  Computer Systems (POS) represent  nowadays a  s ign i f teaut  t rend  in  the professiona~, and amateur use of  	[[90, 93], [52, 56]]	[]	['POS', 'USSR']	[]
1334	Networks A more similar model to the proposed larger-context recurrent language model is a hierarchical recurrent encoder decoder (HRED) proposed recently by Serban et al (2015).	[[131, 135]]	[[91, 129]]	['HRED']	['hierarchical recurrent encoder decoder']
1335	cessing. In Proceedings of the 2nd International Conference on Knowledge Capture(K-CAP). USA.	[[81, 86], [89, 92]]	[[63, 80]]	['K-CAP', 'USA']	['Knowledge Capture']
1336	an example of a low-pass filter. The concept of recursion is next introduced in order to pave the way for a discussion of IIR (Infinite Impulse Response) filters. High-, low-, and	[[122, 125]]	[[127, 152]]	['IIR']	['Infinite Impulse Response']
1337	grammars. In Proceedings of the Fourteenth International Conference on Computational Linguistics (COLING-92), pages 426?432, Nantes, 1992.	[[98, 107]]	[[57, 96]]	['COLING-92']	['Conference on Computational Linguistics']
1338	This method is much simpler than the ILP method, while it can achieve comparable result on the CLANG (Coach Language) and Query corpus. 	[[95, 100], [37, 40]]	[[102, 116]]	['CLANG', 'ILP']	['Coach Language']
1339	AVERAGE 3.31 316 72.58% 78.02% 84.65% Table 2: Word sense disambiguation results, including two baselines (MFS = most frequent sense; LeskC = Lesk-corpus) and the word sense disam-	[[107, 110], [134, 139]]	[[113, 132], [142, 153]]	['MFS', 'LeskC']	['most frequent sense', 'Lesk-corpus']
1340	ifications) to be the first on the COMPS list, and further assigns a positive value for an additional feature INV (inverted) on verbs. This feature may	[[110, 113], [35, 40]]	[[115, 123]]	['INV', 'COMPS']	['inverted']
1341	tion access tasks. Current approaches to AZ rely on supervised machine learning (ML). 	[[81, 83], [41, 43]]	[[63, 79]]	['ML', 'AZ']	['machine learning']
1342	(? 2.2), we propose an induction algorithm based on Integer Linear Programming (ILP). Figure 2	[[80, 83]]	[[52, 78]]	['ILP']	['Integer Linear Programming']
1343	portance of the edge features and the resultant largemargin constraint, we also compare against a standard binary Support Vector Machine (SVM) which uses node features alone to predict whether each	[[138, 141]]	[[114, 136]]	['SVM']	['Support Vector Machine']
1344	In frame semantics, the meaning of words or word expressions, also called target words (TW), comprises aspects of conceptual structures, or frames, that de-	[[88, 90]]	[[74, 86]]	['TW']	['target words']
1345	maps: 2-d space-filling approach. ACM Transactions on Graphics (TOG), 11(1):92?99. 	[[64, 67], [34, 37]]	[[38, 62]]	['TOG', 'ACM']	['Transactions on Graphics']
1346	tion of predicate signs. Ramchand divides events into a maximum of three hierarchical phrases: an initiation phrase (InitP), a process phrase (ProcP), and a result phrase (ResP).	[[117, 122], [143, 148]]	[[98, 115], [127, 141]]	['InitP', 'ProcP']	['initiation phrase', 'process phrase']
1347	 (Choudhury et al, 2007) modeled each standard English word as a hidden Markov model (HMM) and calculated the probability of observing the noisy-	[[86, 89]]	[[65, 84]]	['HMM']	['hidden Markov model']
1348	parts of speech, and for different confidence levels. We compare our method to the Semantic Orientation from PMI (SO-PMI) method described in Turney (2002), the Spin model described in Takamura, Inui, and Okumura (2005), the shortest path method	[[114, 120]]	[[83, 112]]	['SO-PMI']	['Semantic Orientation from PMI']
1349	[5] Corbett, J. C., M. B. Dwyer, J. Hatcliff, S. Laubach, C. S. Pasareanu, Robby and H. Zheng, Bandera: Extracting finite-state models from java source code, in: Proceedings of the International Conference on Software Engineering (ICSE), 2000. 	[[231, 235]]	[[181, 229]]	['ICSE']	['International Conference on Software Engineering']
1350	Web Search 1 Figure 1: Architecture of the Multi-task Deep Neural Network (DNN) for Representation Learning: The lower layers are shared across all tasks, while top layers are task-specific.	[[75, 78]]	[[54, 73]]	['DNN']	['Deep Neural Network']
1351	non-terminals as leaves. Later, Moschitti (2006) introduced the Partial Tree Kernels (PTK), by allowing fragments with partial rule expansions.	[[86, 89]]	[[64, 84]]	['PTK']	['Partial Tree Kernels']
1352	 The motivation for that work is twofold: on the one hand it builds on the strength of the first sense heuristic in Word Sense Disambiguation (WSD) (i.e. the heuristic of choosing themost commonly used sense of a word, irrespective of the context in which	[[143, 146]]	[[116, 141]]	['WSD']	['Word Sense Disambiguation']
1353	F-measure between precision and recall over the sure and possibles. They argue that it is a better alternative to the commonly used Alignment Error Rate (AER), which does not sufficiently penalize unbalanced precision and recall.9 As our corpus is mono-	[[154, 157]]	[[132, 152]]	['AER']	['Alignment Error Rate']
1354	Linear-chain CRFs correspond to finite state machines, and can be roughly understood as conditionally-trained hidden Markov models (HMMs). This class of CRFs	[[132, 136]]	[[110, 130]]	['HMMs']	['hidden Markov models']
1355	 Opennlp maxent1, an implementation of Maximum Entropy (ME) modeling, is used as the classification tool.	[[56, 58]]	[[39, 54]]	['ME']	['Maximum Entropy']
1356	Introduction  The pro jec t  note  presents  the  computer  program  GECO (GEometry COnsu l te r ) ,  wh ich  generates   exp lanat ions  (descr ip t ions )  o f  geometr i ca l  	[[69, 73]]	[[75, 96]]	['GECO']	['GEometry COnsu l te r']
1357	4.2 Proposed Model : PNB (vs. UM) Figure 1 shows the performances of our new model named Poisson naive Bayes(PNB) classifiers acTable 2: Performances of UM and PNB on the	[[109, 112], [21, 24], [153, 155], [160, 163], [30, 32]]	[[89, 107]]	['PNB', 'PNB', 'UM', 'PNB', 'UM']	['Poisson naive Baye']
1358	rately hand-designing a large set of features (feature engineering) and utilizing the existing supervised natural language processing (NLP) toolkits and resources (i.e name tagger, parsers, gazetteers	[[135, 138]]	[[106, 133]]	['NLP']	['natural language processing']
1359	and Ripper, on the other hand, appear to take more advantage of some feature types than others. For the third task, lexical (LX) and discourse (DS) features apparently have more predictive power for both C4.5 and SVM than the other types.	[[125, 127], [144, 146], [213, 216]]	[[116, 123], [133, 142]]	['LX', 'DS', 'SVM']	['lexical', 'discourse']
1360	IGT-XML. At the heart of the model is a representation of interlinearized glossed text (IGT). Building	[[88, 91], [0, 7]]	[[58, 86]]	['IGT', 'IGT-XML']	['interlinearized glossed text']
1361	son, 2012; Vlas and Robinson, 2011). Due to its expressiveness, natural language (NL) became a popular medium of communication between users and	[[82, 84]]	[[64, 80]]	['NL']	['natural language']
1362	Knowledge-free induction of morphology using latent semantic analysis. In Proceedings of the Conference on Natural Language Learning 2000 (CoNLL-2000), pages 67?72, Lisbon, Portugal.	[[139, 149]]	[[93, 137]]	['CoNLL-2000']	['Conference on Natural Language Learning 2000']
1363	We conducted experiments on a number of different datasets: (1) the English Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al, 1993) with standard POS	[[97, 100], [163, 166]]	[[76, 95]]	['WSJ', 'POS']	['Wall Street Journal']
1364	tual distributions (Miller and Charles, 1991). In concrete, distributional semantic models (DSMs) use vectors that keep track of the contexts (e.g.,	[[92, 96]]	[[60, 90]]	['DSMs']	['distributional semantic models']
1365	We have adapted the list from Rambow et al(2006) for Arabic, and call it here CORE12. It contains the following tags: verb (V), noun (N), adjective (AJ), adverb (AV), proper noun (PN), pronoun (PRO), relative pronoun (REL), preposition (P), conjunction (C), particle	[[149, 151], [162, 164], [180, 182], [194, 197], [218, 221], [78, 84]]	[[138, 147], [118, 122], [128, 132], [154, 160], [167, 178], [185, 192], [200, 216], [224, 235], [241, 252]]	['AJ', 'AV', 'PN', 'PRO', 'REL', 'CORE12']	['adjective', 'verb', 'noun', 'adverb', 'proper noun', 'pronoun', 'relative pronoun', 'preposition', 'conjunction']
1366	lists are derived automatically from the training data.  Frequent Word List (FWL) This list consists of words that occur in more than 5 different documents.	[[77, 80]]	[[57, 75]]	['FWL']	['Frequent Word List']
1367	+ ??????????)  random Markov Clustering Algorithm (MCL)  (Dongen, 2000) 	[[51, 54]]	[[22, 49]]	['MCL']	['Markov Clustering Algorithm']
1368	coord dep coord (c) Previous conjunct headed (PH) Je vois Jean , Paul et Marie	[[46, 48]]	[[20, 44]]	['PH']	['Previous conjunct headed']
1369	TI = fTW; F; ADV; AUX; V A; V C; V V; Pg Each type of the indicators, e.g. TW , contains a set of words, such as TW = twlist = ftw	[[113, 115], [75, 77], [18, 21], [13, 16], [0, 2]]	[[118, 124]]	['TW', 'TW', 'AUX', 'ADV', 'TI']	['twlist']
1370	The  grams in an utterance SSG can be extracted by converting it to a finite state transducer (FST),   .	[[96, 99], [28, 31]]	[[71, 94]]	['FST', 'SSG']	['finite state transducer']
1371	 BIBLIOGRAPHY  ALPAC (Automatic Language Processing Advisory Committee)  1966 Lanquage and Machines - Computers i n  Translation and Linguistics 	[[15, 20]]	[[22, 70]]	['ALPAC']	['Automatic Language Processing Advisory Committee']
1372	edge in Y .  The Wall Street Journal Penn Treebank (PTB) (Marcus et al, 1993) contains parsed constituency	[[52, 55]]	[[37, 50]]	['PTB']	['Penn Treebank']
1373	appropriate.  Terminology Data banks (TD) are the least ambitious  systems because access frequently is not made during a 	[[38, 40]]	[[14, 30]]	['TD']	['Terminology Data']
1374	constructed using only surface Alterf patterns; for the GLM and text versions, we can use either surface patterns, logical form (LF) patterns, or both. 	[[129, 131], [56, 59]]	[[115, 127]]	['LF', 'GLM']	['logical form']
1375	timing. This flexibility is in contrast to speech output in spoken dialogue systems (SDSs) which typically generate, synthesize and deliver speech	[[85, 89]]	[[60, 83]]	['SDSs']	['spoken dialogue systems']
1376	Abstract We present the design, preparation, results and analysis of the Cancer Genetics (CG) event extraction task, a main task of the	[[90, 92]]	[[73, 88]]	['CG']	['Cancer Genetics']
1377	ral probabilistic language model. In Advances in Neural Information Processing Systems (NIPS), 2000.	[[88, 92]]	[[49, 86]]	['NIPS']	['Neural Information Processing Systems']
1378	tical problems in class. It might also be useful in massive open online courses (MOOCs). In this for-	[[81, 86]]	[[52, 79]]	['MOOCs']	['massive open online courses']
1379	These 67? 30 candidate claims were annotated using Amazon?s Mechanical Turk (AMT). In each	[[77, 80]]	[[51, 75]]	['AMT']	['Amazon?s Mechanical Turk']
1380	episodes in lexical processing. In Proceedings of SWAP (Spoken Word Access Processes), A. Cutler, J. M McQueen, and R. Zondervan, ed.,	[[50, 54]]	[[56, 84]]	['SWAP']	['Spoken Word Access Processes']
1381	SR+LM+SB 0.57 0.75 0.65 0.75 0.68 0.71 0.69 0.92 0.87 0.88 0.87 0.85 0.75 0.87 0.81 0.95 A0 - - - 0.85 - - - 0.93 - - - N/A - - - 0.97 Table 5: Alignment results for all datasets and configurations: Using semantic relations (SR), monosemous links (LM) or both (SR+LM).	[[225, 227], [120, 123], [248, 250], [261, 266]]	[[205, 223], [230, 246]]	['SR', 'N/A', 'LM', 'SR+LM']	['semantic relations', 'monosemous links']
1382	Training uses balanced data (50:50). Testing uses two class distributions (C.D.): 50:50 (balanced) and Natural Distribution (N.D.). Improvements of our method are statistically significant with p<0.005 based on paired t-test.	[[125, 129], [75, 78]]	[[103, 123], [54, 72]]	['N.D.', 'C.D']	['Natural Distribution', 'class distribution']
1383	2007). Two 5 DOF sensors - TT (Tongue Tip)  and TB (Tongue Body Back) - were attached on  the midsagittal of the tongue.	[[48, 50], [27, 29], [13, 16]]	[[52, 63], [31, 41]]	['TB', 'TT', 'DOF']	['Tongue Body', 'Tongue Tip']
1384	A Robust Algorithm for the Tree Edit Distance.  Proceedings of the VLDB Endowment (PVLDB), 5(4):334?345.	[[83, 88]]	[[48, 71]]	['PVLDB']	['Proceedings of the VLDB']
1385	mechanism for accurate named entity  (NE) translation in English?Chinese  question answering (QA). This mecha-	[[94, 96], [38, 40]]	[[74, 92], [23, 35]]	['QA', 'NE']	['question answering', 'named entity']
1386	637  Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 27?32, Sofia, Bulgaria, August 9, 2013.	[[70, 77]]	[[36, 68]]	['DiscoMT']	['Discourse in Machine Translation']
1387	Table 1 shows the feature template sets.  For training, we used soft confidence weighted (SCW) (Wang et al., 2012).	[[90, 93]]	[[64, 88]]	['SCW']	['soft confidence weighted']
1388	 As with most modern simulators, DISs are controlled via  graphical user interfaces (GUIs). However, the simulation 	[[85, 89], [33, 37]]	[[58, 83]]	['GUIs', 'DISs']	['graphical user interfaces']
1389	feeling (FE)  food (FO)  group (GR)  location (LO) 	[[32, 34], [9, 11], [20, 22], [47, 49]]	[[25, 30], [0, 7], [14, 18], [37, 45]]	['GR', 'FE', 'FO', 'LO']	['group', 'feeling', 'food', 'location']
1390	match with a hypothesis. These weights are  confidence measures: Logical Sufficiency (LS)  and Logical Necessity (LN).	[[86, 88], [114, 116]]	[[65, 84], [95, 112]]	['LS', 'LN']	['Logical Sufficiency', 'Logical Necessity']
1391	Abstract  This paper describes a heuristic algorithm capable of automatically assigning a label to  each of the senses in a machine readable dictionary (MRD) for the purpose of acquiring a com-  putational-semantic lexicon for treatment of lexical ambiguity.	[[153, 156]]	[[124, 151]]	['MRD']	['machine readable dictionary']
1392	Th~se de l'Universitt~ Joseph  Fourier, Grenoble I, Mars 1990  \[8\] TEl (Text Encoding Initiative), Guidelines for the  Encoding and lnterchange of Machine Readable Texts.	[[69, 72]]	[[74, 98]]	['TEl']	['Text Encoding Initiative']
1393	 Because we don't have any other useful resources except ChineseGigaword(CGW),We first computed mutual information for all 3-character words in two	[[73, 76]]	[[57, 72]]	['CGW']	['ChineseGigaword']
1394	 In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040?1047, Uppsala, Sweden, July.	[[93, 96]]	[[50, 91]]	['ACL']	['Association for Computational Linguistics']
1395	low navigational directions. In Proceedings of the Association for Computational Linguistics (ACL), 2010.	[[94, 97]]	[[51, 92]]	['ACL']	['Association for Computational Linguistics']
1396	on the gender of the user.  User ID: The user ID (UID) labels are inspired by research on Arabic Twitter showing that a consid-	[[50, 53]]	[[41, 48]]	['UID']	['user ID']
1397	hence Srinivas and Joshi, in the context of TAG, refer to supertagging as almost parsing.  The parser is able to parse 20 Wall Street Journal (WSJ) sentences per second on standard hardware, using our best-performing model, which compares very favorably with other	[[143, 146], [44, 47]]	[[122, 141]]	['WSJ', 'TAG']	['Wall Street Journal']
1398	markert@l3s.de Abstract Automatic timeline summarization (TLS) generates precise, dated overviews over	[[58, 61]]	[[34, 56]]	['TLS']	['timeline summarization']
1399	We discuss related work in section 5, and conclude in section 6.  2 Index of Productive Syntax (IPSyn) The Index of Productive Syntax (Scarborough, 1990) evaluates a child?s linguistic development by ana-	[[96, 101]]	[[68, 94]]	['IPSyn']	['Index of Productive Syntax']
1400	ings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). 	[[85, 88]]	[[58, 83]]	['WSD']	['word sense disambiguation']
1401	 2 Extended typed  A-ca lcu lus   CU(\] (Categorial U,,ificAtion (l:ra,nma,r) \[8\] is a,d-  vantageous, compared to other phrase structure 	[[34, 36]]	[[41, 64]]	['CU']	['Categorial U,,ificAtion']
1402	32  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 169?178, Seoul, South Korea, 5-6 July 2012.	[[100, 107]]	[[50, 98]]	['SIGDIAL']	['Special Interest Group on Discourse and Dialogue']
1403	Note also that there is some overlap of infer-  marion between the Lexical Systems analysis and  the Brandeis analysis, such as SUISCAT(TRAN)  and DO.	[[136, 140], [147, 149], [128, 135]]	[]	['TRAN', 'DO', 'SUISCAT']	[]
1404	successful method presented by Bendersky and Croft (2008) for selection and weighting of query noun phrases (NPs). It also extends work for deter-	[[109, 112]]	[[95, 107]]	['NPs']	['noun phrases']
1405	In that case partial semantic mapping will take place where no Logical Form is being built and only referring expressions are asserted in the Discourse Model ? but see below.  3.2 Lexical Information The output of grammatical modules is then fed onto the Binding Module(BM) which activates an algorithm for anaphoric binding in LFG (see [13]) terms using f-structures as domains and grammatical functions as entry points into the structure. We show here below the architecture of the system.	[[270, 272], [328, 331]]	[[255, 268]]	['BM', 'LFG']	['Binding Modul']
1406	al. ( 1997)  and later Harabagiu and Maiorano (HM) (2000)  investigated the acquisition of the lexical concept 	[[47, 49]]	[[23, 45]]	['HM']	['Harabagiu and Maiorano']
1407	There are four goodness algorithms reviewed by Zhao and Kit (2008a). The algorithms, including Description Length Gain (DLG) (Kit and Wilks 1999), Accessor Variety (Feng et al 2004a, 2004b), and Branching Entropy (Tanaka-Ishii	[[120, 123]]	[[95, 118]]	['DLG']	['Description Length Gain']
1408	12  Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137?142, October 25, 2014, Doha, Qatar.	[[82, 86]]	[[46, 80]]	['ANLP']	['Arabic Natural Langauge Processing']
1409	In the SIGHAN Bakeoff 2007, there are five training corpus for word segmentation (WS) task: AS  (Academia Sinica), CityU (City University of  Hong Kong) are traditional Chinese corpus; CTB 	[[115, 120], [82, 84], [92, 94], [185, 188]]	[[122, 137], [63, 80], [97, 112]]	['CityU', 'WS', 'AS', 'CTB']	['City University', 'word segmentation', 'Academia Sinica']
1410	a t tachment  (PP): the attachment ofa PP  in the sequence VP hip PP (VP = verb  phrase, 51P = noun phrase, PP = prepo-  sitional phrase).	[[108, 110], [15, 17], [39, 41], [59, 61], [66, 68], [70, 72]]	[[113, 119], [75, 87]]	['PP', 'PP', 'PP', 'VP', 'PP', 'VP']	['prepo-', 'verb  phrase']
1411	" Unknown  1976 ""The Lexicography Informati on Sys tern (LEXIS) o f  the Bundeswher  Language Service,"" i n  Machine Assisted ""h-ansl ation i n  West "	[[56, 61]]	[[20, 49]]	['LEXIS']	['Lexicography Informati on Sys']
1412	This research has been supported in part by DARPA (under contract number FA8750-13-2-0005), NIH (NICHD award 1R01HD07532801), Keck Foundation (DT123107), NSF (IIS0835797), and	[[92, 95], [44, 49], [73, 75], [143, 145], [154, 157]]	[[97, 102]]	['NIH', 'DARPA', 'FA', 'DT', 'NSF']	['NICHD']
1413	dependency analyzer, respectively.  (V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG =  ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP, 	[[62, 66], [85, 90], [144, 147], [224, 228], [248, 252]]	[[69, 83], [93, 103], [56, 60], [41, 50], [151, 159], [164, 172], [231, 246], [255, 267]]	['AUXV', 'COMPL', 'ERG', 'B-NP', 'I-NP']	['auxiliary verb', 'completive', 'noun', 'main verb', 'ergative', 'singular', 'beginning of NP', 'inside an NP']
1414	Hence, it seems plausible to  utilize a back-off mechanism for these sentences  via a combined system (COMB) incorporating  NB only for the sentences that fail to parse.	[[103, 107], [124, 126]]	[[86, 94]]	['COMB', 'NB']	['combined']
1415	but more often there is only one.  FN=false negative, etc.). I also consider micro- and	[[35, 37]]	[[38, 52]]	['FN']	['false negative']
1416	Abstract Microblogs are a popular way for users to communicate and have recently caught the attention of researchers in the natural language processing (NLP) field. However, regardless of their rising	[[153, 156]]	[[124, 151]]	['NLP']	['natural language processing']
1417	" 2Regularized parses (henceforth, ""parse trees"") are  like F-structures of Lexical Ftmction Grammar (LFG),  except, hat a dependency structure is used."""	[[101, 104]]	[[75, 99]]	['LFG']	['Lexical Ftmction Grammar']
1418	It is based on bilingual phrases, where a bilingual phrase (BP ) is simply two monolingual phrases (MP ) in which each one is supposed to be the translation of each	[[100, 102], [60, 62]]	[[79, 98], [42, 58]]	['MP', 'BP']	['monolingual phrases', 'bilingual phrase']
1419	actual object and \[AI is the word that represents A. CS : = speaking A;yes refers A \[A\] ; yes  (111) Resource Situation(RS)  A resource situation is defined for each individual in a discourse; it 	[[123, 125], [54, 56]]	[[104, 122]]	['RS', 'CS']	['Resource Situation']
1420	belief from the belief tracker in 6.3% of the dialogs.  The mean Word Error Rate (WER) per worker on the test set is 27.5%.	[[82, 85]]	[[65, 80]]	['WER']	['Word Error Rate']
1421	To explore the impact of the quality of annotation resources, we also use a Chinese language analysis tool: Language Technology Platform (LTP) (Che et al, 2010).	[[138, 141]]	[[108, 136]]	['LTP']	['Language Technology Platform']
1422	CrossT values). The regression model predicted  Figure 4: ST alignment crossings (CrossS), as generated  when checking the ST against the TT 	[[82, 88], [0, 6], [58, 60], [123, 125], [138, 140]]	[[71, 80]]	['CrossS', 'CrossT', 'ST', 'ST', 'TT']	['crossings']
1423	translation evaluation metrics such as BLEU score (Papineni et al, 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002).	[[146, 149], [39, 43], [74, 78]]	[[108, 144]]	['PER', 'BLEU', 'NIST']	['Position Independent Word Error Rate']
1424	given topic.  The Maximal Marginal Relevance (MMR) summarization method, which is based on a	[[46, 49]]	[[18, 44]]	['MMR']	['Maximal Marginal Relevance']
1425	In Modern Standard Arabic (MSA), all nouns and adjectives have one of three cases: nominative (NOM), accusative (ACC), or genitive (GEN). What	[[113, 116], [132, 135], [27, 30], [95, 98]]	[[101, 111], [122, 130], [3, 25], [83, 93]]	['ACC', 'GEN', 'MSA', 'NOM']	['accusative', 'genitive', 'Modern Standard Arabic', 'nominative']
1426	Figure 4: Change of global network properties with incremental addition of edges to the directed network of news genre. SCC = Strongly Connected Component, CC = Connected Component. By ?	[[120, 123], [156, 158]]	[[126, 154], [161, 180]]	['SCC', 'CC']	['Strongly Connected Component', 'Connected Component']
1427	 1 LR  Parser  Generat ion   Tree Adjoining Grammars (TAGs) are tree rewrit-  ing systems which combine trees with the sin- 	[[54, 58], [3, 5]]	[[29, 52]]	['TAGs', 'LR']	['Tree Adjoining Grammars']
1428	"tions"" as per the gold standard.  D = True Negatives (TN) = Pairs that were identified as ""Incorrect Transliterations"" by the par-"	[[54, 56]]	[[38, 52]]	['TN']	['True Negatives']
1429	 3 Results and Discussions Chain frequency (CF) and chain length (CL) reflect the dyad?s tweeting behaviors.	[[44, 46], [66, 68]]	[[27, 42], [52, 64]]	['CF', 'CL']	['Chain frequency', 'chain length']
1430	an NP node with the grammatical function (GF) of the pronoun. The GF of the pronoun, in turn, is replaced by HD (head). Such unary branching NPs are added on top of nouns	[[109, 111], [3, 5], [42, 44], [66, 68], [141, 144]]	[[113, 117], [20, 40]]	['HD', 'NP', 'GF', 'GF', 'NPs']	['head', 'grammatical function']
1431	the percentage of tokens that are assigned the correct head and dependency label, as well as the unlabeled attachment score (UAS), that is, the percentage of tokens with the correct head, and the label accuracy (LA), that is, the percentage of tokens with the correct dependency label.	[[212, 214], [125, 128]]	[[196, 210], [97, 123]]	['LA', 'UAS']	['label accuracy', 'unlabeled attachment score']
1432	certainty factor equals to 0.6 (3/5). The general formula for the certainty factor (CF) is shown as follow: CFi = Total number of the answer elements at leaf node i	[[84, 86], [108, 110]]	[[66, 82]]	['CF', 'CF']	['certainty factor']
1433	edge of syntax and semantics.   In connection to conjunct verbs (ConjVs),  (Mohanty, 2010) defines two types of conjunct 	[[65, 71]]	[[49, 63]]	['ConjVs']	['conjunct verbs']
1434	To classify the NPs according to their type in biomedical terms, we have adopted the Sequence Ontology (SO)2 (Eilbeck and Lewis, 2004).	[[104, 106], [16, 19]]	[[85, 102]]	['SO', 'NPs']	['Sequence Ontology']
1435	2 Algorithms and Data  2.1 Task Definition and Data  The named entity (NE) task used for this  evaluation requires the system to identify all 	[[71, 73]]	[[57, 69]]	['NE']	['named entity']
1436	Combining keyphrase and collocation Yamamoto and Church (2001) compare two metrics, MI and Residual IDF (RIDF), and observed that MI is suitable for finding collocation and RIDF	[[105, 109], [84, 86], [130, 132], [173, 177]]	[[91, 103]]	['RIDF', 'MI', 'MI', 'RIDF']	['Residual IDF']
1437	tion using a method similar to FOIL (Quin-  lan, 1990) and bottom-up generalization using  Least General Generalizations (LGG's). Ad- 	[[122, 127], [31, 35]]	[[91, 120]]	"[""LGG's"", 'FOIL']"	['Least General Generalizations']
1438	TOP (PRP ? I?) ( VP (VBP ? NEED?) (	[[17, 19], [0, 3], [5, 8]]	[[21, 24]]	['VP', 'TOP', 'PRP']	['VBP']
1439	calculating the posterior probabilities.  Active SVM with self-training (ASSVM) is an extension of ASVM where each round of training has	[[73, 78], [99, 103]]	[[42, 52]]	['ASSVM', 'ASVM']	['Active SVM']
1440	2 Shared-task evaluation in HLT Over the past twenty years, virtually every field of research in human language technology (HLT) has introduced STECs.	[[124, 127], [28, 31], [144, 149]]	[[97, 122]]	['HLT', 'HLT', 'STECs']	['human language technology']
1441	Y? >l LY?l, s.t. SYl = SY?l (1) 1	[[17, 20]]	[[23, 27]]	['SYl']	['SY?l']
1442	given for each environment. For example, Q appeared eight  times in the context EH--EN ( E - n ,  and a check of the  reference list shows that all these occurrences were in the 	[[84, 86], [80, 82]]	[[89, 94]]	['EN', 'EH']	['E - n']
1443	or more previous turns in the dialogue. The third column shows the mean error and standard error (SE) predicted for the model specified by the first two columns. When	[[98, 100]]	[[82, 96]]	['SE']	['standard error']
1444	3http://www.noslang.com 370 are the New York Times (NYT),4 SMS,5 and Twitter.6 The results are presented in Figure 1.	[[52, 55]]	[[36, 50]]	['NYT']	['New York Times']
1445	2http://www.nist.gov/speech/tests/mt/ Table 1: Training, development and test data from Basic Travel Expression Corpus(BTEC) Japanese English	[[119, 123]]	[[88, 117]]	['BTEC']	['Basic Travel Expression Corpu']
1446	tions. Following Bahdanau et al (2014), we employ the Gated Recurrent Unit (GRU) as our RNN unit due to its capacity in capturing long-distance depen-	[[76, 79]]	[[54, 74]]	['GRU']	['Gated Recurrent Unit']
1447	The columns in the first section of the table represent different settings of the p# parameter, with highest performance for each adjusted count model shown in bold. p# values were selected to show a representative range of performance. P = phoneme model; OR = onset-rhyme model; S = syllable model; IR = iterative re-estimation; LM = local minimum strategy. The best performing local minimum model is shaded.	[[300, 302], [330, 332], [256, 258]]	[[305, 317], [335, 348], [241, 248], [261, 272], [284, 292]]	['IR', 'LM', 'OR']	['iterative re', 'local minimum', 'phoneme', 'onset-rhyme', 'syllable']
1448	 We consider two resources for training the RCM term: the Paraphrase Database (PPDB) (Ganitkevitch et al, 2013) and WordNet (Fell-	[[79, 83]]	[[58, 77]]	['PPDB']	['Paraphrase Database']
1449	These are the second-order prepositional complement (PC) and directional complement (LD) relations, and the first-order direct object (OBJ1) and subject (SU) relations. Finally, the setting SU+OBJ1 joins words obtained from subject	[[154, 156], [53, 55], [85, 87], [135, 138]]	[[145, 152], [27, 51], [127, 133]]	['SU', 'PC', 'LD', 'OBJ']	['subject', 'prepositional complement', 'object']
1450	Natural Language Processing (NLP) techniques can be leveraged in detecting events from voluminous social media data. Events are associated with entities and NLP techniques can be applied to extract the entities that are mentioned in the text that defines an event. To perform Named Entity Recognition (NER) on tweets Ritter et. al. (	[[302, 305]]	[[276, 300]]	['NER']	['Named Entity Recognition']
1451	10 ESA on senses and Wikipedia Link Measure (WLM) compute similarity on a sense-level, however, sim-	[[45, 48], [3, 6]]	[[21, 43]]	['WLM', 'ESA']	['Wikipedia Link Measure']
1452	Test Data Method Accuracy leave-one-out Minnen et al 83.58% Language Model (LM) 86.74% tenfold on development LM 84.72%	[[76, 78], [110, 112]]	[[60, 74]]	['LM', 'LM']	['Language Model']
1453	(TEMPO1) and YOUTH-TIME (TEMPO2) where the choice is a secor~d  order Pa~t and for the pair YOUTH-TIME (TEMPO2) and BUILDING-  TIME (TEMPO3) where the choice is a third order F~Jture. As a 	[[133, 139]]	[[127, 131]]	['TEMPO3']	['TIME']
1454	POS tag distribution. We also use features based on part of speech (POS). We tag using	[[68, 71], [0, 3]]	[[52, 66]]	['POS', 'POS']	['part of speech']
1455	 MEI is one of the four projects elected for  the Johns Hopkins University (JHU) Summer  Workshop 2000.1 Our research focus is on the 	[[76, 79], [1, 4]]	[[50, 74]]	['JHU', 'MEI']	['Johns Hopkins University']
1456	the train and development sets in MC160 and MC500 and split them randomly into a 250-story training set (TRAIN) and a 200-story development set (DEV).	[[105, 110], [34, 39], [44, 49], [145, 148]]	[[91, 99], [128, 139]]	['TRAIN', 'MC160', 'MC500', 'DEV']	['training', 'development']
1457	Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information	[[132, 134]]	[[112, 130]]	['QA']	['Question Answering']
1458	here we only consider clusters which contain at least one ? Person (PER)? entity.	[[68, 71]]	[[60, 66]]	['PER']	['Person']
1459	Certain CT+ (factual) CT? ( counterfactual) CTu (certain but unknown output) Probable PR+ (probable) PR? ( not probable) [NA]	[[86, 89], [8, 11], [22, 24], [44, 47], [101, 104], [122, 125]]	[[91, 99], [111, 119], [49, 75], [28, 42]]	['PR+', 'CT+', 'CT', 'CTu', 'PR?', 'NA]']	['probable', 'probable', 'certain but unknown output', 'counterfactual']
1460	"There have been p,'evious VCl'sions of I,'1"" (l,cpage 1986)  The I,T llSed ill our wo!'k has been implemented on  MacilLtosh with CLOS (Common Lisp Oh.jeer System)  (l,afeurcade 1993) The realizalion is lmscd mainly on "	[[130, 134], [26, 29]]	[[136, 162]]	['CLOS', 'VCl']	['Common Lisp Oh.jeer System']
1461	tion database (OID) and provides the corresponding interface;  - Knowledge Retriever (KR) ? retrieves KSs from 	[[86, 88], [15, 18], [102, 105]]	[[65, 84]]	['KR', 'OID', 'KSs']	['Knowledge Retriever']
1462	noisy and potentially unreliable observations.  While scenario template creation (STC) is a difficult problem, its evaluation is arguably more dif-	[[82, 85]]	[[54, 80]]	['STC']	['scenario template creation']
1463	email: adam@itri.bton.ac.uk  People have been writing programs for auto-  matic Word Sense Disambiguation (WSD) for  forty years now, yet the validity of the task has 	[[107, 110]]	[[80, 105]]	['WSD']	['Word Sense Disambiguation']
1464	Extraction algorithms: ReV = REVERB; Comp = Compression; Data sets: NS = NewsSpike URLs; All = news 2008-2014.	[[83, 87], [23, 26], [37, 41]]	[[73, 82], [29, 35], [44, 55]]	['URLs', 'ReV', 'Comp']	['NewsSpike', 'REVERB', 'Compression']
1465	The dataset used in the CIPS-ParsEval-2010 evaluation is converted from the Tsinghua Chinese Treebank (TCT). There are two subtasks: (1)	[[103, 106], [24, 42]]	[[93, 101]]	['TCT', 'CIPS-ParsEval-2010']	['Treebank']
1466	hardware) but also not highly specialized (implying that it is not limited too severely in scope of use). We believe  an architecture composed of a distributed set of processing elements (PE's), each containing local memory and high  speed DSP processors, with a limited interconnecfion a d communication capability may suit our needs.	[[188, 192], [240, 243]]	[[167, 186]]	"[""PE's"", 'DSP']"	['processing elements']
1467	lates in the treebank; it is erroneous because close to wholesale needs another layer of structure, namely adjective phrase (ADJP) (Bies et al, 1995, p. 179). 	[[125, 129]]	[[107, 123]]	['ADJP']	['adjective phrase']
1468	of the label candidates. For the supervised method, we use a support vector regression (SVR) model (Joachims, 2006) over all of the features.	[[88, 91]]	[[61, 86]]	['SVR']	['support vector regression']
1469	Also,  the dependency framework is arguably closer to  semantics than the phrase structure grammar (PSG)  if the dependency relations are judiciously chosen.	[[100, 103]]	[[74, 98]]	['PSG']	['phrase structure grammar']
1470	error criteria: ? WER (word error rate): The WER is computed as the minimum	[[18, 21], [45, 48]]	[[23, 38]]	['WER', 'WER']	['word error rate']
1471	 ? Fondazione Bruno Kessler (FBK-irst), Italy ?	[[29, 37]]	[[3, 27]]	['FBK-irst']	['Fondazione Bruno Kessler']
1472	segment of a conversation. We will therefore use the  terms initiating conversational participant (ICP) and other  conversational participant(s) (OCP) to distinguish the initi- 	[[99, 102], [146, 149]]	[[60, 97], [108, 141]]	['ICP', 'OCP']	['initiating conversational participant', 'other  conversational participant']
1473	training data (Surdeanu et al, 2008). We report purity (PU), collocation (CO), and their harmonic mean (F1) evaluated on gold arguments in two set-	[[56, 58], [74, 76]]	[[48, 54], [61, 72]]	['PU', 'CO']	['purity', 'collocation']
1474	90 Table 1: Comparison of emotion corpora ordered by the amount of annotations (abbreviations: T=tokenization, POS=part-of-speech tagging, L=lemmatization, DP=dependency parsing, NER=Named Entity Recognition). 	[[156, 158], [179, 182], [111, 114]]	[[159, 177], [183, 207], [97, 109], [115, 129], [141, 154]]	['DP', 'NER', 'POS']	['dependency parsing', 'Named Entity Recognition', 'tokenization', 'part-of-speech', 'lemmatization']
1475	 cs.uni-kassel.de/bibsonomy/dumps Content Relevance (CRM) model (Iwata et al, 2009) and Tag Allocation Model (TAM) (Si et al,	[[53, 56], [110, 113]]	[[34, 51], [88, 108]]	['CRM', 'TAM']	['Content Relevance', 'Tag Allocation Model']
1476	triplet model since it is based on word triplets, is not trained discriminatively but uses the classical maximum likelihood approach (MLE) instead. 	[[134, 137]]	[[105, 123]]	['MLE']	['maximum likelihood']
1477	using the DSO corpus, which contains sentences drawn from two different corpora, namely Brown Corpus (BC) and Wall Street Journal (WSJ). They	[[131, 134], [10, 13], [102, 104]]	[[110, 129], [88, 100]]	['WSJ', 'DSO', 'BC']	['Wall Street Journal', 'Brown Corpus']
1478	Table 3: Experimental Results (Microsoft?s Provided Train and Test Set) sorted the sentences pairs of the MSRP corpus according to the length difference ratio (LDR) defined in Section 3, and partitioned the sorted cor-	[[160, 163], [106, 110]]	[[135, 158]]	['LDR', 'MSRP']	['length difference ratio']
1479	341  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1070?1080, October 25-29, 2014, Doha, Qatar.	[[93, 98]]	[[43, 91]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
1480	Information Retrieval (CLIR). It is also important for Machine Translation (MT), especially when the languages do not use the same scripts.	[[76, 78], [23, 27]]	[[55, 74], [0, 21]]	['MT', 'CLIR']	['Machine Translation', 'Information Retrieval']
1481	vantages while considering the consistency, we further propose a global decoding strategy using Integer Linear Programming(ILP). The constraints	[[123, 126]]	[[96, 121]]	['ILP']	['Integer Linear Programmin']
1482	a seminal work on CCGbank (Hockenmaier and Steedman, 2007), in which the phrase structure trees of the Penn Treebank (PTB) (Marcus et al, 1993) are converted into CCG derivations and a	[[118, 121], [18, 25], [163, 166]]	[[103, 116]]	['PTB', 'CCGbank', 'CCG']	['Penn Treebank']
1483	 In Proceedings of the 14th International Conference on World Wide Web (WWW), pages 342?351, Chiba.	[[72, 75]]	[[56, 70]]	['WWW']	['World Wide Web']
1484	We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven	[[60, 62]]	[[35, 58]]	['RR']	['RelationalRealizational']
1485	tongues. In Proc. of the Text Encoding Initiative 10th Anniversary User Conference (TEI-10). 	[[84, 90], [12, 16]]	[[25, 54]]	['TEI-10', 'Proc']	['Text Encoding Initiative 10th']
1486	In addition, for some nodes it is necessary to insist that adjunction is mandatory at  a node. In such a case, we say that the node has an Obligatory Adjoining (OA)  constraint.	[[161, 163]]	[[139, 159]]	['OA']	['Obligatory Adjoining']
1487	 2 Normalized Compression Distance Normalized compression distance (NCD) is a similarity measure based on the idea that a string x is	[[68, 71]]	[[35, 66]]	['NCD']	['Normalized compression distance']
1488	UMLS Metathesaurus version 2003AC, the string mammectomy has been assigned the concept-unique identifier C0024881 (CUI), the lemma-unique identifier L0024669 (LUI), and the string-unique identifier S0059711 (SUI).	[[115, 118], [0, 4], [27, 33], [159, 162], [208, 211]]	[[105, 113], [125, 157], [173, 206]]	['CUI', 'UMLS', '2003AC', 'LUI', 'SUI']	['C0024881', 'lemma-unique identifier L0024669', 'string-unique identifier S0059711']
1489	........ ? ................... ? ............... + ................... +  CLS = Clause NP = Noun Phrase (BAR 2)  PROD = Predicate $SUBJ -- SUBJECT value of feature ~OL 	[[87, 89], [74, 77], [105, 108], [113, 117], [130, 135], [165, 167]]	[[92, 103], [80, 86], [120, 129], [139, 146]]	['NP', 'CLS', 'BAR', 'PROD', '$SUBJ', 'OL']	['Noun Phrase', 'Clause', 'Predicate', 'SUBJECT']
1490	For example, in (2) we find frames identifying baseform verbs (VB) (2a) and frames identifying cardinal numbers (CD) (2b), despite having a variety of context words.	[[113, 115], [63, 65]]	[[95, 103], [56, 60]]	['CD', 'VB']	['cardinal', 'verb']
1491	z that maps sentences x to logical expressions z. We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xi, zi)|i = 1 . . .	[[106, 110]]	[[87, 104]]	['PCCG']	['probabilistic CCG']
1492	contributions to sentence similarity. In most cases, the longer common sequence (LCS) the two sentences have, the higher similarity score the sentences	[[81, 84]]	[[57, 79]]	['LCS']	['longer common sequence']
1493	1 Introduction  In this paper we address the event extraction task  defined in Automatic Content Extraction (ACE)1  program.	[[109, 112]]	[[79, 107]]	['ACE']	['Automatic Content Extraction']
1494	 Introduction  Categorial Grammars (CGs) consist of two compo-  nents: (i) a lexicon, which assigns syntactic types 	[[36, 39]]	[[15, 34]]	['CGs']	['Categorial Grammars']
1495	rent participation week (Curr) and the second using data from the beginning participation week till the current week (TCurr). For the second setup,	[[118, 123]]	[[100, 111]]	['TCurr']	['the current']
1496	approaches. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pages 419?424. 	[[82, 86]]	[[57, 80]]	['AAAI']	['Artificial Intelligence']
1497	use(USE) social(SOC) body(BOD)  phy_creation(PCR) mental_creation(MCR)  verbal_creagion (VCR)  These are mostly taken from the classifications 	[[89, 92], [4, 7], [16, 19], [26, 29], [45, 48], [66, 69]]	[[72, 87], [0, 3], [9, 15], [21, 25], [32, 44], [50, 65]]	['VCR', 'USE', 'SOC', 'BOD', 'PCR', 'MCR']	['verbal_creagion', 'use', 'social', 'body', 'phy_creation', 'mental_creation']
1498	1  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 42?47, Gothenburg, Sweden, April 27, 2014.	[[72, 77], [81, 85]]	[[38, 70]]	['HyTra', 'EACL']	['Hybrid Approaches to Translation']
1499	4.2 Preprocessing Part?Whole Lexico-Syntactic Patterns Since our discovery procedure is based on the semantic information provided by WordNet, we need to preprocess the noun phrases (NPs) extracted by the three clusters considered and identify the potential part and the whole concepts.	[[183, 186]]	[[169, 181]]	['NPs']	['noun phrases']
1500	trees than the height-one rules of standard contextfree grammars. Tree substitution grammars (TSGs) have been shown to improve upon the standard En-	[[94, 98]]	[[66, 92]]	['TSGs']	['Tree substitution grammars']
1501	based on the design the Princeton English Wordnet.  Arabic Wordnet (AWN) (Elkateb, 2006; Black and Fellbaum, 2006; Elkateb and Fellbaum, 2006) has	[[68, 71]]	[[52, 66]]	['AWN']	['Arabic Wordnet']
1502	That is, it learns some new knowledge.  Static Interactive Learning (SIL): Whenever the  system encounters a sentence out o f  i t s  processing 	[[69, 72]]	[[40, 67]]	['SIL']	['Static Interactive Learning']
1503	AJCL = AmericanJournal of Computational  Linguistics (1974-present)  SNLP = Studies in Natural Language Processin~  (Cambridge University Press Monograph 	[[69, 73], [0, 4]]	[[76, 114], [7, 52]]	['SNLP', 'AJCL']	['Studies in Natural Language Processin~', 'AmericanJournal of Computational  Linguistics']
1504	tl,...,tM i=2 i=N+I   (7)  which is a Nth-order Markovian chain for the language model (MLM). 	[[88, 91]]	[[48, 86]]	['MLM']	['Markovian chain for the language model']
1505	scikit-learn python library 3 : Naive Bayes (NB), Nearest Neighbor (NN), Decision Tree (DT), Ran-	[[45, 47], [68, 70], [88, 90]]	[[32, 43], [50, 66], [73, 86]]	['NB', 'NN', 'DT']	['Naive Bayes', 'Nearest Neighbor', 'Decision Tree']
1506	  (a)  Support vector machine (SVM) (Vapnik,  1998) is a supervised learning algorithm proposed 	[[31, 34]]	[[7, 29]]	['SVM']	['Support vector machine']
1507	 2.1 Modeling Votes Ideal point (IP) models are a mainstay in quantitative political science, often applied to voting records to	[[33, 35]]	[[20, 31]]	['IP']	['Ideal point']
1508	1 ? Active Node List (ANL): a list that records all ?	[[22, 25]]	[[4, 20]]	['ANL']	['Active Node List']
1509	ple need access to information anywhere, anytime. The  Adaptive Information Management (AIM) service in the  FASiL VPA seeks to automatically prioritise and pre-	[[88, 91], [109, 114], [115, 118]]	[[55, 86]]	['AIM', 'FASiL', 'VPA']	['Adaptive Information Management']
1510	tically justified.  Tree adjoining grammars (TAGs) are also a tree-based  system, ltowever, the major composition operation in 	[[45, 49]]	[[20, 43]]	['TAGs']	['Tree adjoining grammars']
1511	tence All the indexes dove ., in which All should be tagged as a predeterminer (PDT).10 Most occurrences of All, however, are as a determiner (DT, 106/135 vs	[[80, 83], [143, 145]]	[[65, 78], [131, 141]]	['PDT', 'DT']	['predeterminer', 'determiner']
1512	The parsers are: the Berkeley parser with gold POS tags as input (Berk-G), the Berkeley product parser with two grammars (Berk-2), the Berkeley parser (Berk-1), the parser of Zhang and Clark (2009) (ZPAR), the Bikel parser (Bikel), the Stanford Factored parser (Stan-F), and the Stanford Unlexicalized PCFG parser (Stan-P). 	[[262, 268], [315, 321], [47, 50], [199, 203]]	[[236, 253], [279, 313], [165, 180]]	['Stan-F', 'Stan-P', 'POS', 'ZPAR']	['Stanford Factored', 'Stanford Unlexicalized PCFG parser', 'parser of Zhang']
1513	would have higher perplexity.  Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally cor-	[[48, 50]]	[[41, 46]]	['SC']	['Score']
1514	Second, in many applications, an effective means of incorporating distributional semantics is Random Indexing (RI). Thus	[[111, 113]]	[[94, 109]]	['RI']	['Random Indexing']
1515	the sentences produced. Additionally, automated machine translation (MT) metrics are explored to quantify the amount of information missing from	[[69, 71]]	[[48, 67]]	['MT']	['machine translation']
1516	 2.1 Tree Substitution Grammars A tree substitution grammar (TSG) is a 4-tuple ?	[[61, 64]]	[[34, 59]]	['TSG']	['tree substitution grammar']
1517	ability of reordering models to capture this tag sequence in system translations. Popovic et al (2006) use the relative difference between WER (word error rate) and PER (position independent word error rate) to indicate reordering errors.	[[139, 142], [165, 168]]	[[144, 159], [170, 206]]	['WER', 'PER']	['word error rate', 'position independent word error rate']
1518	ofwi. Just like the statistical pproaches in many automatic POS tagging programs, our job is to select a  constituent boundary sequence B'with the highest score, P(BIS), from all possible sequences. 	[[164, 167], [60, 63]]	[[136, 160]]	['BIS', 'POS']	"[""B'with the highest score""]"
1519	In Proceedings of the International Conference on World Wide Web (WWW), pages 641?650. 	[[66, 69]]	[[50, 64]]	['WWW']	['World Wide Web']
1520	A Named Entity Labeler for German: Exploiting Wikipedia and Distributional Clusters. In Proceedings of the Conference on Language Resources and Evaluation (LREC), pages 552?556, La Valletta, Malta.	[[156, 160]]	[[121, 139]]	['LREC']	['Language Resources']
1521	     - French/English (fra-eng)    Seven Recognizing Textual Entailment (RTE)  evaluation tracks have already been held: RTE-1 	[[73, 76]]	[[41, 71]]	['RTE']	['Recognizing Textual Entailment']
1522	Dependency-Based Open Information Extraction Pablo Gamallo and Marcos Garcia Centro de Investigac?a?o sobre Tecnologias da Informac?a?o (CITIUS) Universidade de Santiago de Compostela	[[137, 143]]	[[77, 119]]	['CITIUS']	['Centro de Investigac?a?o sobre Tecnologias']
1523	Accept? validations for reliably deliberate (Rel) and unreliable (URel) subsets of the metaphor production data, given that the	[[66, 70], [45, 48]]	[[54, 64], [24, 32]]	['URel', 'Rel']	['unreliable', 'reliably']
1524	In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 417?422.	[[89, 93]]	[[54, 72]]	['LREC']	['Language Resources']
1525	 The graph in figure 2 shows that as the number of relevant documents increases, average precision (AveP) after feedback increases considerably for each extra relevant	[[100, 104]]	[[81, 98]]	['AveP']	['average precision']
1526	 More recently, Galley and Quirk (2011) have introduced linear programming MERT (LP-MERT) as an exact search algorithm that reaches the global op-	[[81, 88]]	[[56, 79]]	['LP-MERT']	['linear programming MERT']
1527	eling approaches. Table 3 shows the corresponding letter error rates (LER). LERs are more compa-	[[70, 73], [76, 80]]	[[50, 68]]	['LER', 'LERs']	['letter error rates']
1528	Winnow and voted-perceptrons (Zhang et al2002;  Collins, 2002), or by using the sequence labeling  models, such as Hidden Markov Models (HMMs)  (Molina and Pla, 2002) and Conditional Random 	[[137, 141]]	[[115, 135]]	['HMMs']	['Hidden Markov Models']
1529	Exact  match rate(EMR), violate match rate(VMR), and  inside match rate(IMR) denote the ratio of three  types of bracketing numbers in all bracketing 	[[72, 75], [18, 21], [43, 46]]	[[54, 70], [0, 17], [24, 42]]	['IMR', 'EMR', 'VMR']	['inside match rat', 'Exact  match rate', 'violate match rate']
1530	sequence of ATN arcs which is matched against the  input string. A pattern arc (PAT) has been added to  the ATN formalism with a form similar to that of oth- 	[[80, 83], [108, 111]]	[[67, 74], [12, 15]]	['PAT', 'ATN']	['pattern', 'ATN']
1531	cific characteristics in form, meaning, function, and distribution. Each entry includes a free text definition, schematic structural description, definitions of construction elements (CEs) and annotated example sentences.	[[184, 187]]	[[161, 182]]	['CEs']	['construction elements']
1532	 3 The V IT  Format   The VIT (short for Verbmobil Interface Term) was  designed as a common output format for the two 	[[26, 29], [7, 11]]	[[41, 65]]	['VIT', 'V IT']	['Verbmobil Interface Term']
1533	 2.1 Random Indexing Our first method is based on Random Indexing (RI), introduced by Kanerva (Kanerva, 1988).	[[67, 69]]	[[50, 65]]	['RI']	['Random Indexing']
1534	 ? Max Similarity (MaxSim): For tuple ? in an	[[19, 25]]	[[3, 17]]	['MaxSim']	['Max Similarity']
1535	vided for training data.  the use of well-motivated Lexical Structures (LS's)  to capture the presuppositional nd anaphoric as- 	[[72, 76]]	[[52, 70]]	"[""LS's""]"	['Lexical Structures']
1536	 The CBDF similarity values between  100,000-word subsets of Original French (OF),  French translated from English (EF), from 	[[78, 80], [5, 9], [116, 118]]	[[61, 76], [84, 114]]	['OF', 'CBDF', 'EF']	['Original French', 'French translated from English']
1537	 4.4 Bag of Words using Maximum Entropy (MaxEnt) Classifier We include Maximum Entropy classifier using sim-	[[41, 47]]	[[24, 39]]	['MaxEnt']	['Maximum Entropy']
1538	 (Markov) that significantly differed from the output of the noisy channel model (NoisyC), which confirms our finding that Markovized models can pro-	[[82, 88]]	[[61, 74]]	['NoisyC']	['noisy channel']
1539	developing a ser ies of inc reas ing ly  soph is t icated natura l  language unders tand ing   systems which will serve as an in tegrated  in ter face  to severa l  faci l i t ies at the Pacif ic  F leet Command Center:  the In tegrated  Data Base (IDB), which conta ins  information  about  ships, the i r  read iness  s tates ,  the i r  capabi l i t ies,  etc.;	[[249, 252]]	[[225, 247]]	['IDB']	['In tegrated  Data Base']
1540	= Europarl). TOOL = grammatical words, PCT/NB = punctuation and numbers, ADJ/ADV = adjectives and adverbs, NAM = proper name, NOM = noun,	[[39, 45], [73, 80], [107, 110], [126, 129]]	[[48, 71], [83, 105], [120, 124], [132, 136]]	['PCT/NB', 'ADJ/ADV', 'NAM', 'NOM']	['punctuation and numbers', 'adjectives and adverbs', 'name', 'noun']
1541	Table 4. Comparative results in AIMed. The number of positive instances (POS) and negative instances (NEG) and macro-averaged precision (ma-P), recall (ma-R) and F1-score (ma-F) are shown.	[[73, 76], [32, 37], [102, 105], [137, 141], [152, 156], [172, 176]]	[[53, 71], [82, 100], [111, 135], [144, 150], [162, 170]]	['POS', 'AIMed', 'NEG', 'ma-P', 'ma-R', 'ma-F']	['positive instances', 'negative instances', 'macro-averaged precision', 'recall', 'F1-score']
1542	fast method to train SVM. SMO breaks the large  quadratic programming (QP) optimization problem needed to be resolved in SVM into a series 	[[71, 73], [21, 24], [26, 29], [121, 124]]	[[48, 69]]	['QP', 'SVM', 'SMO', 'SVM']	['quadratic programming']
1543	each cross level text pair, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (SPh), Phrase to Word (Ph-W) and Word to Sense (W-Se).	[[105, 109], [57, 60], [83, 86], [130, 134]]	[[89, 103], [34, 55], [63, 81], [115, 128]]	['Ph-W', 'P-S', 'SPh', 'W-Se']	['Phrase to Word', 'Paragraph to Sentence', 'Sentence to Phrase', 'Word to Sense']
1544	2012. Disclosure and Quality of Answers in Text and Voice Interviews on iPhones. 67th Annual Conference of the Ameri-can Association for Public Opinion Research (AAPOR) Orlando, FL. Jongho Shin, Shrikanth Narayanan, Laurie Gerber, Abe Kazemzadeh, and Dani Byrd.	[[162, 167], [178, 180]]	[[121, 160]]	['AAPOR', 'FL']	['Association for Public Opinion Research']
1545	DUC 2007. In Proceedings of the Seventh Document Understanding Conference (DUC), Rochester, NY.	[[75, 78], [0, 3], [92, 94]]	[[40, 73]]	['DUC', 'DUC', 'NY']	['Document Understanding Conference']
1546	  2 NII-Speech Resources Consortium  The National Institute of Informatics (NII) was  founded in Tokyo, Japan in April 2000 as an in-	[[76, 79], [4, 7]]	[[41, 74]]	['NII', 'NII']	['National Institute of Informatics']
1547	 5.2 Named Entities As standard named entity recognition (NER) systems do not capture categories that are relevant to	[[58, 61]]	[[32, 56]]	['NER']	['named entity recognition']
1548	" P rev ious  Accomplishments  We have previously constructed a UNIX Consultant (UC), an intelligent NL-capable ""help""  facility that allows naive users to learn about the UNIX operating system."	[[80, 82], [100, 102], [171, 175]]	[[63, 78]]	['UC', 'NL', 'UNIX']	['UNIX Consultant']
1549	ously capture the semantics of words and sentences, such as latent semantic analysis (LSA) or latent Dirichlet alocation (LDA). However, our	[[122, 125], [86, 89]]	[[94, 120], [60, 84]]	['LDA', 'LSA']	['latent Dirichlet alocation', 'latent semantic analysis']
1550	son mari.  Les confessions (CO) is much most faithful to the content, yet, the translator has significantly departed	[[28, 30]]	[[15, 26]]	['CO']	['confessions']
1551	"Artola Xo (1993)o ""ilIZTSUA: lIiztegi-sistema urDrt.lc  a(lime~dunaren sorkuntza eta eraikuntza /Conccption  d'un syst~,me intelligent d'aide dictionnarialc (SIAl))""  Ph.D. Thesis."	[[158, 162]]	[[114, 156]]	['SIAl']	"[""syst~,me intelligent d'aide dictionnarialc""]"
1552	HOO) pro-posed and initiated a shared task in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the problem by developing tools or techniques for the non-native speaker of English, which will automat-ically correct the English prose of the papers so that they can be accepted. This tools and tech-niques may also help native English speakers. This task is simply expressed as text-to-text generation or Natural language Generation (NLG). In the 2011 shared task, all possible errors were covered which made the task enormously huge.	[[441, 444], [0, 3]]	[[412, 439]]	['NLG', 'HOO']	['Natural language Generation']
1553	tools freely available for many languages. That is the case for morphosyntactic analyzers (MSA), but not yet for full or even shallow parsers.	[[91, 94]]	[[64, 89]]	['MSA']	['morphosyntactic analyzers']
1554	The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). 	[[129, 131], [28, 31], [104, 107]]	[[113, 127], [4, 26], [74, 102]]	['PI', 'SRL', 'SRC']	['post inference', 'semantic role labeling', 'semantic role classification']
1555	2003. MDA Guide Version 1.0.1. Technical report, Object Management Group (OMG). 	[[74, 77], [6, 9]]	[[49, 72]]	['OMG', 'MDA']	['Object Management Group']
1556	models use a word embedding size of 200, whereas the hidden layer(s) size is fixed at 400, with all hidden units using the Rectified Linear Unit (ReLu) i.e., f(x) = max(0, x) as activation function.	[[146, 150], [165, 168]]	[[123, 144]]	['ReLu', 'max']	['Rectified Linear Unit']
1557	P2 = < ash, c >}.  3-3 Mixed String Adjunct Language (MAL)  We now have two different styles of rules in G, namely, the 	[[54, 57]]	[[23, 52]]	['MAL']	['Mixed String Adjunct Language']
1558	known that Figure 1(a) can be represented by an equivalent hierarchical Chinese Restaurant Process (CRP) (Aldous, 1985) as in Figure 1(b). 	[[100, 103]]	[[72, 98]]	['CRP']	['Chinese Restaurant Process']
1559	The DM is bracketed between two other components, the Input Manager (IM) and the Output Manager (OM). The	[[97, 99], [4, 6], [69, 71]]	[[81, 95], [54, 67]]	['OM', 'DM', 'IM']	['Output Manager', 'Input Manager']
1560	pears or no other attribute is included.  Surface Text (ST): To measure the effectiveness of the semantic analysis (attribute labels and 	[[56, 58]]	[[42, 54]]	['ST']	['Surface Text']
1561	"up in the bracketed terminal string as insertion o f ""  (* *) "" surround-  ing "" 1970 "".) Inverse noun phrase preposing (NPPREPOS), which  relates such surface pairs as "" GM's sales "" and "" the sales of GM "", "	[[121, 129], [171, 173], [203, 205]]	[[98, 119]]	['NPPREPOS', 'GM', 'GM']	['noun phrase preposing']
1562	each frame from each camera view.  3.3 Dyadic Features (DF)  All of the features discussed above are low-level 	[[56, 58]]	[[39, 54]]	['DF']	['Dyadic Features']
1563	This problem  is of practical interest for the design of various types  of natural anguage interfaces (NLI's) that make use of  different knowledge sources.	[[103, 108]]	[[75, 101]]	"[""NLI's""]"	['natural anguage interfaces']
1564	course Connectives. Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT). 	[[93, 96]]	[[58, 91]]	['TLT']	['Treebanks and Linguistic Theories']
1565	Univers i ty  of Vienna  The first part of this paper is dedicated to an overv iew  of the parser of the system VIE-LANG (Viennese Language  Understanding System).	[[112, 120]]	[[122, 139]]	['VIE-LANG']	['Viennese Language']
1566	ZZ initial optional verb complements  statement(Q) -...  verb_complementsO(VC). 	[[75, 77], [0, 2]]	[[57, 73]]	['VC', 'ZZ']	['verb_complements']
1567	constructions. The notion of construction is similar to the one in Construction Grammar (CxG)4, as in (Goldberg, 1995), where:	[[89, 92]]	[[67, 87]]	['CxG']	['Construction Grammar']
1568	Errors are italicized and marked in red.  LDA with phrases (LDA-P): As aspect-sentiment phrases are often noun phrases, a basic approach	[[60, 65]]	[[42, 58]]	['LDA-P']	['LDA with phrases']
1569	They thus attract research from many different and beneficial perspectives. Dialog acts (DAs) (Searle, 1969), which reflect the functions that ut-	[[89, 92]]	[[76, 87]]	['DAs']	['Dialog acts']
1570	 2. Computer  Aided Wri t ing (CAW)  A computer system for a writer is basically a 	[[31, 34]]	[[4, 29]]	['CAW']	['Computer  Aided Wri t ing']
1571	The most comparable tools to MT-EQuAl are PET (Aziz et al., 2012), COSTA (Chatzitheodorou and Chatzistamatis, 2013), TAUS DQF framework,	[[67, 72], [29, 36], [42, 45], [117, 121], [122, 125]]	[[74, 108]]	['COSTA', 'MT-EQuA', 'PET', 'TAUS', 'DQF']	['Chatzitheodorou and Chatzistamatis']
1572	7-3-1, Hongo, Bunkyo-ku  Tokyo 113, Japan  It is difficult for a natural language understanding system (NLUS) to deal  with ambiguities.	[[104, 108]]	[[65, 102]]	['NLUS']	['natural language understanding system']
1573	clude substitution, splitting and merging statistics. Given  an input (ASCII) word, and the above statistics, candidate  (corrupted) words are generated based on simulating and pro- 	[[71, 76]]	[[61, 69]]	['ASCII']	['an input']
1574	  The project used the Linguamatics Interactive  Information Extraction (I2E) platform. This 	[[73, 76]]	[[49, 71]]	['I2E']	['Information Extraction']
1575	"example, the prepObject )f a LOCATION-PP must be a  PLACE-NOUN. A description of ""on AI"" (as in ""book on  AI"") as a LOCATION-PP c~Id  not be constructed since AI "	[[85, 88], [159, 161], [29, 40], [52, 62], [116, 127]]	[]	"['AI""', 'AI', 'LOCATION-PP', 'PLACE-NOUN', 'LOCATION-PP']"	[]
1576	fying the native language based on the manner of speaking and writing a second language is borrowed from Second Language Acquisition (SLA), where this is known as language transfer.	[[134, 137]]	[[105, 132]]	['SLA']	['Second Language Acquisition']
1577	Figure 2: Heat map of the relevance scores w s, j between the target domain Usenet (UN) with the other domains on ACE 2004 data set.	[[84, 86], [114, 117]]	[[76, 82]]	['UN', 'ACE']	['Usenet']
1578	 Conf. on Language Resources and Evaluation (LREC). 	[[45, 49]]	[[10, 28]]	['LREC']	['Language Resources']
1579	pus (Mitchell et al, 2003)1. Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based par-	[[75, 80], [48, 53]]	[[59, 73], [38, 46]]	['BNEWS', 'NWIRE']	['Broadcast News', 'Newswire']
1580	Background (BKG) the background of the study Problem (PROB) the research problem Method (METH) the methods used Result (RES) the results achieved	[[89, 93], [12, 15], [54, 58], [120, 123]]	[[81, 87], [0, 10], [45, 52], [112, 118]]	['METH', 'BKG', 'PROB', 'RES']	['Method', 'Background', 'Problem', 'Result']
1581	sets used in the fast Tree Kernel.  2.3 A Fast Tree Kernel (FTK) To compute the kernels defined in the previous	[[60, 63]]	[[42, 58]]	['FTK']	['Fast Tree Kernel']
1582	 1 Introduction Coreference resolution (CoRe) is the process of finding markables (noun phrases) referring to the same	[[40, 44]]	[[16, 38]]	['CoRe']	['Coreference resolution']
1583	retrieval with locality information using smart. In  Text retrieval conferenc (TREC-1) (pp. 59-72).	[[79, 85], [88, 90]]	[[53, 77]]	['TREC-1', 'pp']	['Text retrieval conferenc']
1584	A similar embedding method, called ? Global Vector (GloVe)?, was	[[52, 57]]	[[37, 50]]	['GloVe']	['Global Vector']
1585	Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney	[[83, 87]]	[[62, 81]]	['VSMs']	['vector space models']
1586	we use three categories for the identically spelled words: (a) we use the term true equivalents (TE) to refer to the pairs that have the same	[[97, 99]]	[[79, 95]]	['TE']	['true equivalents']
1587	unified into one model. We refer to this model as the Unified Transition(UT) model. 	[[73, 75]]	[[54, 72]]	['UT']	['Unified Transition']
1588	3 HCMUS 6L OpenNLP OpenNLP Dict Rules - Table 2: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer,	[[113, 115]]	[[116, 132]]	['BI']	['Bioinformatician']
1589	mance figures are the standard measures used for this task: F-measure (harmonic mean of recall and precision) and slot error rate (SER), where separate type, extent and content error measures are averaged to get the reported result.	[[131, 134]]	[[114, 129]]	['SER']	['slot error rate']
1590	SVO = Subject-Verb-Object GE = General Event PE = Predefined Event Rule-based	[[45, 47], [0, 3], [26, 28]]	[[50, 66], [6, 25], [31, 44]]	['PE', 'SVO', 'GE']	['Predefined Event', 'Subject-Verb-Object', 'General Event']
1591	with common sense knowledge.       Natural language processing (NLP) techniques  such as part of speech tagging and parse tree gen-	[[64, 67]]	[[35, 62]]	['NLP']	['Natural language processing']
1592	ing Language Understanding Engine), and subsequently analyze its performance on the task?s shared texts. BLUE consists of a pipeline of a parser, logical form (LF) generator, an initial logic generator, and subsequent processing modules.	[[160, 162]]	[[146, 158]]	['LF']	['logical form']
1593	grading.  The Content Assessment Module (CAM) presented in Bailey (2008) and Bailey and Meurers	[[41, 44]]	[[14, 39]]	['CAM']	['Content Assessment Module']
1594	1991), can be characterized asknowledge-rich  in that they presuppose that known lexical  items possess Conceptual Dependence(CD)-  like descriptions.	[[126, 128]]	[[104, 124]]	['CD']	['Conceptual Dependenc']
1595	 Entities On the level of entity extraction, Named Entities (NE) were defined as proper names and quantities of interest. 	[[61, 63]]	[[45, 59]]	['NE']	['Named Entities']
1596	and Causal Relations. In proceedings of the Association for Computational Linguistics (ACL). 	[[87, 90]]	[[50, 85]]	['ACL']	['ation for Computational Linguistics']
1597	PLC = partition left context  (has been done)  PRC = partition right context  (yet to be done) 	[[47, 50], [0, 3]]	[[53, 76], [6, 28]]	['PRC', 'PLC']	['partition right context', 'partition left context']
1598	In our particular application, access to Getty?s Art and Architecture Thesaurus (AAT), to other museum and collection databases or online auction cata-	[[81, 84]]	[[49, 79]]	['AAT']	['Art and Architecture Thesaurus']
1599	 2.3 Named Entity Recognition Named Entity Recognition (NER) is the task of finding all instances of explicitly named entities	[[56, 59]]	[[30, 54]]	['NER']	['Named Entity Recognition']
1600	37  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, page 97, Gothenburg, Sweden, April 27, 2014.	[[73, 78]]	[[39, 71]]	['HyTra']	['Hybrid Approaches to Translation']
1601	The idea is simply to count the mmlber of new  words introduced ow'x a moving interval and 1)roduce  what he calls a vocabulary managemenl profile (VMI'),  or lneasurements at intervals.	[[148, 152]]	[[117, 146]]	"[""VMI'""]"	['vocabulary managemenl profile']
1602	 In order to acquire labeled instances for training, we decompose the gold standard (GS) events into multiple events with single arguments.	[[85, 87]]	[[70, 83]]	['GS']	['gold standard']
1603	 310 X. Fan et al  Causality Na?ve Bayesian Classifier (CNB): For a document represented by a binary-valued vector d=(X1 ,X2 , ?,	[[56, 59]]	[[19, 43]]	['CNB']	['Causality Na?ve Bayesian']
1604	subject and object with the ground truth. ETS/ETO = Emotions towards subject/object, MAS=mean absolute error, and RMSE= root mean square error	[[85, 88], [42, 49], [114, 118]]	[[89, 102], [52, 83], [120, 142]]	['MAS', 'ETS/ETO', 'RMSE']	['mean absolute', 'Emotions towards subject/object', 'root mean square error']
1605	3 Experiments We conducted closed track experiments on three data sources: the Academia Sinica (AS) corpus, the Beijing University (PKU) corpus and the Hong	[[96, 98], [132, 135]]	[[79, 94]]	['AS', 'PKU']	['Academia Sinica']
1606	following are the most frequently used ones: ? MS = Mel?c?uk style used in the MeaningText Theory (MTT): the first conjunct is the	[[47, 49]]	[[52, 66]]	['MS']	['Mel?c?uk style']
1607	can be specified.  The 'source condition(SCND)' represents  conditions on variables in the 'source pattern.'	[[41, 45]]	[[24, 39]]	['SCND']	['source conditio']
1608	Performing proper Arabic dialect identification may positively impact many Natural Language Processing (NLP) application.	[[104, 107]]	[[75, 102]]	['NLP']	['Natural Language Processing']
1609	a baseline.  Language Model (LM): We model the semantic fit of a candidate substitute within the given context	[[29, 31]]	[[13, 27]]	['LM']	['Language Model']
1610	compared to the baseline and stem, respectively.  As for the Kirghiz to Chinese translation (KI-CH) task, improvements seem relative small compared	[[93, 98]]	[[61, 79]]	['KI-CH']	['Kirghiz to Chinese']
1611	are not very demanding on resources: Inverse Consultation (IC) (Tanaka and Umemura, 1994) and Distributional Similarity (DS) (Kaji et al, 2008), their strong points and weaknesses, and proposed	[[121, 123], [59, 61]]	[[94, 119], [37, 57]]	['DS', 'IC']	['Distributional Similarity', 'Inverse Consultation']
1612	work for sentence level feature extraction. In the Window Processing component, each token is further represented as Word Features (WF) and Position Features (PF) (see section 3.4.1 and 3.4.2). Then, the	[[132, 134], [159, 161]]	[[117, 130], [140, 157]]	['WF', 'PF']	['Word Features', 'Position Features']
1613	The seed set is compiled from popular mental and emotional state dictionaries, including the Profile of Mood States (POMS) (McNair et al.,	[[117, 121]]	[[93, 115]]	['POMS']	['Profile of Mood States']
1614	)( bwN We use the corpus provided by IR task of  NTCIR2 (NTCIR 2002) as the training set to  compute the mutual information of words.	[[49, 55], [37, 39]]	[[57, 67]]	['NTCIR2', 'IR']	['NTCIR 2002']
1615	4.3 Nested Expressions No nested expressions will be marked. Even in cases where LOCATION (ENAMEX) expressions occur within  TIMEX and NUMEX expressions, they are not to be tagged.	[[91, 97], [125, 130], [135, 140]]	[[61, 89]]	['ENAMEX', 'TIMEX', 'NUMEX']	['Even in cases where LOCATION']
1616	 3 Bridgeman Art Library Bridgeman Art Library (BAL)2 is one of the world?s top image libraries for art, culture and history.	[[48, 51]]	[[25, 46]]	['BAL']	['Bridgeman Art Library']
1617	1 Introduction Newswire text has long been a primary target for natural language processing (NLP) techniques such as information extraction, summarization, and ques-	[[93, 96]]	[[64, 91]]	['NLP']	['natural language processing']
1618	In this example, only Midas can be chosen for the role of twit, but any member of the class PN (proper names) having the attributes male, brave and handsome can be	[[92, 94]]	[[96, 108]]	['PN']	['proper names']
1619	cannot co-exist on nouns. Next comes the class of particle proclitics (PART+): +  l+ ?	[[71, 76]]	[[50, 69]]	['PART+']	['particle proclitics']
1620	it as a model for compiling their own corpora.  The Russian National Corpus (RNC) has been released by the group of specialists from different organi-	[[77, 80]]	[[52, 75]]	['RNC']	['Russian National Corpus']
1621	The alignments produced by MEBA were  compared to the ones produced by YAWA and  evaluated against the Gold Standard (GS)1 annotations used in the Word Alignment Shared 	[[118, 120], [27, 31], [71, 75]]	[[103, 116]]	['GS', 'MEBA', 'YAWA']	['Gold Standard']
1622	In recent decades, this idea appears in (Curry, 1961) where the interlingua is called tectogrammar, in the Rosetta project (Rosetta, 1994), building on the semantic models of (Montague, 1974), and in the UNL (Universal Networking Language) project. 	[[204, 207]]	[[209, 238]]	['UNL']	['Universal Networking Language']
1623	2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (deter-	[[69, 72], [86, 89], [100, 104], [116, 119]]	[[74, 83], [91, 97], [106, 113], [121, 126]]	['ADJ', 'ADV', 'PRON', 'DET']	['adjective', 'adverb', 'pronoun', 'deter']
1624	ond, task B referred to as task of normalization involves the mapping of each disorder mention to a  UMLS concept unique identifier (CUI).The mapping was limited to UMLS CUI of SNOMED clin-	[[133, 136]]	[[106, 131]]	['CUI']	['concept unique identifier']
1625	 We follow Curran (2004) and use two performance measures: direct matches (DIRECT) and inverse rank (INVR).	[[75, 81], [101, 105]]	[[59, 73], [87, 99]]	['DIRECT', 'INVR']	['direct matches', 'inverse rank']
1626	We further assume that these documents have been tagged with Named Entities (NE)s with coreferences resolved 807	[[77, 79]]	[[61, 75]]	['NE']	['Named Entities']
1627	where P is the Macro Precision and R is the Macro Recall. We also use tree induced error (TIE) in the experiments.	[[90, 93]]	[[70, 88], [21, 30], [50, 56]]	['TIE']	['tree induced error', 'Precision', 'Recall']
1628	Two criteria are used:  1. Overlapping Ambiguity Strings (OAS): the  reference segmentation and the segmenter 	[[58, 61]]	[[27, 56]]	['OAS']	['Overlapping Ambiguity Strings']
1629	Note Table 3: Statistics of training and test corpus for the Canadian Hansards task (PP=perplexity). 	[[85, 87]]	[[88, 98]]	['PP']	['perplexity']
1630	ance improvements in our system.  The OOV recall rates (RRoov) showed in  Table 4 demonstrate that the OOV recognition 	[[56, 61], [38, 41], [103, 106]]	[[42, 54]]	['RRoov', 'OOV', 'OOV']	['recall rates']
1631	 ? REL = relation; ARG = NP/VP/ADJ (6) ACADE?MIQUE = Qui manque d?originalite?,	[[3, 6], [19, 22], [39, 50]]	[[9, 17], [25, 34], [53, 78]]	['REL', 'ARG', 'ACADE?MIQUE']	['relation', 'NP/VP/ADJ', 'Qui manque d?originalite?']
1632	is still room for improvement.  2 The Index of Productive Syntax (IPSyn) The Index of Productive Syntax (Scarborough,	[[66, 71]]	[[38, 64]]	['IPSyn']	['Index of Productive Syntax']
1633	  Parallel corpora Size of English texts (in  million words (MB))  Size of Chinese texts (in 	[[61, 63]]	[[46, 59]]	['MB']	['million words']
1634	Available for SRL tasks are efforts such as PropBank (Palmer et al, 2005) and FrameNet (Fillmore et al, 2003) that have developed semantic role labels (based on differing approaches) and have labeled large corpora for training and testing of SRL systems. PropBank (PB) identifies and labels the semantic arguments of the verb on a verb-by-verb basis, creating a separate frameset that includes verb specific semantic roles to account for each subcategorization frame of the verb. Much like PB, FrameNet (FN) identifies and labels semantic roles, known as Frame Elements, around a relational target, usually a verb.2 But unlike PB, Frame Elements less verb specific, but rather are defined in terms of semantic structures called frames evoked by the verb. That is, one or more verbs can be associated with a single semantic frame.	[[504, 506], [14, 17], [242, 245], [265, 267]]	[[494, 502], [255, 263]]	['FN', 'SRL', 'SRL', 'PB']	['FrameNet', 'PropBank']
1635	A necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference (NLI): the task of determining whether a natural-language	[[138, 141]]	[[110, 136]]	['NLI']	['natural language inference']
1636	transcripts produced with Automatic Speech Recognition  (ASR) systems tend to contain many recognition errors,  leading to low Information Retrieval (IR) performance  (Oard et al, 2007).	[[150, 152], [57, 60]]	[[127, 148], [26, 54]]	['IR', 'ASR']	['Information Retrieval', 'Automatic Speech Recognition']
1637	qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, np), qdet?1.0 DET(the),	[[52, 54], [8, 10], [30, 32], [76, 79]]	[[56, 60]]	['PP', 'NP', 'NP', 'DET']	['prep']
1638	shown by Kusner and colleagues (2015), semantic representations such as Latent Semantic Indexing and Latent Dirichlet Allocation (LDA) can outperform a BoW representation.	[[130, 133], [152, 155]]	[[101, 128]]	['LDA', 'BoW']	['Latent Dirichlet Allocation']
1639	This usage is domain specific and the referent is fixed as a second person subject.  pora: 1) three transcripts of family conversation (FaCon) drawn from Australian English Corpus (Monash University 1996~1998) collecting family interviews about their past holidays (Nariyama 2004); 2) three 30-minute-TV Australian drama transcripts (TV) (Nariyama 2004); 3) Switchboard corpus consisting of telephone conversation on a variety of specified every day topics (Cote 1996).  Referent   FaCon TV dramas Switchboard I we you he/she it they 	[[136, 141], [482, 487], [334, 336], [301, 303]]	[[115, 134]]	['FaCon', 'FaCon', 'TV', 'TV']	['family conversation']
1640	query, and their performance asymptotes by the time they get to the second query.  This effect is confirmed by an analysis of variance (ANOVA)8, which shows a highly significant effect of order of presentation (F = 9.8427; p< .0001).	[[136, 141]]	[[114, 134]]	['ANOVA']	['analysis of variance']
1641	 2 Related Work Dialog act (DA) annotations and tagging, inspired by the speech act theory of Austin (1975) and Searle	[[28, 30]]	[[16, 26]]	['DA']	['Dialog act']
1642	text refer to the same entity in real world or not.  Noun Phrase CRR (NP-CRR) considers all noun  phrases as entities, while Named Entity CRR 	[[70, 76], [138, 141]]	[[53, 68]]	['NP-CRR', 'CRR']	['Noun Phrase CRR']
1643	RLP = rule  learner prediction. RS = Reference Standard   	[[32, 34], [0, 3]]	[[37, 55], [6, 30]]	['RS', 'RLP']	['Reference Standard', 'rule  learner prediction']
1644	follows:  1. If the topic is an individual constant (IC), establish  restrictions (using the Restricts link), if any, on the 	[[53, 55]]	[[32, 51]]	['IC']	['individual constant']
1645	{EVENT 2 {AND {SUBTYPE DIE} {PERSON  $foo}}}  2.4 Graphical User Interface (GUI)  For some applications such as database 	[[76, 79]]	[[50, 74]]	['GUI']	['Graphical User Interface']
1646	The proceedings from CoNLL2004 and  CoNLL2005 detail a wide variety of approaches  to Semantic Role Labeling (SRL).  Many re-	[[110, 113], [21, 30], [36, 45]]	[[86, 108]]	['SRL', 'CoNLL2004', 'CoNLL2005']	['Semantic Role Labeling']
1647	We use the same evaluation metrics as in (McDonald et al, 2005). Dependency accuracy (DA) is the proportion of non-root words that are assigned the	[[86, 88]]	[[65, 84]]	['DA']	['Dependency accuracy']
1648	 1 Overv iew o f  the  IPS  pro jec t   The IPS (Interactive Parsing System) research  project, at the Linguistics Departement of the 	[[44, 47], [23, 26]]	[[49, 75]]	['IPS', 'IPS']	['Interactive Parsing System']
1649	Abstract  This article focuses on the development of  Natural Language Processing (NLP) tools for  Computer Assisted Language Learning 	[[83, 86]]	[[54, 81]]	['NLP']	['Natural Language Processing']
1650	are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from citation texts (CT). 	[[143, 145], [55, 57], [74, 76], [83, 85]]	[[127, 141], [39, 53], [63, 72]]	['CT', 'CT', 'AB', 'DP']	['citation texts', 'citation texts', 'abstracts']
1651	1 Quantification resolution We are concerned with ambiguously quantified noun phrases (NPs) and their interpretation, as illustrated by the following examples:	[[87, 90]]	[[73, 85]]	['NPs']	['noun phrases']
1652	mantic relations between referents. This task has a long tradition in natural language processing (NLP) since the early days of artificial intelligence (Web-	[[99, 102]]	[[70, 97]]	['NLP']	['natural language processing']
1653	bbai@nec-labs.com Abstract We develop a recursive neural network (RNN) to extract answers to arbitrary natural language	[[66, 69]]	[[40, 64]]	['RNN']	['recursive neural network']
1654	sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems. 	[[131, 133]]	[[110, 129]]	['MT']	['machine translation']
1655	 We used Moses (Koehn et al 2007) with lexicalized reordering and a 6-gram language model (LM) trained using SRILM (Stolcke et al 2011) to trans-	[[91, 93], [109, 114]]	[[75, 89]]	['LM', 'SRILM']	['language model']
1656	terms encountered.  Shallow Syntactic (SSyn) features consider the number and ratios of common part-of-speech	[[39, 43]]	[[20, 37]]	['SSyn']	['Shallow Syntactic']
1657	the verb that contained in a subordinate clause.  We use semantic role labeling (SRL) to help  solve this problem in which the coordinated can 	[[81, 84]]	[[57, 79]]	['SRL']	['semantic role labeling']
1658	We performed the same experiments on three dif-  ferent corpora:  Corpus SN (Spanish Novel) train: 15Kw, test:  2Kw, tag set size: 70.	[[73, 75]]	[[77, 90]]	['SN']	['Spanish Novel']
1659	 NONHUMAN (animals), DNA, RNA, PROTEIN, CONTROL (control measures to contain the disease), BACTERIA, CHEMICAL and SYMPTOM.	[[40, 47], [21, 24], [26, 29], [31, 38], [91, 99], [101, 109], [114, 121], [1, 9]]	[[49, 56], [11, 18]]	['CONTROL', 'DNA', 'RNA', 'PROTEIN', 'BACTERIA', 'CHEMICAL', 'SYMPTOM', 'NONHUMAN']	['control', 'animals']
1660	In particular, the work of (Pietra et al, 1997) is inspiring to us, but the improved iterative scaling (IIS) method for parameter estimation and the Gibbs sampler	[[104, 107]]	[[76, 102]]	['IIS']	['improved iterative scaling']
1661	 3 Dataset & Experimental Setup We use the First Certificate in English (FCE) ESOL examination scripts2 (upper-intermediate level as-	[[73, 76]]	[[43, 71]]	['FCE']	['First Certificate in English']
1662	We also mark conjunct clauses with the feature nosubj if they are neither headed by an imperative nor contain a child node with the grammatical function SB (subject) or EP (expletive). This is useful in order to correctly parse	[[153, 155], [169, 171]]	[[157, 164], [173, 182]]	['SB', 'EP']	['subject', 'expletive']
1663	 5.3   Learning Algorithm: Conditional Random Field  Conditional Random Fields (CRF) is a formalism well-suited for learning and prediction on sequential data.	[[80, 83]]	[[53, 78]]	['CRF']	['Conditional Random Fields']
1664	  Abstract  In an attempt to extend Penn Discourse Tree Bank (PDTB) / Turkish Discourse Bank (TDB)  style annotations to spoken Turkish, this paper presents the first attempt at annotating the explicit 	[[62, 66], [94, 97]]	[[36, 60], [70, 92]]	['PDTB', 'TDB']	['Penn Discourse Tree Bank', 'Turkish Discourse Bank']
1665	The relative clause itself has the category S; the incoming edge is labeled RC (relative clause). 	[[76, 78]]	[[80, 95]]	['RC']	['relative clause']
1666	words. We model semantic relatedness between two words using the Information Content (IC) of the pair in a method similar to the one used by Lin	[[86, 88]]	[[65, 84]]	['IC']	['Information Content']
1667	ber of occurrence for this feature per patient narrative is obtained based on the frequency of the coordinating conjunction PoS tag (CC) detected in the parse tree structure.	[[133, 135]]	[[112, 123]]	['CC']	['conjunction']
1668	Economics neighborhood fbank  bank  Subject Code EC = Economies  account cheque money by 	[[49, 51]]	[[54, 63]]	['EC']	['Economies']
1669	five folds and evaluate them on the fifth 29 Table 1: Results for passage retrieval for TREC-QA using disjoint windows (DW) and sliding windows (SW). ??	[[120, 122], [88, 95], [145, 147]]	[[102, 118]]	['DW', 'TREC-QA', 'SW']	['disjoint windows']
1670	Having presented the sequent parser, we now show its embedding in the learning algorithm GraSp (Grammar of Speech). 	[[89, 94]]	[[96, 113]]	['GraSp']	['Grammar of Speech']
1671	2007) from 4 domains, labeled positive or negative. We apply logistic regression (LR) and SVM using unigram and bigram features.	[[82, 84], [90, 93]]	[[61, 80]]	['LR', 'SVM']	['logistic regression']
1672	"  -movement (mostly wh-movement: WH), empty complementizers (COMP), empty units (UNIT), and traces representing pseudo-attachments"	[[61, 65], [33, 35], [81, 85]]	[[44, 59], [74, 78]]	['COMP', 'WH', 'UNIT']	['complementizers', 'unit']
1673	They  ha 1 Personal Name (PN); Date or Time (DT); Location Name  (LN); Team Name (TN); Competition Title (CT); Personal 	[[26, 28], [45, 47], [66, 68], [82, 84], [106, 108]]	[[11, 24], [31, 43], [50, 63], [71, 80], [87, 104]]	['PN', 'DT', 'LN', 'TN', 'CT']	['Personal Name', 'Date or Time', 'Location Name', 'Team Name', 'Competition Title']
1674	Topic 1 Other modules Greeting(GR) Keep silence(KS) Figure 2: Overview of the information navigation	[[31, 33], [48, 50]]	[[22, 29], [35, 46]]	['GR', 'KS']	['Greetin', 'Keep silenc']
1675	Proceedings of the Fourteenth  International Joint Conference in Artificial  Intelligence (IJCAI?95), pp. 1395 ?	[[91, 99]]	[[77, 89]]	['IJCAI?95']	['Intelligence']
1676	for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland. 	[[145, 149], [49, 52]]	[[106, 143]]	['BUCC', 'DSL']	['Building and Using Comparable Corpora']
1677	Figure 1: Sample transcript from a TD child 3 Narrative Topic Analysis Using LDA Latent Dirichlet Allocation (LDA) (Blei et al 2003) has been used in NLP to model topics within	[[110, 113]]	[[81, 108]]	['LDA']	['Latent Dirichlet Allocation']
1678	of which are limited in scope. We here restrict inferables to the particular subset de-  fined by Hahn, Markert, and Strube (1996), which we call functional anaphora (FA). 	[[167, 169]]	[[146, 165]]	['FA']	['functional anaphora']
1679	sulting matrices be M1 and M2, respectively. In step (3), SentIDs (sentences where the two words appear with the specified relation) are obtained by	[[58, 65]]	[[67, 96]]	['SentIDs']	['sentences where the two words']
1680	One is manually typing the text transcriptions which we regarded as the Correct Recognition Result (CRR) transcription, and another is the ASR result which	[[100, 103], [139, 142]]	[[72, 98]]	['CRR', 'ASR']	['Correct Recognition Result']
1681	1985) in which Tree Adjoining Grammars fall. 3 Since the  set of Tree Adjoining Languages (TALs) is a strict super-  set of the set of Context Free Languages, in order to define 	[[91, 95]]	[[65, 89]]	['TALs']	['Tree Adjoining Languages']
1682	 In our experiments, we have applied the COLLINS (Collins, 1999) parser to generate the syntactic tree of both pieces of text.	[[41, 48]]	[[50, 57]]	['COLLINS']	['Collins']
1683	The generator operates from a declarative know-  ledge base of linguistic knowledge, common to that used  by PHRAN (PHRasal ANalyzer; Wilensky and Arens,  1980).	[[109, 114]]	[[116, 132]]	['PHRAN']	['PHRasal ANalyzer']
1684	the method described in Section 3.2). We also present the number of linear equations (L.Eq.) used	[[86, 91]]	[[68, 84]]	['L.Eq.']	['linear equations']
1685	word penalty The 8 features have weights adjusted on the tuning data using minimum error rate training (MERT) (Och, 2003).	[[104, 108]]	[[75, 102]]	['MERT']	['minimum error rate training']
1686	5.1 Data  We have two kinds of training data from general  domain: Labeled Data (LD) and Unlabeled Data  (UD).	[[81, 83], [106, 108]]	[[67, 79], [89, 103]]	['LD', 'UD']	['Labeled Data', 'Unlabeled Data']
1687	known formalisms uch as Head Driven Phrase  Structure Grammar (HPSG), Lexical Functional  Grammar (LFG) or Slot Grammars (SG), because  SUG allows a modular and computational 	[[122, 124], [63, 67], [99, 102], [136, 139]]	[[107, 120], [24, 61], [70, 97]]	['SG', 'HPSG', 'LFG', 'SUG']	['Slot Grammars', 'Head Driven Phrase  Structure Grammar', 'Lexical Functional  Grammar']
1688	 The single product opinion summarizer we consider is the Sentiment Aspect Match model (SAM) described and evaluated in (Lerman et al, 2009).	[[88, 91]]	[[58, 80]]	['SAM']	['Sentiment Aspect Match']
1689	cognition(COG) feeling(FEEL)  motivation(MOT) abstraction(ABS)  time(TIME) space(SPA) attribute(ATT)  relation(REL) social_relation(SREL) 	[[96, 99], [10, 13], [23, 27], [41, 44], [58, 61], [69, 73], [81, 84], [111, 114], [132, 136]]	[[86, 94], [0, 9], [15, 22], [30, 40], [46, 57], [64, 68], [75, 80], [102, 110], [116, 131]]	['ATT', 'COG', 'FEEL', 'MOT', 'ABS', 'TIME', 'SPA', 'REL', 'SREL']	['attribut', 'cognition', 'feeling', 'motivation', 'abstraction', 'time', 'space', 'relation', 'social_relation']
1690	2008).  The semantic role labeler (SRL) consists of a pipeline of independent, local classifiers that iden-	[[35, 38]]	[[12, 33]]	['SRL']	['semantic role labeler']
1691	rameter set to decode WSMT 2006 Europal test set (TEST1) and used the second on WSMT news commentary test set 2007 (TEST2)6. Table 6 shows the	[[116, 121], [80, 84], [22, 26], [50, 55]]	[[101, 114], [40, 48]]	['TEST2', 'WSMT', 'WSMT', 'TEST1']	['test set 2007', 'test set']
1692	strat1 40.1 24.4 15.0 strat2 38.2 22.5 14.5 Table 4: Word Error Rate (WER), Concept Error Rate (CER) and Interpretation Error Rate (IER) ac-	[[70, 73], [96, 99], [132, 135]]	[[53, 68], [76, 94], [105, 130]]	['WER', 'CER', 'IER']	['Word Error Rate', 'Concept Error Rate', 'Interpretation Error Rate']
1693	using a set of data-driven terms.  We investigated how likely term frequency (TF) based RF is to discover HEWs.	[[78, 80], [88, 90], [106, 110]]	[[62, 76]]	['TF', 'RF', 'HEWs']	['term frequency']
1694	Way (1991) emphasizes the importance of this taxonomy by positing a central role for a dynamic type hierarchy (DTH) in metaphor, one that can create new and com-	[[111, 114]]	[[87, 109]]	['DTH']	['dynamic type hierarchy']
1695	Multi-document person name resolution, Proceedings of 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Reference Resolution Workshop.	[[124, 127]]	[[81, 122]]	['ACL']	['Association for Computational Linguistics']
1696	 In Proceedings of the 16th International Conference on Computational Linguistics (COLING), volume I, pages 466?471.	[[83, 89]]	[[56, 81]]	['COLING']	['Computational Linguistics']
1697	 PRECISE (Popescu, Etzioni, and Kautz 2003) 88.0 88.0 ZC05 (Zettlemoyer and Collins 2005) 79.3 ? 	[[54, 58]]	[[60, 88]]	['ZC05']	['Zettlemoyer and Collins 2005']
1698	Using the alignments from HIER, we created phrase tables using model probabilities (MOD), and heuristic extraction on words (HEUR-W), blocks (HEUR-B), and minimal phrases (HEUR-P) as de-	[[125, 131], [26, 30], [84, 87], [142, 148], [172, 178]]	[[94, 123]]	['HEUR-W', 'HIER', 'MOD', 'HEUR-B', 'HEUR-P']	['heuristic extraction on words']
1699	to the Markov Logic system. At each step, we compute both the maximum a posteriori (MAP) assignment of coreference relationships as well	[[84, 87]]	[[62, 82]]	['MAP']	['maximum a posteriori']
1700	CORROLA TION  NP1 = (COR) (APP) (ADJ) (NC) N  NP2 = (NC) N  NP3 = N 	[[53, 55], [21, 24], [27, 30], [33, 36], [39, 41], [14, 17]]	[[46, 51], [0, 12]]	['NC', 'COR', 'APP', 'ADJ', 'NC', 'NP1']	['NP2 =', 'CORROLA TION']
1701	by each of these strategies.  The four Word Sense (WS) disambiguation  strategies resolve sense ambiguity errors.	[[51, 53]]	[[39, 49]]	['WS']	['Word Sense']
1702	These are selected from the LDC English Gigaword corpus. AFP = Agence France-Presse; AFW = Associated Press Worldstream; NYT = New York Times; XIN = Xinhua News Agency; and CNA = Central News Agency of Taiwan denote the sections of the LDC English Gigaword	[[121, 124], [143, 146], [28, 31], [57, 60], [85, 88], [173, 176], [236, 239]]	[[127, 141], [149, 160], [63, 83], [91, 119], [179, 198]]	['NYT', 'XIN', 'LDC', 'AFP', 'AFW', 'CNA', 'LDC']	['New York Times', 'Xinhua News', 'Agence France-Presse', 'Associated Press Worldstream', 'Central News Agency']
1703	knowledge about the structuf.e of the world into account.  - The Data Base Language ( DBL ) , which contains conatants that correspond  to data base primitives . (	[[86, 89]]	[[65, 83]]	['DBL']	['Data Base Language']
1704	Term Frequency-Inverse Document Frequency (TF-IDF) is a widely used similarity measure in Information Retrieval(IR). It has also been shown	[[112, 114], [43, 49]]	[[90, 110], [0, 41]]	['IR', 'TF-IDF']	['Information Retrieva', 'Term Frequency-Inverse Document Frequency']
1705	word-level features described in Section 5.  4.1 Ranking by regression (RR) The first ranking strategy is based on training a re-	[[72, 74]]	[[49, 70]]	['RR']	['Ranking by regression']
1706	of the log-likelihood.     We used the tempered EM (TEM) as described  by Hofmann (1999).	[[52, 55]]	[[39, 50]]	['TEM']	['tempered EM']
1707	2.1 Knowledge Source We employ BabelNet 2.5.1 as our reference knowledge base (KB). BabelNet is a multilingual	[[79, 81]]	[[63, 77]]	['KB']	['knowledge base']
1708	size ? is fixed to 0.0001. We refer to this model as Orthogonal Matrix Factorization (OrMF). 	[[86, 90]]	[[53, 84]]	['OrMF']	['Orthogonal Matrix Factorization']
1709	or certain part-of-speech tags (e.g., interjection).8 4.2.3 Performance of the formality classifier We trained a Maximum Entropy (MaxEnt) classifier in the Mallet package (McCallum, 2002).	[[130, 136]]	[[113, 128]]	['MaxEnt']	['Maximum Entropy']
1710	77 .73  Problem 2nd 1.45 3.00  GS = Group significant cannot pool by individual  DISCUSSION OF THE RESULTS 	[[31, 33]]	[[36, 53]]	['GS']	['Group significant']
1711	ear (lin) kernel, second degree polynomial kernel (d=2), and RBF kernel (rbf); SVM with transductive inference (TSVM) and linear (lin) kernel or second degree polynomial (d=2) ker-	[[112, 116]]	[[88, 110]]	['TSVM']	['transductive inference']
1712	The word lattices of the HUB-1 corpus are directed acyclic graphs in the HTK Standard Lattice Format (SLF), consisting of a set of vertices and a set of edges.	[[102, 105], [25, 30], [73, 76]]	[[77, 100]]	['SLF', 'HUB-1', 'HTK']	['Standard Lattice Format']
1713	2011). The default loss function is expected error (EE) (Och, 2003; Cherry and Foster, 2012).	[[52, 54]]	[[36, 50]]	['EE']	['expected error']
1714	 3.1.1 Pre-Processing For Feature Extraction Phrase Analysis(PA): Using basic syntactic analysis (shallow parsing), the PA module re-builds	[[61, 63], [120, 122]]	[[45, 59]]	['PA', 'PA']	['Phrase Analysi']
1715	plains the text? Our approach is related to minimum description length (MDL). We formulate our	[[72, 75]]	[[48, 70]]	['MDL']	['mum description length']
1716	pattern, up to mi example questions.  Question Pattern (QPi):  When do Q_PRN Q_MVerb Q_BNP?	[[56, 59], [71, 76], [77, 84], [85, 90]]	[[38, 54]]	['QPi', 'Q_PRN', 'Q_MVerb', 'Q_BNP']	['Question Pattern']
1717	Sample DCR entries specifying enumerated values for SynFeatureName, etc. The specification uses the Ontology Web Language (OWL) to list valid values for objects of the defined class. 	[[123, 126], [7, 10]]	[[100, 121]]	['OWL', 'DCR']	['Ontology Web Language']
1718	{FirstName.SecondName}@dfki.de Abstract The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system	[[121, 123], [44, 48]]	[[97, 119]]	['IE', 'IDEX']	['Information Extraction']
1719	smoothing as the evaluation metric.  Best v.s. Rest (BR) To score the best hypothesis in the n-best set	[[53, 55]]	[[37, 51]]	['BR']	['Best v.s. Rest']
1720	Translation  The first method compared is a transitive  translation using MT (machine translation). The 	[[74, 76]]	[[78, 97]]	['MT']	['machine translation']
1721	to inspect and easily modify discourse-planning specifications for rapid  iterative refinement. The Explanation Design Package (EDP) formalism  is a convenient, schema-like (McKeown 1985; Paris 1988) programming 	[[128, 131]]	[[100, 126]]	['EDP']	['Explanation Design Package']
1722	function (Section 4.1).  Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of	[[55, 57]]	[[45, 53]]	['PB']	['PropBank']
1723	 3 Conditional Random Fields  Conditional Random Fields (CRFs) are a type of  discriminative probabilistic model proposed for 	[[57, 61]]	[[30, 55]]	['CRFs']	['Conditional Random Fields']
1724	(pre-nominal or post-nominal) and predicative functions; (ii) a unigram distribution (level uni), independently encoding the parts of speech (POS) of the words preceding and following the adjective, respec-	[[142, 145]]	[[125, 140]]	['POS']	['parts of speech']
1725	{yvchen, yww, anatoleg, air}@cs.cmu.edu Abstract Spoken dialogue systems (SDS) typically require a predefined semantic ontology	[[74, 77]]	[[49, 72]]	['SDS']	['Spoken dialogue systems']
1726	this, in addition to the four actions of the NonInc algorithm, we introduce two new actions: Left Reveal (LRev) and Right Reveal (RRev). For this,	[[130, 134], [106, 110]]	[[116, 128], [93, 104]]	['RRev', 'LRev']	['Right Reveal', 'Left Reveal']
1727	 4.1 Lexical Sample Tasks We have evaluated our system on SensEval-2 (SE2) and SensEval-3 (SE3) lexical sample tasks and also	[[70, 73], [91, 94]]	[[58, 68], [79, 89]]	['SE2', 'SE3']	['SensEval-2', 'SensEval-3']
1728	all characters are included as features; full remove (P4), where all special Twitter features like user names, URLs, hashtags, retweet (RT ) tags, and emoticons are stripped; and replacing Twitter fea-	[[136, 138], [54, 56], [111, 115]]	[[127, 134]]	['RT', 'P4', 'URLs']	['retweet']
1729	sampling the outputs at random locations.  INTENSITY: HOG (histogram of gradients) (Dalal and Triggs, 2005) describes the direction	[[54, 57]]	[[59, 81]]	['HOG']	['histogram of gradients']
1730	Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus. 	[[146, 148], [15, 17], [36, 39], [80, 85], [120, 128], [181, 183]]	[[149, 168], [18, 34], [40, 67], [86, 111], [129, 137], [184, 186]]	['SD', 'BI', 'NLP', 'McCCJ', 'Charniak', 'GE']	['Stanford Dependency', 'Bioinformatician', 'Natural Language Processing', 'McClosky-Charniak-Johnson', 'Charniak', 'GE']
1731	 respectively display the results obtained without and with the use of subcat features (SF). The sec-	[[88, 90]]	[[71, 86]]	['SF']	['subcat features']
1732	exposed through the feature HOOK to facilitate further composition. These properties include pointers to the local top handle (LTOP), the constituent?s primary index (INDEX), and the external argument, if any (XARG).	[[127, 131], [167, 172], [210, 214], [28, 32]]	[[109, 118], [160, 165], [183, 199]]	['LTOP', 'INDEX', 'XARG', 'HOOK']	['local top', 'index', 'external argumen']
1733	for Answer Search (IYAS) project, conducted by the Arabic Language Technologies (ALT) group at Qatar Computing Research Institute (QCRI) within the Hamad Bin Khalifa University and	[[131, 135], [19, 23], [81, 84]]	[[95, 129], [51, 79], [0, 17]]	['QCRI', 'IYAS', 'ALT']	['Qatar Computing Research Institute', 'Arabic Language Technologies', 'for Answer Search']
1734	SEMANTICS. The main component of SeSyn is a rule system (the syntax) which transforms the Semantic Analysis  (SA) of any given sentence into a Surface Structure (SS) of that sentence. The SAs represent meanings ina higher order 	[[162, 164], [33, 38], [110, 113], [188, 191]]	[[143, 160], [90, 107]]	['SS', 'SeSyn', 'SA)', 'SAs']	['Surface Structure', 'Semantic Analysis']
1735	  Daniel S. Leite1, Lucia H. M. Rino1, Thiago A. S. Pardo2, Maria das Gra?as V. Nunes2  N?cleo Interinstitucional de Ling??stica Computacional (NILC)  http://www.nilc.icmc.usp.br 	[[144, 148]]	[[88, 142]]	['NILC']	['N?cleo Interinstitucional de Ling??stica Computacional']
1736	 4 Classifier We used a linear support vector machine (SVM) classifier, which is standard for text data.	[[55, 58]]	[[31, 53]]	['SVM']	['support vector machine']
1737	2 Related Work There have been several studies on supervised dialogue act (DA) modeling. To the best of	[[75, 77]]	[[61, 73]]	['DA']	['dialogue act']
1738	syntactically correct) to 1.0 (completely wrong).  ISER (information item semantic error rate): The test sentences are segmented into information items; for each of these items, the translation candidates	[[51, 55]]	[[57, 93]]	['ISER']	['information item semantic error rate']
1739	 1 Introduct ion  Some natural anguage processing (NLP) tasks can  be performed with only coarse-grained semantic in- 	[[51, 54]]	[[23, 49]]	['NLP']	['natural anguage processing']
1740	ble. The model makes heavy use of single-category Ambiguity Classes (AC)3, which (being independent on the tagger?s intermediate decisions) can be	[[69, 71]]	[[50, 67]]	['AC']	['Ambiguity Classes']
1741	Mauser et al 2009). One promising approach is the Discriminative Word Lexicon (DWL). In this	[[79, 82]]	[[50, 77]]	['DWL']	['Discriminative Word Lexicon']
1742	identification. In Proceedings of the 24th International Conference on Computational Linguistics (COLING), pages 2585?2602, Mumbai, India.	[[98, 104]]	[[71, 96]]	['COLING']	['Computational Linguistics']
1743	 2.2 W3C Semantic Web The World Wide Web (WWW) was once designed to be as simple, as decentralized and as interop-	[[42, 45], [5, 8]]	[[26, 40]]	['WWW', 'W3C']	['World Wide Web']
1744	quency and its character string frequency is  less than or equal to 1%, it is a SWBS;  BMM-ASM (BMM ambiguity string mapping  table: the BMM-ASM table lists all the 	[[87, 94], [137, 144], [80, 84]]	[[96, 124]]	['BMM-ASM', 'BMM-ASM', 'SWBS']	['BMM ambiguity string mapping']
1745	lined previously, in that its target is entity extraction from raw news wires from the news agency Agence France Presse (AFP), and not only linking relying on gold NER annotations: the input	[[121, 124], [164, 167]]	[[99, 119]]	['AFP', 'NER']	['Agence France Presse']
1746	is indicated by the dotted black line.  The receiver operating characteristic (ROC) curves in Figures 2 and 3 demonstrate perfor-	[[79, 82]]	[[44, 77]]	['ROC']	['receiver operating characteristic']
1747	tive of the gold standard data.  Finally, the alignment error rate (AER) is lower (and hence better) for English?French than Romanian?	[[68, 71]]	[[46, 66]]	['AER']	['alignment error rate']
1748	 Parsed* Recall t Prec/Rec t MLP Prob t  Left Corner (LC) 21797 91.75 9000 .76399 .78156 .175928  LB o LC 53026 96.75 7865 .77815 .78056 .359828 	[[54, 56], [29, 32], [98, 100], [103, 105]]	[[41, 52]]	['LC', 'MLP', 'LB', 'LC']	['Left Corner']
1749	C = connector  TR = terse reply  FS = false start  E = echo 	[[33, 35], [15, 17]]	[[38, 49], [4, 13], [20, 31], [55, 59]]	['FS', 'TR']	['false start', 'connector', 'terse reply', 'echo']
1750	provided as input. The crawler generates the  Universal Resource Locator (URL) address for the  index (first) page of any particular date.	[[74, 77]]	[[46, 72]]	['URL']	['Universal Resource Locator']
1751	vides brief details of each annotation dimension.   2.1 Knowledge Type (KT)  This dimension is responsible for capturing the 	[[72, 74]]	[[56, 70]]	['KT']	['Knowledge Type']
1752	e-maih ide@cs,  vassar ,  edu   Abstract. The Text Encoding Initiative (TEl) is an  international project established in 1988 to develop 	[[72, 75]]	[[42, 70]]	['TEl']	['The Text Encoding Initiative']
1753	We developed three ensemble learning approaches for recognizing disorder entities and a Vector Space Model based method for encoding. Our approaches achieved top rank in both subtasks, with the best F measure of 0.813 for entity recognition and the best accuracy of 74.1% for encoding, indicating the proposed approaches are promising.  1 Introduction  In recent years, clinical natural language processing (NLP) has received great attention for its critical role in unlocking information embedded in clinical documents. Leveraging such information can facilitate the secondary1 use of electronic health record (EHR) data to                                                      This work is licensed under a Creative Commons Attribution 4.0 International Licence.	[[408, 411], [612, 615]]	[[379, 406], [586, 610]]	['NLP', 'EHR']	['natural language processing', 'electronic health record']
1754	Comprehension in 100 days,? published by  Chung Hwa Book Co., (H.K.) Ltd.  The  ChungHwa training set includes 100 English 	[[63, 67]]	[[48, 56]]	['H.K.']	['Hwa Book']
1755	In Proc. of the Association for Computational Linguistics (ACL), pages 523?530.	[[59, 62]]	[[16, 57]]	['ACL']	['Association for Computational Linguistics']
1756	these basic models for email conversations.  4.1 Latent Dirichlet Allocation (LDA) Our first model is the probabilistic LDA model	[[78, 81], [120, 123]]	[[49, 76]]	['LDA', 'LDA']	['Latent Dirichlet Allocation']
1757	expunged to meet United States HIPAA standards, (U.S. Health, 2002) and approved for release by the local Institutional Review Board (IRB); the sample must represent problems that medical records coders	[[134, 137], [49, 52], [31, 36]]	[[106, 132]]	['IRB', 'U.S', 'HIPAA']	['Institutional Review Board']
1758	PrecisionCorrectTransliterations (PTrans)  2. RecallCorrectTransliteration (RTrans)  3.	[[76, 82], [34, 40]]	[[46, 74], [0, 32]]	['RTrans', 'PTrans']	['RecallCorrectTransliteration', 'PrecisionCorrectTransliterations']
1759	Each accuracy measure is shown in a column, including the segmentation F-score (SF ), the overall tagging 894	[[80, 82]]	[[58, 72]]	['SF']	['segmentation F']
1760	while the NB+E extractor has the worst. Training the CRF with negative examples (CRF+E) gave better precision in extracted information then train-	[[81, 86], [10, 14]]	[[53, 79]]	['CRF+E', 'NB+E']	['CRF with negative examples']
1761	on Chinese FrameNet is divided into the subtasks of boundary identification(BI) and semantic role classification(SRC). 	[[113, 116], [76, 79]]	[[84, 112], [52, 75]]	['SRC', 'BI)']	['semantic role classification', 'boundary identification']
1762	Learning attitudes and attributes from multi-aspect reviews. In Data Mining (ICDM), 2012 IEEE 12th International Conference on, pages 1020?1025.	[[77, 81], [89, 93]]	[[61, 75]]	['ICDM', 'IEEE']	['In Data Mining']
1763	posed web-based semantic similarity measures: Jaccard, Dice, Overlap, PMI (Bollegala et al, 2007), Normalized Google Distance (NGD) (Cilibrasi and Vitanyi, 2007), Sahami and Heil-	[[127, 130], [70, 73]]	[[99, 125]]	['NGD', 'PMI']	['Normalized Google Distance']
1764	data with which to test research hypotheses. We de-  scribe the Air Travel Information System (ATIS) pilot  corpus, a corpus designed to measure progress in Spo- 	[[95, 99]]	[[64, 93]]	['ATIS']	['Air Travel Information System']
1765	EN 94,725 2.58 Table 2: Corpus statistics: SR=Serbian, SL=Slovene, EN=English, BG=Bulgarian Tagset The Multext-East corpus is manually an-	[[67, 69], [79, 81], [0, 2], [43, 45], [55, 57]]	[[70, 77], [82, 91], [46, 53], [58, 65]]	['EN', 'BG', 'EN', 'SR', 'SL']	['English', 'Bulgarian', 'Serbian', 'Slovene']
1766	http://www.ukp.tu-darmstadt.de Abstract In this paper, we present a machine learning approach for word sense alignment (WSA) which combines distances between senses in the graph representations of lexical-semantic resources	[[120, 123]]	[[98, 118]]	['WSA']	['word sense alignment']
1767	through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06-	[[123, 130], [8, 11], [50, 53], [167, 170]]	[[95, 121], [20, 48], [140, 165]]	['AQUAINT', 'IBM', 'DTO', 'BAA']	['Answering for Intelligence', 'Disruptive Technology Office', 'Broad Agency Announcement']
1768	prove SRL performance.  Template Generation (TG)  Our template generation (TG) algorithm extracts 	[[45, 47], [6, 9], [75, 77]]	[[24, 43], [54, 73]]	['TG', 'SRL', 'TG']	['Template Generation', 'template generation']
1769	.  Reattachment Heuristic (RH) targets nonargument head errors that occur if a TL argument	[[27, 29], [79, 81]]	[[3, 25]]	['RH', 'TL']	['Reattachment Heuristic']
1770	I_tt is raining.,  OBJ = Object: He read a book., 	[[19, 22]]	[[25, 31]]	['OBJ']	['Object']
1771	 1 Introduction  When a natural language processing (NLP) system  is created in a modular fashion, it can be relatively 	[[53, 56]]	[[24, 51]]	['NLP']	['natural language processing']
1772	? A chunking rule:  PP = prep, NP#1, if (pythontest(#1)). 	[[20, 22]]	[[25, 29]]	['PP']	['prep']
1773	On Complexity of Word Order. Traitement Automatique des Langues (TAL), 41(1):273?300. 	[[65, 68]]	[[29, 62]]	['TAL']	['Traitement Automatique des Langue']
1774	target translation: the gunman was killed by police .  The Penn English Treebank (PTB) (Marcus et al, 1993) is our source of syntactic information, largely	[[82, 85]]	[[59, 80]]	['PTB']	['Penn English Treebank']
1775	 Most commonly, recurrent neural networks are trained with stochastic gradient descent (SGD), where the gradient of the training criterion is com-	[[88, 91]]	[[59, 86]]	['SGD']	['stochastic gradient descent']
1776	rule in a CFG. It can  therefore fill the ACity (ArrivalCity) or the  DCity (DepartureCity) slot, and instantiate a 	[[42, 47], [10, 13], [70, 75]]	[[49, 60], [77, 90]]	['ACity', 'CFG', 'DCity']	['ArrivalCity', 'DepartureCity']
1777	of Data-to-Speech systems have been and are be-  ing developed on the basis of D2S. Examples are  the Dial Your Disc (DYD)-system, which presents  information in English about Mozart compositions 	[[118, 121], [79, 82]]	[[102, 116]]	['DYD', 'D2S']	['Dial Your Disc']
1778	Experimental results on Europarl with different translation directions (BLEU% on WMT08).  RW=Random Walk. * indicates the results are significantly better than the baseline (p<0.05).	[[90, 92], [72, 77], [81, 86]]	[[93, 104]]	['RW', 'BLEU%', 'WMT08']	['Random Walk']
1779	l (CAUSER sally-l)  (OBJECT paint-l)  (PATH (path-1 (DESTINATION wall-l))))  Sally sprayed the wall with paint.	[[39, 43], [10, 17], [28, 35]]	[[45, 51]]	['PATH', 'sally-l', 'paint-l']	['path-1']
1780	 2.2 LNRE Nature  o f  the  Data   The LNRE (Large Number of Rare Events)  zone (Chitashvili & Baayen, 1993) is defined as 	[[39, 43], [5, 9]]	[[45, 72]]	['LNRE', 'LNRE']	['Large Number of Rare Events']
1781	gls: the definition of the verb  They also defined two alternate search protocols: rich hierarchy exploration (RHE) with no  more than six links and shallow hierarchy explo-	[[111, 114]]	[[83, 109]]	['RHE']	['rich hierarchy exploration']
1782	"F6: ""TO PRODUCE GOLF CLUBS""  (VP (AUX (TO ""TO""))  (VP (V ""PRODUCE"")  (NP (N ""GOLF"") (N ""CLUBS"")))) "	[[51, 53], [30, 32], [34, 37], [70, 72]]	[[55, 58]]	['VP', 'VP', 'AUX', 'NP']	"['V ""']"
1783	The current   representat ion fo r  t h a t  sentence i n  pur system would be:  Z V l  =Ncorn(elephant,X1) PI =P.P(size,X1 ,small)  & =Ncom(animal,X1) P2 =P(size ,XI ,large) 	[[108, 110], [89, 94], [81, 86], [136, 140], [112, 115], [152, 154], [164, 166], [104, 106]]	[]	['PI', 'Ncorn', 'Z V l', 'Ncom', 'P.P', 'P2', 'XI', 'X1']	[]
1784	 Association for Computational Linguistics.          ACL Special Interest Group in Computational Phonology (SIGPHON), Philadelphia,        Morphological and Phonological Learning: Proceedings of the 6th Workshop of the	[[108, 115]]	[[57, 106]]	['SIGPHON']	['Special Interest Group in Computational Phonology']
1785	 50 missed OG events were labeled as Past (PA) while FU events were commonly mislabeled as both PA	[[43, 45], [11, 13], [53, 55], [96, 98]]	[[37, 41]]	['PA', 'OG', 'FU', 'PA']	['Past']
1786	5 Conclusion We have presented an efficient extension of the posterior regularization (PR) framework to a more general class of penalty functions.	[[87, 89]]	[[61, 85]]	['PR']	['posterior regularization']
1787	2013 Association for Computational Linguistics FCG offers a similar grammar engineering framework that follows the principles of Construction Grammar (CxG) (Goldberg 2003; Hoffmann and Trousdale 2013). CxG	[[151, 154], [47, 50], [202, 205]]	[[129, 149]]	['CxG', 'FCG', 'CxG']	['Construction Grammar']
1788	and Innovation action). This research is part of the Interactive sYstems for Answer Search (IYAS) project, conducted by the Arabic Language Tech-	[[92, 96]]	[[53, 90]]	['IYAS']	['Interactive sYstems for Answer Search']
1789	translation from one source language to multiple target languages, inspired by the recently proposed neural machine translation(NMT) framework proposed by Bahdanau et al (2014).	[[128, 131]]	[[101, 126]]	['NMT']	['neural machine translatio']
1790	 VP  Figure 3: A tree with some of its partial trees (PTs). 	[[54, 57]]	[[39, 52]]	['PTs']	['partial trees']
1791	are assigned the correct head and dependency type ? and unlabeled attachment score (UAS) ? the per-	[[84, 87]]	[[56, 82]]	['UAS']	['unlabeled attachment score']
1792	3 Estimation  We estimate a model?s distributions with  probabilistic decision trees (DTs).4 We build  decision trees using the WinMine toolkit 	[[86, 89]]	[[70, 84]]	['DTs']	['decision trees']
1793	~eather(WEA) ingestion(ING)  use(USE) social(SOC) body(BOD)  phy_creation(PCR) mental_creation(MCR)  verbal_creagion (VCR) 	[[74, 77], [95, 98], [8, 11], [23, 26], [33, 36], [45, 48], [55, 58], [118, 121]]	[[61, 72], [79, 93], [1, 7], [13, 22], [29, 32], [38, 44], [50, 54], [101, 116]]	['PCR', 'MCR', 'WEA', 'ING', 'USE', 'SOC', 'BOD', 'VCR']	['phy_creatio', 'mental_creatio', 'eather', 'ingestion', 'use', 'social', 'body', 'verbal_creagion']
1794	 1 Introduction In this paper, we propose TroFi (Trope Finder), a nearly unsupervised clustering method for sep-	[[42, 47]]	[[49, 61]]	['TroFi']	['Trope Finder']
1795	In Proceedings of the International Conference on Data Engineering (ICDE). 	[[68, 72]]	[[22, 66]]	['ICDE']	['International Conference on Data Engineering']
1796	At  the top level, >,sb,,~ denotes the basic relation for the  overall ranking of information structure (IS) patterns. 	[[105, 107]]	[[82, 103]]	['IS']	['information structure']
1797	"sented in the graphic. The first strategy can be applied when the data set contains a  functionally independent attribute (FIA) that is used as an organizing device or ""an-  chor"" for the entire graphic."	[[123, 126]]	[[87, 121]]	['FIA']	['functionally independent attribute']
1798	5.2 Results We evaluate SO of words on three different sized corpora: Gigaword (GW) 6.2GB, GigaWord + 50% of web data (GW+WB1) 21.2GB and Gi-	[[80, 82], [24, 26], [119, 121], [122, 125]]	[[70, 78]]	['GW', 'SO', 'GW', 'WB1']	['Gigaword']
1799	Table 5: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion, GE=GE task corpus.	[[94, 97], [73, 75], [138, 143], [178, 186], [204, 206], [239, 241]]	[[98, 125], [76, 92], [144, 169], [187, 195], [242, 244], [207, 226]]	['NLP', 'BI', 'McCCJ', 'Charniak', 'SD', 'GE']	['Natural Language Processing', 'Bioinformatician', 'McClosky-Charniak-Johnson', 'Charniak', 'GE', 'Stanford Dependency']
1800	Implicit Incongruity (IMP) Boolean Incongruity of extracted implicit phrases (Rilof et.al, 2013) Explicit Incongruity (EXP) Integer Number of times a word follows a word of opposite polarity	[[119, 122], [22, 25]]	[[97, 105]]	['EXP', 'IMP']	['Explicit']
1801	on Artificial Intelligence (KI2002), volume 2479 of Lecture Notes in Artificial Intelligence (LNAI), pages 18?32, Aachen, Germany, September. 	[[94, 98], [28, 34]]	[[52, 92]]	['LNAI', 'KI2002']	['Lecture Notes in Artificial Intelligence']
1802	One of  the most commonly used methods is the  Latent Semantic Analysis (LSA). In this 	[[73, 76]]	[[47, 71]]	['LSA']	['Latent Semantic Analysis']
1803	unable to touch the robot?s screen or to verbalize a speech command (e.g. after a stroke) is the brain computer interface (BCI) of the robot (Hintermu?ller et al, 2011).	[[123, 126]]	[[97, 121]]	['BCI']	['brain computer interface']
1804	functions. The latter three correspond to the three Gene Ontology (GO) (Ashburner et al, 2000) toplevel sub-ontologies, and terms of these types were	[[67, 69]]	[[52, 65]]	['GO']	['Gene Ontology']
1805	Noun I Nominalized verb(NIO  Determinative modifier ::= Adjective I Differentiable Adjective(DA) I Verb I Noun I  Location I String l Numeral + Classifier 	[[93, 95], [24, 27]]	[[68, 91]]	['DA', 'NIO']	['Differentiable Adjectiv']
1806	and get close to the top level in several other tracks.  Recently, Maximum Entropy model(ME) and CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai	[[89, 91], [97, 101]]	[[67, 82]]	['ME', 'CRFs']	['Maximum Entropy']
1807	new opportunity: part of Attempto Controlled English (ACE) was mapped to OWL (Kaljurand and Fuchs, 2007), and Processable English (PENG) evolved to Sydney OWL Syntax (SOS) (Cregan et	[[131, 135]]	[[110, 129]]	['PENG']	['Processable English']
1808	 verb. The third is end position (EP), after a predi-  cate.	[[34, 36]]	[[20, 32]]	['EP']	['end position']
1809	n - c-dow British English American English Table 1: Example of tags assigned with coarse-grained Universal Tagset (UT) and fine-grained lexical type tagset (LTT).	[[115, 117], [157, 160]]	[[97, 113], [136, 155]]	['UT', 'LTT']	['Universal Tagset', 'lexical type tagset']
1810	user?s state in the given session. In this research, the support vector machine (SVM) is used as a classifier.	[[81, 84]]	[[57, 79]]	['SVM']	['support vector machine']
1811	on a questionnaire provided to them. And a Mean Opinion Score(MoS) of 62.27% was achieved.	[[62, 65]]	[[43, 61]]	['MoS']	['Mean Opinion Score']
1812	cf. Webber 1987b), representing the narrative's unfold-  ing contents, and the l inear text structure (LTS), whose  components are linked by rhetorical relations such as 	[[103, 106]]	[[79, 101]]	['LTS']	['l inear text structure']
1813	PROP  VP  I PROP = proposition  These-fragments would match a locative object use of a p r epos~ .~ lu r~  arlu L H ~ .	[[12, 16]]	[[19, 30]]	['PROP']	['proposition']
1814	The feature av is derived from unsupervised segmentation as in (Zhao and Kit, 2008a), and the accessor variety (AV) (Feng et al, 2004) is adopted as the unsupervised segmentation crite-	[[112, 114]]	[[94, 110]]	['AV']	['accessor variety']
1815	lowing three metrics are used in this experiment.  (a) EPN in total (EPN-T): The number of the expanded problems which are generated in the	[[69, 74]]	[[55, 67]]	['EPN-T']	['EPN in total']
1816	ENT E1 E2 (b) Feature Paired Tree(FPT) ENT	[[34, 37]]	[[14, 32]]	['FPT']	['Feature Paired Tre']
1817	most u n e x p a n d e d  node o f  TI: for  3 b t h i s  r e s u l t s  i n :   3 e .  (S ( V  m a i l )  (NP ( N P  ( N  B o x e s )  (N*)) P P * )  ( P P * ) ) .  	[[108, 110], [142, 145]]	[[113, 116]]	['NP', 'P P']	['N P']
1818	The methods for scoring the Template Element, Template Relation, Scenario Template, and Named Entity tasks are  very similar. From the standpoint of calculating scores, The template element (TE) task is the basic task of these four. 	[[191, 193]]	[[173, 189]]	['TE']	['template element']
1819	 Experiments are performed on two datasets, the English Penn Treebank (PTB) dataset using the standard train, dev and test splits, and the ARK	[[71, 74]]	[[56, 69]]	['PTB']	['Penn Treebank']
1820	and (?) denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking.	[[87, 89], [100, 102], [121, 123]]	[[90, 98], [103, 119], [124, 139]]	['FL', 'FS', 'DS']	['Flagging', 'Feature stacking', 'Domain stacking']
1821	tempered EM (TEM) [8] algorithm, instead of a naive one, to avoid this problem. TEM algorithm is closely related to the deterministic annealing EM (DAEM) algorithm [17], and helps avoid local extrema by introducing inverse temperature ?.	[[148, 152], [13, 16], [80, 83]]	[[120, 146], [0, 11]]	['DAEM', 'TEM', 'TEM']	['deterministic annealing EM', 'tempered EM']
1822	CRF) A wide range of contextual information, such as surrounding words (GREF), dependency or case structure (GTREF), and dependency path (GREF ), has been utilized for similarity calculation, and achieved considerable success.	[[138, 142], [0, 3], [72, 76]]	[[109, 114]]	['GREF', 'CRF', 'GREF']	['GTREF']
1823	ceedin.qs, IEEE-1ECEJ-ASJ htternational Con-  ference on Acoustics, Speech, and Signal Process-  ing (ICASSP), 2bkyo, April 1986. 	[[102, 108], [11, 25]]	[[26, 100]]	['ICASSP', 'IEEE-1ECEJ-ASJ']	['htternational Con-  ference on Acoustics, Speech, and Signal Process-  ing']
1824	al., ( 2013) but replace Markov Logic Networks with Probabilistic Soft Logic (PSL) (Kimmig et al.,	[[78, 81]]	[[52, 76]]	['PSL']	['Probabilistic Soft Logic']
1825	We apply LDA on  the user-word matrix UW:  UW = UM * MW  , where UM is the user-hidden matrix, MW is the 	[[48, 50], [9, 12], [38, 40], [43, 45], [53, 55], [65, 67], [95, 97]]	[[51, 52], [21, 37]]	['UM', 'LDA', 'UW', 'UW', 'MW', 'UM', 'MW']	['*', 'user-word matrix']
1826	The development of efficient estimation procedures for context-dependent acoustic models revolutionised the field of Automatic Speech Recognition (ASR) (Young et al.,	[[147, 150]]	[[117, 145]]	['ASR']	['Automatic Speech Recognition']
1827	 Figure 1: Graphical representation of the phrase pair topic (PPT) model. 	[[62, 65]]	[[43, 60]]	['PPT']	['phrase pair topic']
1828	(O?Shaughnessy, 2000), lip aperture (LA) is the normalized Euclidean distance between the lips, and lip protrusion (LP) is the normalized 2nd principal component of the midpoint between the lips.	[[116, 118], [36, 39]]	[[100, 114], [23, 35]]	['LP', '(LA']	['lip protrusion', 'lip aperture']
1829	For example, PropBank annotates 8,037 ARGM-MNR relations (10.7%) out of 74,980 adjunct-like arguments (ARGMs). There are verbs	[[103, 108], [38, 46]]	[[92, 101]]	['ARGMs', 'ARGM-MNR']	['arguments']
1830	v2i = v3i ? a circular convolution model (CON) v1 ? v2 = v3	[[42, 45]]	[[23, 34]]	['CON']	['convolution']
1831	CoTrain vs. BaseCN2 1.8E-07 0.00257 0.000182 CoTrain vs. BaseCN3 1.27E-06 0.00922 0.000765 CoTrain vs. LEX(CN) 6.09E-29 3.72E-21 1.61E-24 CoTrain vs. LEX(EN) 0.0018 0.0276 0.00329	[[107, 109], [103, 106], [150, 153], [154, 156]]	[[91, 98]]	['CN', 'LEX', 'LEX', 'EN']	['CoTrain']
1832	 Definition 2.5  Given a grammar, G, define MCL(G) (Maximum Change in Length) as:  MCL(G) = max { m \] A (.. q/1. . .	[[44, 47], [83, 86], [92, 95]]	[[52, 76], [25, 32]]	['MCL', 'MCL', 'max']	['Maximum Change in Length', 'grammar']
1833	steels@arti.vub.ac.be Abstract Fluid Construction Grammar (FCG) is a new linguistic formalism designed to ex-	[[59, 62]]	[[31, 57]]	['FCG']	['Fluid Construction Grammar']
1834	matical device for handling coordination in computa-  tional linguistics has been the SYSCONJ facility for  augmented transition networks (ATNs) (Woods 1973;  Bates 1978).	[[139, 143], [86, 93]]	[[108, 137]]	['ATNs', 'SYSCONJ']	['augmented transition networks']
1835	Extended Markup Language (XML) is a pro-  posed standard (XML, 1997) specified by the World  Wide Web Consortium (W3C). In XML, tags and 	[[114, 117]]	[[93, 112]]	['W3C']	['Wide Web Consortium']
1836	de Marneffe, Manning, and Potts The Pragmatic Complexity of Veridicality Assessment under an attitude predicate (say), the events in Examples (6a) and (6b) are assessed as certain (CT+), whereas the words highly confident in Example (6c) trigger PR+, and may in Example (6d) leads to PS+.	[[181, 184], [246, 249], [284, 287]]	[[172, 179]]	['CT+', 'PR+', 'PS+']	['certain']
1837	Adobe website:2.03 Adobe Systems:1.82 Data mining (DM), also known as Knowledge-Discovery in Databases (KDD) or Knowledge-Discovery and Data Mining (KDD), is the process of automatically searching large volumes of data for patterns.	[[51, 53], [104, 107], [149, 152]]	[[38, 49], [70, 101], [112, 140]]	['DM', 'KDD', 'KDD']	['Data mining', 'Knowledge-Discovery in Database', 'Knowledge-Discovery and Data']
1838	Table 9 Number of times a core grammatical function was annotated more than once in the treebank (TRBK) by the model using gold morphology (GOLD-M), and by the model using predicted morphology (PRED-M).	[[140, 146], [98, 102], [194, 200]]	[[123, 138], [88, 96], [172, 192]]	['GOLD-M', 'TRBK', 'PRED-M']	['gold morphology', 'treebank', 'predicted morphology']
1839	ciently or accurately than alternative approaches.  Constraint Programming (CP) is a field of research that develops algorithms and tools for	[[76, 78]]	[[52, 74]]	['CP']	['Constraint Programming']
1840	121 domain adaptation algorithm mentioned in (Daume, 2007) based on Maximum Entropy model (MaxEnt) (Ratnaparkhi, 1996).	[[91, 97]]	[[68, 83]]	['MaxEnt']	['Maximum Entropy']
1841	Disco-En-Gold consists of 349 expressions divided into training (TrainD), validation (ValD), and test data (TestD) manually assigned scores from 0 to 100, indicating the level of compositionality (the	[[108, 113], [86, 90], [65, 71]]	[[97, 106], [74, 84], [42, 63]]	['TestD', 'ValD', 'TrainD']	['test data', 'validation', 'divided into training']
1842	Domains: HT = human transcription factors in blood cells, TCS = two-component systems, BB = bacteria biology, BS = Bacillus subtilis	[[87, 89], [9, 11], [58, 61], [110, 112]]	[[92, 108], [14, 33], [64, 85], [115, 132]]	['BB', 'HT', 'TCS', 'BS']	['bacteria biology', 'human transcription', 'two-component systems', 'Bacillus subtilis']
1843	shown in Figure 2. It is observed that the numbers of instances of Conjunct Verb (ConjV),  Passives (Pass), Auxiliary Construction (AC) 	[[82, 87], [101, 105], [132, 134]]	[[67, 80], [108, 130], [91, 99]]	['ConjV', 'Pass', 'AC']	['Conjunct Verb', 'Auxiliary Construction', 'Passives']
1844	 4.1.7 Doctors? Prescriptions (PRESC) Some of our food-health relations are also men-	[[31, 36]]	[[16, 29]]	['PRESC']	['Prescriptions']
1845	did. We used files 1-270, 400-554, and 600-931 as source domain training data (STrain), files 271300 as source domain testing data (STest) and files	[[79, 85], [132, 137]]	[[50, 72], [104, 130]]	['STrain', 'STest']	['source domain training', 'source domain testing data']
1846	mantic representation is not so clear cut. Generalising only verbs to semantic files (SFv) was the best option in most of the experiments, particularly	[[86, 89]]	[[70, 84]]	['SFv']	['semantic files']
1847	TGTM PR=pr ,  pkr ,  b r   TGTM PL =p l ,  ph l ,  b l   TGTM PW=pw, pkw, bw  Figure 26 	[[62, 64], [0, 4], [5, 7], [27, 31], [32, 34], [57, 61]]	[[65, 67], [8, 10], [36, 39]]	['PW', 'TGTM', 'PR', 'TGTM', 'PL', 'TGTM']	['pw', 'pr', 'p l']
1848	 2011b. Overview of the entity relations (REL) supporting task of BioNLP Shared Task 2011.	[[42, 45]]	[[31, 40]]	['REL']	['relations']
1849	 2.3 Tree Insertion Grammar Tree Insertion Grammars (TIGs) are a longstanding compromise between the intuitive expressivity	[[53, 57]]	[[28, 51]]	['TIGs']	['Tree Insertion Grammars']
1850	translation quality include the ridge regression (RR) and support vector regression (SVR) with RBF (radial basis functions) kernel (Smola and Scho?lkopf, 2004).	[[95, 98], [50, 52], [85, 88]]	[[100, 122], [32, 48], [58, 83]]	['RBF', 'RR', 'SVR']	['radial basis functions', 'ridge regression', 'support vector regression']
1851	77 (a) README.txt file (d) RPM Spec PACKAGE section (metadata) Bean Scripting Framework (BSF) is a set of Java classes which provides an easy to use scripting language support	[[89, 92], [27, 30]]	[[63, 87]]	['BSF', 'RPM']	['Bean Scripting Framework']
1852	a generalization of the Semitic root-and-template modeling. We use Egyptian Arabic (EGY), and German (GER) as our test languages.	[[84, 87], [102, 105]]	[[67, 75], [94, 100]]	['EGY', 'GER']	['Egyptian', 'German']
1853	2013).  Currently the most active framenet research teams are working on Swedish FrameNet (SweFN) (Borin et al.,	[[91, 96]]	[[73, 89]]	['SweFN']	['Swedish FrameNet']
1854	emoeion(ENO) perception(PER)  possession(POSS) stat ive(STA)  ~eather(WEA) ingestion(ING)  use(USE) social(SOC) body(BOD) 	[[85, 88], [8, 11], [24, 27], [41, 45], [56, 59], [70, 73], [85, 88], [95, 98], [107, 110], [117, 120]]	[[75, 84], [0, 7], [13, 23], [30, 40], [47, 56], [63, 69], [91, 94], [100, 106], [112, 116]]	['ING', 'ENO', 'PER', 'POSS', 'STA', 'WEA', 'ING', 'USE', 'SOC', 'BOD']	['ingestion', 'emoeion', 'perception', 'possession', 'stat ive(', 'eather', 'use', 'social', 'body']
1855	notation is illustrated in Figure 3.  3.3 Positional Unknown Model (PosUnk) The main weakness of the PosAll model is that	[[68, 74]]	[[42, 60]]	['PosUnk']	['Positional Unknown']
1856	1982, 1984; Clark 1992; Cremers 1996; Arts 2004). The present article will examine its consequences for the generation of referring expressions (GRE). In doing this, we dis-	[[145, 148]]	[[108, 143]]	['GRE']	['generation of referring expressions']
1857	If we look at the permutations, we have in 2. to-  picalization, with OBJect NP in focus structure (FS) I in I. the grammatical relations of 25 are preserved 	[[100, 102], [77, 79]]	[[83, 98]]	['FS', 'NP']	['focus structure']
1858	 Data set We evaluate segmentation performance on the Penn Arabic Treebank (ATB).5 It consists of about 4,500 sentences of modern Arabic obtained	[[76, 79]]	[[59, 74]]	['ATB']	['Arabic Treebank']
1859	was supported in part by JSPS Research Fellowships for Young Scientists and in part by CREST, JST (Japan Science and Technology Agency). 	[[94, 97], [25, 29], [87, 92]]	[[99, 127]]	['JST', 'JSPS', 'CREST']	['Japan Science and Technology']
1860	which predicts the aligned source positions for every target word, and (c) the Positional Unknown (PosUnk) ?	[[99, 105]]	[[79, 97]]	['PosUnk']	['Positional Unknown']
1861	semantic F1 of 85.63 for English.  Time Expression Identification (TEI) and Normalization (TEN): We use the time module	[[67, 70]]	[[35, 65]]	['TEI']	['Time Expression Identification']
1862	mostly context-free, with some context-sensit ive and  some transformational  rules, written in a modif ied  Backus Normal Form (BNF). Each rule contains the 	[[129, 132]]	[[109, 127]]	['BNF']	['Backus Normal Form']
1863	The  motivation for this work is presented in section 4. Unsupervised Morphology Learner (UML)  framework is presented in section 5.	[[90, 93]]	[[57, 88]]	['UML']	['Unsupervised Morphology Learner']
1864	called D2S. D2S has been used as the foundation of a number of language-generating systems, including GOALGETTER, a system that generates soccer reports in Dutch.1 D2S consists of two modules: (1) a language generation module (LGM) and (2) a speech generation module (SGM) which turns the generated text into a speech signal.	[[227, 230], [12, 15], [7, 10], [164, 167], [268, 271]]	[[199, 225], [242, 266]]	['LGM', 'D2S', 'D2S', 'D2S', 'SGM']	['language generation module', 'speech generation module']
1865	assignment for each annotator. We then performed an analysis of variance (ANOVA) on the outcomes of our experiment.	[[74, 79]]	[[52, 72]]	['ANOVA']	['analysis of variance']
1866	 We split annotated data into two parts: the BLOB (Binary Large OBject) and the XML annotations that refer to specific regions of the BLOB.	[[45, 49], [80, 83], [134, 138]]	[[51, 70]]	['BLOB', 'XML', 'BLOB']	['Binary Large OBject']
1867	5.1 Overall Results Table 4 shows the parsing results for the StateSplit (SP) PCFG, the Head-Driven (HD) PCFG and the Relational-Realizational (RR) PCFG	[[74, 76], [78, 82], [101, 103], [105, 109], [144, 146], [148, 152]]	[[67, 72], [88, 99], [118, 142]]	['SP', 'PCFG', 'HD', 'PCFG', 'RR', 'PCFG']	['Split', 'Head-Driven', 'Relational-Realizational']
1868	amples in 3.2).  In section 4, we describe the  specification of Korean TimeML (KTimeML). 	[[80, 87]]	[[65, 78]]	['KTimeML']	['Korean TimeML']
1869	BlogSum-generated summary content using ROUGE and compared the results with the original candidate list (OList). The t-test re-	[[105, 110], [40, 45]]	[[80, 103]]	['OList', 'ROUGE']	['original candidate list']
1870	(domain specific) region. The upper region of the on-  tology is called the Ontology Base (OB) and contains  approximately 400 items that represent generalizations 	[[91, 93]]	[[76, 89]]	['OB']	['Ontology Base']
1871	this results in minimum expected word error rate (WER) hypothesis (Mangu et al, 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim	[[122, 125]]	[[102, 120]]	['MBR']	['minimum Bayes risk']
1872	We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely FLORIAN (Florian et al., 	[[99, 106], [68, 73]]	[[108, 115]]	['FLORIAN', 'CoNLL']	['Florian']
1873	3. Maximization of 1)arameters, A of at:tire fea-  tures 1)y I IS(hnproved Iterative Sealing) algo-  rithm.	[[63, 65]]	[[75, 92]]	['IS']	['Iterative Sealing']
1874	In  Proceedings of the 16th International Conference  on World Wide Web (WWW), pages 697-706. 	[[73, 76]]	[[57, 71]]	['WWW']	['World Wide Web']
1875	 1  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random Sig+Reply	[[58, 60], [81, 84], [105, 108]]	[[37, 56], [61, 67]]	['ID', 'FIR', 'SVE']	['information density', 'Fisher']
1876	pointing and lacks systematic evaluation.  This paper employs a label propagation (LP)  algorithm for global learning of NP anaphoricity.	[[83, 85], [121, 123]]	[[64, 81]]	['LP', 'NP']	['label propagation']
1877	1998) as the reference. In Chinese FrameNet, the predicates, called lexical units (LU), evoke frames which roughly correspond to different	[[83, 85]]	[[68, 81]]	['LU']	['lexical units']
1878	see chapter 4.3.  WIV(1): Weighted Identity Value (with the weight 1):  see chapter 2.2.	[[18, 21]]	[[26, 49]]	['WIV']	['Weighted Identity Value']
1879	pick PRON up?, where PRON is the part of speech (POS) tag for pronouns.	[[49, 52], [21, 25], [5, 9]]	[[33, 47]]	['POS', 'PRON', 'PRON']	['part of speech']
1880	tor machines: learning with many relevant features. In European Conference on Machine Learning (ECML). 	[[96, 100]]	[[55, 94]]	['ECML']	['European Conference on Machine Learning']
1881	This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs). To	[[86, 90]]	[[61, 84]]	['SVMs']	['Support Vector Machines']
1882	for candidate summary sentence selection  by standard page rank algorithms used in  Information Retrieval (IR). As Bengali is 	[[107, 109]]	[[84, 105]]	['IR']	['Information Retrieval']
1883	ous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine transla-	[[83, 86], [89, 92]]	[[54, 81]]	['NLP', 'WSD']	['natural language processing']
1884	or database .  Superficially, DEFT resembles a Natural language Understanding (NLUI) system ; however, there are key differences .	[[79, 83]]	[[47, 77]]	['NLUI']	['Natural language Understanding']
1885	J08b 97.74 93.37 N07 97.83 93.32 SF = segmentation F-score; JF = joint segmentation and POS-tagging F-score. 	[[33, 35], [60, 62], [88, 91]]	[[38, 52], [65, 70]]	['SF', 'JF', 'POS']	['segmentation F', 'joint']
1886	each new domain and scenario, as discussed in the next section.  The lexical analysis module (LexAn) is responsible for splitting the document into sentences, and the sentences into tokens.	[[94, 99]]	[[69, 85]]	['LexAn']	['lexical analysis']
1887	ing different training methods. The effects of discriminative training (CRF) and extended feature sets (lower section) are more than additive.	[[72, 75]]	[[50, 70]]	['CRF']	['criminative training']
1888	WordNet Domains (Magnini and Cavagli a`, 2000).  Conceptual Density (CD) is a measure of the correlation among the sense of a given word and its	[[69, 71]]	[[49, 67]]	['CD']	['Conceptual Density']
1889	Chinese Semantic Dictionary (CSD) for  Chinese-English machine translation, the  Chinese Concept Dictionary (CCD) for  cross-language text processing, the multi-level 	[[109, 112], [29, 32]]	[[81, 107], [0, 27]]	['CCD', 'CSD']	['Chinese Concept Dictionary', 'Chinese Semantic Dictionary']
1890	tagging, lemmatization, etc.). For corpus query, we employ the Corpus Query Processor (CQP) (CWB; Evert, 2004) which works on the basis of	[[87, 90]]	[[63, 85]]	['CQP']	['Corpus Query Processor']
1891	partially completed subproof or function of the system. The implementation f this  was the IPSIM (Interruptible Prolog SIMulator) theorem prover, which can maintain  a set of partially completed proofs and jump to the appropriate one as dialog pro- 	[[91, 96]]	[[98, 128]]	['IPSIM']	['Interruptible Prolog SIMulator']
1892	 Introduction  The DARPA ATIS Spoken Language System (SLS) task  represents ignificant new challenges for speech and natural 	[[54, 57], [19, 24], [25, 29]]	[[30, 52]]	['SLS', 'DARPA', 'ATIS']	['Spoken Language System']
1893	applied this formula to a vocabulary of single terms.  Subiect Field Code (SFC). This system applies a 	[[75, 78]]	[[55, 73]]	['SFC']	['Subiect Field Code']
1894	of Electrical and Computer Engineering Pohang University of Science and Technology (POSTECH) Advanced Information Technology Research Center (AITrc) San 31, Hyoja-Dong, Pohang, Republic of Korea, 790-784	[[142, 147], [84, 91]]	[[93, 140], [39, 82]]	['AITrc', 'POSTECH']	['Advanced Information Technology Research Center', 'Pohang University of Science and Technology']
1895	 Another syntactic phenomena crucial to the parser is known as the complex NP  Constraint (CNPC) (Radford 1981); i.e., no transformation rule can move any element  out of a complex NP, where a complex NP (CNP) is an NP containing a relative clause.	[[91, 95], [181, 183], [201, 203], [205, 208], [216, 218], [75, 77]]	[[79, 89]]	['CNPC', 'NP', 'NP', 'CNP', 'NP', 'NP']	['Constraint']
1896	otherwise as uniform as possible (Berger et al, 1996). maximum entropy model (MaxEnt) is known to easily combine diverse features and	[[78, 84]]	[[55, 70]]	['MaxEnt']	['maximum entropy']
1897	al., 2007) use the document-level topics extracted with Latent Dirichlet Allocation (LDA) as indicators of meanings for word sense disambigua-	[[85, 88]]	[[56, 83]]	['LDA']	['Latent Dirichlet Allocation']
1898	n?5WZWZ7V?Zo+Y#?ZWA<E  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216?1224, October 25-29, 2014, Doha, Qatar.	[[113, 118]]	[[63, 111]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
1899	In this paper, we propose  methods to detect and correct WOEs in Chinese sentences. Conditional random fields (CRFs)  based WOEs detection models identify the sentence segments containing WOEs.	[[111, 115], [57, 61], [124, 128], [188, 192]]	[[84, 109]]	['CRFs', 'WOEs', 'WOEs', 'WOEs']	['Conditional random fields']
1900	 ? The Match Rate(MR): The match rate is the match number normalized	[[18, 20]]	[[7, 16]]	['MR']	['Match Rat']
1901	Abstract In this paper we present results from the METER (MEasuring TExt Reuse) project whose aim is to explore issues	[[51, 56]]	[[58, 78]]	['METER']	['MEasuring TExt Reuse']
1902	"less than linear is the sample size, m. We formalize  this as a variant of ""Set Cover"" problem which we call  ""Weighted Set Cover~(WSC), and prove the existence of  an approximation algorithm with a performance guar- "	[[131, 134]]	[[111, 129]]	['WSC']	['Weighted Set Cover']
1903	ping observed sequences to possible ground truth sequences.  We do not use the Character Error Rate (CER) metric, since for almost all NLP applications the unit of	[[101, 104], [135, 138]]	[[79, 99]]	['CER', 'NLP']	['Character Error Rate']
1904	3.3  Parameter  es t imat ion   In supervised lcarning~ the simpliest parameter  estimation is the maximum likelihood(ML) cs-  t imation(Duda et al, 1973) which lnaximizes 	[[118, 120]]	[[99, 116]]	['ML']	['maximum likelihoo']
1905	 1 Introduction Word Sense Disambiguation(WSD) is the process of assigning a meaning to a word based on the context	[[42, 45]]	[[16, 41]]	['WSD']	['Word Sense Disambiguation']
1906	NEDcost = EDcost/length (4) ? The Match Number(MN): The match number is the number of words	[[47, 49], [0, 7], [10, 16]]	[[34, 45]]	['MN', 'NEDcost', 'EDcost']	['Match Numbe']
1907	son to visit Udaipur.  Parse: September to March is [NP (np the  best season) [SBAR [S (dcP to visit Udaipur)]]] .	[[53, 55], [79, 83]]	[[57, 59]]	['NP', 'SBAR']	['np']
1908	stood statistical models?statistical dependency parsers, probabilistic context-free grammars (PCFGs), and word translation models (TMs)?can be effectively combined into a unified framework that jointly searches for the best	[[131, 134], [94, 99]]	[[111, 129], [57, 92]]	['TMs', 'PCFGs']	['translation models', 'probabilistic context-free grammars']
1909	A workaround is to restrict the possible tag candidates per position by using either morphological analyzers (MAs), dictionaries or heuristics (Hajic?,	[[110, 113]]	[[85, 108]]	['MAs']	['morphological analyzers']
1910	In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together (ACL), pages 50?57, Stroudsburg, PA.	[[100, 103], [132, 134]]	[]	['ACL', 'PA']	[]
1911	is that of the closest centroid.  The Naive Bayes (NB) classifier is based on a probabilistic model which assumes conditional in-	[[51, 53]]	[[38, 49]]	['NB']	['Naive Bayes']
1912	Concepts across categories Hilke Reckman and Crit Cremers Leiden University Centre for Linguistics (LUCL) Leiden, Netherlands	[[100, 104]]	[[58, 98]]	['LUCL']	['Leiden University Centre for Linguistics']
1913	translators with the help of computer-aided translation tools (CAT), (3) rule-based MT systems (RBMT) and (4) statistical MT systems (SMT). 	[[134, 137], [63, 66], [96, 100]]	[[110, 132], [29, 55], [73, 86]]	['SMT', 'CAT', 'RBMT']	['statistical MT systems', 'computer-aided translation', 'rule-based MT']
1914	and documents created by three or four New York Times columnists (TF = Thomas Friedman, PK = Paul Krugman, MD = Maureeen Dowd, GC = Gail Collins).	[[88, 90], [66, 68], [107, 109], [127, 129]]	[[93, 105], [71, 86], [112, 125], [132, 144]]	['PK', 'TF', 'MD', 'GC']	['Paul Krugman', 'Thomas Friedman', 'Maureeen Dowd', 'Gail Collins']
1915	A closer, more detailed, look at the  LOCATION data suggests that he high payoff indicated  by the average REC and precision (PRE) scores was  achieved because most of the data were listable.	[[126, 129], [107, 110]]	[[115, 124]]	['PRE', 'REC']	['precision']
1916	various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by Feng et al. (	[[126, 128], [69, 72]]	[[108, 124], [44, 67]]	['AV', 'DLG']	['Accessor Variety', 'Description Length Gain']
1917	In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-07), pages 2670?2676. 	[[86, 94]]	[[27, 84]]	['IJCAI-07']	['International Joint Conference on Artificial Intelligence']
1918	discourse analysis phase, the situation  frame is interpreted, resulting in one or  more instantiated knowledge base (KB)  objects, which are state or event 	[[118, 120]]	[[102, 116]]	['KB']	['knowledge base']
1919	method of ADN. ADN is constructed by  Restricted Boltzmann Machines (RBM)  with unsupervised learning using labeled 	[[69, 72], [10, 13], [15, 18]]	[[38, 67]]	['RBM', 'ADN', 'ADN']	['Restricted Boltzmann Machines']
1920	stituent category labels expressing adverbials (RB), coordinations (CC), various types of interjections (UH, INTJ) and adverbial phrases (ADVP). We may	[[138, 142], [48, 50], [68, 70], [105, 107], [109, 113]]	[[119, 136], [53, 66], [25, 46], [90, 103]]	['ADVP', 'RB', 'CC', 'UH', 'INTJ']	['adverbial phrases', 'coordinations', 'expressing adverbials', 'interjections']
1921	They are not constrained  by features of any previous utterance in the  discourse segment (DS), and the elements of Cf(Un)  are partially ordered to reflect relative prominence 	[[91, 93], [119, 121], [116, 118]]	[[72, 89]]	['DS', 'Un', 'Cf']	['discourse segment']
1922	set we trained on both glosses and statistical MT data, for the OnWN and FNWN test sets we trained on glosses only (OnWN), and for the SMT test set we trained on statistical MT data only (MTnews and	[[116, 120], [47, 49], [64, 68], [73, 77], [135, 138], [174, 176], [188, 194]]	[[99, 114]]	['OnWN', 'MT', 'OnWN', 'FNWN', 'SMT', 'MT', 'MTnews']	['on glosses only']
1923	For 1)reprocessing the dictionary definitions, we  have experimented with two ditDrent Caggers: the Xe-  rox PAR(J part-of-speech tagger \[8\], and the Chop-  per \[9\], an optimizing finit, e state luachine-hased tag- 	[[109, 112], [78, 86]]	[[115, 119]]	['PAR', 'ditDrent']	['part']
1924	"number of correcVi'abeled-constituents in proposed parse  number of correct matched constituent inproposed parse  6) Sentence parsing ratio(SPg) =  number"" of sentences having a proposed parse by parser "	[[140, 143]]	[[117, 133]]	['SPg']	['Sentence parsing']
1925	 2. Effort of Association (EA): a mc~sure of the effort  required to associate some entity with lira description 	[[27, 29]]	[[4, 25]]	['EA']	['Effort of Association']
1926	Argument Filtering Argument  Boundary Detection (ABD) module ?)???????	[[49, 52]]	[[29, 47]]	['ABD']	['Boundary Detection']
1927	Reverse Gap 0.072 0.033 Table 1: Percentage of reordering patterns ` reverse gap (RG): The two source phrases are not adjacent, and are in the reverse order as	[[82, 84]]	[[69, 80]]	['RG']	['reverse gap']
1928	Text REtrieval Conference (TREC)1. The TREC 1The Text REtrieval Conference (TREC) is a series of evaluations of fully automatic Q/A systems	[[76, 80], [27, 31], [39, 43], [128, 131]]	[[49, 74]]	['TREC', 'TREC', 'TREC', 'Q/A']	['Text REtrieval Conference']
1929	ror rate (WWER), which gives a weight on errors from the viewpoint of IR, instead of word error rate (WER), which treats all words uniformly.	[[102, 105], [10, 14], [70, 72]]	[[85, 100]]	['WER', 'WWER', 'IR']	['word error rate']
1930	(Bikel et al, 1997), Decision Trees (Sekine, 1998), Maximum Entropy Models (Borthwick and Sterling, 1998), Support Vector Machines (SVM) (Asahara and Matsumoto, 2003), and also semi-supervised	[[132, 135]]	[[107, 130]]	['SVM']	['Support Vector Machines']
1931	Since by default we return up to RT=100  search engine results to user, we will extract the  top RQ=RT/(#newQuery+1) entries from results of  each new query and original query.	[[97, 99], [33, 35]]	[[100, 115]]	['RQ', 'RT']	['RT/(#newQuery+1']
1932	The IE results are stored in a database which  is the basis for IE-related applications like QA,  BR (Browsing, threading and visualization) and  AS (Automatic Summarization).	[[98, 100], [4, 6], [64, 66], [93, 95], [146, 148]]	[[102, 110], [150, 173]]	['BR', 'IE', 'IE', 'QA', 'AS']	['Browsing', 'Automatic Summarization']
1933	TDP (target only) 62.60 33.04 Table 2: Results Generalized average precision (GAP) is a more precise measure than P	[[78, 81], [0, 3]]	[[47, 76]]	['GAP', 'TDP']	['Generalized average precision']
1934	1 Introduction Linguistics studies have shown that action verbs often denote some change of state (CoS) as the result of an action, where the change of state of-	[[99, 102]]	[[82, 97]]	['CoS']	['change of state']
1935	2007). The practical NLP application based evaluations are automatic speech recognition (ASR), information retrieval (IR) and statistical machine	[[89, 92], [21, 24], [118, 120]]	[[59, 87], [95, 116]]	['ASR', 'NLP', 'IR']	['automatic speech recognition', 'information retrieval']
1936	contribution: Functional GENDER and NUMBER features contribute more than their form-based counterparts, in both gold and predicted conditions; rationality (RAT) as a single feature on top of the POS tag set helps in gold (and with Easy-First Parser, also in predicted conditions)?but when used in combination with	[[156, 159], [195, 198]]	[[143, 154]]	['RAT', 'POS']	['rationality']
1937	this mode\]., the linguistic facts that pertain solely  to the source language (SL) are supposed to be  clearly separated from the facts that pertain solely 	[[80, 82]]	[[63, 78]]	['SL']	['source language']
1938	We set aside the blind TEST set for evaluating the final performance of our named entity recognition (NER) and relation extraction (RE) 2http://code.google.com/apis/ajaxsearch	[[132, 134], [102, 105]]	[[111, 130], [76, 100]]	['RE', 'NER']	['relation extraction', 'named entity recognition']
1939	systems that learn new representations for opendomain NLP using latent-variable language models like Hidden Markov Models (HMMs). In POS-	[[123, 127], [54, 57], [133, 136]]	[[101, 121]]	['HMMs', 'NLP', 'POS']	['Hidden Markov Models']
1940	      The system integrates both dependency parse  tree pattern and semantic role labeler (SRL) results  of each input sentence when extracting the triples.	[[91, 94]]	[[68, 89]]	['SRL']	['semantic role labeler']
1941	 6 Experiments and Results We use the Wall Street Journal (WSJ) section of the Penn Treebank as our labeled source domain	[[59, 62]]	[[38, 57]]	['WSJ']	['Wall Street Journal']
1942	 2.2 CoSeC CoSeC (Comparing Semantics in Context) performs meaning comparison on the basis of an underspec-	[[11, 16]]	[[18, 37]]	['CoSeC']	['Comparing Semantics']
1943	from Si.  Feature Causality Diagram (FCD): CNB allows each feature Y, which occurs in a  given document, to have a Feature Causality Diagram (FCD).	[[37, 40], [43, 46], [142, 145]]	[[10, 35], [115, 140]]	['FCD', 'CNB', 'FCD']	['Feature Causality Diagram', 'Feature Causality Diagram']
1944	 658     We investigate the effect of thyroid transcription factor 1 (TTF-1) ...x: a	[[70, 75]]	[[46, 68]]	['TTF-1']	['transcription factor 1']
1945	represented in an n ? n matrix of objects by a  multidimensional scaling (MDS) of the distance  between each object.	[[74, 77]]	[[48, 72]]	['MDS']	['multidimensional scaling']
1946	nutcracker 155 22 62 312 449 0.0467 0.8342 39.5% (60% w/o B.O.) srl 0 487 437 63 13 0.9740 0.1260 55.0% Table 1: Results of the three systems on the SSI-testsuite ( TN = true negatives, FN = false negatives, TP = true positives, FP = false positives, N = TN + FP, P = TP + FN, Prec = Precision, ERROR: no	[[165, 167], [186, 188], [208, 210], [229, 231], [255, 257], [260, 262], [268, 270], [273, 275], [277, 281]]	[[170, 184], [191, 206], [213, 227], [234, 249], [284, 293]]	['TN', 'FN', 'TP', 'FP', 'TN', 'FP', 'TP', 'FN', 'Prec']	['true negatives', 'false negatives', 'true positives', 'false positives', 'Precision']
1947	of the Annual Meeting of the ACL and the International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP). 	[[117, 120], [29, 32], [110, 115]]	[[120, 121]]	['ACL', 'ACL', 'AFNLP']	['-']
1948	knowledge that we can get from these examples the required information to parse a new input sentence .  In our  approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema  where each SSTC describes a sentence, a representation tree as well as the correspondence b tween substrhzgs in 	[[193, 197], [229, 233]]	[[154, 191]]	['SSTC', 'SSTC']	['Structured String Tree Correspondence']
1949	removed for expository reasons.  rewrites into an (optional) sentence adjunct (SA), a  subject, a verbphrase and subject's right adjunct 	[[79, 81]]	[[61, 77]]	['SA']	['sentence adjunct']
1950	derivation decoding (Best MAX), the best single system minimum Bayes risk decoding (Best MBR) and minimum Bayes risk system combination (MBR-SC) combining three systems.	[[137, 143], [26, 29], [89, 92]]	[[98, 135], [55, 73]]	['MBR-SC', 'MAX', 'MBR']	['minimum Bayes risk system combination', 'minimum Bayes risk']
1951	The straight case is the one mentioned above, treating all elements on the PARTS list equally (EQUAL). As a second op-	[[95, 100]]	[[86, 93]]	['EQUAL']	['equally']
1952	(:all it as word accuracy(W.A.). We use one  more measure, called character accuracy(C.A.)  that measures the character edit distance be- 	[[85, 89], [26, 29]]	[[66, 83], [12, 25]]	['C.A.', 'W.A']	['character accurac', 'word accuracy']
1953	tance Metric from Relative Comparisons. Advances in Neural Information Processing Systems (NIPS).. J. Weeds, D. Weir and D. McCarthy.	[[91, 95]]	[[52, 89]]	['NIPS']	['Neural Information Processing Systems']
1954	 1 Introduction  Spoken language translation (SLT) has become  more important due to globalization.	[[46, 49]]	[[17, 44]]	['SLT']	['Spoken language translation']
1955	\[ Class (Tag) Kernel Nouns  act (AC)  an~ (AN)  art~fact (AR) 	[[44, 46]]	[[39, 42]]	['AN']	['an~']
1956	 These algorithms are now getting keen atten-  tion from the natural anguage processing (NLP)  research community since the huge text corpus 	[[89, 92]]	[[61, 87]]	['NLP']	['natural anguage processing']
1957	s+trsl Alhnd qmrA<STnAEyA <lY Almryx ? India will send a satellite to Mars [in 2013]?. In every tree node, the terms above the line arepart of the CATiB annotations: the word, POS (VRB = verb, PRT = particle, PROP = proper noun, NOM = nominal)and relation (MOD = modifier, SBJ = subject, OBJ = object). The terms under the line are the Buckwalter POS tag, thelemma and the gloss, respectively.	[[181, 184], [193, 196], [209, 213], [229, 232], [257, 260], [273, 276], [288, 291], [176, 179], [147, 152], [347, 350]]	[[187, 191], [199, 207], [216, 222], [235, 246], [263, 271], [279, 286], [294, 300]]	['VRB', 'PRT', 'PROP', 'NOM', 'MOD', 'SBJ', 'OBJ', 'POS', 'CATiB', 'POS']	['verb', 'particle', 'proper', 'nominal)and', 'modifier', 'subject', 'object']
1958	Hidden topic markov models. In Artificial Intelligence and Statistics (AISTATS), San Juan, Puerto Rico. 	[[71, 78]]	[[31, 69]]	['AISTATS']	['Artificial Intelligence and Statistics']
1959	 Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful	[[72, 75], [104, 106]]	[[51, 70], [81, 96]]	['HMM', 'ME']	['Hidden Markov Model', 'Maximum Entropy']
1960	 Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly better result.	[[66, 69]]	[[34, 64]]	['PPM']	['prediction by partial matching']
1961	There are four basic phrases in Korean: noun phrase (NP), verb phrase (VP), adverb phrase (ADVP), and independent phrase (IP). Thus, chunking by rules is	[[122, 124], [53, 55], [71, 73], [91, 95]]	[[102, 120], [40, 51], [58, 69], [76, 89]]	['IP', 'NP', 'VP', 'ADVP']	['independent phrase', 'noun phrase', 'verb phrase', 'adverb phrase']
1962	Natural Language Generation (NLG). For example, STOP is a Natural Language Generation (NLG) system that generates tailored smoking cessation let-	[[87, 90], [29, 32]]	[[58, 85], [0, 27]]	['NLG', 'NLG']	['Natural Language Generation', 'Natural Language Generation']
1963	matic and paradigmatic associations on the results of the clustering step. We conduct two experiments on SemEval-2012 task 2 and Scholastic Assessment Test (SAT) analogy quizzes to measure relational similarity to evaluate our model.	[[157, 160]]	[[129, 155]]	['SAT']	['Scholastic Assessment Test']
1964	Expanding  on a suggestion of Nfichieis (1982), we classify verbs  as subject equi (SEqui), object equi (OEqul), sub-  ject raising (SRals ing)  or object raising (ORuls ing) 	[[84, 89], [105, 110], [164, 173], [133, 142]]	[[70, 82], [92, 103], [148, 162], [113, 131]]	['SEqui', 'OEqul', 'ORuls ing', 'SRals ing']	['subject equi', 'object equi', 'object raising', 'sub-  ject raising']
1965	AT(- +)  Stems to which the suffixes +ation and +ative may  attach are marked as (AT +), while those taking the  corresponding forms +ion and +ive are (AT -).	[[82, 86], [152, 154], [0, 2]]	[[60, 66]]	['AT +', 'AT', 'AT']	['attach']
1966	 3.4 MAP Inference Maximum a posteriori (MAP) inference seeks the solution to	[[41, 44], [5, 8]]	[[19, 39]]	['MAP', 'MAP']	['Maximum a posteriori']
1967	 BBLT Input Screen      We originally developed BBLT for ourselves as machine translation (MT) developers and evaluators, to rapidly see the meanings of Arabic strings	[[91, 93], [1, 5], [48, 52]]	[[70, 89]]	['MT', 'BBLT', 'BBLT']	['machine translation']
1968	(2) dobj? det:DT NN prep:IN 7DT/det=determiner, NN=noun, IN/prep=preposition, dobj=direct object	[[48, 50], [14, 16], [17, 19], [60, 64], [78, 82], [28, 35], [4, 8], [20, 24]]	[[51, 55], [65, 76], [83, 96], [36, 46]]	['NN', 'DT', 'NN', 'prep', 'dobj', '7DT/det', 'dobj', 'prep']	['noun', 'preposition', 'direct object', 'determiner']
1969	PropBank defines core roles ARG0 through ARG5, which receive different interpretations for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional), as shown in Figure 2(a).	[[154, 162], [28, 32], [41, 45], [139, 145], [178, 186]]	[[164, 172], [188, 199]]	['ARGM-TMP', 'ARG0', 'ARG5', 'ARGM-*', 'ARGM-DIR']	['temporal', 'directional']
1970	1 (TICCL) we gradually developed in prior projects is now TICCLops (TICCL online processing system). TICCLops is a fully	[[58, 66], [3, 8], [101, 109]]	[[68, 98]]	['TICCLops', 'TICCL', 'TICCLops']	['TICCL online processing system']
1971	3.1.1 Directed Acyclic Graph The general SPRITE model can be thought of as a dense directed acyclic graph (DAG), where every document or topic is connected to every compo-	[[107, 110]]	[[83, 105]]	['DAG']	['directed acyclic graph']
1972	ining both BLEU and NIST scores? relationship to their Unlabeled Accuracy Score(UAS). 	[[80, 83], [11, 15], [20, 24]]	[[55, 79]]	['UAS', 'BLEU', 'NIST']	['Unlabeled Accuracy Score']
1973	automatique, GDR I3 ATALA, Paris, November 1999.  Tang E.K., Natural languages Analysis in machine translation (MT) based on the STCG, PhD thesis, Sains Malaysia University, Penang, March 1994	[[112, 114], [13, 16], [20, 25], [129, 133], [135, 138], [55, 58]]	[[91, 110]]	['MT', 'GDR', 'ATALA', 'STCG', 'PhD', 'E.K']	['machine translation']
1974	~  = unrecognized input token.)  (ABC) = (A(BC)) = ((AB)C). Tim singletonbidi- 	[[53, 57]]	[[42, 46]]	['AB)C']	['A(BC']
1975	  Here the parameters are set using an algorithm  whose uniform resource name (URN),  xyz.edu/algo-1, is declared as an attribute of the 	[[79, 82]]	[[56, 77]]	['URN']	['uniform resource name']
1976	3.2 To resolve gapping under serial verb construction Serial verb construction (SVC) (Baker, 1989) is construction in which a sequence of verbs appears in	[[80, 83]]	[[54, 78]]	['SVC']	['Serial verb construction']
1977	We also wanted to determine if information about 6http://www.isi.edu/?ravichan/YASMET.html dialog acts (DA) helps the ranking task. If we	[[104, 106]]	[[91, 102]]	['DA']	['dialog acts']
1978	 2 Question Classification We define Question Classification(QC) here to be the task that, given a question, maps it to one of	[[61, 63]]	[[37, 59]]	['QC']	['Question Classificatio']
1979	  MTI. The original Medical Text Indexer (MTI)  system, shown in Figure 1, consists of an infra-	[[42, 45], [2, 5]]	[[20, 40]]	['MTI', 'MTI']	['Medical Text Indexer']
1980	nese kanji and words. The currently available  JWAD Version 1 (JWAD-V1) consists of  104,800 free word association responses col-	[[63, 70]]	[[47, 61]]	['JWAD-V1']	['JWAD Version 1']
1981	For attribute selection on the composed vector, we use two methods we found to perform best in Hartung and Frank (2010): Entropy Selection (ESel) and Most Prominent Component (MPC).	[[140, 144], [176, 179]]	[[121, 138], [150, 174]]	['ESel', 'MPC']	['Entropy Selection', 'Most Prominent Component']
1982	we used dialog features derived from manual annotations ? dialog acts (DA) and overt displays of power (ODP) ?	[[71, 73], [104, 107]]	[[58, 69], [79, 102]]	['DA', 'ODP']	['dialog acts', 'overt displays of power']
1983	not enter into speech recognition. Tillmann, Vogel, Ney, and Zubiaga (1997) proposes a dynamic programming (DP)?based search algorithm for statistical MT that monotonically translates the input sentence from left to right.	[[108, 110], [151, 153]]	[[87, 106]]	['DP', 'MT']	['dynamic programming']
1984	All the words were categorized  into three types: Lexicon words (LWs), Factoid  words (FTs), Named Entity (NEs). Accordingly, 	[[107, 110], [65, 68], [87, 90]]	[[93, 105], [50, 63], [71, 85]]	['NEs', 'LWs', 'FTs']	['Named Entity', 'Lexicon words', 'Factoid  words']
1985	task yet to be tackled by TM but identified as an important potential application for it (Lewin et al 2008): Cancer Risk Assessment (CRA). Over the	[[133, 136], [26, 28]]	[[109, 131]]	['CRA', 'TM']	['Cancer Risk Assessment']
1986	" After detecting a new indefinite description (as ETA(x) : unlversity(x)) ReP  creates a new ""referential object'"" (RefO). During the discours6 (after the "	[[116, 120], [50, 56], [74, 77]]	[[94, 112]]	['RefO', 'ETA(x)', 'ReP']	['referential object']
1987	only from the corresponding source language segment. We use the Moses statistical MT (SMT) toolkit (Koehn et al.,	[[86, 89]]	[[70, 84]]	['SMT']	['statistical MT']
1988	large and is often simplified.  Because we use belief propagation (BP) as baseline to compare to, and as a subroutine in our pro-	[[67, 69]]	[[47, 65]]	['BP']	['belief propagation']
1989	proach of (Liu et al, 2004), in which IDs (category seeds) and instances are represented by vectors in a usual IR-style Vector Space Model (VSM), and similarity is measured by the cosine function:	[[140, 143], [111, 113]]	[[120, 138]]	['VSM', 'IR']	['Vector Space Model']
1990	The IBM Model 4 search space cannot be efficiently enumerated; therefore it cannot be trained directly using Expectation Maximization (EM). In practice, a sequence	[[135, 137], [4, 7]]	[[109, 133]]	['EM', 'IBM']	['Expectation Maximization']
1991	2 Symmetrical Tversky?s Ratio Model In the field of mathematical psychology Tversky proposed the ratio model (TRM) (Tversky, 1977) motivated by the imbalance that humans have on	[[110, 113]]	[[93, 108]]	['TRM']	['the ratio model']
1992	1 Introduction As they are normally conceived, many tasks relevant to Computational Linguistics (CL), such as text categorization, clustering, and information retrieval, ignore the con-	[[97, 99]]	[[70, 95]]	['CL']	['Computational Linguistics']
1993	(Figure 2). Argviz is a web-based application, built using Google Web Toolkit (GWT),4 which allows users to visualize and manipulate SITS?s outputs en-	[[79, 82], [133, 138]]	[[59, 77]]	['GWT', 'SITS?']	['Google Web Toolkit']
1994	of the reparandum coincides with the termination of  the fluent portion of the utterance, which we term the  INTERRUPTION SITE (IS). The DISFLUENCY INTERVAL 	[[128, 130]]	[[109, 126]]	['IS']	['INTERRUPTION SITE']
1995	pseudo-terms?. We also discuss the use of Hidden Markov Models (HMMs) to capture contextual information.	[[64, 68]]	[[42, 62]]	['HMMs']	['Hidden Markov Models']
1996	Afterward we name the TD composed of words from gold training set and tagged test set and as Na??ve TD (NTD) for its unbalanced coverage in training and test set.	[[104, 107], [22, 24]]	[[93, 102]]	['NTD', 'TD']	['Na??ve TD']
1997	819 location (LO) of the incident (e.g. airport name), and the country (CO) where the incident occurred. 	[[72, 74], [14, 16]]	[[63, 70], [4, 12]]	['CO', 'LO']	['country', 'location']
1998	~  I n  recent  years  the  prob lem o f  man 'mach ine  communicat ion  by  means   o f  natura l  language (NL) i s  becoming  a pract i ca l  one .  And the  	[[110, 112]]	[[90, 108]]	['NL']	['natura l  language']
1999	phrase structure grammar (PSG) as thc tagging  formalisms(Lecch & Garside 1991), and some  adopt dependency grammar(DG) 1993, Komatsu,  Jin, & Yasuhara, 1993).	[[116, 118], [26, 29]]	[[97, 114], [0, 24]]	['DG', 'PSG']	['dependency gramma', 'phrase structure grammar']
2000	ian.fletcher, peter.maguire@cs.man.ac.uk Abstract Dialogue Acts (DAs) which explicitly ensure mutual understanding are frequent	[[65, 68]]	[[50, 63]]	['DAs']	['Dialogue Acts']
2001	Instead of using graph-based consensus  confidence as features in the log-linear model, we  perform structured label propagation (Struct-LP) to  re-rank the n-best list directly, and the similarity 	[[130, 139]]	[[100, 128]]	['Struct-LP']	['structured label propagation']
2002	1 In t roduct ion   Finding base noun phrases is a sensible first step  for many natural anguage processing (NLP) tasks:  Accurate identification of base noun phrases is ar- 	[[109, 112]]	[[81, 107]]	['NLP']	['natural anguage processing']
2003	The paper first provides a brief overview of Lexical Functional Grammar, and the Penn Arabic Treebank (ATB). The next section presents	[[103, 106]]	[[86, 101]]	['ATB']	['Arabic Treebank']
2004	0.467 (+126%)?  Total Document Reciprocal Rank (TDRR) PubMed 0.495 0.137 0.038 0.331	[[48, 52]]	[[16, 46]]	['TDRR']	['Total Document Reciprocal Rank']
2005	before the start of the current utterance.  Overlapping label (OL) an utterance on another channel with a particular DA tag overlaps the	[[63, 65], [117, 119]]	[[44, 61]]	['OL', 'DA']	['Overlapping label']
2006	The most common and obvious way to  deal with disjunctive constraints i to expand the grammat-  ical description to disjunctive normal form (DNF) during a  pre-processing step, thereby eliminating disjunction from the 	[[141, 144]]	[[116, 139]]	['DNF']	['disjunctive normal form']
2007	 ? System integration, through SGML (the Standard Generalized Markup Language), both at the leve l of meaning analysis and at the overall application level .	[[31, 35]]	[[41, 77]]	['SGML']	['Standard Generalized Markup Language']
2008	 To overcome this problem, Gliozzo et al (2005) introduced the domain model (DM) and show how to define a domain VSM in which texts and terms	[[77, 79], [113, 116]]	[[63, 75]]	['DM', 'VSM']	['domain model']
2009	 Most of lexical networks, as networks extracted from real world, are small worlds (SW) networks.	[[84, 86]]	[[70, 82]]	['SW']	['small worlds']
2010	We encode the target state in the  similar way. Like the Vector Space Model(VSM),  we use a label matrix to represent each class as in 	[[76, 79]]	[[57, 74]]	['VSM']	['Vector Space Mode']
2011	a wordbreak (WB). In other words, we model Chinese word segmentation as wordbreak (WB) identification which takes all CB?s as candidates and	[[83, 85], [13, 15], [118, 122]]	[[72, 81], [2, 11]]	['WB', 'WB', 'CB?s']	['wordbreak', 'wordbreak']
2012	The last column lists the Spearman rank order correlation (?) of the rankings with the Berlin and Kay (B&K) ranks. 	[[103, 106]]	[[87, 101]]	['B&K']	['Berlin and Kay']
2013	Then the lexicon Chinese Semantic  Dictionary (CSD) containing sense descriptions  and the corpus Chinese Senses Pool (CSP) annotated with senses are built interactively, simulta-	[[119, 122], [47, 50]]	[[98, 117], [17, 45]]	['CSP', 'CSD']	['Chinese Senses Pool', 'Chinese Semantic  Dictionary']
2014	ferent setups with this parameter. We compare the following setups: (1) The majority baseline (BL) i.e., choosing the most frequent label (SR). (	[[95, 97], [139, 141]]	[[85, 93]]	['BL', 'SR']	['baseline']
2015	the ratio of system?s moves stating that the requested information is not available; Number of abandoned requests (NAR) and abandoned-request ratio (ARR), i.e., the number and the ratio of the information-providing games	[[115, 118]]	[[85, 113]]	['NAR']	['Number of abandoned requests']
2016	have been opened.  Named entity recognition (NER) is one of the  many fields of NLP that rely on machine learn?	[[45, 48], [80, 83]]	[[19, 43]]	['NER', 'NLP']	['Named entity recognition']
2017	For exam-  ple, an analysis of the texts using Mann and Thomp-  son's (1987) Rhetorical Structure Theory (RST) would  result primarily in the relations sequence  and jo in t  	[[106, 109]]	[[77, 104]]	['RST']	['Rhetorical Structure Theory']
2018	It  is embedded to the C-value approach for  automatic term recognition (ATR), in the  form of weights constructed from statisti- 	[[73, 76], [23, 30]]	[[45, 71]]	['ATR', 'C-value']	['automatic term recognition']
2019	paradigm (Berners-Lee, 2006), which requires the use of uniform resource identifiers (URIs), the hypertext transfer protocol (HTTP), standard representation formats (such as RDF) and links to	[[126, 130], [86, 90], [174, 177]]	[[97, 124], [56, 84]]	['HTTP', 'URIs', 'RDF']	['hypertext transfer protocol', 'uniform resource identifiers']
2020	 1 Introduction Electroencephalography (EEG) and magnetoencephalography (MEG) are similar methods for	[[40, 43], [73, 76]]	[[16, 38], [49, 71]]	['EEG', 'MEG']	['Electroencephalography', 'magnetoencephalography']
2021	is necessary to train sentence prediction models, a third approach that uses labeled comment data for training (CTr) but sentences for testing (STe) is included in the CTR/STE row.	[[144, 147], [112, 115], [168, 175]]	[[121, 142], [85, 110]]	['STe', 'CTr', 'CTR/STE']	['sentences for testing', 'comment data for training']
2022	 Therefore it makes sense to also extract data from machine readable dictionaries (MRDs). 	[[83, 87]]	[[52, 81]]	['MRDs']	['machine readable dictionaries']
2023	Clear And Simple English (CASE) Caterpillar Fundamental English (CFE) Caterpillar Technical English (CTE) Diebold Controlled English (DCE)	[[101, 104], [26, 30], [65, 68], [134, 137]]	[[70, 99], [0, 24], [32, 63], [106, 132]]	['CTE', 'CASE', 'CFE', 'DCE']	['Caterpillar Technical English', 'Clear And Simple English', 'Caterpillar Fundamental English', 'Diebold Controlled English']
2024	Table 1: First five SentiWordNet entries for cold#a In our experiments we use two different versions of SWN: SentiWordNet 1.0 (SWN1), the first release of SWN, and its updated version SentiWord-	[[127, 131], [104, 107], [155, 158]]	[[109, 125]]	['SWN1', 'SWN', 'SWN']	['SentiWordNet 1.0']
2025	there is a link to the next node).  Prompts (PT) occur when the tutor attempts to elicit a meaningful contribution from the student.	[[45, 47]]	[[36, 43]]	['PT']	['Prompts']
2026	Section 7 concludes this article.  2 Automatic Speech Recognition (ASR)  Thai ASR research focused on two major topics.	[[67, 70], [78, 81]]	[[37, 65]]	['ASR', 'ASR']	['Automatic Speech Recognition']
2027	string is a false positive (FP). Each gold standard gene mention is counted as a false negative (FN) if it is not identified by the approach.	[[97, 99], [28, 30]]	[[81, 95], [12, 26]]	['FN', 'FP']	['false negative', 'false positive']
2028	 1 Introduction Todays natural user interfaces (NUI) for applications running on smart devices, e.g, phones (SIRI,	[[48, 51], [109, 113]]	[[23, 46]]	['NUI', 'SIRI']	['natural user interfaces']
2029	3.2 Coordination Structures Among the most controversial annotation schemes are those of coordination structures (CS), which are groups of two or more tokens that are in coordina-	[[114, 116]]	[[89, 112]]	['CS']	['coordination structures']
2030	For WSD evaluation, three measures are used: (1) Jaccard Index (JI), which measures the degree of overlap between the induced senses and the gold	[[64, 66], [4, 7]]	[[49, 62]]	['JI', 'WSD']	['Jaccard Index']
2031	 1 Introduction Medical relation (MR) classification, an information extraction task in the clinical domain that was recently defined in the 2010 i2b2/VA Challenge (Uzuner et al.,	[[34, 36]]	[[16, 32]]	['MR']	['Medical relation']
2032	for a comprehensive comparison: ? Mean absolute error (MAE) measures how closely predictions resemble their observed	[[55, 58]]	[[34, 53]]	['MAE']	['Mean absolute error']
2033	a user interface for the production of wordlevel annotations for an opinion-mining task in the information technology (IT) domain. 	[[119, 121]]	[[95, 117]]	['IT']	['information technology']
2034	Litkowski, K. C.: Syntactic Clues and Lexical Resources in Question-Answering. In E. M. Voorhees & D. K. Harman (eds.), The Ninth Text Retrieval Conference (TREC-9). ( 2001) 157-166 10.	[[157, 163]]	[[120, 155]]	['TREC-9']	['The Ninth Text Retrieval Conference']
2035	ptishes a rather inconsequential change with respect o a  previously non-existent link or with respect to a link  No impairment (NI) Q Confusion (C)  Q Mislearning (ML) Q Insufficient Learning (IL) 	[[129, 131], [165, 167], [194, 196]]	[[114, 127], [152, 163], [171, 192], [135, 144]]	['NI', 'ML', 'IL']	['No impairment', 'Mislearning', 'Insufficient Learning', 'Confusion']
2036	five different linear classifiers to extract PPI from AIMed: L2-SVM, 1-norm soft-margin SVM (L1-SVM), logistic regression (LR) (Fan et al, 2008), averaged perceptron (AP) (Collins,	[[123, 125], [45, 48], [61, 67], [88, 91], [93, 99], [167, 169]]	[[102, 121], [146, 165]]	['LR', 'PPI', 'L2-SVM', 'SVM', 'L1-SVM', 'AP']	['logistic regression', 'averaged perceptron']
2037	974   Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875?879, October 25-29, 2014, Doha, Qatar.	[[94, 99]]	[[44, 92]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
2038	11  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675, October 25-29, 2014, Doha, Qatar.	[[92, 97]]	[[42, 90]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
2039	6. Demonstrative pronoun labels are collapsed to DEM PRON (person and number information is easily recovered)	[[53, 57], [49, 52]]	[[59, 65]]	['PRON', 'DEM']	['person']
2040	al., 2005; Joachims et al, 2009) formulation, as shown in Optimization Problem 1 (OP1), to learn a weight vector w.	[[82, 85]]	[[58, 80]]	['OP1']	['Optimization Problem 1']
2041	every lost arc translates to a set of lost parts, we can avoid repeating computations by storing the partial loss of every arc in a data structure (DS): e ?? 	[[148, 150]]	[[132, 146]]	['DS']	['data structure']
2042	We conduct an extrinsic evaluation to compare  the different versions of ArSenL on the task of  subjectivity and sentiment analysis (SSA). We 	[[133, 136], [73, 79]]	[[96, 131]]	['SSA', 'ArSenL']	['subjectivity and sentiment analysis']
2043	For example, both the terms chiaroscuro and collage are classified under picture, image, icon in WordNet, but in the Art & Architecture Thesaurus (AA&T)16 chiaroscuro is categorized under perspective and shading techniques whereas collage is classified under image-making processes and	[[147, 151]]	[[117, 145]]	['AA&T']	['Art & Architecture Thesaurus']
2044	Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios Piperidis, Mike Rosner, & Daniel Tapias, 310?314, Valletta, Malta. European Language Resources Association (ELRA).Ferra?ndez, Oscar, Michael Ellsworth, Rafael Mun?oz, & Collin F. Baker. 2010b.	[[183, 187]]	[[142, 181]]	['ELRA']	['European Language Resources Association']
2045	2 Pivot Translation Pivot translation is a translation from a source language (SRC) to a target language (TRG) through an intermediate pivot (or bridging) language (PVT).	[[106, 109], [79, 82], [165, 168]]	[[89, 104], [62, 77], [135, 163]]	['TRG', 'SRC', 'PVT']	['target language', 'source language', 'pivot (or bridging) language']
2046	92  NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 28?36, Montre?al, Canada, June 7?8, 2012.	[[90, 95]]	[[31, 88]]	['SLPAT']	['Speech and Language Processing for Assistive Technologies']
2047	Figure 3: The system architecture.   CA = communicative act. 	[[37, 39]]	[[42, 59]]	['CA']	['communicative act']
2048	programming alignment on the recognizer?s  hypothesis (HYP) and the non-literal transcription  that is used as reference (REF).  The alignment 	[[122, 125], [55, 58]]	[[111, 120], [43, 53]]	['REF', 'HYP']	['reference', 'hypothesis']
2049	volved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.	[[85, 88]]	[[61, 83]]	['TAG']	['Tree Adjoining Grammar']
2050	 To overcome the difficulty, we build a new Multilayer Search Mechanism (MSM). Different	[[73, 76]]	[[44, 71]]	['MSM']	['Multilayer Search Mechanism']
2051	Abstract We consider the problem of correcting errors made by English as a Second Language (ESL) writers and address two issues that are essen-	[[92, 95]]	[[62, 90]]	['ESL']	['English as a Second Language']
2052	The third is end position (EP), after a predi-  cate. Pre position (PreP) and post position (PostP)  are provided for adverbs as modifiers.	[[68, 72], [93, 98], [27, 29]]	[[54, 66], [78, 91], [13, 25]]	['PreP', 'PostP', 'EP']	['Pre position', 'post position', 'end position']
2053	"constituents. For example, discussing the possible adaptation of Phillips' algorithm to incremental gener-  ation, Lager and Black (1994) point out that some versions of Categorial Grammar (CG) would make the  generator more talkative, by giving rise to ""a more generous notion of constituency""."	[[190, 192]]	[[170, 188]]	['CG']	['Categorial Grammar']
2054	sic and Young, 2011; Williams, 2010; Young et al., 2010) and Bayesian network (BN)-based methods (Raux and Ma, 2011; Thomson and Young,	[[79, 81]]	[[61, 77]]	['BN']	['Bayesian network']
2055	is placed sixth out of seventeen systems according to Mean Absolute Error (MAE) and third according to Root Mean Squared Error (RMSE). The	[[128, 132], [75, 78]]	[[103, 126], [54, 73]]	['RMSE', 'MAE']	['Root Mean Squared Error', 'Mean Absolute Error']
2056	{zhongzhi, nght}@comp.nus.edu.sg Abstract Word sense disambiguation (WSD) systems based on supervised learning	[[69, 72]]	[[42, 67]]	['WSD']	['Word sense disambiguation']
2057	DEP = dependency type ? MOR = morphological features (set) ?	[[24, 27], [0, 3]]	[[30, 43], [6, 16]]	['MOR', 'DEP']	['morphological', 'dependency']
2058	TF (Term Frequency)  is the word frequency within a document;  IDF (Inverse Document Frequency) is the  logarithm of the ratio of the total number of 	[[63, 66], [0, 2]]	[[68, 94], [4, 18]]	['IDF', 'TF']	['Inverse Document Frequency', 'Term Frequency']
2059	of conditional random fields. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 870?878.	[[98, 101]]	[[55, 96]]	['ACL']	['Association for Computational Linguistics']
2060	Setting P0.1 P0.25 P0.33 P0.5 Best-F1 ContextSim (CS) 42.9 69.6 60.7 58.7 49.6 SpellingSim (SS) 90.5 74.2 69.9 64.6 50.9 (a) from baseline models	[[92, 94], [50, 52]]	[[79, 90], [38, 48]]	['SS', 'CS']	['SpellingSim', 'ContextSim']
2061	participate in the interpretation f the CLS (e.g., elements bearing the thematic roles assigned by  the predicate, etc.). DPSs (DP structures) semantically characterize noun phrases. They consist 	[[122, 126], [40, 43]]	[[128, 141]]	['DPSs', 'CLS']	['DP structures']
2062	V is the vocabulary size.  The question difficulty estimation (QDE) task aims to automatically learn the question difficul-	[[63, 66]]	[[31, 61]]	['QDE']	['question difficulty estimation']
2063	Constituents are tagged with IsA class labels from a large, automatically extracted lexicon, using a probabilistic context free grammar (PCFG). 	[[137, 141]]	[[101, 135]]	['PCFG']	['probabilistic context free grammar']
2064	CONN =  nil;  konj( KONJ )  FUNDF = fundf n( NOMINAL ); /* No nil */ 	[[20, 24], [0, 4], [28, 33]]	[[14, 18]]	['KONJ', 'CONN', 'FUNDF']	['konj']
2065	on three official testsets.  NIST 2008 Open Machine Translation (OpenMT) Evaluation9 has distributed test data from 2 domains: Newswire and Web.	[[65, 71]]	[[39, 63]]	['OpenMT']	['Open Machine Translation']
2066	by adapting the baseline model to four adaptation domains. In particular, we attempt to interpret the results given in terms of the character error rate (CER) by  correlating them with the characteristics of the adaptation domain measured us-	[[154, 157]]	[[132, 152]]	['CER']	['character error rate']
2067	In Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems (SENSEVAL-2), pages 1?5. 	[[101, 111]]	[[22, 99]]	['SENSEVAL-2']	['Second International Workshop on Evaluating Word Sense Disambiguation Systems']
2068	comparison with SMO-n) 6 Conclusions Automatic Text Simplification (ATS) aims to convert complex texts into a simpler form, which is more accessible to a wider audience.	[[68, 71], [16, 21]]	[[37, 66]]	['ATS', 'SMO-n']	['Automatic Text Simplification']
2069	defined linguistic context, the task is to predict the class of a token. Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model.	[[98, 102]]	[[73, 96]]	['SVMs']	['Support Vector Machines']
2070	the exploration and verbalization history; and (4) it then sends semantic representations in the form of preverbal messages (PVMs) to the Formulation & Articulation components.	[[125, 129]]	[[105, 123]]	['PVMs']	['preverbal messages']
2071	 In Proceedings of the International Conference on Computational Linguistics (COLING-04). 	[[78, 87]]	[[51, 76]]	['COLING-04']	['Computational Linguistics']
2072	Branching quantification in DTS has partially been discussed in [7] and [8], in which we compared DTS with First Order Logic (FOL). However, FOL is limited in that it allows to	[[126, 129], [28, 31], [98, 101], [141, 144]]	[[107, 124]]	['FOL', 'DTS', 'DTS', 'FOL']	['First Order Logic']
2073	Tables 79 show, for each emotion classification, the mean accuracy (%correct) and standard error (SE) for our 10 feature sets.	[[98, 100]]	[[82, 96]]	['SE']	['standard error']
2074	validation. This is carried out for the tweet text (TEXT), user-declared location (MB-LOC) and user-declared time zone (MB-TZ).	[[52, 56], [83, 89], [120, 125]]	[[46, 50], [59, 81], [95, 118]]	['TEXT', 'MB-LOC', 'MB-TZ']	['text', 'user-declared location', 'user-declared time zone']
2075	2004. Evaluation of a Deidentification (De-Id) Software Engine to Share Pathology Reports and Clinical Documents for	[[40, 45]]	[[22, 38]]	['De-Id']	['Deidentification']
2076	Norm = Normalisation of input prior to tagging. SUC = Subset of Stockholm-Umea? 	[[48, 51]]	[[54, 73]]	['SUC']	['Subset of Stockholm']
2077	Abstract In this paper, we address the problem of converting Dialectal Arabic (DA) text that is written in the Latin script (called	[[79, 81]]	[[61, 77]]	['DA']	['Dialectal Arabic']
2078	76 4 Multi-media Information Networks A Multimedia Information Network (MINet) is a structured collection made up of a set of multimedia documents (e.g., texts and images) and links between these documents.	[[72, 77]]	[[40, 70]]	['MINet']	['Multimedia Information Network']
2079	20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M List Y 9.0 3.3 0.9 13.3 16030 Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%), D = deletions (%), I = insertions (%), T = total WER (%).	[[137, 139], [99, 104], [227, 230]]	[[142, 153], [182, 191], [201, 211], [221, 226]]	['OP', 'HUB-1', 'WER']	['overparsing', 'deletions', 'insertions', 'total']
2080	in the lexicon to the following categories: protesters : NP seized : (S\NP )/NP several : NP/NP	[[70, 74], [57, 59], [77, 79], [90, 95]]	[[60, 68]]	['S\\NP', 'NP', 'NP', 'NP/NP']	['seized :']
2081	ious learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas.	[[76, 79], [117, 120]]	[[51, 74]]	['SVM', 'FFL']	['support vector machines']
2082	 1 Introduction Information retrieval (IR) has been studied since an earlier stage [e.g., (Menzel, 1966)] and sev-	[[39, 41]]	[[16, 37]]	['IR']	['Information retrieval']
2083	FA8750-09-C-0181. The second author also thanks the Vietnam Education Foundation (VEF) for its sponsorship.	[[82, 85]]	[[52, 80]]	['VEF']	['Vietnam Education Foundation']
2084	special right category before COO > (5) the common left coordination category > (6) the other special right category > (7) the free cross-clause clausal category (IC) > (8) the common left cross-clause category > (9) the free cross-clause punctuations (PUS). 	[[253, 256], [30, 33], [163, 165]]	[[239, 251], [56, 68]]	['PUS', 'COO', 'IC']	['punctuations', 'coordination']
2085	adapted to a new domain.  Word sense disambiguation (WSD), on the other hand, is the closely related task of assigning a sense	[[53, 56]]	[[26, 51]]	['WSD']	['Word sense disambiguation']
2086	The method seems to be a simple pattern  matching technique in a left-to-right fashion  but it helps in case of conjunct verbs (ConjVs). 	[[128, 134]]	[[112, 126]]	['ConjVs']	['conjunct verbs']
2087	Recently, a number of deep-learning based models have been proposed for the task of Visual Question Answering (VQA). The per-	[[111, 114]]	[[84, 109]]	['VQA']	['Visual Question Answering']
2088	DEPICTION (DPC) TOPIC (TPC) SYNONYMY-NAME (SYN) CAUSALITY (CSL) PART-WHOLE (PW) MANNER (MNR) ANTONYMY (ANT) JUSTIFICATION (JST) HYPERNYMY (ISA) MEANS (MNS) PROBABILITY OF EXISTENCE (PRB) GOAL (GOL) ENTAIL (ENT) ACCOMPANIMENT (ACC) POSSIBILITY (PSB) BELIEF (BLF)	[[182, 185], [43, 46], [11, 14], [23, 26], [193, 196], [206, 209], [226, 229], [244, 247], [257, 260], [59, 62], [76, 78], [88, 91], [103, 106], [123, 126], [151, 154], [139, 142]]	[[156, 167], [28, 36], [0, 9], [16, 21], [187, 191], [198, 204], [211, 224], [231, 242], [249, 255], [48, 57], [64, 74], [80, 86], [93, 101], [108, 121], [144, 149]]	['PRB', 'SYN', 'DPC', 'TPC', 'GOL', 'ENT', 'ACC', 'PSB', 'BLF', 'CSL', 'PW', 'MNR', 'ANT', 'JST', 'MNS', 'ISA']	['PROBABILITY', 'SYNONYMY', 'DEPICTION', 'TOPIC', 'GOAL', 'ENTAIL', 'ACCOMPANIMENT', 'POSSIBILITY', 'BELIEF', 'CAUSALITY', 'PART-WHOLE', 'MANNER', 'ANTONYMY', 'JUSTIFICATION', 'MEANS']
2089	Internet: chris@lsi .com NLP OBJECTIVES LSI's overall natural language processing (NLP) objective is the development of a broad coverage, reusable system which is readily transportable to additional domains, applications, and sublanguages in English, as well as	[[83, 86], [25, 28], [40, 43]]	[[54, 81]]	['NLP', 'NLP', 'LSI']	['natural language processing']
2090	argument structure agreement (Das, 2009), the  analysis of Non-MonoClausal Verb (NMCV) or  Serial Verb, Control Construction (CC),  Modal Control Construction (MCC), Passives 	[[126, 128], [81, 85], [160, 163]]	[[104, 124], [59, 79], [132, 158]]	['CC', 'NMCV', 'MCC']	['Control Construction', 'Non-MonoClausal Verb', 'Modal Control Construction']
2091	PP) MDI Missed Samples (MS) Bigram Missed Samples (MS) Figure 4: Values of PP and MS for automata for ad-hoc automata	[[51, 53], [0, 2], [4, 7], [24, 26], [82, 84], [75, 77]]	[[35, 49], [8, 22]]	['MS', 'PP', 'MDI', 'MS', 'MS', 'PP']	['Missed Samples', 'Missed Samples']
2092	dominates the other.  Marcu?s Nuclearity Principle (NP) Marcu 1996 provides an alternative to the immediate interpretation and	[[52, 54]]	[[30, 50]]	['NP']	['Nuclearity Principle']
2093	we will describe in detail in Section 3. They then  introduced a ClueWordSummarizer (CWS), a  graph-based unsupervised summarization ap-	[[85, 88]]	[[65, 83]]	['CWS']	['ClueWordSummarizer']
2094	The lexical features used are word bigrams. The Part of Speech (PoS) of the target word and its neighbors make up the the syntactic	[[64, 67]]	[[48, 62]]	['PoS']	['Part of Speech']
2095	6 Scope Resolution One way of dealing with scope ambiguities is by using underspecified representations (URs). A	[[105, 108]]	[[73, 103]]	['URs']	['underspecified representations']
2096	Most common approaches to language model adaptation, such as count merging and model interpolation, are special cases of maximum a posteriori (MAP) estimation (Bacchiani and Roark, 2003).	[[143, 146]]	[[121, 141]]	['MAP']	['maximum a posteriori']
2097	 2 CFN and Its SRL task Chinese FrameNet(CFN) (You et al, 2005) is a research project that has been developed by Shanxi	[[41, 44], [3, 6], [15, 18]]	[[24, 39]]	['CFN', 'CFN', 'SRL']	['Chinese FrameNe']
2098	were computed for each scenario: bilingual evaluation under study (BLEU), position independent error rate (PER) and word error rate (WER). 	[[133, 136], [67, 71], [107, 110]]	[[116, 131], [33, 59], [74, 105]]	['WER', 'BLEU', 'PER']	['word error rate', 'bilingual evaluation under', 'position independent error rate']
2099	patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction pat-	[[80, 83]]	[[54, 78]]	['NSP']	['Ngram Statistics Package']
2100	propose a new inference method ? collective iterative classification (CIC), to find the maximum a posteriori (MAP) assignments for both entities	[[70, 73], [110, 113]]	[[33, 68], [88, 108]]	['CIC', 'MAP']	['collective iterative classification', 'maximum a posteriori']
2101	EUD1.2 has the added benefit of being natively annotated with gold-standard Universal Dependencies (UD) parses (Nivre et al, 2015).	[[100, 102], [0, 3]]	[[76, 98]]	['UD', 'EUD']	['Universal Dependencies']
2102	tegrating more linguistic and structural knowledge with modern statistical techniques, and in particular, for syntax-based machine translation (MT) systems. 	[[144, 146]]	[[123, 142]]	['MT']	['machine translation']
2103	representation, a Partial-Lattice Markov Random Field (PL-MRF), which is a tractable variation of a Factorial Hidden Markov Model (HMM) for language modeling.	[[131, 134], [55, 61]]	[[110, 129], [18, 53]]	['HMM', 'PL-MRF']	['Hidden Markov Model', 'Partial-Lattice Markov Random Field']
2104	namely the overall accuracy (Total-A) and the recall with respect to in-vocabulary words (IV-R),  OOV words (OOV-R) or multi-POS words (MTR).	[[109, 114], [90, 94], [125, 128], [136, 139]]	[[98, 107], [69, 82]]	['OOV-R', 'IV-R', 'POS', 'MTR']	['OOV words', 'in-vocabulary']
2105	573   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 51?59, Sofia, Bulgaria, August 9, 2013.	[[71, 78]]	[[37, 69]]	['DiscoMT']	['Discourse in Machine Translation']
2106	 ? Argument Similarity (ArgSim): This baseline computes the cosine similarity of the vectors for	[[24, 30]]	[[3, 22]]	['ArgSim']	['Argument Similarity']
2107	Instead to measure topic coherence we follow (Newman et al, 2009) to compute the Pointwise Mutual Information (PMI) of topic words w.r.t wikipedia articles.	[[111, 114]]	[[81, 109]]	['PMI']	['Pointwise Mutual Information']
2108	rules, along with a few lexical rules involving a list of stop phrases, discourse cue phrases and wordlevel parts of speech (POS) tags. First, paragraph	[[125, 128]]	[[108, 123]]	['POS']	['parts of speech']
2109	The projection  of the root node on the active leaves is referred to  as the M-BDU (Main BDU). Only syntactic infor-	[[77, 82]]	[[84, 92]]	['M-BDU']	['Main BDU']
2110	 A project that is based on a roughly similar notion of text meaning representation (TMR) concepts is the ?	[[85, 88]]	[[56, 83]]	['TMR']	['text meaning representation']
2111	Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien	[[139, 144], [64, 67], [170, 177]]	[[126, 137]]	['L-LDA', 'LDA', 'DiscLDA']	['Labeled LDA']
2112	In this paper, we describe a mechanism which gen-  erates rebuttals to such rejoinders in the context of  arguments generated from Bayesian etworks (BNs)  (Pearl, 1988).	[[149, 152]]	[[131, 147]]	['BNs']	['Bayesian etworks']
2113	Nearly 2,500 sets of related words in  LLOCE are organized according to 14 subjects and 129 topics (TOP). Cross references (REF) between sets,  topics, and subjects are also given to show various inter-sense r lations not captured within the same topic.	[[124, 127]]	[[112, 122]]	['REF']	['references']
2114	 4.1 BRAT The brat rapid annotation tool (BRAT) is an opensource web-based annotation tool that supports a	[[42, 46]]	[[14, 40]]	['BRAT']	['brat rapid annotation tool']
2115	ity. In Proceedings of Treebanks and Linguistic Theories (TLT) 2003, Vaxjo, Sweden. 	[[58, 61], [71, 74]]	[[23, 56]]	['TLT', 'axj']	['Treebanks and Linguistic Theories']
2116	 We focus on the following languages: German (DE), French (FR), Italian (IT), and Dutch (NL).	[[59, 61], [73, 75], [46, 48], [89, 91]]	[[51, 57], [64, 71], [38, 44], [82, 87]]	['FR', 'IT', 'DE', 'NL']	['French', 'Italian', 'German', 'Dutch']
2117	context dependence and mutual information. Yamamoto and Church (2001) experiment with both mutual information and residual inverse document frequency (RIDF)1 as criteria for deciding Japanese words, and their main contribution is in affording	[[151, 155]]	[[114, 149]]	['RIDF']	['residual inverse document frequency']
2118	representative popular heterogeneous corpora, i.e. 232 Penn Chinese Treebank (CTB) and PKU?s People?s Daily (PPD).	[[78, 81], [109, 112]]	[[60, 76], [87, 107]]	['CTB', 'PPD']	['Chinese Treebank', 'PKU?s People?s Daily']
2119	 260 SentiWordNet(SWN) (Baccianella et al., 	[[18, 21]]	[[5, 16]]	['SWN']	['SentiWordNe']
2120	Discourse Relations (DR) 48.04 Entity Grid (EG) 67.74 Lexical Cohesion (LC) 61.63 Document Length 69.40	[[72, 74], [21, 23], [44, 46]]	[[54, 70], [0, 19], [31, 42]]	['LC', 'DR', 'EG']	['Lexical Cohesion', 'Discourse Relations', 'Entity Grid']
2121	For this task we train and test three different statistical models: an n-gram language model, a maximum entropy model (MaxEnt) and a (linear) support vector machine (SVM).	[[119, 125], [166, 169], [71, 77]]	[[96, 111], [142, 164]]	['MaxEnt', 'SVM', 'n-gram']	['maximum entropy', 'support vector machine']
2122	R5   95 7 Antecedent Contained Deletion(ACD)  Further evidence for the proposed analysis comes 	[[40, 43]]	[[10, 39]]	['ACD']	['Antecedent Contained Deletion']
2123	Figure 2: Accuracy by sentence length for Method 5 measured on separate grammatical and ungrammatical data: Gr = Grammatical, AG = Agreement, RW = Real-Word, EW = Extra Word, MW = Missing	[[126, 128], [108, 110], [142, 144], [158, 160], [175, 177]]	[[131, 140], [113, 124], [147, 156], [163, 173], [180, 187]]	['AG', 'Gr', 'RW', 'EW', 'MW']	['Agreement', 'Grammatical', 'Real-Word', 'Extra Word', 'Missing']
2124	the sentence. The segmentation model is a chain LVM (latent variable model) that aims to maximize a linear objective defined by:	[[48, 51]]	[[53, 74]]	['LVM']	['latent variable model']
2125	(Roelofs, 2004), experiments on priming (Schvaneveldt et al., 1976) or the tip of the tongue problem (TOT) (Brown and McNeill, 1996).	[[102, 105]]	[[75, 92]]	['TOT']	['tip of the tongue']
2126	For the nouns, 31 basic types are selected from  WordNet top categories (unique beginners): 2  entity(ENT) life~orm(LIF)  causal_agent(AGT) human(HUN) 	[[102, 105]]	[[95, 100]]	['ENT']	['entit']
2127	bilities. Experience has shown that this kind of  full-fledged question answering (QA) over texts  from a wide range of domains is so difficult for 	[[83, 85]]	[[63, 81]]	['QA']	['question answering']
2128	4  At the highest level, the text is a request addressed to CCC  members to vote against making the nuclear freeze initiative (NFI)  one of the issues about which CCC actively lobbies and promotes 	[[127, 130], [60, 63], [163, 166]]	[[100, 125]]	['NFI', 'CCC', 'CCC']	['nuclear freeze initiative']
2129	 ? New York Times (NYT) archive: a set of around 1.8 million news article from the archives	[[19, 22]]	[[3, 17]]	['NYT']	['New York Times']
2130	 8. Adjacent Variety(AV) of the candidate. We	[[21, 23]]	[[4, 19]]	['AV']	['Adjacent Variet']
2131	3 Polylingual Topic Model The polylingual topic model (PLTM) is an extension of latent Dirichlet alocation (LDA) (Blei et al.,	[[108, 111], [55, 59]]	[[80, 106]]	['LDA', 'PLTM']	['latent Dirichlet alocation']
2132	morphologically very rich. Different suffixes  may be attached to a Light Verb (LVs) (in this  case [YYY]) depending on the various features 	[[80, 83]]	[[68, 78]]	['LVs']	['Light Verb']
2133	for si ? Bm do Use Breadth First Search (BFS) to check if ?	[[41, 44]]	[[19, 39]]	['BFS']	['Breadth First Search']
2134	scheme that includes: (1) a pre-annotation that segments the dialogue into turns which are further segmented into Elementary Discourse Units (EDUs) with the author of each turn automatically given;	[[142, 146]]	[[114, 140]]	['EDUs']	['Elementary Discourse Units']
2135	2.3 Approach BUAP-RUN-3: Random Indexing and Bag of Concepts The vector space model (VSM) for document representation supporting search is probably the most	[[85, 88], [13, 23]]	[[65, 83]]	['VSM', 'BUAP-RUN-3']	['vector space model']
2136	few open source programs. Since we are interested in a fully supervised WSD tool, IMS (It Makes Sense) (Zhong and Ng, 2010) is selected in our	[[82, 85], [72, 75]]	[[87, 95]]	['IMS', 'WSD']	['It Makes']
2137	Measuring and estimating post-editing effort is therefore a growing concern addressed by Confidence Estimation (CE) (Specia, 2011). 	[[112, 114]]	[[89, 110]]	['CE']	['Confidence Estimation']
2138	 246  AO = all objects  MO = matched objects 	[[6, 8], [24, 26]]	[[11, 22], [29, 43]]	['AO', 'MO']	['all objects', 'matched object']
2139	validate the performance of our method:  1. Precision@N (P@N). P@N measures how 	[[57, 60], [63, 66]]	[[44, 55]]	['P@N', 'P@N']	['Precision@N']
2140	as source domain training data (STrain), files 271300 as source domain testing data (STest) and files 590-596 as target domain testing data (TTest). We	[[141, 146], [32, 38], [85, 90]]	[[113, 139], [3, 30], [57, 83]]	['TTest', 'STrain', 'STest']	['target domain testing data', 'source domain training data', 'source domain testing data']
2141	716 Figure 5: The NUMBERS System Architecture (CA = communicative act) The module network topology of the system is	[[47, 49], [18, 25]]	[[52, 69]]	['CA', 'NUMBERS']	['communicative act']
2142	y) 7. Mutual dependency (MD) log P (xy)2P (x?)P (? y)	[[25, 27]]	[[6, 23]]	['MD']	['Mutual dependency']
2143	/NN ?? /NN ]   Input: wi: word index (ID) in a given sentence. 	[[38, 40]]	[[31, 36]]	['ID']	['index']
2144	"  EFF (effect): We made her the secretary.    ORIG (origin): She made a cake from apples. "	[[46, 50], [2, 5]]	[[52, 58], [7, 13]]	['ORIG', 'EFF']	['origin', 'effect']
2145	Rule 3: Question word followed immediately by a verb (Example (3)).   Qp = question word + headword in the following Verb Phrase(VP) or NP chunk  Rule 4: Question word followed by a passive VP (Example (4)).	[[129, 131], [136, 138], [190, 192]]	[[117, 127]]	['VP', 'NP', 'VP']	['Verb Phras']
2146	1 Introduction In the past few years, a number of studies have focused on verbal semantic role labeling (SRL). 	[[105, 108]]	[[81, 103]]	['SRL']	['semantic role labeling']
2147	Above all, our goal is to integrate cross-media inference and create the linkage among the information extracted from those heterogenous data. Our novel Multi-media Information Networks (MiNets) representation initializes our idea about a basic ontology of the ranking system.	[[187, 193]]	[[159, 185]]	['MiNets']	['media Information Networks']
2148	apply shallow semantic (selectlonal) constraints, to filter out semantically anomalous parses, in a  second experiment. This procedure used PUNDIT's Selection Pattern Query and Response (SPQR)  component ~Lang1988\].	[[187, 191]]	[[149, 185]]	['SPQR']	['Selection Pattern Query and Response']
2149	tives falls in the middle range and what causes the large and small divergence of the document collection pairs with different topics (DT) and the same topic (ST) or perspective (SP), respectively.	[[135, 137], [159, 161], [179, 181]]	[[117, 133], [147, 157], [166, 177]]	['DT', 'ST', 'SP']	['different topics', 'same topic', 'perspective']
2150	The application of the program is demonstrated using the Aberdeen Report Judgment Scales (ARJS; Sporer, 2004) with a set of 72 deceptive and true accounts of a driving examination. Data on different types of inter-coder reliabilities are presented and implications for future research with computer-assisted qualitative coding procedures as well as training of coders are outlined. Credits This research has been supported by a grant from the German Science Foundation (Deutsche Forschungsgemeinschaft (DFG): Sp262/3-2) to the present author. The author would like to thank Edda Niederstadt and Nina F. Petermann for the coding of the data, and to Jaume Masip, Valerie Hauch, and Sarah Treiber for comments on an earlier version of this manuscript.	[[503, 506], [90, 94]]	[[470, 501], [57, 88]]	['DFG', 'ARJS']	['Deutsche Forschungsgemeinschaft', 'Aberdeen Report Judgment Scales']
2151	gathered training data from parallel texts for the set of most frequently occurring noun, adjective, and verb types in the Brown Corpus (BC). These word	[[137, 139]]	[[123, 135]]	['BC']	['Brown Corpus']
2152	Tipster (ADEPT) Program is a demonstration project  aimed at alleviating problems currently being faced by  the Office of Information Resources (OIR). OIR has 	[[145, 148], [9, 14], [151, 154]]	[[112, 143]]	['OIR', 'ADEPT', 'OIR']	['Office of Information Resources']
2153	most blogged about articles? of the New York Times (NYT)1. 	[[52, 55]]	[[36, 50]]	['NYT']	['New York Times']
2154	outer: the perceived external frame or point of reference for  the action, event, or state as a whole  Means (MNS):  inner: the perceived immediate affeetor or effeetor of the 	[[110, 113]]	[[103, 108]]	['MNS']	['Means']
2155	Ihe maohine translation problem has recently been replaced  by much narrower goals and computer processing of language has  become part df artificial intelligence (AI), speech recognition,  and structural pattern recognition.	[[164, 166]]	[[139, 162]]	['AI']	['artificial intelligence']
2156	ers; (ii) to design an initial policy for reinforcement learning of multimodal clarifications.4 We use the Nite XML Toolkit (NXT) (Carletta et al, 2003) to represent and browse the data and to de-	[[125, 128]]	[[107, 123]]	['NXT']	['Nite XML Toolkit']
2157	guided learning. The approach taken has been to en-  code an artificial neural network (ANN) in a genome  which stores its architecture and learning rules.	[[88, 91]]	[[61, 86]]	['ANN']	['artificial neural network']
2158	" 5Note that this is a recursive lexical rule, which  Adjunct Extraposition Lexical Rule (AELR)  ""r,oc \[\] ICATIHEAD nou,~ Vverb\] "	[[89, 93]]	[[53, 87]]	['AELR']	['Adjunct Extraposition Lexical Rule']
2159	observed in Dutch. Dutch shows a pattern in which  an arbitrary number of noun phrases (NP's) may be  followed by a finite verb and an arbitrary number 	[[88, 92]]	[[74, 86]]	"[""NP's""]"	['noun phrases']
2160	stem of JUMP = <jump>.   sense of JUMP = jumping. 	[[34, 38], [8, 12]]	[[41, 48], [16, 20]]	['JUMP', 'JUMP']	['jumping', 'jump']
2161	For the training of the SMT engines, we used two parallel Spanish-English corpora provided by the organizers: the Europarl (EP) corpus (Koehn, 2005), which consists of 1.4M paral-	[[124, 126], [24, 27]]	[[114, 122]]	['EP', 'SMT']	['Europarl']
2162	 4.3 Counting and Calculation The SRI Language Modelling Toolkit (SRILM) (Stolcke and others, 2002) is used to count the frequencies in our work.	[[66, 71]]	[[34, 56]]	['SRILM']	['SRI Language Modelling']
2163	Center for Language Technology. After accomplishing the task concerning named entity (NE)  identification, we go on studying identification 	[[86, 88]]	[[72, 84]]	['NE']	['named entity']
2164	ical relations may be at the head of multiple arcs.  For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ).	[[87, 92], [140, 145]]	[[70, 85], [124, 138]]	['S-SBJ', 'L-OBJ']	['surface subject', 'logical object']
2165	tailment pairs to train an entailment classifier, our model was only trained on the 800 text-hypothesis pairs found in the RTE-3 Development Set (DevSet). 	[[146, 152], [123, 128]]	[[129, 144]]	['DevSet', 'RTE-3']	['Development Set']
2166	crimination information (MDI) estimation criterion for speech recognition and notes an improvement in terms of perplexity and word error rate (WER). 	[[143, 146], [25, 29]]	[[126, 141]]	['WER', 'MDI)']	['word error rate']
2167	sions to identify stylistic shifts in paraphrase, allowing us to differentiate stylistic properties in the Paraphrase Database (PPDB) with high accuracy. Sec-	[[128, 132]]	[[89, 126]]	['PPDB']	['properties in the Paraphrase Database']
2168	Jiang, Hua. Xu} @uth.tmc.edu tangbuzhou@gmail.com yukun.chen@Vanderbilt. Edu      Abstract This work describes the participation of the University of Texas Health Science Center at Houston (UTHealth) team on the SemEval 2014 ? Task 7 analysis of clinical text challenge.	[[190, 198]]	[[136, 188]]	['UTHealth']	['University of Texas Health Science Center at Houston']
2169	making procedures.  Latent semantic analysis (LSA) (Deerwester et al.,	[[46, 49]]	[[20, 44]]	['LSA']	['Latent semantic analysis']
2170	(? baseline?) and MT (Madnani et al, 2012). RAE	[[18, 20], [44, 47]]	[[22, 32]]	['MT', 'RAE']	['Madnani et']
2171	t have also been used.  2.2 EasyAdapt (EA) In this section, we give a brief overview of	[[39, 41]]	[[28, 37]]	['EA']	['EasyAdapt']
2172	  Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 120?125, Boulder, Colorado, June 2009.	[[87, 92]]	[[46, 85]]	['CoNLL']	['Computational Natural Language Learning']
2173	In addition, we also  experimented with different combinations of  translation models (TM), phrase-based and  factor-based, trained on various datasets to 	[[87, 89]]	[[67, 85]]	['TM']	['translation models']
2174	ules, distributed on two computers.  The graphical user interface (GUI) has a close link to the dialogue manager since it integrates sev-	[[67, 70]]	[[41, 65]]	['GUI']	['graphical user interface']
2175	Within Acquilex IP Project, a unification framework  based on typed feature structures \[4\] was ddveloped, the  LKB (Lexical Knowledge Base), in order to represent  conceptual units corresponding to lexieal senses, lexical 	[[113, 116], [16, 18]]	[[118, 140]]	['LKB', 'IP']	['Lexical Knowledge Base']
2176	     1 Introduction  Most of the natural language generation (NLG)  components in current dialog systems are imple-	[[62, 65]]	[[33, 60]]	['NLG']	['natural language generation']
2177	structure by first computing the similarity of each proposition to the others using a Latent Dirichlet Allocation (LDA) model. LDA is a genera-	[[115, 118], [127, 130]]	[[86, 113]]	['LDA', 'LDA']	['Latent Dirichlet Allocation']
2178	In E.M. Voorhees and  D.K. Harman, editors, The 3d Text RE-  trieval Conference (TREC-3). 	[[81, 87], [3, 6], [22, 25]]	[[48, 79]]	['TREC-3', 'E.M', 'D.K']	['3d Text RE-  trieval Conference']
2179	System and Datasets We use the Moses phrasebased MT system (Koehn et al, 2007) and consider Urdu?English (UR?EN), Chinese?English (ZH?EN) translation, and Arabic?English	[[106, 111]]	[[92, 104]]	['UR?EN']	['Urdu?English']
2180	have to be induced from parallel corpora.  An inversion transduction grammar (ITG) strikes a good balance between STGs and SDTGs,	[[78, 81], [114, 118], [123, 128]]	[[46, 76]]	['ITG', 'STGs', 'SDTGs']	['inversion transduction grammar']
2181	? P2E1N3S2, C S D G ASD Simplified Technical English (ASD-STE) (ASD 2013), often abbreviated to Simplified Technical English (STE) or just Simplified English, is a CNL for the aerospace	[[54, 61], [0, 10], [126, 129], [164, 167], [64, 67]]	[[20, 52], [96, 124]]	['ASD-STE', '? P2E1N3S2', 'STE', 'CNL', 'ASD']	['ASD Simplified Technical English', 'Simplified Technical English']
2182	domain-oriented semantics of the GENIA event corpus, and suggests a factor for utilizing NLP techniques for Text Mining (TM) in the bio-medical domain.	[[121, 123], [33, 38], [89, 92]]	[[108, 119]]	['TM', 'GENIA', 'NLP']	['Text Mining']
2183	trieval process. ( Zhou and Wade, 2009b) proposed a Latent Dirichlet Allocation (LDA)based method to model the latent structure of 	[[81, 84]]	[[52, 79]]	['LDA']	['Latent Dirichlet Allocation']
2184	                                                    2  In our system, we define ten types of factoid: date, time (TIME), percentage, money, number (NUM), measure, e-mail, phone  number, and WWW.	[[148, 151], [114, 118], [190, 193]]	[[140, 146], [108, 112]]	['NUM', 'TIME', 'WWW']	['number', 'time']
2185	The contextual information about social status and  sentence-external individuals can bc included in the  attribute CONTEXT (CONX). Ill order to see values the 	[[125, 129]]	[[116, 123]]	['CONX']	['CONTEXT']
2186	1 Motivation  Question Answering has emerged as a key area in  natural language processing (NLP) to apply question parsing, information extraction, summariza-	[[92, 95]]	[[63, 90]]	['NLP']	['natural language processing']
2187	2.4 Optimisation and Sampling from a WCFG Optimisation in a weighted CFG (WCFG)3, that is, finding the maximum derivation, is well stud-	[[74, 78], [37, 41]]	[[60, 72]]	['WCFG', 'WCFG']	['weighted CFG']
2188	recording  information pertinent to treatment of a patient that consists of a number of subsections such as Chief Complaint (CC), History of Present Illness (HPI),	[[125, 127], [158, 161]]	[[108, 123], [130, 156]]	['CC', 'HPI']	['Chief Complaint', 'History of Present Illness']
2189	get node and another node on the dependency parsed tree: ANC (ancestor), DES (descendant), SIB (sibling), and TARGET (target word). Figure 5 shows	[[110, 116], [57, 60], [73, 76], [91, 94]]	[[118, 124], [62, 70], [78, 88], [96, 103]]	['TARGET', 'ANC', 'DES', 'SIB']	['target', 'ancestor', 'descendant', 'sibling']
2190	 1 Introduction Many problems in natural language processing (NLP) involve optimizing some objective function over a set of	[[62, 65]]	[[33, 60]]	['NLP']	['natural language processing']
2191	Table 3: Statistics of training and test corpus for the Canadian Hansards task (PP=perplexity, SL=sentence length). 	[[95, 97]]	[[98, 113]]	['SL']	['sentence length']
2192	 1 Introduction  Relation Extraction (RE) aims to identify a set of  predefined relations between pairs of entities in 	[[38, 40]]	[[17, 36]]	['RE']	['Relation Extraction']
2193	2 Methods In this IRB-approved study, we obtained the Shared Annotated Resource (ShARe) corpus originally generated from the Beth Israel Dea-	[[81, 86]]	[[54, 79]]	['ShARe']	['Shared Annotated Resource']
2194	2 Data In this study, we use a collection of blog posts from five blogs: Carpetbagger(CB)1, Daily Kos(DK)2, Matthew Yglesias(MY)3, Red State(RS)4, and Right	[[86, 88], [102, 104], [125, 127], [141, 143]]	[[73, 84], [92, 100], [108, 124], [131, 140]]	['CB', 'DK', 'MY', 'RS']	['Carpetbagge', 'Daily Ko', 'Matthew Yglesias', 'Red State']
2195	it can therefore mean ? prostrated on the threshold  and respectfully (AD) paid visits three times? or 	[[71, 73]]	[[53, 56]]	['AD']	['and']
2196	logical structure of the text is a boundary between two logical segments (see Figure 1).  The method is called Logical TextTiling (LTT), due to some similarities with the TextTiling algorithm for topic shifts detection (Hearst, 1997).	[[131, 134]]	[[111, 129]]	['LTT']	['Logical TextTiling']
2197	We evaluated our Chinese word segmenter in the open track, on all 4 corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and	[[101, 103], [136, 141], [164, 167]]	[[84, 99], [106, 121], [144, 162]]	['AS', 'CITYU', 'MSR']	['Academia Sinica', 'City University', 'Microsoft Research']
2198	Both Russian and Czech have relatively free word order, so it may seem an odd choice to use a Markov model (MM) tagger. Why should second order	[[108, 110]]	[[101, 106]]	['MM']	['model']
2199	In this paper we present our entry to the WMT?13 shared task: Quality Estimation (QE) for machine translation (MT). 	[[111, 113], [42, 48], [82, 84]]	[[90, 109], [62, 80]]	['MT', 'WMT?13', 'QE']	['machine translation', 'Quality Estimation']
2200	tant of these is the controlled vocabulary terms assigned by human indexers. NLM?s controlled vocabulary thesaurus, Medical Subject Headings (MeSH),2 contains approximately 23,000 descriptors arranged in a hierarchical structure and more than 151,000	[[142, 146], [77, 82]]	[[116, 140]]	['MeSH', 'NLM?s']	['Medical Subject Headings']
2201	 Table 1: Classifier features in predicate disambiguation (PredDis), argument identification (ArgId), and argument labeling (ArgLab).	[[94, 99], [59, 66], [125, 131]]	[[69, 92], [33, 57], [106, 123]]	['ArgId', 'PredDis', 'ArgLab']	['argument identification', 'predicate disambiguation', 'argument labeling']
2202	by using multiple learners and a label integrator.  We have developed a forward (FR) and a backward relationship (BR) learner to learn relation-	[[81, 83], [114, 116]]	[[72, 79], [91, 112]]	['FR', 'BR']	['forward', 'backward relationship']
2203	 3.1.3 TBL Transformation based learning(TBL), first introduced by Eric Brill(Brill, 1995), is mainly	[[41, 44], [7, 10]]	[[11, 39]]	['TBL', 'TBL']	['Transformation based learnin']
2204	duction to the two matrices. In particular, we adopt the Singular Value Decomposition (SVD), one of the most effective methods to approximate the original	[[87, 90]]	[[57, 85]]	['SVD']	['Singular Value Decomposition']
2205	ambiguous verb structure in a garden-path in two ways; one is as a subordinate clause (MV), the other is a Reduced Relative (RR). He defined	[[125, 127], [87, 89]]	[[107, 123]]	['RR', 'MV']	['Reduced Relative']
2206	 Based on the statistics shown in Table 3, the likelihood ratio tests (LRT) model captures the statistical association between a pattern p and a word	[[71, 74]]	[[47, 69]]	['LRT']	['likelihood ratio tests']
2207	relates only loosely to the semantics of natural language. The work we present in  this paper differs from all previous work in natural anguage processing (NLP) in at  least two respects.	[[156, 159]]	[[128, 154]]	['NLP']	['natural anguage processing']
2208	news headlines (headlines); mapping of lexical resources from Ontonotes to Wordnet (OnWN) and from FrameNet to WordNet (FNWN); and evaluation of machine translation (SMT).	[[120, 124]]	[[99, 118]]	['FNWN']	['FrameNet to WordNet']
2209	various subsets of the documents in the English Gigaword corpus, chiefly drawn from New York Times (NYT) and Agence France Presse (AFP).1 2.1 Are Discounts Constant?	[[131, 134], [100, 103]]	[[109, 129], [84, 98]]	['AFP', 'NYT']	['Agence France Presse', 'New York Times']
2210	based on a probabilistic model. We investigate two methods using Latent Dirichlet Allocation (LDA) (Blei, 2003) in ?	[[94, 97]]	[[65, 92]]	['LDA']	['Latent Dirichlet Allocation']
2211	tagging. These errors in the training corpora affects badly to the machine learning (ML) based models. 	[[85, 87]]	[[67, 83]]	['ML']	['machine learning']
2212	Where is that??).  Finally, the task features (TASK) reflect conflicting instructions in the domain.	[[47, 51]]	[[32, 36]]	['TASK']	['task']
2213	3.3 Aspect term extraction Our approach for aspect term extraction is based on Conditional Random Fields (CRF). The choice	[[106, 109]]	[[79, 104]]	['CRF']	['Conditional Random Fields']
2214	tourist resources was made by the Direcc?a?o Geral de Turismo (DGT) and afterwards the Inventory of Tourist Resources (IRT) emerged. 	[[119, 122], [63, 66]]	[[87, 117], [45, 61]]	['IRT', 'DGT']	['Inventory of Tourist Resources', 'Geral de Turismo']
2215	 OOV Handling Techniques and their Combination We compare our baseline system (BASELINE) to each of our basic techniques and their full combi-	[[79, 87], [1, 4]]	[[62, 77]]	['BASELINE', 'OOV']	['baseline system']
2216	 2.3 IR Similarity Measures (IR) The information retrieval?based features (IR) were based on a dump of English Wikipedia from Novem-	[[75, 77], [5, 7], [29, 31]]	[[37, 58]]	['IR', 'IR', 'IR']	['information retrieval']
2217	for re-ranking in the context of name tagging.  Maximum Entropy modeling (MaxEnt) has  been extremely successful for many NLP classifi-	[[74, 80], [122, 125]]	[[48, 63]]	['MaxEnt', 'NLP']	['Maximum Entropy']
2218	their semantic deviation values. The result is a  list of pairs called the ICS (Initial Cluster Set). 	[[75, 78]]	[[80, 99]]	['ICS']	['Initial Cluster Set']
2219	  1 Introduction  INTERA (Integrated European language data  Repository Area, Contract 22076Y2C2DMAL2) is 	[[18, 24]]	[[26, 76]]	['INTERA']	['Integrated European language data  Repository Area']
2220	Table 1: Mean relative frequencies and standard deviation for each class (A(nimate) vs. I(nanimate)) from feature extraction (SUBJ=Transitive Subject, OBJ=Object, GEN=Genitive -s, PASS=Passive byphrase, ANAAN=Anaphoric reference by animate pronoun, ANAIN=Anaphoric reference by inanimate pronoun, REFL=Anaphoric reference by reflexive pronoun).	[[203, 208], [126, 130], [151, 154], [163, 166], [180, 184], [249, 254], [297, 301]]	[[209, 239], [155, 161], [167, 175], [185, 192], [255, 287], [325, 334]]	['ANAAN', 'SUBJ', 'OBJ', 'GEN', 'PASS', 'ANAIN', 'REFL']	['Anaphoric reference by animate', 'Object', 'Genitive', 'Passive', 'Anaphoric reference by inanimate', 'reflexive']
2221	 3 The TMop framework TMop (Translation Memory open-source purifier) is an open-source TM cleaning software written	[[22, 26], [87, 89]]	[[28, 46]]	['TMop', 'TM']	['Translation Memory']
2222	cabbage 9 chou 1 chou blossom 25 fleur 73 commande carpet 39 tapis 1 tapis bitter 59 amer 1 amer hammer 67 marteau 1 marteau bread 82 pain 1 pain citizen 115 citoyen 1 citoyen bath 178 bain 1 bain butterfly 201 papillon 1 papillon eat 208 manger 1 manger butter 220 beurre 59 terre eagle 282 aigle 1 aigle cheese 527 fromage 1 fromage cold 539 froid 1 froid deep 585 profond 1 profond cottage 624 cabanon 1 cabanon earth 702 terre 53 tabac child 735 enfant 1 enfant bed 806 lit 2 table beautiful 923 beau 1 beau care 1267 soin 1 soin hand 1810 main 2 main city 2610 ville 1 ville girl 2673 fille 1 fille green 2861 vert 1 vert blue 2914 bleu 1 bleu hard 3615 dur 1 dur black 9626 noir 1 noir Bible 17791 Bible 1 Bible foot 23548 pied 8 siffler chair 24027 chaise 1 chaise fruit 38544 fruit 1 fruit  Table 2: Results for the language pair English ? French. The meaning of the columns is as follows: ESW = English source word; CF = corpus frequency of English source word; ET = expected translation according to gold standard; RE = computed rank of expected translation; CT = computed translation. 	[[925, 927], [1069, 1071], [898, 901], [971, 973], [1025, 1027]]	[[930, 946], [1074, 1094], [904, 923], [976, 996], [1039, 1055]]	['CF', 'CT', 'ESW', 'ET', 'RE']	['corpus frequency', 'computed translation', 'English source word', 'expected translation', 'rank of expected']
2223	from Association. ACM Transactions on Information Systems (TOIS) 21:315-346. 	[[59, 63], [18, 21]]	[[22, 57]]	['TOIS', 'ACM']	['Transactions on Information Systems']
2224	2   TASK A: Question Generation from Paragraphs  1.1   Task Definition  The Question Generation from Paragraphs (QGP) task challenges participants to  generate a list of 6 questions from a given input paragraph.	[[113, 116]]	[[76, 111]]	['QGP']	['Question Generation from Paragraphs']
2225	The number of sentences with product features  ? Word level (WL)  ?	[[61, 63]]	[[49, 59]]	['WL']	['Word level']
2226	2 Description of the France Telecom 3000 Voice Agency corpus The France Telecom 3000 (FT3000) Voice Agency service, the first deployed vocal service at France	[[86, 92]]	[[65, 84]]	['FT3000']	['France Telecom 3000']
2227	 2 Sign language phenomena Sign Languages (SLs) involve simultaneous manual and non-manual components for conveying mean-	[[43, 46]]	[[27, 41]]	['SLs']	['Sign Languages']
2228	(DE), Greek (EL), English (EN), Spanish (ES), French (FR), Italian (IT), Korean (KO), Dutch (NL), Portugese (PT), Russian (RU), Swedish (SV) and Chinese (ZH) ?	[[123, 125], [1, 3], [13, 15], [27, 29], [41, 43], [54, 56], [68, 70], [81, 83], [93, 95], [109, 111], [137, 139], [154, 156]]	[[114, 121], [18, 25], [32, 39], [46, 52], [59, 66], [73, 79], [86, 91], [98, 107], [128, 135], [145, 152]]	['RU', 'DE', 'EL', 'EN', 'ES', 'FR', 'IT', 'KO', 'NL', 'PT', 'SV', 'ZH']	['Russian', 'English', 'Spanish', 'French', 'Italian', 'Korean', 'Dutch', 'Portugese', 'Swedish', 'Chinese']
2229	pute probability scores of word sequences. The general conversational language model (LM) is based on data from the SWITCHBOARD corpus and a small	[[86, 88]]	[[70, 84]]	['LM']	['language model']
2230	tic models, which in this case are hidden Markov models (HMM), and described in terms of wellknown Mel frequency cepstral coefficients (MFCCs) (Benesty et al, 2008).	[[136, 141], [57, 60]]	[[99, 134], [35, 54]]	['MFCCs', 'HMM']	['Mel frequency cepstral coefficients', 'hidden Markov model']
2231	con using label propagation and ground truth EPA values (POS= part-of-speech, W= the number of the induced words, MAS=mean absolute error, and RMSE= root mean squared error	[[114, 117], [45, 48], [57, 60], [143, 147]]	[[118, 131], [62, 76], [107, 112], [149, 172]]	['MAS', 'EPA', 'POS', 'RMSE']	['mean absolute', 'part-of-speech', 'words', 'root mean squared error']
2232	Nincc NIA ~ \]laS started moving from toy  problems to ,'eal applications one of the biggest  difficully has been Knowledge Acquisition (KA)  of different lypes (lexical, grammatical, domain 	[[137, 139]]	[[114, 135]]	['KA']	['Knowledge Acquisition']
2233	lingual extensions of approaches based on latent (LSI), generative (LDA, PLSI) as well as explicit (ESA) topic modelling can induce an interlingual topic space allowing documents	[[100, 103], [50, 53], [68, 71], [73, 77]]	[[90, 98]]	['ESA', 'LSI', 'LDA', 'PLSI']	['explicit']
2234	We chose Gaussian distributions. If the parents of node X are Y, P (X|Y ) = N(m + W ?	[[68, 71]]	[[56, 63]]	['X|Y']	['X are Y']
2235	4.0 release. This was also the training data used in the experiments in the Parsing the Web (PTW) shared task at NAACL 2012.2 In the shared task	[[93, 96], [113, 118]]	[[76, 91]]	['PTW', 'NAACL']	['Parsing the Web']
2236	processing applications, su ch as larg e-vocab u lary speech recog nition (L V CS R), statistical machine translation (S M T ) and information retrieval (IR), is the morpholog ical analy sis of w ords.	[[154, 156], [119, 124], [75, 83]]	[[131, 152], [86, 117], [34, 73]]	['IR', 'S M T', 'L V CS R']	['information retrieval', 'statistical machine translation', 'larg e-vocab u lary speech recog nition']
2237	project.  In section 2 we provide an overview of the automatic compound processing (AuCoPro) project, which forms the background of this research.	[[84, 91]]	[[53, 82]]	['AuCoPro']	['automatic compound processing']
2238	Table 5: Final test accuracies for Chinese. UAS = unlabeled attachment score; UEM = unlabeled exact match; LAS = labeled attachment score.	[[78, 81]]	[[84, 105]]	['UEM']	['unlabeled exact match']
2239	Q-based Intersection PTSD stands for posttraumatic stress disorder and is a psychological disorder.  Generic Union Posttraumatic stress disorder (PTSD) is a psychological disorder, which is classified as an anxiety disorder in the DSM-IV, caused by a mental trauma (also called psychotrauma) that	[[146, 150], [21, 25], [231, 237]]	[[115, 144]]	['PTSD', 'PTSD', 'DSM-IV']	['Posttraumatic stress disorder']
2240	 1 Introduction Using natural language processing (NLP) techniques to mine software corpora such as code com-	[[51, 54]]	[[22, 49]]	['NLP']	['natural language processing']
2241	 3 Background Slot filling (SF) is a query-oriented relation extraction (RE) task in the Knowledge Base Popu-	[[28, 30], [73, 75]]	[[14, 26], [52, 71]]	['SF', 'RE']	['Slot filling', 'relation extraction']
2242	3.2 Result of Chinese NER We evaluated our named entity recognizer on the SIGHAN Microsoft Research Asia(MSRA) corpus in both closed and open track.	[[105, 109], [22, 25], [74, 80]]	[[81, 104]]	['MSRA', 'NER', 'SIGHAN']	['Microsoft Research Asia']
2243	  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 67?68, Gothenburg, Sweden, April 27, 2014.	[[71, 76]]	[[37, 69]]	['HyTra']	['Hybrid Approaches to Translation']
2244	functions to SGML-mark the input.  Fast Partial Parser (FPP) .  The ultimate 	[[56, 59], [13, 17]]	[[35, 54]]	['FPP', 'SGML']	['Fast Partial Parser']
2245	contrast, L/RMC = left/right-most char,  L/RMW = left/right-most word, VS = vowel  sequences, HYPH = hyphenation, CASE =  case, PM = parenthesized material.	[[94, 98], [10, 15], [41, 46], [71, 73], [114, 118], [128, 130]]	[[101, 112], [18, 38], [49, 69], [76, 92], [122, 126], [133, 155]]	['HYPH', 'L/RMC', 'L/RMW', 'VS', 'CASE', 'PM']	['hyphenation', 'left/right-most char', 'left/right-most word', 'vowel  sequences', 'case', 'parenthesized material']
2246	The next-to-last  column shows the precision (PRE)--the true positives divided by all verbs that Lerner  judged to be +S. The final column shows the recall (REC)--the true positives divided  by all verbs that were judged +S by hand.	[[157, 160], [46, 49]]	[[149, 155], [35, 44]]	['REC', 'PRE']	['recall', 'precision']
2247	Transactions of the Association for Computational Linguistics (TACL), 1:25?36. 	[[63, 67]]	[[0, 61]]	['TACL']	['Transactions of the Association for Computational Linguistics']
2248	 The improvement of PAS analysis would benefit many natural language processing (NLP) applications, such as information extraction, summariza-	[[81, 84], [20, 23]]	[[52, 79]]	['NLP', 'PAS']	['natural language processing']
2249	we use the DUC2002 and DUC2004 data sets, both of which are open benchmark data sets from Document Understanding Conference (DUC) for generic automatic summarization evaluation.	[[125, 128], [11, 14], [23, 26]]	[[90, 123]]	['DUC', 'DUC', 'DUC']	['Document Understanding Conference']
2250	characters ? are often referred to by pronouns or definite noun phrases (NPs) instead of explicit repetition. 	[[73, 76]]	[[59, 71]]	['NPs']	['noun phrases']
2251	 The two-level analysis of the cited forms ap-  pears below ST = sm'face tape, PT -- pattern  tape, 115.\[' -- root tape, VT : vocal{sin tat)e , and 	[[60, 62]]	[[65, 77]]	['ST']	"[""sm'face tape""]"
2252	exploited in (Goldberg and Zhu, 2006), which seeks document sentiments as an output of an optimisation problem (OPTIM) and the algorithm adopted by (Wu et al2009), that uses ranking	[[112, 117]]	[[90, 110]]	['OPTIM']	['optimisation problem']
2253	 4KEY: E1S=singular first person ergative, INC=incompletive, PART=particle, PREP=preposition, PRON=pronoun, NEG=negation, 37	[[61, 65], [76, 80], [94, 98], [108, 111], [7, 10], [43, 46]]	[[66, 74], [81, 92], [99, 106], [112, 120], [47, 59], [11, 41]]	['PART', 'PREP', 'PRON', 'NEG', 'E1S', 'INC']	['particle', 'preposition', 'pronoun', 'negation', 'incompletive', 'singular first person ergative']
2254	and above 95% for determiners (DT). In addition, subjects (SS) have a score above 90%. In all these	[[59, 61], [31, 33]]	[[49, 57], [18, 28]]	['SS', 'DT']	['subjects', 'determiner']
2255	2.4.1 English Gigaword We created large-scale n-gram language models using English Gigaword Second Edition6 (EGW). 	[[109, 112]]	[[75, 91]]	['EGW']	['English Gigaword']
2256	For POS tagging, the three main error categories are the confusion between adverbs (AD) and verbs with an  adverbial force, between measure words (M) and 	[[84, 86], [4, 7]]	[[75, 82], [132, 139]]	['AD', 'POS']	['adverbs', 'measure']
2257	three stages. A source-language input string is rewritten to form an information retrieval (IR) query.	[[92, 94]]	[[69, 90]]	['IR']	['information retrieval']
2258	The open class  word (host) tends to be uninflected, and only  the light verb (LV) carries tense, agreement  and aspect markers.	[[79, 81]]	[[67, 77]]	['LV']	['light verb']
2259	Table 1. Coverage and accuracy of each derived feature for RTE3 revised development collection  (RTE3Devmt), the RTE3 Test collection (RTE3Test ) and the entire RTE2 collection (RTE2All). 	[[135, 143], [59, 63], [97, 106], [161, 165], [178, 185]]	[[113, 133]]	['RTE3Test', 'RTE3', 'RTE3Devmt', 'RTE2', 'RTE2All']	['RTE3 Test collection']
2260	1.2 The Binding Module The output of grammatical modules is then fed onto the Binding Module(BM) which activates an algorithm for anaphoric binding in LFG terms	[[93, 95], [151, 154]]	[[78, 92]]	['BM', 'LFG']	['Binding Module']
2261	"followed by a verbal suffix"". This is to cover general  verb inflection, for both auxiliaries (AUX +) and main  verbs (AUX -)."	[[95, 100], [119, 122]]	[[82, 93]]	['AUX +', 'AUX']	['auxiliaries']
2262	directed approach according to (Ephraim and Malah, 1985) based on two different noise estimation schemes, i.e. the minimum statistics approach (MS) as described in (Martin, 2001) and the minimum	[[144, 146]]	[[115, 133]]	['MS']	['minimum statistics']
2263	Recently, a larger set of word relatedness judgments was obtained by (Finkelstein et al, 2002) in the WordSimilarity-353 (WS-353) collection. Despite the	[[122, 128]]	[[102, 120]]	['WS-353']	['WordSimilarity-353']
2264	ferent levels. For each text pair on four cross levels, i.e., Paragraph to Sentence (P-S), Sentence to Phrase (S-Ph), Phrase to Word (Ph-W) and Word	[[85, 88], [111, 115], [134, 138]]	[[62, 83], [91, 109], [118, 132]]	['P-S', 'S-Ph', 'Ph-W']	['Paragraph to Sentence', 'Sentence to Phrase', 'Phrase to Word']
2265	 Acknowledgments This research was supported by the Deutsche Forschungsgemeinschaft (DFG) in the Center of Excellence in ?	[[85, 88]]	[[52, 83]]	['DFG']	['Deutsche Forschungsgemeinschaft']
2266	5 Conclusions We have presented a sequential semantic role labeling system for the Semeval-2007 task 17 (SRL). 	[[105, 108]]	[[68, 95]]	['SRL']	['system for the Semeval-2007']
2267	mentary ASR systems, a technique first proposed in the context of NIST?s ROVER system (Fiscus, 1997) with a 12% relative error reduction (RER), and subsequently widely employed in many ASR	[[138, 141], [8, 11], [185, 188]]	[[112, 136]]	['RER', 'ASR', 'ASR']	['relative error reduction']
2268	 The application will eventually be deployed using a Software as a Service (SaaS) model. It will	[[76, 80]]	[[53, 74]]	['SaaS']	['Software as a Service']
2269	Collins et al. ( 2008) proposed simple exponentiated gradient (EG) algorithm for Conditional Random Feild (CRF).	[[63, 65], [107, 110]]	[[39, 61], [81, 105]]	['EG', 'CRF']	['exponentiated gradient', 'Conditional Random Feild']
2270	The meaning of the abbreviations is as follows (for definitions see Section 1): Incr = Incrementality; DP = Discriminatory Power; Train = Trainability; Type = Hardwired Type Selection; Hum = Human Preference Modelling; FB = Full Brevity .	[[219, 221], [103, 105], [130, 135], [185, 188], [169, 173]]	[[224, 236], [108, 128], [138, 150], [191, 196], [152, 156]]	['FB', 'DP', 'Train', 'Hum', 'Type']	['Full Brevity', 'Discriminatory Power', 'Trainability', 'Human', 'Type']
2271	FEDERAL DATA ENCRYPTION STANDARD APPROVED BY COMMERCE DEPARTMENT  A data encryption algorithm, designed to protect digital information, was  approved in November ad a Federal .Information Processing Standard (FIPS)  by the Department of Commerce.	[[209, 213]]	[[167, 207]]	['FIPS']	['Federal .Information Processing Standard']
2272	Free word associations are the words people spontaneously come up with in re-sponse to a stimulus word. Such informa-tion has been collected from test persons and stored in databases.  A well known example is the Edinburgh Associative Thesaurus (EAT). We will show in this paper that this kind of knowledge can be acquired automatically from corpora, en-abling the computer to produce similar associative responses as people do.	[[246, 249]]	[[213, 244]]	['EAT']	['Edinburgh Associative Thesaurus']
2273	(Joachims, 1999) software). In it, we implemented: the String Kernel (SK), the Syntactic Tree Kernel (STK), the Shallow Semantic Tree Kernel	[[70, 72]]	[[55, 68]]	['SK']	['String Kernel']
2274	storing our storage. We built a pattern matching  system based on Finite State Automata(FSA). After 	[[88, 91]]	[[66, 87]]	['FSA']	['Finite State Automata']
2275	The target set is built using the ? 88-?89 Wall Street Journal Corpus (WSJ) tagged using the (Ratnaparkhi, 1996) tagger and	[[71, 74]]	[[43, 62]]	['WSJ']	['Wall Street Journal']
2276	lexicon tool, with a classification phase based on  Featured-Based kernel such as SL kernel and TreeBased kernel such as Dependency tree (DT) kernel  (Culotta and Sorensen, 2004) and Phrase Structure 	[[138, 140], [82, 84]]	[[121, 136]]	['DT', 'SL']	['Dependency tree']
2277	Symbol Descriptor Example  If. I Simple connection between (IMPEDANCE) GA TAF~I  I|.2 	[[60, 69]]	[[31, 58]]	['IMPEDANCE']	['I Simple connection between']
2278	 1 Introduction Synchronous contex free grammars (SCFGs) generalize traditional context-free grammars to generate	[[50, 55]]	[[16, 48]]	['SCFGs']	['Synchronous contex free grammars']
2279	Association measure Following the aforementioned studies, we implemented these popular measures: pointwise mutual information (PMI), log-likelihood ratio (LL) and chi-square (?	[[127, 130], [155, 157]]	[[97, 125], [133, 147]]	['PMI', 'LL']	['pointwise mutual information', 'log-likelihood']
2280	Given an input pair (q,a), where q is a question and a is a candidate answer, first we retrieve the word embeddings (WEs) of both q and a. Then, we separately apply a	[[117, 120]]	[[100, 115]]	['WEs']	['word embeddings']
2281	Therefore, the standard metrics widely used in sequential topic segmentation for monologues and dialogs, such as Pk and WindowDiff(WD), are also not applicable.	[[131, 133], [113, 115]]	[[120, 129]]	['WD', 'Pk']	['WindowDif']
2282	 English For English dataset, we follow the standard splits of Penn Treebank (PTB), using sections 2-21 for training, section 22 as de-	[[78, 81]]	[[63, 76]]	['PTB']	['Penn Treebank']
2283	1 Introduction For the past three decades, there has been a great deal of work on the automatic identification (ID) of languages from the speech signal alone.	[[112, 114]]	[[96, 110]]	['ID']	['identification']
2284	2004. On clusterings: Good, bad and spectral. Journal of the ACM (JACM), 51(3):497?515.	[[66, 70]]	[[46, 64]]	['JACM']	['Journal of the ACM']
2285	Broadcast News (BN), Newswire (NW), Broadcast Conversation (BC), Telephone Conversation (TC), Web Blog (WB) and Magazine (MZ). 	[[104, 106], [122, 124]]	[[94, 102], [112, 120]]	['WB', 'MZ']	['Web Blog', 'Magazine']
2286	IDF Approach. Proceedings of International Conference on Language Resources and Evaluation (LREC) 2012, European Language Resources Association	[[92, 96], [0, 3]]	[[57, 75]]	['LREC', 'IDF']	['Language Resources']
2287	This paper presents the UNITOR system that participated in the *SEM 2013 shared task on Semantic Textual Similarity (STS). The task is	[[117, 120], [24, 30], [64, 67]]	[[88, 115]]	['STS', 'UNITOR', 'SEM']	['Semantic Textual Similarity']
2288	figure 1. To avoid confusion, we refer to this basic  unit throughout as a Temporal Unit (TU). 	[[90, 92]]	[[75, 88]]	['TU']	['Temporal Unit']
2289	(Koehn, 2004a). Furthermore, they extendedWSD to phrase sense disambiguation (PSD) (Carpuat and Wu, 2007a).	[[78, 81]]	[[49, 76]]	['PSD']	['phrase sense disambiguation']
2290	in a fully automatic fashion. Again, this is an exciting possibility that would solve the current bottleneck of supervised word sense disambiguation (WSD) methods (namely, that sense-tagged corpora are very costly to acquire).	[[150, 153]]	[[123, 148]]	['WSD']	['word sense disambiguation']
2291	Hypernym Hyponyms Co-Hyponyms Figure 2: The proposed semantic word embedding (SWE) learning framework (The left part denotes the state-of-the-art skip-gram model; The right part represents the semantic constraints).	[[78, 81]]	[[53, 76]]	['SWE']	['semantic word embedding']
2292	Stephanie Strassel and Zhiyi Song and Joe Ellis University of Pennsylvania Linguistic Data Consortium (LDC) Philadelphia, PA, USA	[[103, 106], [126, 129], [122, 124]]	[[75, 101]]	['LDC', 'USA', 'PA']	['Linguistic Data Consortium']
2293	CoTrain vs. BaseCN2 0.000144 4.77E-05 0.000247 CoTrain vs. BaseCN3 0.0009 0.000287 0.00139 CoTrain vs. LEX(CN) 9.53E-10 7.15E-11 1.17E-09 CoTrain vs. LEX(EN) 1.87E-05 1.64E-05 8.92E-07	[[107, 109], [150, 153], [154, 156]]	[[91, 98]]	['CN', 'LEX', 'EN']	['CoTrain']
2294	Patrizia Paggio University of Copenhagen Centre for Language Technology (CST) Njalsgade 140, 2300-DK Copenhagen	[[73, 76], [93, 100]]	[[41, 71]]	['CST', '2300-DK']	['Centre for Language Technology']
2295	 ? Named entity (NE) representation in KBs poses another NED challenge.	[[17, 19], [39, 42], [57, 60]]	[[3, 15]]	['NE', 'KBs', 'NED']	['Named entity']
2296	post-process of the internal diacritization task  using the same machine learning approach that  was trained on Base phrase (BP)-Chunk as well  as POS features of individual tokens with correct 	[[125, 127], [147, 150]]	[[112, 123]]	['BP', 'POS']	['Base phrase']
2297	ODQA system, SPIQA. The system derives disambiguating queries (DQs) that draw out additional information.	[[63, 66], [0, 4], [13, 18]]	[[39, 61]]	['DQs', 'ODQA', 'SPIQA']	['disambiguating queries']
2298	initions of state cannot be sensitive to (sometimes critical) aspects of the dialogue context, such as the user?s last dialogue move (DM) (e.g. requesthelp) unless that move directly affects the status of	[[134, 136]]	[[119, 132]]	['DM']	['dialogue move']
2299	English?German 45.59 43.72 Automatically aligned corpora average 47.99?4.20 45.75?3.64 Table 1: The grammatical coverage (GC) of NF-ITG for different corpora dependent on the interpretation of word alignments: contiguous Translation Equivalence or discontiguous Translation Equivalence	[[122, 124], [129, 135]]	[[100, 120]]	['GC', 'NF-ITG']	['grammatical coverage']
2300	   grandmother. CL.1SG.GEN ALL ART=airport  my grandmother to the airport 	[[31, 34], [16, 26]]	[[35, 42]]	['ART', 'CL.1SG.GEN']	['airport']
2301	of the set of terminal symbols) or empty strings. A Phrase Structure Tree (PST) is a tree in which all and only the leaf nodes are labeled with words or	[[75, 78]]	[[52, 73]]	['PST']	['Phrase Structure Tree']
2302	the literature. In a recent Japanese NE workshop, a maximum entropy (ME) system outperformed decision tree sys-	[[69, 71]]	[[52, 67]]	['ME']	['maximum entropy']
2303	 The pattern we used consists of a mix between the  part of speech (POS) tags and the mention tags for  the words in the training instance.	[[68, 71]]	[[52, 66]]	['POS']	['part of speech']
2304	the impact of various kinds of physical degradation that pages may endure before they are scanned and processed using optical character recognition (OCR) software. 	[[149, 152]]	[[118, 147]]	['OCR']	['optical character recognition']
2305	2007), to machine translation (Wu and Fung 2009) and summarization (Melli et al 2005).  Most semantic role labeling (SRL) systems to date conceptualize the task as a supervised learning problem and rely on role-annotated data for model training.	[[117, 120]]	[[93, 115]]	['SRL']	['semantic role labeling']
2306	Elfardy H. and Diab M. 2012. Simplified guidelines for the creation of large scale dialectal arabic annotations. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC), Istanbul, Turkey. Elfardy H. and Diab M. 2013.	[[202, 206]]	[[167, 185]]	['LREC']	['Language Resources']
2307	tial state for HMM, then experiment with different inference algorithms such as ExpectationMaximization (EM), Variational Bayers (VB) or Gibbs sampling (GS).5 Gao and Johnson (2008) compare	[[130, 132], [15, 18], [105, 107], [153, 155]]	[[110, 128], [80, 103], [137, 151]]	['VB', 'HMM', 'EM', 'GS']	['Variational Bayers', 'ExpectationMaximization', 'Gibbs sampling']
2308	sub-tasks: ? Multimedia Information Network (MiNet) Construction: Construct MiNet from cross-media and cross-genre information (i.e. tweets, images, sentences of web doc-	[[45, 50], [76, 81]]	[[13, 43]]	['MiNet', 'MiNet']	['Multimedia Information Network']
2309	rithm to handle this setting. To do so, we use dynamic programming (DP) together with greedy search.	[[68, 70]]	[[47, 66]]	['DP']	['dynamic programming']
2310	ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrSF-12 27.03 30.20 21.62 24.81 28.57 34.01SF-123 26.83 30.34 21.34 24.64 28.77 34.24SF-NE 26.51 29.86 21.42 24.55 27.96 33.26SF-GD 26.22 29.48 20.33 23.72 28.30 33.83Gold 27.92 31.15 22.56 25.69 29.65 35.08 Table 5: Results with supervised selection of catenae with specified length (SF-12, SF-123) are more effective than combinations of SFeat with heuristic NomEnd (SF-NE) or GovDep (SF-GD). 	[[412, 417], [430, 435]]	[[383, 410]]	['SF-NE', 'SF-GD']	['SFeat with heuristic NomEnd']
2311	On the basis of these specifications, a mapping between VAML and Concrete AML (CAML) can be made. CAML	[[79, 83], [56, 60], [98, 102]]	[[65, 77]]	['CAML', 'VAML', 'CAML']	['Concrete AML']
2312	 Phrasometer ? The phrasometer feature (PM) is the summed log-likelihood of all n-grams the word	[[40, 42]]	[[19, 30]]	['PM']	['phrasometer']
2313	accessing semantic information represented in  input specifications, written in the form of the  Sentence Plan Language (SPL) (Kasper, 1989;  Bateman, 1997a), and in the knowledge base of 	[[121, 124]]	[[97, 119]]	['SPL']	['Sentence Plan Language']
2314	4.1 Compositional Neural Language Model (C-NLM) Compositional Neural Language Model (C-NLM) is a combination of a word representation learning	[[85, 90], [41, 46]]	[[48, 83], [4, 39]]	['C-NLM', 'C-NLM']	['Compositional Neural Language Model', 'Compositional Neural Language Model']
2315	system architecture  The data is stored in one central Resource  Repository (RR). As training data may change (for 	[[77, 79]]	[[55, 75]]	['RR']	['Resource  Repository']
2316	" At the shallowest level of attachment we find the conjunctions (CONJ+) +  w+ ? and?"	[[65, 70]]	[[51, 63]]	['CONJ+']	['conjunctions']
2317	oleary@cs.umd.edu Abstract The Text Analysis Conference (TAC) ranks summarization systems by their average score	[[57, 60]]	[[31, 55]]	['TAC']	['Text Analysis Conference']
2318	Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC?14), Reykjavik, Iceland, may. European Language Resources Association (ELRA). 	[[203, 207], [127, 134]]	[[162, 201], [92, 125]]	['ELRA', 'LREC?14']	['European Language Resources Association', 'Language Resources and Evaluation']
2319	adaptation led to a considerable improvement of +4.1 BLEU and large improvements in terms of METEOR and Translation Edit Rate (TER). We	[[127, 130], [53, 57], [93, 99]]	[[104, 125]]	['TER', 'BLEU', 'METEOR']	['Translation Edit Rate']
2320	EOPAS (PARADISEC tool) for text interlinear text and media analysis  2. NLTK (Natural Language Toolkit) for text analytics with linguistic data (Bird, Klein, & Loper, 2009)  3.	[[72, 76], [0, 5], [7, 16]]	[[78, 102]]	['NLTK', 'EOPAS', 'PARADISEC']	['Natural Language Toolkit']
2321	tract syntactic features for FB-LTAG.  We use with Sejong Treebank (SJTree) which  contains 32 054 eojeols (the unity of segmenta-	[[68, 74]]	[[51, 66]]	['SJTree']	['Sejong Treebank']
2322	lem, and our system adopted a supervised learning approach with Maximum Entropy classifier, which is widely used in natural language processing(NLP). 	[[144, 147]]	[[116, 142]]	['NLP']	['natural language processin']
2323	In Proc. of the Conference on Computational Natural Language Learning (CoNLL), 7.	[[71, 76]]	[[30, 69]]	['CoNLL']	['Computational Natural Language Learning']
2324	there are usually three kinds of named entities (NEs) to be dealt with: names of persons (PER) , locations (LOC) and organizations (ORG).	[[108, 111], [49, 52], [90, 93], [132, 135]]	[[97, 106], [33, 47], [81, 87], [117, 129]]	['LOC', 'NEs', 'PER', 'ORG']	['locations', 'named entities', 'person', 'organization']
2325	2011.  Icelandic Parsed Historical Corpus (IcePaHC). 	[[43, 50]]	[[7, 41]]	['IcePaHC']	['Icelandic Parsed Historical Corpus']
2326	easily inspectable. The generalizing ability of the evolutionary reinforcement learning (RL) algorithm, XCS, can dramatically reduce the size of the opti-	[[89, 91], [104, 107]]	[[65, 87]]	['RL', 'XCS']	['reinforcement learning']
2327	 1 Introduction  Lexical Acquisition (LA) processes strongly rely on  basic assumptions embodied by the source informa- 	[[38, 40]]	[[17, 36]]	['LA']	['Lexical Acquisition']
2328	simardm@iro.umontreal.ca Abstract The term translation spotting (TS) refers to the task of identifying the target-language (TL)	[[65, 67]]	[[43, 63], [107, 122]]	['TS']	['translation spotting', 'target-language']
2329	Theoretically, the expressive power of converting the cospecs of a GLS into DCG parse rules i s equivalent to the power of a Lexicalized Tree Adjoining Grammar with collocations (Shieber[14]) , what we have termed Hyper Lexicalized Tree Adjoining Grammars (HTAGs) (Pustejovsky[13]) . 	[[257, 262], [67, 70], [76, 79]]	[[214, 255]]	['HTAGs', 'GLS', 'DCG']	['Hyper Lexicalized Tree Adjoining Grammars']
2330	7 8  Figure 2: The Sense Distribution  the help of a graphical user intefface(GUI) scans a  parsed sample article and indicates a series of se- 	[[78, 81]]	[[53, 76]]	['GUI']	['graphical user inteffac']
2331	tice to both characteristics mentioned above. The central construct in this framework is  that of context factor (CF). A CF is defined by a scope, which is a collection of individ- 	[[114, 116], [121, 123]]	[[98, 112]]	['CF', 'CF']	['context factor']
2332	2003) ? Minimum Error Rate Training (MERT) (Och, 2003) on a held-out development set, target	[[37, 41]]	[[8, 35]]	['MERT']	['Minimum Error Rate Training']
2333	ring Expression Generation Task (TUNA-REG) organised by Albert Gatt, Anja Belz and Eric Kow; the two GREC Challenges, GREC Main Subject Reference Generation (GREC-MSR) and GREC Named Entity Generation (GREC-NEG), organised by Anja Belz, Eric Kow, Jette Viethen and Albert Gatt; and the Giving Instructions in	[[202, 210], [33, 41], [101, 105], [118, 122], [158, 166]]	[[172, 200]]	['GREC-NEG', 'TUNA-REG', 'GREC', 'GREC', 'GREC-MSR']	['GREC Named Entity Generation']
2334	velopment and 23 for testing. Chinese data were extracted from the Penn Chinese Treebank (CTB) (Xue et al, 2005); articles 001-270 and 440-	[[90, 93]]	[[72, 88]]	['CTB']	['Chinese Treebank']
2335	 These classifiers are based on a discriminative model: Support Vector Machine (SVM)6 (Vapnik, 1995).	[[80, 83]]	[[56, 78]]	['SVM']	['Support Vector Machine']
2336	There is no person boiling noodles A woman is boiling noodles in water Example 9051 (ENTAILMENT) A pair of kids are sticking out blue and green colored tongues	[[85, 95]]	[[71, 83]]	['ENTAILMENT']	['Example 9051']
2337	qnp?0.6 NP(qdet, qn), qnp?0.4 NP(qnp, qpp), qpp?1.0 PP(qprep, qnp), qdet?1.0 DET(the),	[[52, 54], [77, 80], [30, 32], [8, 10]]	[[56, 60]]	['PP', 'DET', 'NP', 'NP']	['prep']
2338	WordNet  In another related effort, SRI performed experiments  in utilizing WordNet (WN) as a knowledge source for  IE.	[[85, 87], [36, 39], [116, 118]]	[[76, 83]]	['WN', 'SRI', 'IE']	['WordNet']
2339	 One suggestion is is to use as a natural anguage  grammar the Core Language Engine (CLE)  (Alshawi 1992).	[[85, 88]]	[[63, 83]]	['CLE']	['Core Language Engine']
2340	variance distribution. In the feature space, a support vector machine (SVM) classifier (Vapnik, 1995) is used to determine the likelihoods	[[71, 74]]	[[47, 69]]	['SVM']	['support vector machine']
2341	 1 I n t roduct ion   The development of Natural Language (NL) systems  for data retrieval has been a central issue in NL Pro- 	[[59, 61], [119, 121]]	[[41, 57]]	['NL', 'NL']	['Natural Language']
2342	O-ADVL = Object Adverbial: lie ran two miles.  APP = Apposition: Helsinki, the capital of Finland,  N = Title: King George and Mr. 	[[47, 50], [0, 6]]	[[53, 63], [9, 25], [104, 109]]	['APP', 'O-ADVL']	['Apposition', 'Object Adverbial', 'Title']
2343	learning is straightforward.  Very Reduced Regular Expression (VRRE):  Given a finite alphabet E,  the set of very 	[[63, 67]]	[[30, 61]]	['VRRE']	['Very Reduced Regular Expression']
2344	pendency and constituency tracks are shown in table 1. The label attachment score (LAS) was used by the organizer for evaluating the dependency versions,	[[83, 86]]	[[59, 81]]	['LAS']	['label attachment score']
2345	2.1 Download Initial Collection        The Yahoo Full Coverage Collection (YFCC) was  downloaded from http://fullcoverage.yahoo.com during 	[[75, 79]]	[[43, 73]]	['YFCC']	['Yahoo Full Coverage Collection']
2346	processing. In Proceedings of the Conference on  Knowledge Capture (K-CAP), pages 70-77, 2003. 	[[68, 73]]	[[49, 66]]	['K-CAP']	['Knowledge Capture']
2347	 2 In Pisa two dictionaries of Italian, ,are being used: the  Nuovo Dizionario Garzanti (GRZ) and the Dlzionario-  Macchina dell'ltaliano (DMI), a MRD mainly based on the 	[[89, 92], [139, 142], [147, 150]]	[[79, 87], [102, 137]]	['GRZ', 'DMI', 'MRD']	"['Garzanti', ""Dlzionario-  Macchina dell'ltaliano""]"
2348	knowledge from a corpus\[2\]\[6\]\[91.  Machine translation (MT) systems are no  exception.	[[61, 63]]	[[40, 59]]	['MT']	['Machine translation']
2349	lines and web-gathered word lists. Theses grammars are represented by Finite State Machines (FSMs) (thanks to the AT&T GRM/FSM toolkit (Allauzen et	[[93, 97], [114, 118], [119, 126]]	[[70, 91]]	['FSMs', 'AT&T', 'GRM/FSM']	['Finite State Machines']
2350	 This named-entity tagger program is based on a first order Maximum Entropy Markov Model (MEMM) and is described in Yoshida and Tsujii (2007).	[[90, 94]]	[[60, 88]]	['MEMM']	['Maximum Entropy Markov Model']
2351	the first reference in this study. ( 3) a small dataset of Wikipedia articles (WIKI) to extend our corpus and metric evaluation to topics beyond the	[[79, 83]]	[[59, 77]]	['WIKI']	['Wikipedia articles']
2352	2008; Metzler and Cai, 2011).  Work in Content Based Image Retrieval (CBIR) (Datta et al., 2008) has progressed from systems that	[[70, 74]]	[[39, 68]]	['CBIR']	['Content Based Image Retrieval']
2353	There are two machine learning tasks in our problem. The first is Dialogue Act (DA) Tagging, in which we assign DAs to every Dialogue Func-	[[80, 82], [112, 115]]	[[66, 78]]	['DA', 'DAs']	['Dialogue Act']
2354	1 This work has been developed in the project KFr-FAST (KIT = Kilnstliche  Intelligenz und Textverstehen (Artificial Intelligence and Text  Understanding); FAST = Functor Argument S ructure for Translation), which  constitutes the Berlin component of the complementary research project of 	[[156, 160]]	[[163, 189]]	['FAST']	['Functor Argument S ructure']
2355	 The obtained Spanish scores as compared to the  scores from the initial English experiment (E-E-E)  are shown in figure 5.	[[93, 98]]	[[73, 91]]	['E-E-E']	['English experiment']
2356	(approx. 68,000 sentences, 1.4 million tokens), (2) Brown Corpus (BROWN) (approx. 60,000	[[66, 71]]	[[52, 57]]	['BROWN']	['Brown']
2357	This makes it hard to find particular information of in-terest (say, a mention of a particular company in a set of thousands of YouTube comments), or to un-derstand the gist of the discussion at a high-level. Our goal in this work was to create a simple tool which would allow people to rapidly ingest useful information contained in large community-created comment threads, where the volume of data precludes manual inspection. To this end, we created CoFi (Comment Filter), a language-independent, web-based interactive browser for single comment threads. 2 How CoFi works For a given set of comments, we create a distinct CoFi instance.	[[453, 457], [625, 629], [564, 568]]	[[459, 473]]	['CoFi', 'CoFi', 'CoFi']	['Comment Filter']
2358	a mistake when we generate the final output that results in a lower score of 56.31% in term of Labeled Attachment Score (LAS), reported by organizers.	[[121, 124]]	[[95, 119]]	['LAS']	['Labeled Attachment Score']
2359	and embedded phrase levels: ? Object reordering (ObjR), in which the objects and their dependents are moved in front	[[49, 53]]	[[30, 47]]	['ObjR']	['Object reordering']
2360	Part of FrameNet is also a corpus of 135,000 annotated example sentences from the British National Corpus (BNC). 	[[107, 110]]	[[82, 105]]	['BNC']	['British National Corpus']
2361	and tests 2 and 3 are open tests performed on different test data. DM (i.e., Default Model) assigns all  incoming cases with the most likely class and it is 	[[67, 69]]	[[77, 90]]	['DM']	['Default Model']
2362	task. As reported in (Liu et al 2013a), we used a genetic algorithm (GA) (Cormen et al 2001) to au80	[[69, 71]]	[[50, 67]]	['GA']	['genetic algorithm']
2363	for the task: a training dataset (TrainSet) with 9675 messages directly retrieved from Twitter; a development dataset (DevSet), with 1654 messages; the testing dataset from 2013 run, which	[[119, 125], [34, 42]]	[[98, 117], [16, 32]]	['DevSet', 'TrainSet']	['development dataset', 'training dataset']
2364	Suffixes (S): able, est, ful, ic, ing, ive, ness etc.  Word Sentiment Polarity (SP): POS, NEG, NEU Pivoting on the head aspect, we look forward and	[[80, 82]]	[[60, 78]]	['SP']	['Sentiment Polarity']
2365	2.2 Keystroke Ratio (KSR) In addition to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric (Och et al, 2003).	[[127, 130], [21, 24]]	[[110, 125], [4, 19]]	['KSR', 'KSR']	['keystroke ratio', 'Keystroke Ratio']
2366	tagger (Cutting et al 1992) and LT POS tagger  (Mikheev 1997). Maximum Entropy (MaxEnt)  based taggers also seem to perform very well        	[[80, 86], [32, 34], [35, 38]]	[[63, 78]]	['MaxEnt', 'LT', 'POS']	['Maximum Entropy']
2367	This paper presents the UNL graph matching method for the Semantic Textual Similarity(STS) task. 	[[86, 89], [24, 27]]	[[58, 84]]	['STS', 'UNL']	['Semantic Textual Similarit']
2368	tends the comparison set to players of AS Roma.  Prepositional phrases (PPs), gerunds, and relative clauses introduce additional complexity.	[[72, 75], [39, 41]]	[[49, 70]]	['PPs', 'AS']	['Prepositional phrases']
2369	Score(E), where E is an example of Pat To improve ranking, we also try to find the longest similar subsequence (LSS) between the user input, Sent and retrieved example, Exm	[[112, 115]]	[[83, 110]]	['LSS']	['longest similar subsequence']
2370	exp (14)    Short word difference penalty (SWDP): a  good translation should have roughly the same 	[[43, 47]]	[[12, 41]]	['SWDP']	['Short word difference penalty']
2371	DSTG = Adverb s t r i r ig   mRTOVO = For + Subject -+ to -+ Object  NA = N t- Adjective  NASOBJBE - N + as - t- Object  of be -	[[69, 71], [0, 4], [29, 35], [90, 98]]	[[74, 88]]	['NA', 'DSTG', 'mRTOVO', 'NASOBJBE']	['N t- Adjective']
2372	 1 Introduction Traditionally, Information Retrieval (IR) and Statistical Natural Language Processing (NLP) applica-	[[54, 56], [103, 106]]	[[31, 52], [74, 101]]	['IR', 'NLP']	['Information Retrieval', 'Natural Language Processing']
2373	This paper explores the use of the homotopy method for training a semi-supervised Hidden Markov Model (HMM) used for sequence labeling.	[[103, 106]]	[[82, 101]]	['HMM']	['Hidden Markov Model']
2374	from the output of the parser we adopt a uniform meaning representation which is a structured Logical Form(LF). In other words we map our f-	[[107, 109]]	[[94, 106]]	['LF']	['Logical Form']
2375	The resulting unit denominates a concept which belongs to the language for special purposes (LSP). 	[[93, 96]]	[[62, 91]]	['LSP']	['language for special purposes']
2376	2011.  Overview of the Infectious Diseases (ID) task of BioNLP Shared Task 2011.	[[44, 46], [56, 62]]	[[23, 42]]	['ID', 'BioNLP']	['Infectious Diseases']
2377	1425  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 75?81, Gothenburg, Sweden, April 27, 2014.	[[75, 80], [84, 88]]	[[41, 73]]	['HyTra', 'EACL']	['Hybrid Approaches to Translation']
2378	Var. 47.9 60.7 67.9 70.8 75.0 77.3 Pronunciation (PHL) with Pron. Var.	[[50, 53], [66, 69], [60, 64]]	[[35, 48]]	['PHL', 'Var', 'Pron']	['Pronunciation']
2379	the reference than the rest. Considering this, we use a Longest Common Subsequence(LCS) based criterion to calculate s(x, y).	[[83, 86]]	[[56, 81]]	['LCS']	['Longest Common Subsequenc']
2380	 1 Introduction Statistical machine translation (SMT) starts from sequence-based models.	[[49, 52]]	[[16, 47]]	['SMT']	['Statistical machine translation']
2381	Upsala College  INTRODUCTION  A computerized conference (CC) is a form of co~znunica-  tion in which participants type into and read frc~ a 	[[57, 59]]	[[32, 55]]	['CC']	['computerized conference']
2382	redundancy at low and medium allophonic complexities, estimated by the Jaccard indices between their false positives (FP) and false negatives (FN). 	[[118, 120], [143, 145]]	[[101, 116], [126, 141]]	['FP', 'FN']	['false positives', 'false negatives']
2383	entries in an existing knowledge base is called entity linking and has been proposed and studied in the Knowledge Base Population (KBP) track of the Text Analysis Conference (TAC) (McNamee and Dang,	[[131, 134], [175, 178]]	[[104, 129], [149, 173]]	['KBP', 'TAC']	['Knowledge Base Population', 'Text Analysis Conference']
2384	left AV and right AV. For a string s with length l, we define the left accessor variety (LAV) as the types of distinct characters preceding s in	[[89, 92], [5, 7], [18, 20]]	[[66, 87]]	['LAV', 'AV', 'AV']	['left accessor variety']
2385	ACL 2006 paper (see experiments). Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities	[[62, 65], [0, 3]]	[[34, 60]]	['CDC', 'ACL']	['Cross document coreference']
2386	 ? Backward Looking (BL)/Forward Looking (FL) features (14 to 22) are mostly extracted from ut-	[[42, 44], [21, 23]]	[[25, 40], [3, 19]]	['FL', 'BL']	['Forward Looking', 'Backward Looking']
2387	  We have also made a preliminary attempt to transfer a thesaurus entry from the Collins Thesaurus (CT) into  Italian by means of the English-Italian and Italian- 	[[100, 102]]	[[81, 98]]	['CT']	['Collins Thesaurus']
2388	 Temporal Types Possible Values (tags) Timeline (TL) past, present, future Day of Week (DOW) Mon, Tue, . . . ,	[[49, 51], [88, 91]]	[[39, 47], [75, 86]]	['TL', 'DOW']	['Timeline', 'Day of Week']
2389	There are three options: French (FR), Spanish (SP), or, Merged languages (ML), where the results are obtained by merging the English output of FR	[[74, 76], [33, 35], [47, 49], [143, 145]]	[[56, 72], [25, 31], [38, 45]]	['ML', 'FR', 'SP', 'FR']	['Merged languages', 'French', 'Spanish']
2390	4-gram + LSA using linear interpolation  with ? LSA = 0.11 (LI). 	[[60, 62], [9, 12]]	[[48, 51], [19, 39]]	['LI', 'LSA']	['LSA', 'linear interpolation']
2391	experiments can be listed as follows.  Head Word (HW.) The predicate?s head word as	[[50, 53]]	[[39, 48]]	['HW.']	['Head Word']
2392	The two main Modern Standard Arabic dependency treebanks currently available are the Columbia Arabic Treebank (CATiB) (Habash and Roth, 2009) and the Prague Arabic Dependency	[[111, 116]]	[[85, 109]]	['CATiB']	['Columbia Arabic Treebank']
2393	Many researchers have attempted several tech-  niques to deal with extragrammatical sentences  such as Augmented Transition Network(ATN)  (Kwasny and Sondheimer, 1981), network-based 	[[132, 135]]	[[103, 130]]	['ATN']	['Augmented Transition Networ']
2394	>60 ICE ICE ICE ICE 10C 1~ 10C I0C  >.Y0 ICE ICE ICE ICE ICE IOC I(E 1~  >40 IO(J ICE ICE lO0 ICE ICE 1~ ICE  >35 ICE lO(l ICE ICE lie lOC IOC lOC 	[[77, 79]]	[[82, 93]]	['IO']	['ICE ICE lO0']
2395	 IHMM-based: He et al (2008) propose an  indirect hidden Markov model (IHMM) for hypothesis alignment.	[[71, 75], [1, 5]]	[[41, 69]]	['IHMM', 'IHMM']	['indirect hidden Markov model']
2396	We apply the combination patterns to the training corpus, and count pairs of True Positives (TP) and False Positives (FP). The scores are calculated	[[118, 120], [93, 95]]	[[101, 116], [77, 91]]	['FP', 'TP']	['False Positives', 'True Positives']
2397	ing via schema matching and lexicon extension. In Association for Computational Linguistics (ACL). 	[[93, 96]]	[[50, 91]]	['ACL']	['Association for Computational Linguistics']
2398	1125 VNC token expressions (CFS07 has 1180).  We then split them into a development (DEV) set and a test (TEST) set.	[[85, 88], [5, 8], [28, 33], [106, 110]]	[[72, 83], [100, 104]]	['DEV', 'VNC', 'CFS07', 'TEST']	['development', 'test']
2399	Although there is a modest cost associated with annotating data, we show that a reduction of 40% relative in alignment error (AER) is possible over the GIZA++ aligner (Och and Ney, 2003).	[[126, 129], [152, 158]]	[[109, 124]]	['AER', 'GIZA++']	['alignment error']
2400	At the semantic level, we have included three different families which operate using named entities (NE), semantic roles (SR), and discourse representations (DR).	[[101, 103], [122, 124], [158, 160]]	[[85, 99], [106, 120], [131, 156]]	['NE', 'SR', 'DR']	['named entities', 'semantic roles', 'discourse representations']
2401	and GL.  GL = GR or GL unspec. CC	[[14, 16], [4, 6], [9, 11], [31, 33]]	[[17, 19]]	['GR', 'GL', 'GL', 'CC']	['or']
2402	We use the following label set: S-O (not in maze); S-M (single word maze); B-M (beginning of multi-word 72	[[75, 78], [32, 35], [51, 54]]	[[80, 98], [56, 72]]	['B-M', 'S-O', 'S-M']	['beginning of multi', 'single word maze']
2403	In (Raymond and Riccardi, 2007), the SFST-based model is compared with Support Vector Machines (SVM) (Vapnik, 1998) and Conditional Random Fields (CRF) (Laf-	[[96, 99], [37, 41], [147, 150]]	[[71, 94], [120, 145]]	['SVM', 'SFST', 'CRF']	['Support Vector Machines', 'Conditional Random Fields']
2404	We used five retrieval systems to generate  relevance scores for query-document pairs:  Fuzzy Boolean (FB). This system translates a query 	[[103, 105]]	[[88, 101]]	['FB']	['Fuzzy Boolean']
2405	C, N, X, Y } (V = verb, AV = auxiliary verb, EV = verb with Ersatzinfinitiv, Vfin = finite verb, Vinf	[[24, 26], [45, 47], [77, 81]]	[[29, 43], [50, 75], [84, 95], [18, 22]]	['AV', 'EV', 'Vfin']	['auxiliary verb', 'verb with Ersatzinfinitiv', 'finite verb', 'verb']
2406	directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech	[[85, 88]]	[[63, 83]]	['DTW']	['dynamic time warping']
2407	"fornls of words. For instance, the rule  \[ S= ion I=  (NNS VBZ) R= (NN) M=8\]  says if by deleting the suffix ""ion"" from a word "	[[60, 63], [69, 71]]	[[56, 59]]	['VBZ', 'NN']	['NNS']
2408	evaluate the quality of the paraphrase collection.  In parcitular, Amazon?s Mechanical Turk1 (MTurk) provides a way to pay people small amounts of	[[94, 99]]	[[76, 92]]	['MTurk']	['Mechanical Turk1']
2409	we sample polysemous words from wide-domain {French,Chinese}-English corpora, and use Amazon?s Mechanical Turk (MTurk) to annotate word sense on the English side.	[[112, 117]]	[[95, 110]]	['MTurk']	['Mechanical Turk']
2410	scribe the relation R. Most previous systems perform these steps by first using named entity recognition (NER) to identify possible arguments and then using a simple string match, but this crude	[[106, 109]]	[[80, 104]]	['NER']	['named entity recognition']
2411	 1 The following abbreviations are used POSS = possessive prefix/suffix; LOC = locative suffix; OBV = obviative suffix;	[[40, 44], [73, 76], [96, 99]]	[[47, 57], [79, 87], [102, 111]]	['POSS', 'LOC', 'OBV']	['possessive', 'locative', 'obviative']
2412	and include them in the training data for the SMT.  Corpus Combination (CComb) The easiest method is to use these n newly created paral-	[[72, 77]]	[[52, 70]]	['CComb']	['Corpus Combination']
2413	Our systems use both corpus-based and knowledge-based approaches: Maximum Entropy(ME) (Lau et al, 1993; Berger et al, 1996; Ratnaparkhi, 1998) is	[[82, 84]]	[[66, 81]]	['ME']	['Maximum Entropy']
2414	fn is tfi, and foov is fq in Feature (1).  v. Df_Rank (D-Rank): It is similar to SRank and computed based on Rank(i)= 	[[55, 61], [81, 86]]	[[46, 53]]	['D-Rank', 'SRank']	['Df_Rank']
2415	Associated Press Worldstream English Service (APW) ? The New York Times Newswire Service (NYT) ? The Xinhua News Agency English Service (XIE) For each source, Gigaword articles are classified into several types, including newswire advisories, etc.  We restricted our investigations to actual news stories.	[[137, 140], [46, 49], [90, 93]]	[[101, 135], [0, 28], [57, 71]]	['XIE', 'APW', 'NYT']	['Xinhua News Agency English Service', 'Associated Press Worldstream', 'New York Times']
2416	 The task addressed in this paper is also related to the Semantic Textual Similarity (STS) task (Agirre et al, 2012).	[[86, 89]]	[[57, 84]]	['STS']	['Semantic Textual Similarity']
2417	dresses such conflicting constraints. In this method, the owner of the TM generates a Phrase Table (PT) from it, and makes it accessible to the user following	[[100, 102], [71, 73]]	[[86, 98]]	['PT', 'TM']	['Phrase Table']
2418	approaches (Gasic and Young, 2011; Lee and  Eskenazi, 2012; Williams, 2010; Young et al  2010) and Bayesian network (BN)-based  methods (Raux and Ma, 2011; Thomson and 	[[117, 119]]	[[99, 115]]	['BN']	['Bayesian network']
2419	 In addition, NE alignment can be very useful for  Statistical Machine Translation (SMT) and CrossLanguage Information Retrieval (CLIR).	[[84, 87], [14, 16], [130, 134]]	[[51, 82], [93, 128]]	['SMT', 'NE', 'CLIR']	['Statistical Machine Translation', 'CrossLanguage Information Retrieval']
2420	The tag B-X (Begin) represents the first word of a named entity of type X, for example, PER (Person) or LOC (Location). The tag I-X (In-	[[88, 91], [104, 107]]	[[93, 99], [109, 117]]	['PER', 'LOC']	['Person', 'Location']
2421	and Buckley, 1988), ResidualIDF(RIDF), Variance, Burstiness and Gain, are based on derivations from term frequency (TF) and document frequency (DF). 	[[116, 118], [144, 146], [32, 36]]	[[100, 114], [124, 142], [20, 31]]	['TF', 'DF', 'RIDF']	['term frequency', 'document frequency', 'ResidualIDF']
2422	Amongst the various learning algorithms available in QUEST, to make our results comparable we selected SVR with radial basis function (RBF) kernel, which has been shown to perform very well	[[135, 138], [53, 58], [103, 106]]	[[112, 133]]	['RBF', 'QUEST', 'SVR']	['radial basis function']
2423	from the remaining pool of data.  The intrinsic stopping criterion (ISC) we propose here focuses on the latter aspect of the ideal stop-	[[68, 71]]	[[38, 66]]	['ISC']	['intrinsic stopping criterion']
2424	 Snow et al (2008) explored the use of the Amazon Mechanical Turk (MTurk) web service for gathering annotations for a variety of natural lan-	[[67, 72]]	[[50, 65]]	['MTurk']	['Mechanical Turk']
2425	The results of dependency (ZPar-eager, Ours-standard, Ours-PS and Mate-tools) and constituent parsers (BerkeleyParser and ZPar-con) are measured by the unlabeled accuracy score (UAS), labeled accuracy score (LAS) and bracketing f-measure (BF), respectively. 	[[208, 211], [239, 241], [54, 61], [178, 181], [122, 130], [27, 37]]	[[184, 206], [217, 229], [152, 176]]	['LAS', 'BF', 'Ours-PS', 'UAS', 'ZPar-con', 'ZPar-eager']	['labeled accuracy score', 'bracketing f', 'unlabeled accuracy score']
2426	(A~.TG, ~ROC)I ^  (SUBS, SUSU, VT),  (ARTG, PR.OC)I ^  (SUBS, SUSU, VT)2  , ARTG = article g~n&al  SUBS = substantif  compl~ment 	[[76, 80], [99, 103], [4, 6], [9, 12], [19, 23], [25, 29], [31, 33], [38, 42], [44, 49], [56, 60], [62, 66], [68, 70]]	[[83, 97], [106, 116]]	['ARTG', 'SUBS', 'TG', 'ROC', 'SUBS', 'SUSU', 'VT', 'ARTG', 'PR.OC', 'SUBS', 'SUSU', 'VT']	['article g~n&al', 'substantif']
2427	grammatical frameworks (HLG). Combinatory  Categorial Grammars (CCG) (Steedman, 1987;  Steedman, 1996; Steedman, 1998; Steedman and 	[[64, 67], [24, 27]]	[[30, 62]]	['CCG', 'HLG']	['Combinatory  Categorial Grammars']
2428	 1 Introduction Coreference resolution (CR) ? the task of determin-	[[40, 42]]	[[16, 38]]	['CR']	['Coreference resolution']
2429	sures for each portion of the results. One is a relevance score (RS) with the target document  	[[65, 67]]	[[48, 63]]	['RS']	['relevance score']
2430	be compared, for any section of the corpus. The tool also calculates the majority tag (MajTag). Av-	[[87, 93]]	[[73, 85]]	['MajTag']	['majority tag']
2431	"planes, the ""READ""-units by AND-planes. The flip-  flops (FF) are simple register units and the shift  register is a simple PLA network of well  known "	[[58, 60], [124, 127]]	[[44, 56]]	['FF', 'PLA']	['flip-  flops']
2432	future research which are suggested by some af the techniques used in this program.  The SFRAME (semantic frame) concept. in which a sernantirl interpretation 	[[89, 95]]	[[97, 111]]	['SFRAME']	['semantic frame']
2433	guage varieties: the language of native speakers (N), the language of advanced, highly fluent nonnative speakers (NN), and translationese (T). We	[[114, 116], [50, 52], [139, 141]]	[[97, 103], [33, 39], [123, 137]]	['NN', 'N)', 'T)']	['native', 'native', 'translationese']
2434	First, we provide background on Open IE and how it relates to Semantic Role Labeling (SRL). Section 3 de-	[[86, 89], [37, 39]]	[[62, 84]]	['SRL', 'IE']	['Semantic Role Labeling']
2435	didate substitutes, as described below.  Lexical Baseline (LB): In this approach we use the pre-existing lexical resources to provide a rank-	[[59, 61]]	[[41, 57]]	['LB']	['Lexical Baseline']
2436	Understanding (NLU) framework (see below), while ASR includes features such as speech/nonspeech (SNS) detection and automatic gain control (AGC).	[[97, 100], [15, 18], [49, 52], [140, 143]]	[[79, 95], [116, 138]]	['SNS', 'NLU', 'ASR', 'AGC']	['speech/nonspeech', 'automatic gain control']
2437	can formulate natural language-like patterns as exploratory queries for relations against a text corpus.  We draw inspiration from the information seeking paradigm of Exploratory Search (ES) (Marchionini, 2006; White and Roth, 2009), where users start with a vaguely defined information need and - with a mix	[[187, 189]]	[[167, 185]]	['ES']	['Exploratory Search']
2438	The language is defined by about 50 simple grammar rules. ? P5E2N3S4, F W A Standard Language (SLANG). See Section 4.1. ?	[[95, 100]]	[[76, 93]]	['SLANG']	['Standard Language']
2439	Metonymy Often a sentence relates entities in a way inconsistent with the target ontology. For example, with the Component Library (CLib) ontology,movement properties (e.g., speed, acceleration) are defined as properties of the movement events, rather	[[132, 136]]	[[113, 130]]	['CLib']	['Component Library']
2440	stable functional definition across languages. These categories include NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition,	[[84, 87], [101, 104], [115, 118], [129, 132]]	[[90, 99], [107, 113], [121, 127], [135, 145]]	['ADJ', 'ADV', 'NUM', 'ADP']	['adjective', 'adverb', 'number', 'adposition']
2441	cal Dirichlet Process (HDP) (Teh et al, 2006), a Bayesian nonparametric variant of Latent Dirichlet Allocation (LDA), to automatically infer the number of topics.	[[112, 115], [23, 26]]	[[83, 110], [0, 21]]	['LDA', 'HDP']	['Latent Dirichlet Allocation', 'cal Dirichlet Process']
2442	Taggers have been developed for a variety of languages, including Modern Standard Arabic (MSA) (Khoja, 2001; Diab et al, 2004).	[[90, 93]]	[[66, 88]]	['MSA']	['Modern Standard Arabic']
2443	From every argument to the predicate, we extract all child  noun phrases (NP) and adjectival phrases (ADJP)  as candidate gaps as well.	[[74, 76], [102, 106]]	[[60, 72], [82, 100]]	['NP', 'ADJP']	['noun phrases', 'adjectival phrases']
2444	 2004. Document understanding conferences (DUC). 	[[43, 46]]	[[7, 41]]	['DUC']	['Document understanding conferences']
2445	 3.1 LSTMs for sequence generation A Recurrent Neural Network (RNN) is a generalization of feed forward neural networks to se-	[[63, 66], [5, 10]]	[[37, 61]]	['RNN', 'LSTMs']	['Recurrent Neural Network']
2446	con. In Proceedings of the 31st Annual Conference of the Cognitive Science Society (CogSci). 	[[84, 90]]	[[57, 82]]	['CogSci']	['Cognitive Science Society']
2447	audiences have little trouble mapping a collection  of noun phrases onto the same entity, this task of  noun phrase (NP) coreference r solution can present  a formidable challenge to an NLP system.	[[117, 119], [186, 189]]	[[104, 115]]	['NP', 'NLP']	['noun phrase']
2448	its right neighbour. Hence, the tree in Figure 2 is assumed to have the structure ((AB)C). 	[[84, 88]]	[[52, 81]]	['AB)C']	['assumed to have the structure']
2449	4.4 Word Sense Induction In this section, we present an evaluation of our model on the word sense induction (WSI) tasks. The	[[109, 112]]	[[87, 107]]	['WSI']	['word sense induction']
2450	response- and reference-based scoring methods. All models use support vector regression (SVR) (Smola and Sch?olkopf, 2004), with the complexity parame-	[[89, 92]]	[[62, 87]]	['SVR']	['support vector regression']
2451	Table 1: Labelled attachment score on the two test sets of the best single parse, blended with weights set to PoS labelled attachment score (LAS) and blended with learned weights.	[[141, 144], [110, 113]]	[[114, 139]]	['LAS', 'PoS']	['labelled attachment score']
2452	AO = all objects  MO = matched objects  TF = text filtering  FM = F-measures 	[[40, 42], [0, 2], [18, 20], [61, 63]]	[[45, 59], [5, 16], [23, 38], [66, 76]]	['TF', 'AO', 'MO', 'FM']	['text filtering', 'all objects', 'matched objects', 'F-measures']
2453	6.1 Corpus The training and test data sets were derived from the EventCorefBank (ECB) corpus5 created by Bejan and Harabagiu (2010) to study event coreference	[[81, 84]]	[[65, 79]]	['ECB']	['EventCorefBank']
2454	 2004. The Automatic Content Extraction (ACE) Program?Tasks, Data, and Evaluation.	[[41, 44]]	[[11, 39]]	['ACE']	['Automatic Content Extraction']
2455	 We use LibSVM (Chang and Lin, 2011), an implementation of Support Vector Machines (SVM) (Cortes and Vapnik, 1995), as the underlying tech-	[[84, 87], [8, 14]]	[[59, 82]]	['SVM', 'LibSVM']	['Support Vector Machines']
2456	Typically, the weights of the log-linear combination in Equation 3 are optimised by means of Minimum Error Rate Training (MERT) (Och, 2003).	[[122, 126]]	[[93, 120]]	['MERT']	['Minimum Error Rate Training']
2457	"O N S  *****  SCAN CALLED AT 1 I  ANTEST CALL'EC FOR 'I ""REDVO U (AACC) ,SD= 2 .  RES= 6."	[[49, 52], [66, 70], [73, 75], [82, 85]]	[[34, 48]]	['FOR', 'AACC', 'SD', 'RES']	"[""ANTEST CALL'EC""]"
2458	One part of the work is directed towards developing computational methods to facilitate the manual construction of SweFN. We have so far focused on three tasks: (1) semantic role labeling (SRL) (Johansson et al.,	[[189, 192], [115, 120]]	[[165, 187]]	['SRL', 'SweFN']	['semantic role labeling']
2459	The SCBD structure.  prepositional phrases (PPs), verb phrases (VPs), and adverbial phrases  (APs).	[[44, 47], [64, 67], [4, 8], [94, 97]]	[[21, 42], [50, 62], [74, 91]]	['PPs', 'VPs', 'SCBD', 'APs']	['prepositional phrases', 'verb phrases', 'adverbial phrases']
2460	SN = Sa-inflection oun (nominal verb)  CM = case marker (-nom/-acc argument)  PT = particle (other arguments)  VB = verb 	[[78, 80], [0, 2], [39, 41], [111, 113]]	[[83, 91], [5, 22], [44, 55], [116, 120]]	['PT', 'SN', 'CM', 'VB']	['particle', 'Sa-inflection oun', 'case marker', 'verb']
2461	evidences, making the coupled approach efficient enough to be applied to themore complex task of joint word segmentation (WS) and POS tagging for the first time.	[[122, 124], [130, 133]]	[[103, 120]]	['WS', 'POS']	['word segmentation']
2462	Researchers have ex-plored the topic of CLI in the areas of lexical style (Jarvis et al 2012a), lexical n-grams (Jarvis & Paquot, 2012), character n-grams (Tsur & Rappo-prot, 2007), using variables related to cohesion, lexical sophistication, syntactic complexity and conceptual knowledge (Crossley & McNamara, 2012), error patterns (Bestgen, et al 2012; Wong & Dras, 2009), and a combination of these ap-proaches (Jarvis et al 2012b; Koppel et al 2005; Mayfield Tomokiyo & Jones, 2001, Wong & Dras, 2009).  Such studies have demonstrated relatively strong success rates for classifying an L2 writing sample based on the L1 of the writer. For instance, Jarvis and Paquot (2012), using 1-4-grams as pre-dictor variables on a subset of argumentative es-says included in the International Corpus of Learner English (ICLE) (Granger et al 2009) achieved a 53.6% classification accuracy for 12 groups of L1s. Crossley and McNamara (2012) used features related to cohesion, lexical sophisti-cation, syntactic complexity, and conceptual knowledge taken from the computational tool Coh-	[[813, 817], [40, 43], [590, 592], [621, 623]]	[[772, 811]]	['ICLE', 'CLI', 'L2', 'L1']	['International Corpus of Learner English']
2463	any parser; the third requirement is not easily met  in all languages, but even in those languages where  nonrestrictives are not easily identifiable, (II)  works reasonable well.	[[152, 154]]	[[137, 149]]	['II']	['identifiable']
2464	The effectiveness of customer care in the email channel is measured using two competing metrics: Average Handling Time (AHT) and Customer Experience Evaluation (CEE).	[[120, 123], [161, 164]]	[[97, 118], [129, 159]]	['AHT', 'CEE']	['Average Handling Time', 'Customer Experience Evaluation']
2465	2004. A maxi-mum-entropy Chinese parser augmented by transformation-based learning. ACM Transactions on Asian Language Information Processing (TALIP) 3(2): 159-168. Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu.	[[143, 148]]	[[88, 141]]	['TALIP']	['Transactions on Asian Language Information Processing']
2466	higher indicating better.  We used Amazon?s Mechanical Turk (MTurk)5 to collect the human judgements.	[[61, 66]]	[[44, 59]]	['MTurk']	['Mechanical Turk']
2467	 We excluded only punctuation; we did no filtering for part of speech (POS). Each word was actually	[[71, 74]]	[[55, 69]]	['POS']	['part of speech']
2468	information. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 317?325, Columbus, USA.	[[100, 103], [131, 134]]	[[57, 98]]	['ACL', 'USA']	['Association for Computational Linguistics']
2469	tially freely-available sources: Family Practitioner Inquiry Network (FPIN)2, Parkhurst Exchange Forum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4 were used to design and develop the presented test	[[137, 143], [70, 74], [104, 106]]	[[114, 135], [33, 68], [78, 102]]	['BMJ-CE', 'FPIN', 'PE']	['BMJ Clinical Evidence', 'Family Practitioner Inquiry Network', 'Parkhurst Exchange Forum']
2470	 It considers all pronouns (PRP, PRP$), noun phrases (NP) and heads of verb phrases (VP) as potential mentions.	[[85, 87], [28, 31], [54, 56], [33, 37]]	[[71, 83], [40, 51]]	['VP', 'PRP', 'NP', 'PRP$']	['verb phrases', 'noun phrase']
2471	3 Bayes ian  networks   A Bayes ian  network  (Pearl, 1988), or  Bayesian 1)el|el nel;work (BBN),  eonsisi;s of a sol;  of var iab les  and a sel; of d i rec ted  edges  (:on- 	[[92, 95]]	[[65, 90]]	['BBN']	['Bayesian 1)el|el nel;work']
2472	Saccade Length (SL) Real Sum of saccade lengths (measured by number of words) divided by word count Simple Regression Count (REG) Real Total number of gaze regressions Gaze Skip count (SKIP) Real Number of words skipped divided by total word count	[[125, 128], [16, 18], [185, 189]]	[[107, 117], [0, 14]]	['REG', 'SL', 'SKIP']	['Regression', 'Saccade Length']
2473	to find words with the same meanings. We use a simple approach called the Direct Reversal (DR) approach in (Lam and Kalita, 2013) to create	[[91, 93]]	[[74, 89]]	['DR']	['Direct Reversal']
2474	 0.7  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random CoNLL-03	[[60, 62], [107, 110], [118, 126], [83, 86]]	[[39, 58], [63, 69]]	['ID', 'SVE', 'CoNLL-03', 'FIR']	['information density', 'Fisher']
2475	1. Introduction  Named entity(NE) recognition is important for recent  sophisticated information service such as question answering 	[[30, 32]]	[[17, 28]]	['NE']	['Named entit']
2476	monolingual applications and have been used in commercial grammar checkers.1 These parsers produce a logical form (LF) representation that is compatible across multiple languages (see	[[115, 117]]	[[101, 113]]	['LF']	['logical form']
2477	"Learning"". In Proceedings of the 3 rd ACL  Workshop on Very Large Corpora (WVLC95). "	[[75, 81]]	[[43, 73]]	['WVLC95']	['Workshop on Very Large Corpora']
2478	ceedings of the 10th International Conference on Text, Speech, and Dialogue (TSD-2007), Lecture Notes in Computer Science (LNCS), Springer-Verlag.	[[123, 127], [77, 80]]	[[88, 121]]	['LNCS', 'TSD']	['Lecture Notes in Computer Science']
2479	cos(d, c).  Candidate Rank (CR) The features described so far disambiguate every surface form s ?	[[28, 30]]	[[12, 26]]	['CR']	['Candidate Rank']
2480	4 Learning Algorithms We evaluated four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and de-	[[97, 100], [153, 155], [134, 137]]	[[72, 95], [140, 150], [103, 111]]	['SVM', 'NB', 'AdB']	['Support Vector Machines', 'Naive Baye', 'AdaBoost']
2481	1998. Protein folding in the hydrophobic-hydrophilic(HP) model is NPcomplete.	[[53, 55], [66, 68]]	[[29, 52]]	['HP', 'NP']	['hydrophobic-hydrophilic']
2482	Suppose that the feature f2 is an agreement feature and that a local  tree t which is a projection of this ID rule has been constructed, then  the Agreement Principle (AP) forces X = Y = Z and therefore the  AP has to consider three cases 6: 	[[168, 170], [107, 109], [208, 210]]	[[147, 166]]	['AP', 'ID', 'AP']	['Agreement Principle']
2483	900  Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 48?57, Gothenburg, Sweden, April 27, 2014.	[[74, 79], [83, 87]]	[[40, 72]]	['HyTra', 'EACL']	['Hybrid Approaches to Translation']
2484	a r e  the fo l lowing  -- Performer,  Object, Goal, Source,  Locat ion,   Means, Cause, and Enabler  -- and (2) s t r u c t u r a l  c a s e s ,  which a r e   R E E L  ( r e l a t i v e  c l a u s e )  and COMP (compound). I w i l l  not  e x p l a i n  	[[208, 212]]	[[214, 222]]	['COMP']	['compound']
2485	and the neural language model (NLM), for each phrase combination: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj). For each	[[100, 103], [122, 127], [31, 34], [82, 87]]	[[90, 98], [109, 120], [8, 29], [66, 80]]	['N-N', 'V-Obj', 'NLM', 'Adj-N']	['nounnoun', 'verb object', 'neural language model', 'adjective noun']
2486	to be explained by a set of unobserved (latent) topics. Hidden Markov Model LDA (HMM-LDA) (Griffiths et al, 2005) is a topic model that simul-	[[81, 88]]	[[56, 79]]	['HMM-LDA']	['Hidden Markov Model LDA']
2487	SELF = talking to oneself  TQ = terse question  TI = terse information  INT = interrupted 	[[48, 50], [0, 4], [27, 29], [72, 75]]	[[53, 70], [7, 25], [32, 46], [78, 89]]	['TI', 'SELF', 'TQ', 'INT']	['terse information', 'talking to oneself', 'terse question', 'interrupted']
2488	gorithm used belongs to the family of algorithms described by Covington (2001), and the classifiers are trained using support vector machines (SVM) (Vapnik, 1995).	[[143, 146]]	[[118, 141]]	['SVM']	['support vector machines']
2489	mvzaanen@uvt.nl Gerhard van Huyssteen Centre for Text Technology (CTexT) North-West University	[[66, 71]]	[[38, 64]]	['CTexT']	['Centre for Text Technology']
2490	capability [and t o  go] interregional without involving the private sector.  The General Services Administratioq (GSA) l a s t  month amended its M v a o y  Gu--&de-  Zines adding privacy a d  security considerations for use i n  ADP o r  tqlecom- 	[[115, 118], [231, 234]]	[[82, 113]]	['GSA', 'ADP']	['General Services Administratioq']
2491	-4, -12, and -109 are all disjoint speaker sets.)  (Codes: SD=speaker dependent (2400 training sentences for RM2), MS=multi-speaker, SI=speaker independent, -4=all 4  RM2 speakers combined, -12=all 12 RM1 SD speakers combined, -109=109 RM1 SI training speakers, SDG=SD Gaussians, 	[[59, 61], [133, 135], [109, 111], [115, 117], [167, 169], [201, 203], [205, 207], [236, 238], [240, 242], [262, 265], [266, 268]]	[[62, 79], [136, 155], [118, 131]]	['SD', 'SI', 'RM', 'MS', 'RM', 'RM', 'SD', 'RM', 'SI', 'SDG', 'SD']	['speaker dependent', 'speaker independent', 'multi-speaker']
2492	A mobile real-time speech-to-speech translation (S2ST) device is one of the grand challenges in natural language processing (NLP). It involves	[[125, 128], [49, 53]]	[[96, 123], [19, 47]]	['NLP', 'S2ST']	['natural language processing', 'speech-to-speech translation']
2493	 ? ALGN (alignment-based): We ran a sentence alignment algorithm (Gale and Church, 1993)	[[3, 7]]	[[9, 18], [45, 54]]	['ALGN']	['alignment', 'alignment']
2494	Abstract This paper explores the use of set expansion (SE) to improve question answering (QA) when the expected answer is a list of entities	[[90, 92], [55, 57]]	[[70, 88], [40, 53]]	['QA', 'SE']	['question answering', 'set expansion']
2495	back. We define a currency for annotation cost as Annotation cost Units (AUs). For an annotation bud-	[[73, 76]]	[[50, 71]]	['AUs']	['Annotation cost Units']
2496	sible classes we show the accuracy of the correct hashtag being amongst the top 1,5 or 50 hashtags as well as the Mean Reciprocal Rank (MRR). The	[[136, 139]]	[[114, 134]]	['MRR']	['Mean Reciprocal Rank']
2497	54  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 325?335, October 25-29, 2014, Doha, Qatar.	[[92, 97]]	[[42, 90]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
2498	 3 Data The RST Discourse Treebank (RST-DT) (Carlson et al, 2002) was used for training and testing.	[[36, 42]]	[[12, 34]]	['RST-DT']	['RST Discourse Treebank']
2499	When ready and mature,  technology and language processing techniques will be  incorporated into Foreign Broadcast Information Service (FBIS)  processing.	[[136, 140]]	[[97, 134]]	['FBIS']	['Foreign Broadcast Information Service']
2500	3.2 Formalizing Paradigmatic Relations with Lexical Functions Lexical functions (LF) are a formal tool designed to describe all types of genuine lexical relations	[[81, 83]]	[[62, 79]]	['LF']	['Lexical functions']
2501	Evaluating the quality of language output tasks such as Machine Translation (MT) and Automatic Summarisation (AS) is a challenging topic in Natural Language Processing	[[110, 112], [77, 79]]	[[85, 108], [56, 75]]	['AS', 'MT']	['Automatic Summarisation', 'Machine Translation']
2502	tent both in their living rooms and on their mobile devices. Digital video recorders (DVRs) allow people to record TV programs from hundreds of chan-	[[86, 90], [115, 117]]	[[61, 84]]	['DVRs', 'TV']	['Digital video recorders']
2503	 2.1 Named Entity Recognition We regard named entity recognition (NER) as a standalone task, independent of language identification.	[[66, 69]]	[[40, 64]]	['NER']	['named entity recognition']
2504	Context-Free Grammars 2.1 Minimalist Grammars A Minimalist Grammar (MG) (Stabler and Keenan, 2003)1 is a five-tuple	[[68, 70]]	[[48, 66]]	['MG']	['Minimalist Grammar']
2505	As shown in the table, all models perform equally well on identification, which is determined by the frame matcher (FM); i.e., any extracted argument receiving one or more candidate roles is ?	[[116, 118]]	[[101, 114]]	['FM']	['frame matcher']
2506	The CoreSC scheme consists of three layers; the first layer corresponds to eleven concepts (Background (BAC), Hypothesis (HYP), Motivation (MOT), Goal (GOA), Object (OBJ), Method (MET), Model	[[122, 125], [140, 143], [4, 10], [104, 107], [152, 155], [166, 169], [180, 183]]	[[110, 120], [128, 138], [92, 102], [146, 150], [158, 164], [172, 178]]	['HYP', 'MOT', 'CoreSC', 'BAC', 'GOA', 'OBJ', 'MET']	['Hypothesis', 'Motivation', 'Background', 'Goal', 'Object', 'Method']
2507	Feature F1,344 d Automated Readability Index (ARI) 0.187 0.047 Average Sentence Length (ASL) 3.870 0.213 Sentence Complexity (COM) 10.93 0.357	[[88, 91], [46, 49], [126, 129]]	[[63, 86], [17, 44], [114, 124]]	['ASL', 'ARI', 'COM']	['Average Sentence Length', 'Automated Readability Index', 'Complexity']
2508	to do the testing on real emotions. The Berlin Emotional Database (EMO-DB) contains the set of emotions from the MPEG-4 standard (anger,	[[67, 73], [113, 119]]	[[47, 65]]	['EMO-DB', 'MPEG-4']	['Emotional Database']
2509	Queue to for user intervention.  4.2,  Document Processor (DP)   The DP identifies and extracts all SGML tags de- 	[[59, 61], [100, 104]]	[[39, 57]]	['DP', 'SGML']	['Document Processor']
2510	systematic way.  The MIME (Managing Information in Medical Emergencies)1 project is developing technology to	[[21, 25]]	[[27, 70]]	['MIME']	['Managing Information in Medical Emergencies']
2511	07 a Single 0.71 1.00 0.83 0.00 0.00 0.00 0.42 1.39 9  Table 9  Constituent parsing evaluation results of Task 2-2 (Close Track), ranked with Tot-F1  (S_S=simple sentence, C_S=complex sentence)  ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A Rank	[[151, 154], [172, 175], [253, 258], [227, 234], [198, 204]]	[[155, 170], [176, 192]]	['S_S', 'C_S', 'POS-A', 'S_S C_S', 'Sys-ID']	['simple sentence', 'complex sentence']
2512	2214  Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 1?9, Gothenburg, Sweden, April 26-30 2014.	[[74, 76], [32, 36]]	[[54, 72]]	['DM', 'EACL']	['Dialogue in Motion']
2513	In Proceedings of the Conference of the Pacific Association for Computational Linguistics (PACLING), pages 120?128. 	[[91, 98]]	[[40, 89]]	['PACLING']	['Pacific Association for Computational Linguistics']
2514	2 Long Short-Term Memory Networks 2.1 Overview Recurrent neural networks (RNNs) are able to process input sequences of arbitrary length via the re-	[[74, 78]]	[[47, 72]]	['RNNs']	['Recurrent neural networks']
2515	determine the appropriate xpressional form. Hovy's text structurer (Hovy 1988b),  for example, uses rhetorical relations as defined in Rhetorical Structure Theory (RST)  (Mann and Thompson 1987) to order a set of propositions to be expressed.	[[164, 167]]	[[135, 162]]	['RST']	['Rhetorical Structure Theory']
2516	seeds more complex models with simpler models, and the parameters of each model are estimated through an Expectation Maximization (EM) procedure.	[[131, 133]]	[[105, 129]]	['EM']	['Expectation Maximization']
2517	vincent.claveau@irisa.fr christian.raymond@irisa.fr R?SUM?Dans cet article, nous d?crivons notre participation au D?fi Fouille de Texte (DeFT) 2012. Ced?fi consistait en l?attribution automatique de mots-cl?s ?	[[137, 141], [52, 57]]	[[81, 135]]	['DeFT', 'R?SUM']	['d?crivons notre participation au D?fi Fouille de Texte']
2518	EUROTYP + + + + + + Leipzig Glossing Rules + + + + + + + + Penn Treebank (POS) + + + + + + + + STTS + + + + + + +	[[74, 77], [0, 7], [95, 99]]	[[59, 72]]	['POS', 'EUROTYP', 'STTS']	['Penn Treebank']
2519	We first experiment with the separately trained supertagger and parser, which are then combined using belief propagation (BP) and dual decomposition (DD).	[[122, 124], [150, 152]]	[[102, 120], [130, 148]]	['BP', 'DD']	['belief propagation', 'dual decomposition']
2520	We show that languageindependent features can be used for regression with Support Vector Machines (SVMs) and the Margin-Infused Relaxed Algorithm (MIRA), and	[[99, 103], [147, 151]]	[[74, 97], [113, 145]]	['SVMs', 'MIRA']	['Support Vector Machines', 'Margin-Infused Relaxed Algorithm']
2521	tional words and notional words. In  the field  of Natural Language Processing(NLP), many  studies on text computing or word meaning 	[[79, 82]]	[[51, 77]]	['NLP']	['Natural Language Processin']
2522	wards a Shared Task for Multiword Expressions. ACL Special Interest Group on the Lexicon (SIGLEX), Marrakech.	[[90, 96], [47, 50]]	[[51, 88]]	['SIGLEX', 'ACL']	['Special Interest Group on the Lexicon']
2523	kept on a ventilator for medical reasons.     Change of state (COS) is most often understood  as an aspectual difference that is reflected in verb 	[[63, 66]]	[[46, 61]]	['COS']	['Change of state']
2524	, with different projection and SRL training methods. SP=Supplement; OW=Overwrite. 	[[54, 56], [32, 35], [69, 71]]	[[72, 81], [57, 67]]	['SP', 'SRL', 'OW']	['Overwrite', 'Supplement']
2525	NER model was shown in Table 4. We use the Peking University (PKU) named entity corpus to train the models.	[[62, 65], [0, 3]]	[[43, 60]]	['PKU', 'NER']	['Peking University']
2526	pre-rendered animations.  The Natural Language Understanding (NLU) module needs to cope with both chat and military	[[62, 65]]	[[30, 60]]	['NLU']	['Natural Language Understanding']
2527	Interpreting news requires identifying its constituent events. Information extraction (IE) makes this feasible by considering only events of a specified type,	[[87, 89]]	[[63, 85]]	['IE']	['Information extraction']
2528	must be stored at each step of the decoding algorithm.  This information includes: the current score (SCORE),  the pointer to the previous lexical item (BPO) on the best 	[[102, 107], [153, 156]]	[[95, 100]]	['SCORE', 'BPO']	['score']
2529	tities e1, e2, ei,.., eE on translated English documents through aforementioned step, meanwhile, we consider all noun phrases(NP) in original Chinese documents and generate mention candidates	[[126, 128]]	[[113, 124]]	['NP']	['noun phrase']
2530	Japanese Translated SemEval 2007 Test Corpus (in %)  Before Morphology [After Morphology]  Emotion Score (ES) ? 0 Emotion Score (ES) ?	[[106, 108], [129, 131]]	[[91, 104], [114, 127]]	['ES', 'ES']	['Emotion Score', 'Emotion Score']
2531	2.3.3 Name List Generated using Double Propagation We implement the Double Propagation (DP) algorithm described in Qiu et al. (	[[88, 90]]	[[68, 86]]	['DP']	['Double Propagation']
2532	the (Penn Treebank) annotation style, (3) the (LexTract) extraction tool, (4) possible unsuitability of the (TAG) model, and (5) annotation errors. We	[[109, 112], [47, 55]]	[]	['TAG', 'LexTract']	[]
2533	The ISO 639-3 language codes for our eight languages are as follows: Urdu (URD), Thai (THA), Bengali (BEN), Tamil (TAM), Punjabi (PAN), Tagalog (TGL), Pashto	[[75, 78], [102, 105], [4, 7], [87, 90], [115, 118], [130, 133], [145, 148]]	[[69, 73], [93, 100], [81, 85], [108, 113], [121, 128], [136, 143]]	['URD', 'BEN', 'ISO', 'THA', 'TAM', 'PAN', 'TGL']	['Urdu', 'Bengali', 'Thai', 'Tamil', 'Punjabi', 'Tagalog']
2534	icantly better performance than GIZA++. We also evaluated Support Vector Machines (SVM) classifiers on the same first order feature space and	[[83, 86], [32, 38]]	[[58, 81]]	['SVM', 'GIZA++']	['Support Vector Machines']
2535	user gender (GEN), the user identity (UID) (e.g. the user could be a person or an organization), and the source document ID (DID). We also mark the lan-	[[125, 128], [13, 16], [38, 41]]	[[112, 123], [5, 11], [23, 36]]	['DID', 'GEN', 'UID']	['document ID', 'gender', 'user identity']
2536	In this work, we apply Dirichlet Process Mixture Models (DPMMs) to a learning task in natural language processing (NLP): lexical-semantic verb clustering.	[[115, 118], [57, 62]]	[[86, 113], [23, 55]]	['NLP', 'DPMMs']	['natural language processing', 'Dirichlet Process Mixture Models']
2537	190  troductory phase (GREET-INTRODUCE-TOPIC), the  negotiation phase (NEGOTIATE) and the closing  phase (FINISH).	[[71, 80], [106, 112]]	[[52, 69], [90, 104]]	['NEGOTIATE', 'FINISH']	['negotiation phase', 'closing  phase']
2538	In Proceedings of the NAACL/AMTA Workshop on Syntax and Structure in Statistical Translation (SSST), pages 33?40, Rochester, NY.	[[94, 98], [22, 32], [125, 127]]	[[45, 92]]	['SSST', 'NAACL/AMTA', 'NY']	['Syntax and Structure in Statistical Translation']
2539	fered to punched cards. Abbreviated alphabetical symbols  are used for the syntactic analysis (AP=adJective phrase)  because of the program's 24unlt search limitation.	[[95, 97]]	[[98, 114]]	['AP']	['adJective phrase']
2540	relative-resource?, i.e.  EuroWordNet (EWN).1   In this paper we start by briefly recalling the 	[[39, 42]]	[[26, 37]]	['EWN']	['EuroWordNet']
2541	They are the One-error Loss (O-Loss) function, the Symmetric Loss (S-Loss) function, and the Hierarchical Loss (H-Loss) function: ?	[[112, 118], [29, 35], [67, 73]]	[[93, 110], [13, 27], [51, 65]]	['H-Loss', 'O-Loss', 'S-Loss']	['Hierarchical Loss', 'One-error Loss', 'Symmetric Loss']
2542	other side, corpus-based measures such as Latent Semantic Analysis (LSA) (Landauer et al 1997), Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), Salient Semantic Analysis	[[124, 127], [68, 71]]	[[96, 122], [42, 66]]	['ESA', 'LSA']	['Explicit Semantic Analysis', 'Latent Semantic Analysis']
2543	   FIGURE 1: Community of Inquiry (CoI) model   (Adapted from: Garrison et al 2000) 	[[35, 38]]	[[13, 33]]	['CoI']	['Community of Inquiry']
2544	sented by the S node) are extracted. ( 2) VPs (NPs) in a path on which all the nodes are VPs (NPs) are also recursively extracted and regarded as hav-	[[94, 97], [42, 45], [47, 50], [89, 92]]	[]	['NPs', 'VPs', 'NPs', 'VPs']	[]
2545	during this period: Carpetbagger (CB),1 Daily Kos (DK),2 Matthew Yglesias (MY),3 Red State (RS),4 and Right Wing News (RWN).5 CB and MY ceased as independent bloggers in August 2008.6 Because	[[119, 122], [34, 36], [51, 53], [75, 77], [92, 94], [133, 135], [126, 128]]	[[102, 117], [20, 32], [40, 49], [57, 73], [81, 90]]	['RWN', 'CB', 'DK', 'MY', 'RS', 'MY', 'CB']	['Right Wing News', 'Carpetbagger', 'Daily Kos', 'Matthew Yglesias', 'Red State']
2546	Cognitive Science Department at Xiamen University (XMU) ? ?  Harbin Institute of Technology Shenzhen Graduate School (HITSZGS)    National Taipei University of Technology (NTUT)   	[[118, 125], [51, 54], [172, 176]]	[[61, 116], [32, 49], [130, 170]]	['HITSZGS', 'XMU', 'NTUT']	['Harbin Institute of Technology Shenzhen Graduate School', 'Xiamen University', 'National Taipei University of Technology']
2547	Turian et al. ( 2010) applied word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (	[[88, 91]]	[[62, 86]]	['NER']	['Named Entity Recognition']
2548	J = Japanese ????? S = Spanish  JV = Joint Venture ?????????? ME = Microelectronics 	[[32, 34], [62, 64]]	[[37, 50], [4, 12], [67, 83]]	['JV', 'ME']	['Joint Venture', 'Japanese', 'Microelectronics']
2549	0 ROOT Figure 2: Example of a CoNLL-style annotated sentence. Each word (FORM) is numbered (ID), lemmatized (LEMMA), annotated with two levels of part-of-speech tags (CPOSTAG and POSTAG), annotated with morpho-	[[92, 94], [30, 35], [73, 77], [109, 114], [167, 174], [179, 185]]	[[79, 90], [97, 107], [146, 165]]	['ID', 'CoNLL', 'FORM', 'LEMMA', 'CPOSTAG', 'POSTAG']	['is numbered', 'lemmatized', 'part-of-speech tags']
2550	dimensional space, in which both texts and terms are represented by means of Domain Vectors (DVs), i.e. vectors representing the domain relevances among the linguistic object and	[[93, 96]]	[[77, 91]]	['DVs']	['Domain Vectors']
2551	SIZE = sizesensorreading85 SHAPE = shapesensorreading62 COLOUR = coloursensorreadning78 ?	[[56, 62], [0, 4], [27, 32]]	[[65, 87], [7, 26], [35, 55]]	['COLOUR', 'SIZE', 'SHAPE']	['coloursensorreadning78', 'sizesensorreading85', 'shapesensorreading62']
2552	languages studied differ widely, there is a quasistandard for presenting the material, in the form of interlinearized glossed text (IGT). IGT typically	[[132, 135], [138, 141]]	[[102, 130]]	['IGT', 'IGT']	['interlinearized glossed text']
2553	 This work addresses a major bottleneck in the development of Statistical MT (SMT) systems: the lack of sufficiently large parallel corpora for most	[[78, 81]]	[[62, 76]]	['SMT']	['Statistical MT']
2554	In Proc. of the 4th Workshop  on Treebanks and Linguistic Theories (TLT), pages  149?160.	[[68, 71]]	[[33, 66]]	['TLT']	['Treebanks and Linguistic Theories']
2555	 4. Template Relation(TR) recognition: Finding the relation between TEs and a question	[[22, 24]]	[[4, 20]]	['TR']	['Template Relatio']
2556	 3.2.6. Candidate word number (WNum)  Because there are candidates that are a multi-	[[31, 35]]	[[18, 29]]	['WNum']	['word number']
2557	cognition(COG) competition(COMP)  contact(CeNT) motion(MOT)  emoeion(ENO) perception(PER)  possession(POSS) stat ive(STA) 	[[85, 88], [10, 13], [27, 31], [42, 46], [55, 58], [69, 72], [102, 106], [117, 120]]	[[74, 84], [0, 9], [15, 26], [34, 41], [48, 54], [61, 68], [91, 101], [108, 116]]	['PER', 'COG', 'COMP', 'CeNT', 'MOT', 'ENO', 'POSS', 'STA']	['perception', 'cognition', 'competition', 'contact', 'motion', 'emoeion', 'possession', 'stat ive']
2558	colour histograms derived from images.  In the RGB (Red Green Blue) colour model, each pixel is represented as an integer in range of	[[47, 50]]	[[52, 66]]	['RGB']	['Red Green Blue']
2559	documents. Thus, HTMM can be seen both as a variation of Hidden Markov Model (HMM) and a variation of LDA.	[[78, 81], [17, 21], [102, 105]]	[[57, 76]]	['HMM', 'HTMM', 'LDA']	['Hidden Markov Model']
2560	Route INJECTION ORAL SMOKING SNORTING Aspect CHEMISTRY (Pharmacology, TEK) CULTURE (Culture, Setting, Social, Spiritual) EFFECTS (Effects)	[[75, 82], [70, 73], [121, 128]]	[[84, 91], [130, 137]]	['CULTURE', 'TEK', 'EFFECTS']	['Culture', 'Effects']
2561	1 Introduction After two decades of steady progress, research in statistical machine translation (SMT) started to cross its path with translation industry with tan-	[[98, 101]]	[[65, 96]]	['SMT']	['statistical machine translation']
2562	=   where,   ij  is the term frequency(TF) of the j-th word  in the vocabulary in the document , i.e. the 	[[39, 41]]	[[24, 37]]	['TF']	['term frequenc']
2563	  1 Introduction  Sign Language (SL) is a visual-gestural language, using the whole upper body articulators 	[[33, 35]]	[[18, 31]]	['SL']	['Sign Language']
2564	"some plan P such that, if H executes P. then in the re-  sulting state, there exists a \['F identifiable term P' such  that H knows that Denotation(Pl = Dem;tation(DI),  and 5"" intends that H execute P. "	[[164, 166]]	[[153, 163]]	['DI']	['Dem;tation']
2565	"though this is never the case.) The second such  feature ""Theme as Chomeuf  (TAC) is the only  non-trinary-valued feature in our learner; it spec- "	[[77, 80]]	[[58, 74]]	['TAC']	['Theme as Chomeuf']
2566	HLT/EMNLP, 2005  http://www.nist.gov/speech/tests/ace/ace07/doc, The  ACE 2007 (ACE07) Evaluation Plan, Evaluation of  the Detection and Recognition of ACE Entities, Val-	[[80, 85], [0, 9], [152, 155]]	[[70, 78]]	['ACE07', 'HLT/EMNLP', 'ACE']	['ACE 2007']
2567	]} (7) This optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).	[[78, 80]]	[[52, 76]]	['EM']	['expectation maximization']
2568	 1 Introduction Word Sense Disambiguation (WSD) is considered one of the most important prob-	[[43, 46]]	[[16, 41]]	['WSD']	['Word Sense Disambiguation']
2569	? TFN Bank(TFNB): 38,769 samples  CPN Bank(CPNB): 17,637 samples  The difficulty of identifying unknown words in 	[[43, 47], [11, 15]]	[[34, 42], [2, 10]]	['CPNB', 'TFNB']	['CPN Bank', 'TFN Bank']
2570	paper is the following:  Def. A generative system (GS) is a sequence TI,... ,Tn of TS,  whe~'~-TR(Tl,... ,Tn) is a relation between strings and D-trees and 	[[51, 53], [83, 85], [95, 97]]	[[32, 49]]	['GS', 'TS', 'TR']	['generative system']
2571	 First we used the C&C Combinatory Categorial Grammar (CCG) parser5 (C&C) by Clark and Curran (2004) using the biomedical model described in	[[69, 72], [19, 22], [55, 58]]	[[23, 53], [77, 93]]	['C&C', 'C&C', 'CCG']	['Combinatory Categorial Grammar', 'Clark and Curran']
2572	Figure 1: Na??ve Bayes Model The model described above is commonly known as a na??ve Bayes (NB) model. NB models have	[[92, 94], [103, 105]]	[[78, 90], [10, 22]]	['NB', 'NB']	['na??ve Bayes', 'Na??ve Bayes']
2573	5 System Description The PEZ system consists of three components, viz (i) a Web Translation Memory (WebTM) crawler, (ii) the XLING reranker and (iii) a longest	[[100, 105], [25, 28], [125, 130]]	[[76, 98]]	['WebTM', 'PEZ', 'XLING']	['Web Translation Memory']
2574	our proposed STRAIN approach. The results of using sentence training (STr) and sentence testing (STe) are shown in the STR/STE row of Table 5.	[[70, 73], [13, 19], [97, 100], [119, 126]]	[[51, 68], [79, 95]]	['STr', 'STRAIN', 'STe', 'STR/STE']	['sentence training', 'sentence testing']
2575	? Toral (2013) explores the selection of data to train domain-specific language models (LM) from non-domain specific corpora by means	[[88, 90]]	[[71, 86]]	['LM']	['language models']
2576	P i jPMI P i P j=   Equation 2: Pointwise Mutual Information (PMI)  between two terms i and j. 	[[62, 65], [5, 8]]	[[32, 60]]	['PMI', 'PMI']	['Pointwise Mutual Information']
2577	2: boston sweep colorado to win world series 3: rookies respond in first crack at the big time C-LR=C-LexRank; WDS=Word Distributional Similarity Table 4: Top 3 ranked summaries of the redsox cluster	[[111, 114], [95, 99]]	[[115, 145], [100, 109]]	['WDS', 'C-LR']	['Word Distributional Similarity', 'C-LexRank']
2578	" X '1""~,. \[-1  Def in i t ion 2.2 A N:M sur face coerc ion  (SC)  ru le ix a quadruple (/,c/,c~,r) where l and r "	[[62, 64]]	[[41, 59]]	['SC']	['sur face coerc ion']
2579	 1 Introduction Spoken Dialogue Systems (SDSs) play a key role in achieving natural human-machine interaction.	[[41, 45]]	[[16, 39]]	['SDSs']	['Spoken Dialogue Systems']
2580	Table 10: A=acoustic, P=psycholinguistic, POS=part-of-speech, C=complexity, F=fluency, VR=vocabulary richness, CFG=CFG production rule features.	[[87, 89], [42, 45], [111, 114], [115, 118]]	[[90, 109], [12, 20], [24, 40], [46, 60], [64, 74], [78, 85]]	['VR', 'POS', 'CFG', 'CFG']	['vocabulary richness', 'acoustic', 'psycholinguistic', 'part-of-speech', 'complexity', 'fluency']
2581	 9 Here, we present the generation-oriented PG Workbench (PGW), which assists grammar developers, among other things, in testing whether the implemented syntactic and lexical knowledge allows all and only well-formed permutations. In Section 2, we describe PG?s topology-based linearizer implemented in the PGW gen-erator, whose software design is sketched in Section 3.	[[58, 61], [307, 310], [257, 261]]	[[44, 56]]	['PGW', 'PGW', 'PG?s']	['PG Workbench']
2582	Furthermore, to create a fully text-bound subset, family memberships relations (MEMBER) were resolved into single edges and suitable references to	[[80, 86]]	[[57, 78]]	['MEMBER']	['memberships relations']
2583	tation for the joint learning process. Specifically, we make use of the latent structural SVM (LS-SVM) (Yu and Joachims, 2009) formulation.	[[95, 101]]	[[72, 93]]	['LS-SVM']	['latent structural SVM']
2584	2013 temporal summarization. In Proceedings of the 22nd Text Retrieval Conference (TREC), November.	[[83, 87]]	[[56, 81]]	['TREC']	['Text Retrieval Conference']
2585	Fortunately, learning the reward function using IRL methods have already been proposed for the general (PO)MDP framework (Ng and Russell, 2000; Kim et al.,	[[104, 106], [48, 51], [107, 110]]	[[78, 90]]	['PO', 'IRL', 'MDP']	['proposed for']
2586	 156 The Basque Dependency Treebank (BDT) is a dependency treebank in its original design, due to	[[37, 40]]	[[9, 35]]	['BDT']	['Basque Dependency Treebank']
2587	? Negat ive Precis ion ( I~P)  :  * F -measure  (FM) ? ( ~2+I)?PP?PR /32 ?	[[49, 51], [25, 28]]	[[36, 46]]	['FM', 'I~P']	['F -measure']
2588	In Proc. IEEE Automatic Speech Recognition and Understanding (ASRU), Merano, Italy, December. 	[[62, 66], [9, 13]]	[[14, 60]]	['ASRU', 'IEEE']	['Automatic Speech Recognition and Understanding']
2589	Table 4: Evaluation results. Abbreviations: TVN (Tone & Vowel Normalization); N-LM (N-order Language Modelling); DS (Dataset); PK (Prior Knowledge); WC (Weighting-based Corrector). 	[[113, 115], [127, 129], [44, 47], [78, 82], [149, 151]]	[[117, 124], [131, 146], [84, 110], [153, 178]]	['DS', 'PK', 'TVN', 'N-LM', 'WC']	['Dataset', 'Prior Knowledge', 'N-order Language Modelling', 'Weighting-based Corrector']
2590	 3 Baseline SMT system The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sen-	[[68, 71], [12, 15]]	[[35, 66]]	['SMT', 'SMT']	['statistical machine translation']
2591	transduction and matching words approximately.  Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma.	[[57, 61]]	[[48, 55]]	['UTF8']	['Unicode']
2592	Technology (NAST), Lao PDR  ? Madan Puraskar Pustakalaya (MPP),  Nepal  	[[58, 61]]	[[30, 56]]	['MPP']	['Madan Puraskar Pustakalaya']
2593	capture all the types of entities. Typical structures  of Chinese person name (CN), location name (LN)  and organization name (ON) are as follows: 	[[79, 81], [99, 101]]	[[58, 77], [84, 97]]	['CN', 'LN']	['Chinese person name', 'location name']
2594	2.1 Purver, Ginzburg and Healey (PGH) Purver, Ginzburg and Healey (2003) investigated CRs in the British National Corpus (BNC) (Burnard, 2000).	[[122, 125], [33, 36], [86, 89]]	[[97, 120], [4, 31]]	['BNC', 'PGH', 'CRs']	['British National Corpus', 'Purver, Ginzburg and Healey']
2595	 1 Introduction Natural Language Processing (NLP) systems often consist of a series of NLP components, each trained	[[45, 48], [87, 90]]	[[16, 43]]	['NLP', 'NLP']	['Natural Language Processing']
2596	 2 (Durrett and Klein, 2013) call this error false new (FN). 	[[56, 58]]	[[45, 54]]	['FN']	['false new']
2597	ity, to validate Boosting NER hypotheses. We also use three Markov chain Monte Carlo (MCMC) algorithms for probabilistic inference in MLNs.	[[86, 90], [26, 29], [134, 138]]	[[60, 84]]	['MCMC', 'NER', 'MLNs']	['Markov chain Monte Carlo']
2598	by the connectives will yield better readability.  Entity Grid (EG) Along with the previous work (Pitler and	[[64, 66]]	[[51, 62]]	['EG']	['Entity Grid']
2599	For each bracketed phrase, if its FF label does not  fit into the corresponding default pattern, (like for  the noun phrase(NP), the default grammatical  structure is that the last noun in the phrase is the 	[[124, 126], [34, 36]]	[[112, 123]]	['NP', 'FF']	['noun phrase']
2600	i );3 MFS = Maximal Freq Sequences(d1 i	[[6, 9]]	[[12, 37]]	['MFS']	['Maximal Freq Sequences(d1']
2601	attempted to cut down on certain items in the process.  Figure 2: Translation examples (SRC = source, BASE = baseline system, BACKOFF = backoff	[[88, 91]]	[[94, 100]]	['SRC']	['source']
2602	adjectives, and specifies the participants and properties of the situation it describes, the so called frame elements (FEs). 	[[119, 122]]	[[103, 117]]	['FEs']	['frame elements']
2603	WI = wide; NA = narrow; CR = critical; CL = closed; ALV = alveolar; P-A = palato-alveolar; RET = retroflex. 	[[91, 94], [0, 2], [11, 13], [24, 26], [39, 41], [52, 55], [68, 71]]	[[97, 106], [5, 9], [16, 22], [29, 37], [44, 50], [58, 66], [74, 89]]	['RET', 'WI', 'NA', 'CR', 'CL', 'ALV', 'P-A']	['retroflex', 'wide', 'narrow', 'critical', 'closed', 'alveolar', 'palato-alveolar']
2604	ranking models on this data set, including Support Vector Machine (SVM) with a linear kernel, SVM with a radial basis function (RBF) kernel and Logistic Regression (LR).	[[128, 131], [67, 70], [94, 97], [165, 167]]	[[105, 126], [43, 65], [144, 163]]	['RBF', 'SVM', 'SVM', 'LR']	['radial basis function', 'Support Vector Machine', 'Logistic Regression']
2605	the specified length limit. This idea is realized using the integer linear programming-based (ILP) optimization framework, with objective function set to	[[94, 97]]	[[60, 86]]	['ILP']	['integer linear programming']
2606	quency weighted recall evaluation. We used a  Japanese frequency dictionary (FD) generated  from the Japanese EDR corpus (Isahara, 2007) to 	[[77, 79]]	[[55, 75]]	['FD']	['frequency dictionary']
2607	model. We tuned the parameters of these features with Minimum Error Rate Training (MERT) (Och, 2003) on the NIST MT03 Evaluation data set (919	[[83, 87], [108, 112], [113, 117]]	[[54, 81]]	['MERT', 'NIST', 'MT03']	['Minimum Error Rate Training']
2608	4 Experiments 4.1 Event Extraction We extract events by running the Semantic Role Labeling (SRL) tool in SENNA (Collobert et al., 2011).	[[92, 95], [105, 110]]	[[68, 90]]	['SRL', 'SENNA']	['Semantic Role Labeling']
2609	Figure 1: An entity and relation example (Roth and Yih, 2004). Person (PER) and location (LOC) entities are connected by Live in and Located in	[[71, 74], [90, 93]]	[[63, 69], [80, 88]]	['PER', 'LOC']	['Person', 'location']
2610	(3) The measure introduced by Resnik (Resnik, 1995) (RES) returns the information content (IC) of the LCS of two concepts:	[[91, 93], [53, 56], [102, 105]]	[[70, 89]]	['IC', 'RES', 'LCS']	['information content']
2611	ley (2004). This framework for linguistic semantics is called Unied Eventity Representation (UER), because it is a true extension of the UML and not	[[94, 97], [138, 141]]	[[62, 92]]	['UER', 'UML']	['Uni\x02ed Eventity Representation']
2612	 (d) The word?s position in the sentence (e) The word?s Part of speech (POS) tag, based on the Stanford POS tagger2	[[72, 75], [104, 107]]	[[56, 70]]	['POS', 'POS']	['Part of speech']
2613	Table 3: Participants and summary of system descriptions. Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, SDE=Software Development Engineer, CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, Snowball=Snowball stemmer, WN-lemma=WordNet lemmatization, McCCJ=McClosky-Charniak-Johnson	[[138, 141], [73, 75], [94, 97], [173, 180], [199, 205], [222, 230], [249, 257], [281, 286]]	[[142, 171], [76, 92], [98, 125], [181, 197], [206, 220], [231, 247], [258, 279], [287, 312]]	['SDE', 'BI', 'NLP', 'CoreNLP', 'Porter', 'Snowball', 'WN-lemma', 'McCCJ']	['Software Development Engineer', 'Bioinformatician', 'Natural Language Processing', 'Stanford CoreNLP', 'Porter stemmer', 'Snowball stemmer', 'WordNet lemmatization', 'McClosky-Charniak-Johnson']
2614	the Extraction of Potential Opinion Phrases. Notation: po=potential opinion, M=modifier, NP=noun phrase, S=subject, P=predicate, O=object.	[[89, 91], [55, 57]]	[[92, 103], [58, 75], [79, 87], [107, 114], [118, 127], [131, 137]]	['NP', 'po']	['noun phrase', 'potential opinion', 'modifier', 'subject', 'predicate', 'object']
2615	Here, we assume :  P(t i I G~ )  IP(tt I Pi-i PiWi) Pi-I piwi E dp  \[ P(ti / Pi ) Pi-I Pi Wi ~ 1I) 	[[33, 35]]	[[39, 43]]	['IP']	['I Pi']
2616	and labeled by the people on Amazon Mechanical Turk, a web service. Amazon Mechanical Turk (MTurk) allows individuals to post jobs on MTurk with a set fee that are	[[92, 97]]	[[75, 90]]	['MTurk']	['Mechanical Turk']
2617	facts or splits the  goal into new subgo~ls uch as to show the facts in the premises of n. The derivation of a fact is  conveyed by so-called mathematics ommunicating acts (MCAs) and accompanied by storing the  fact as a chunk in the declarative memory.	[[173, 177]]	[[142, 171]]	['MCAs']	['mathematics ommunicating acts']
2618	Brighton, BN1 9QN, England  Abstract  Generalised phrase structure grammars (GPSG's)  appear to offer a means by which the syntactic 	[[77, 83], [14, 17], [10, 13]]	[[38, 75]]	"[""GPSG's"", '9QN', 'BN1']"	['Generalised phrase structure grammars']
2619	and opportunities. In Proceedings of the 1st International Temporal Web Analytics Workshop (TWAW), pages 1?8.	[[92, 96]]	[[59, 90]]	['TWAW']	['Temporal Web Analytics Workshop']
2620	Model of Argumentation. Proceedings of American Association .for  Artificial Intelligence (AAAI) Conference: 313-315. 	[[91, 95]]	[[39, 89]]	['AAAI']	['American Association .for  Artificial Intelligence']
2621	to ? constraints? in interactive topic models (ITM) (Hu et al, 2014).	[[47, 50]]	[[21, 45]]	['ITM']	['interactive topic models']
2622	 Support Vector Machine Support Vector Machines (SVMs) have been shown to be an effective classifier in text	[[49, 53]]	[[24, 47]]	['SVMs']	['Support Vector Machines']
2623	When using only the former type of feature function, our classifier is equivalent to a maximum entropy (MaxEnt) model. 	[[104, 110]]	[[87, 102]]	['MaxEnt']	['maximum entropy']
2624	that of Visweswariah et al(2011) ? hereby called Travelling Salesman Problem (TSP) model ? with	[[78, 81]]	[[49, 76]]	['TSP']	['Travelling Salesman Problem']
2625	computed from the rewrite rules by the examination of the interdependencies of the rules with the help of  KIT = Ktinsdiche lntelligenz und Textverstehen  (artificial intelligence and text understanding), FAST = 	[[107, 110], [205, 209]]	[[113, 135]]	['KIT', 'FAST']	['Ktinsdiche lntelligenz']
2626	Table 2: Overall scores of whole task as well as separately for each annotation format in terms of labeled precision (LP), recall (LR) and F 1	[[118, 120], [131, 133]]	[[99, 116], [123, 129]]	['LP', 'LR']	['labeled precision', 'recall']
2627	4 Supervised Named Entity Recognition In the first part of this work, we adopt a supervised named entity recognition (NER) framework for the attribute extraction problem from eBay listing titles.	[[118, 121]]	[[92, 116]]	['NER']	['named entity recognition']
2628	step of segmentation is presented in Section 3 with two variants: stochastic word alignment (GIZA) and integer linear programming (ILP). Then evaluations	[[131, 134], [93, 97]]	[[103, 129]]	['ILP', 'GIZA']	['integer linear programming']
2629	description where i t  is useful.  The posit ion of Linear Precedence (LP) state-  ments in th i s  formalism must now be c la r i f ied .	[[71, 73]]	[[52, 69]]	['LP']	['Linear Precedence']
2630	(see, e.g., (Moschitti, 2006) for more details).  Syntactic Tree Kernel (STK), also known as a subset tree kernel (Collins and Duffy, 2002), maps	[[73, 76]]	[[50, 71]]	['STK']	['Syntactic Tree Kernel']
2631	Computational Linguistics, Volume 15, Number 1, March 1989 33  Michael C. McCord \]Design of LMT: A Prolog-Based Machine Translation System  sions in a logical form language (LFL)  (McCord 1985a,  1987).	[[175, 178], [93, 96]]	[[152, 173]]	['LFL', 'LMT']	['logical form language']
2632	2013. Overview of the pathway curation (PC) task of bioNLP shared task 2013.	[[40, 42]]	[[22, 38]]	['PC']	['pathway curation']
2633	Bielefeld University  2 Center of Excellence ? Cognitive Interaction Technology?(CITEC), Bielefeld University     	[[81, 86]]	[[47, 79]]	['CITEC']	['Cognitive Interaction Technology']
2634	c?2009 Association for Computational Linguistics Report on the First NLG Challenge on Generating Instructions in Virtual Environments (GIVE) Donna Byron	[[135, 139], [69, 72]]	[[86, 133]]	['GIVE', 'NLG']	['Generating Instructions in Virtual Environments']
2635	Abstract  This paper provides a description of the Hong  Kong Polytechnic University (PolyU) System  that participated in the task #5 of SemEval-2, 	[[86, 91]]	[[62, 84]]	['PolyU']	['Polytechnic University']
2636	sHDP 0.162 0.046 0.442 0.102 Table 2: Average topic coherence for various baselines (HDP, Gaussian LDA (G-LDA)) and sHDP. 	[[104, 109], [116, 120], [85, 88], [0, 4]]	[[90, 102]]	['G-LDA', 'sHDP', 'HDP', 'sHDP']	['Gaussian LDA']
2637	In second method we compare CLIR performance of the two systems using Cross Language Evaluation Forum (CLEF) 2007 ad-hoc bilingual track (Hindi-English) docu-	[[103, 107], [28, 32]]	[[70, 101]]	['CLEF', 'CLIR']	['Cross Language Evaluation Forum']
2638	 A baseline system was also implemented using  the principle of most frequent sense (MFS),  where each word sense distribution was retrieved 	[[85, 88]]	[[64, 83]]	['MFS']	['most frequent sense']
2639	extraction. Using the alignments from HIER, we created phrase tables using model probabilities (MOD), and heuristic extraction on words (HEUR-W), blocks	[[96, 99], [38, 42], [137, 143]]	[[75, 80], [106, 135]]	['MOD', 'HIER', 'HEUR-W']	['model', 'heuristic extraction on words']
2640	 2 Methodo logy   A User Centered (UC) approach was adopted for the  design of GEPPETTO.	[[35, 37], [79, 87]]	[[20, 33]]	['UC', 'GEPPETTO']	['User Centered']
2641	 The joint model is trained in max-margin fashion using a latent structural SVM (LSSVM) where the answer-entailing structures are latent.	[[81, 86]]	[[58, 79]]	['LSSVM']	['latent structural SVM']
2642	translations. Although initially intended for  learners of English as Foreign Language (EFL)  in Taiwan, it is a gold mine of texts in English 	[[88, 91]]	[[59, 86]]	['EFL']	['English as Foreign Language']
2643	tiguous correspondence. The Structure Reordering Rules (SRR) and Discontiguous Phrase Rules (DPR) mentioned by (Zhang et al, 2008a) can be regarded	[[93, 96], [56, 59]]	[[65, 91], [28, 54]]	['DPR', 'SRR']	['Discontiguous Phrase Rules', 'Structure Reordering Rules']
2644	Lioma C. and Ounis I., A Syntactically-Based Query  Reformulation Technique for Information Retrieval,  Information Processing and Management (IPM), Elsevier Science, 2007 	[[143, 146]]	[[104, 141]]	['IPM']	['Information Processing and Management']
2645	 2 Story Segmentation using Modified Kmeans (MKM) Clustering The first step in multi-document summarization is	[[45, 48]]	[[28, 43]]	['MKM']	['Modified Kmeans']
2646	mance.  We investigated chopping criteria based on a fixed number of words (FIXED), at  speaker changes (TURN), at pauses (PAUSE), and, for reference, at actual sentence 	[[76, 81], [105, 109], [123, 128]]	[[53, 74], [115, 121]]	['FIXED', 'TURN', 'PAUSE']	['fixed number of words', 'pauses']
2647	2.1 Conditional Random Fields  Conditional random field (CRF) was an extension  of both Maximum Entropy Model (MEMs) and  Hidden Markov Models (HMMs) that was firstly 	[[111, 115]]	[[88, 109]]	['MEMs']	['Maximum Entropy Model']
2648	Lima or Jessica Alba??. Therefore, we decided to employ a Conditional Random Fields (CRF) tagger (Lafferty et al, 2001) to the task, since CRF	[[85, 88], [139, 142]]	[[58, 83]]	['CRF', 'CRF']	['Conditional Random Fields']
2649	and (W-1,W0,W1) ? Gazetteers (GAZ): We use two sets of gazetteers.	[[30, 33], [5, 8], [9, 11], [12, 14]]	[[18, 28]]	['GAZ', 'W-1', 'W0', 'W1']	['Gazetteers']
2650	Best-Scoring-Choice Realization Pablo Gerva?s, Raquel Herva?s, Carlos Leo?n Natural Interaction based on Language (NIL) Universidad Complutense de Madrid	[[115, 118]]	[[76, 113]]	['NIL']	['Natural Interaction based on Language']
2651	word lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from	[[84, 87]]	[[65, 82]]	['OOV']	['out of vocabulary']
2652	CDD, which comprises a total of 173 gold annotated cues, we find that Classifier I mislabels 11 false positives (FPs) and seven false negatives (FNs). 	[[113, 116], [145, 148], [0, 3]]	[[96, 111], [128, 143]]	['FPs', 'FNs', 'CDD']	['false positives', 'false negatives']
2653	A Combinatory Categorial Grammar parsing (CCG) (Steedman, 2000) tool and a Tree Kernel (TK) classifier constitute the core of the system.	[[88, 90], [42, 45]]	[[75, 86], [2, 32]]	['TK', 'CCG']	['Tree Kernel', 'Combinatory Categorial Grammar']
2654	gorithm (PAS-PTK), which is highly more efficient and more accurate than the SSTK and (ii) a new kernel called Part of Speech sequence kernel (POSSK), which proves very accurate to represent shallow syn-	[[143, 148], [9, 16], [77, 81]]	[[111, 141]]	['POSSK', 'PAS-PTK', 'SSTK']	['Part of Speech sequence kernel']
2655	The word nchi is dis-  ambiguated with a rule relying on the Ncl of the  following genitive connector (GEN-CON). 	[[103, 110], [61, 64]]	[[83, 101]]	['GEN-CON', 'Ncl']	['genitive connector']
2656	We begin by describing how for our typical model, the Viterbi EM objective can be formulated as a mixed integer quadratic programming (MIQP) problem with nonlinear constraints (Figure 2).	[[135, 139]]	[[98, 133]]	['MIQP']	['mixed integer quadratic programming']
2657	Topic: Short, usually controversial statement that defines the subject of interest.   Context Dependent Claim (CDC): General, and concise statement, that directly supports or contests  the given Topic.	[[111, 114]]	[[86, 109]]	['CDC']	['Context Dependent Claim']
2658	100 Another measure of accuracy that is frequently used is the so called Out Of Vocabulary (OOV) measure, which represents the percentage of words that was not recog-	[[92, 95]]	[[73, 90]]	['OOV']	['Out Of Vocabulary']
2659	  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477?487, October 25-29, 2014, Doha, Qatar.	[[90, 95]]	[[40, 88]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
2660	 In order to overcome these limitations, some  techniques like word sense induction (WSI) have  been proposed for discovering words?	[[85, 88]]	[[63, 83]]	['WSI']	['word sense induction']
2661	is shown in PLATE 1. The second part is the  Prompt Piano Server (PPS), which is an IVR  (interactive voice response) server with a Dialogic 	[[66, 69], [84, 87]]	[[45, 64]]	['PPS', 'IVR']	['Prompt Piano Server']
2662	nority preference algorithm that models bridging recognition as a subtask of learning finegrained information status (IS). We substan-	[[118, 120]]	[[98, 116]]	['IS']	['information status']
2663	One means of achieving a fiat structure with extrinsic  ordering is by using the ID/LP formalism, a subformalism of  GPSG that allows immediate dominance (ID) information to be  specified separately from linear precedence (LP) notions. (	[[155, 157], [81, 86], [117, 121], [223, 225]]	[[134, 153], [204, 221]]	['ID', 'ID/LP', 'GPSG', 'LP']	['immediate dominance', 'linear precedence']
2664	ded systems. It is widely used by software certification authorities such as FAA (Federal Aviation Association), and it establishes some guidelines and	[[77, 80]]	[[82, 110]]	['FAA']	['Federal Aviation Association']
2665	knowledge resources (e.g., WordNet), and (ii) corpus-based that do not require any external knowledge source. Corpus-based metrics are formalized as Distributional Semantic Models (DSMs) (Baroni and Lenci, 2010) based on the distributional hypothesis of meaning (Harris, 1954).	[[181, 185]]	[[149, 179]]	['DSMs']	['Distributional Semantic Models']
2666	Several numerical algorithms, such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limitedmemory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright	[[131, 134]]	[[112, 129]]	['IIS']	['Iterative Scaling']
2667	which allows POS tagged and chunked data to be represented (including recursion), and Shakti Standard Format (SSF)2. The editor allows	[[110, 113], [13, 16]]	[[86, 108]]	['SSF', 'POS']	['Shakti Standard Format']
2668	This system tags, lemmatizes and parses corpus data using the current version of the RASP (Robust Accurate Statistical Parsing) toolkit (Briscoe et al, 2006), and on the basis of resulting	[[85, 89]]	[[91, 126]]	['RASP']	['Robust Accurate Statistical Parsing']
2669	It predicts four type of reordering patterns, namely MA (monotone adjacent), MG (monotone gap), RA (reverse adjacent), and RG (reverse gap).	[[96, 98], [53, 55], [77, 79], [123, 125]]	[[100, 116], [57, 74], [81, 93], [127, 138]]	['RA', 'MA', 'MG', 'RG']	['reverse adjacent', 'monotone adjacent', 'monotone gap', 'reverse gap']
2670	<AbstractText Label=?RESULTS? NlmCategory=?RESULTS?>Premature delivery rate was higher (p = 0.048) in the CKC group (14/36, 38.88%) than in control group (14/68, 20.5%) with a odds ratio (OR) of 2.455 (1.007 - 5.985); and premature delivery was related to cone depth, OR was significantly increased when the cone depth was more than	[[188, 190], [106, 109], [268, 270]]	[[176, 186]]	['OR', 'CKC', 'OR']	['odds ratio']
2671	sides identity (IDENT) we only marked up three associative relations (Hawkins, 1978): set membership (ELEMENT), subset (SUBSET), and ? gen-	[[120, 126], [16, 21], [102, 109]]	[[112, 118], [6, 14]]	['SUBSET', 'IDENT', 'ELEMENT']	['subset', 'identity']
2672	 A manually  prepared seed list that is used to frame the  lexical patterns for conjunct verbs (ConjVs)  contains frequently used Light Verbs (LVs).	[[96, 102], [143, 146]]	[[80, 94], [130, 141]]	['ConjVs', 'LVs']	['conjunct verbs', 'Light Verbs']
2673	pairments. Many have advocated the potential benefits of language sample analysis (LSA) (Johnston, 2006; Dunn et al.,	[[83, 86]]	[[57, 81]]	['LSA']	['language sample analysis']
2674	email: allan.ramsay@manchester.ac.uk Debora Field University of Sheffield (UK) email: D.Field@sheffield.ac.uk	[[75, 77]]	[[50, 73]]	['UK']	['University of Sheffield']
2675	6 BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire; UN = Usenet Newsgroups; and WL = Weblogs.	[[53, 56], [92, 94], [2, 4], [32, 34], [107, 109], [135, 137]]	[[59, 90], [97, 105], [7, 30], [37, 51], [112, 129], [140, 147]]	['CTS', 'NW', 'BC', 'BN', 'UN', 'WL']	['Conversational Telephone Speech', 'Newswire', 'Broadcast Conversations', 'Broadcast News', 'Usenet Newsgroups', 'Weblogs']
2676	In Proceedings of the 2005 International conference on Intelligent User Interfaces (IUI), pages 137?144. ACM Press.	[[84, 87], [105, 108]]	[[55, 82]]	['IUI', 'ACM']	['Intelligent User Interfaces']
2677	model.  Case Frame Editor (CFE): Maintains the lexical  Case Frame Factbase, a data file of how to infer 	[[27, 30]]	[[8, 25]]	['CFE']	['Case Frame Editor']
2678	location(LOC) psych_feature(PSY)  cognition(COG) feeling(FEEL)  motivation(MOT) abstraction(ABS)  time(TIME) space(SPA) attribute(ATT) 	[[75, 78], [92, 95], [9, 12], [28, 31], [44, 47], [57, 61], [103, 107], [115, 118], [130, 133]]	[[64, 73], [80, 90], [0, 8], [14, 27], [34, 43], [49, 56], [98, 102], [109, 114], [120, 129]]	['MOT', 'ABS', 'LOC', 'PSY', 'COG', 'FEEL', 'TIME', 'SPA', 'ATT']	['motivatio', 'abstractio', 'location', 'psych_feature', 'cognition', 'feeling', 'time', 'space', 'attribute']
2679	shown in (1).2  2~Vc use lhe fo l low ing  abbrev ia t ions :  NOM : nominat ive ;   ACC = accusat ive ;  AI)N = adnomina l ;  CI. = c lass i l ier ;  ARGSTR 	[[85, 88], [106, 110]]	[[91, 98], [113, 121]]	['ACC', 'AI)N']	['accusat', 'adnomina']
2680	tity in the contrast set until no distractors are left.  Dale & Reiter speaker frequency (DR-sf) uses a different preferred attribute list for each speaker,	[[90, 95]]	[[57, 88]]	['DR-sf']	['Dale & Reiter speaker frequency']
2681	3.1 Div is ion  and  L inear l i za t ion  o f   Cases   At first, we define a translation pattern (TPi) as fol-  lows.	[[100, 103]]	[[79, 98]]	['TPi']	['translation pattern']
2682	(UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance). 	[[90, 92], [1, 3], [46, 48]]	[[95, 111], [6, 15], [51, 60]]	['SU', 'UT', 'UR']	['System utterance', 'Utterance', 'Utterance']
2683	The model consists of two subtasks of boundary identification(BI) and semantic role classification(SRC). 	[[99, 102], [62, 64]]	[[70, 97], [38, 61]]	['SRC', 'BI']	['semantic role classificatio', 'boundary identification']
2684	vide a significant degree of control. Perhaps nowhere is this observation more keenly  felt than in weak lexical ontologies like Princeton WordNet (PWN). In PWN [1], 	[[148, 151], [157, 160]]	[[129, 146]]	['PWN', 'PWN']	['Princeton WordNet']
2685	form (FFC); from this decision he/she formulates a natural language utterance with certain features including the sentence type (SeTp) the subject type (SuTp) and punctuation (Punct).	[[129, 133], [6, 9], [153, 157], [176, 181]]	[[114, 127], [139, 151], [163, 174]]	['SeTp', 'FFC', 'SuTp', 'Punct']	['sentence type', 'subject type', 'punctuation']
2686	Construct word representation model for  corpus in the base time, D(TB), and in the  target time, D(TT). ( Section 2.1) 	[[100, 102]]	[[85, 96]]	['TT']	['target time']
2687	84  Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), pages 144?151, Ann Arbor, June 2005.	[[82, 87]]	[[41, 80]]	['CoNLL']	['Computational Natural Language Learning']
2688	(1992). Grammars  are defined over typed fea-  twre .structures (TFSs) which can be viewed as  generalizations of first-order terms (Carpenter, 	[[65, 69]]	[[35, 63]]	['TFSs']	['typed fea-  twre .structures']
2689	 FERGUS was originally trained on the Penn Tree Bank corpus consisting of Wall Street Journal text (WSJ). The results on	[[100, 103], [1, 7]]	[[74, 93]]	['WSJ', 'FERGUS']	['Wall Street Journal']
2690	for alignments.  Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art per-	[[43, 46]]	[[17, 41]]	['RNN']	['Recurrent neural network']
2691	Therefore, we refined our reference performance level by combining the ME models (MEM) and handcrafted models (HCM). Suppose the score of a	[[111, 114], [82, 85]]	[[91, 109], [71, 79]]	['HCM', 'MEM']	['handcrafted models', 'ME model']
2692	The entries in the  subjectivity word list have been labeled with  part of speech (POS) tags as well as either  strong or weak subjective tag depending on the 	[[83, 86]]	[[67, 81]]	['POS']	['part of speech']
2693	construct the desired model in a way that allows efficient inference, even for large datasets, using determinantal point processes (DPPs). We begin	[[132, 136]]	[[101, 130]]	['DPPs']	['determinantal point processes']
2694	2.2 Conditional Random Fields Conditional random field (CRF) was an extension of both Maximum Entropy Model (MEMs) and Hidden Markov Models (HMMs) that was firstly	[[109, 113], [56, 59], [141, 145]]	[[86, 107], [30, 54], [119, 139]]	['MEMs', 'CRF', 'HMMs']	['Maximum Entropy Model', 'Conditional random field', 'Hidden Markov Models']
2695	patterns indicative of SLI. In this work, we use Language Models (LMs) for this task since they are a powerful statistical measure of language usage	[[66, 69], [23, 26]]	[[49, 64]]	['LMs', 'SLI']	['Language Models']
2696	The project ? Reference Corpus Middle Low German/ Low Rhenish (1200?1650)?2 transliterates and grammatically annotates the Middle Low German (GML) texts from which we take our examples. Be-	[[142, 145]]	[[95, 133]]	['GML']	['grammatically annotates the Middle Low']
2697	 In addition, the user can supply relevance judgements on  any document by clicking Rel (relevant), NRel (not rel-  evant), or PRel (probably relevant).	[[100, 104], [84, 87], [127, 131]]	[[106, 121], [89, 97], [133, 150]]	['NRel', 'Rel', 'PRel']	['not rel-  evant', 'relevant', 'probably relevant']
2698	not explicit about this. ? P5E2N4S3, F W A Computer Processable Language (CPL) (Clark et al. 2005) is a controlled variant of	[[74, 77], [27, 35]]	[[43, 72]]	['CPL', 'P5E2N4S3']	['Computer Processable Language']
2699	RERANKED 92.7 42.9 92.0 32.6 ORACLE 97.6 81.2 96.7 72.5 Table 4: Word accuracies and error rate reductions (ERR) in percentages for CELEX G2P augmented by Combilex	[[108, 111]]	[[85, 106]]	['ERR']	['error rate reductions']
2700	 1 Introduction Word Sense Disambiguation (WSD) is an important component in many information organization	[[43, 46]]	[[16, 41]]	['WSD']	['Word Sense Disambiguation']
2701	 7.1 Support vector machines  Support vector machines (SVMs) were introduced by (Vapnik, 1995) as an instantiation 	[[55, 59]]	[[30, 53]]	['SVMs']	['Support vector machines']
2702	ailehor ~  The following entry is associated with the class of  verbs taking an NP as indirect objects(IOBJ) which  may be possibly found within a prepositional phrase or 	[[103, 107], [80, 82]]	[[86, 101]]	['IOBJ', 'NP']	['indirect object']
2703	First, we investigate how laypeople intuitively recognize metaphor by conducting Amazon Mechanical Turk (MTurk) experiments.	[[105, 110]]	[[88, 103]]	['MTurk']	['Mechanical Turk']
2704	Table 9 (Hindi). Here, precision measures the number of correct Named Entities (NEs) in the machine tagged file over the total number of NEs in the ma-	[[80, 83], [137, 140]]	[[64, 78]]	['NEs', 'NEs']	['Named Entities']
2705	 53 Creative Information Retrieval (CIR) can be used as a platform for the design of many Web services that offer linguistic creativity on de-mand. By enabling the flexible retrieval of n-gram data for non-literal queries, CIR allows a wide variety of creative tasks to be reimagined as simple IR tasks (Veale 2013).	[[36, 39], [223, 226]]	[[4, 34]]	['CIR', 'CIR']	['Creative Information Retrieval']
2706	 5.1 Calculation of Emotion Tag weights  Sense_Tag_Weight (STW): The tag weight has  been calculated using SentiWordNet.	[[59, 62]]	[[41, 57]]	['STW']	['Sense_Tag_Weight']
2707	description model, the Dublin Core Metadata Set, together with an interchange method provided by the Open Archives Initiative (OAI), make it possible to construct a union catalog over	[[127, 130]]	[[101, 125]]	['OAI']	['Open Archives Initiative']
2708	appear although ModP and FocP are optional.  projections such as NegP (negation phrase) will  not be discussed although we assume there must 	[[65, 69], [16, 20], [25, 29]]	[[71, 86]]	['NegP', 'ModP', 'FocP']	['negation phrase']
2709	We show that hierarchies of this type can be  automatical!y constructed, by using the semantic ategory codes and the subject codes of the  Longman Dictionary of Contemporary English (LDOCE) to disambiguate the genus terms in  noun definitkms.	[[183, 188]]	[[139, 181]]	['LDOCE']	['Longman Dictionary of Contemporary English']
2710	Table 1 provides an overview of all entity classes and relations. The workflow consists of two steps: Firstly, rule- and ontology-based named entity recognition (NER) is performed (cf. Section	[[162, 165]]	[[136, 160]]	['NER']	['named entity recognition']
2711	and why they should be adhered to? involving a coordinated phrase in the object position consisting of an NP (najprostsze zasady ? the most basic principles?)	[[106, 108]]	[[110, 121]]	['NP']	['najprostsze']
2712	The score measures the maximum overlap between a hypothesized cluster (HYP) and a corresponding gold standard cluster (GOLD), and computes a weighted average across all the HYP clus-	[[119, 123], [71, 74], [173, 176]]	[[96, 109], [49, 61]]	['GOLD', 'HYP', 'HYP']	['gold standard', 'hypothesized']
2713	3. Generation of Crisp Descriptions Arguably the most fundamental task in the generation of referring expressions (GRE), content determination (CD) requires finding a set of properties that jointly identify the in-	[[115, 118], [144, 146]]	[[78, 113], [121, 142]]	['GRE', 'CD']	['generation of referring expressions', 'content determination']
2714	 1 Introduction Biomedical Text Mining (TM) has become increasingly popular due to the pressing need to provide	[[40, 42]]	[[27, 38]]	['TM']	['Text Mining']
2715	calizations.  Direct responses (DS) are essentially characterized by introductory markers like yes/no/this is pos-	[[32, 34]]	[[14, 30]]	['DS']	['Direct responses']
2716	"	 strategies(Lewis, 1992).  We use probability threshold(PT) strategy where each document is assigned to the categories above a thresh-"	[[65, 67]]	[[43, 64]]	['PT']	['probability threshold']
2717	CP OP LC RC where CP = Correspondence Part; OP = Operator; LC = Left Context; RC = Right Context There are four different kinds of rules that may be	[[59, 61], [78, 80], [0, 2], [3, 5], [6, 8], [9, 11], [18, 20], [44, 46]]	[[64, 76], [83, 96], [23, 42], [49, 57]]	['LC', 'RC', 'CP', 'OP', 'LC', 'RC', 'CP', 'OP']	['Left Context', 'Right Context', 'Correspondence Part', 'Operator']
2718	In this paper, I present a lexical representation  of the light  verb  ha  'do'  used in two types of  Korean light verb constructions (LVCs). These 	[[136, 140]]	[[110, 134]]	['LVCs']	['light verb constructions']
2719	an algorithm that combines the reference choice rules for  reason and the reference choice rules for methods, to pro-  duce preverbal messages (PMs) from PCAs. As such, the 	[[144, 147]]	[[124, 142]]	['PMs']	['preverbal messages']
2720	No normalization 68.52 (70.45) 84.34 (82.8) 37.4 (45.2) 67.8 (67.1) Table 2: Binary logistic regression 10-fold cross validation with different feature normalization approaches: Scores within brackets are when the female speaker data is removed; S = Stressed, US = Unstressed, MCB = Majority Class Baseline. 	[[260, 262], [277, 280]]	[[265, 275], [283, 306]]	['US', 'MCB']	['Unstressed', 'Majority Class Baseline']
2721	j.nerbonne@rug.nl Abstract Pair Hidden Markov Models (PairHMMs) are trained to align the pronunciation tran-	[[54, 62]]	[[27, 52]]	['PairHMMs']	['Pair Hidden Markov Models']
2722	phrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E). For all baselines we	[[116, 119], [80, 83]]	[[94, 114], [58, 78]]	['S2E', 'E2C']	['Spanish into English', 'English into Chinese']
2723	If we put these two constraints together we obtain the constraint MINS = MAXS, which means that the area where quantifiers take scope (the MAXS-	[[73, 77], [66, 70], [139, 143]]	[]	['MAXS', 'MINS', 'MAXS']	[]
2724	the discrimination of similar languages: The DSL corpus collection. In Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland. 	[[145, 149]]	[[106, 143]]	['BUCC']	['Building and Using Comparable Corpora']
2725	 ? REL = relation + property; ARG = NP/VP/ADJ	[[3, 6], [30, 33]]	[[9, 17], [36, 45]]	['REL', 'ARG']	['relation', 'NP/VP/ADJ']
2726	Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN ? CREST, JST (Japan Science and Technology Corporation) Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012 JAPAN	[[54, 57]]	[[59, 87]]	['JST']	['Japan Science and Technology']
2727	categories include NOUN, VERB, ADJ = adjective, ADV = adverb, NUM = number, ADP = adposition, CONJ = conjunction, DET = determiner, PRON = 1http://www.wiktionary.org/	[[94, 98], [114, 117], [31, 34], [48, 51], [62, 65], [76, 79], [132, 136]]	[[101, 112], [120, 130], [37, 46], [54, 60], [68, 74], [82, 92]]	['CONJ', 'DET', 'ADJ', 'ADV', 'NUM', 'ADP', 'PRON']	['conjunction', 'determiner', 'adjective', 'adverb', 'number', 'adposition']
2728	fer to semantically similar words. We have applied the Markov Cluster algorithm (MCL) (van Dongen, 2000) to group semantically similar terms together.	[[81, 84]]	[[55, 79]]	['MCL']	['Markov Cluster algorithm']
2729	PRN = Pronoun A-FLEX = Adjectival Inflexion  CA = Case CL = Inflsxion Class GD = Gender MD = Mode  PF ~ Predicate Form PS = Person TN = Tense  NU = Number 	[[131, 133], [0, 3], [14, 20], [45, 47], [55, 57], [76, 78], [88, 90], [99, 101], [119, 121], [143, 145]]	[[136, 141], [6, 13], [23, 43], [50, 54], [70, 75], [81, 87], [93, 97], [104, 118], [124, 130], [148, 154]]	['TN', 'PRN', 'A-FLEX', 'CA', 'CL', 'GD', 'MD', 'PF', 'PS', 'NU']	['Tense', 'Pronoun', 'Adjectival Inflexion', 'Case', 'Class', 'Gender', 'Mode', 'Predicate Form', 'Person', 'Number']
2730	In this paper, we address the problem of parsing transcribed spoken Levantine Arabic (LA).We do not assume the existence of any anno-	[[86, 88]]	[[68, 84]]	['LA']	['Levantine Arabic']
2731	See Table 3 for the complete list of non-predicate filters describing restrictions on the role text (RT), role span (RS), and predicate frame (PF) in terms of the semantic type	[[101, 103], [117, 119], [143, 145]]	[[90, 99], [106, 115], [126, 141]]	['RT', 'RS', 'PF']	['role text', 'role span', 'predicate frame']
2732	Knowing the precise identity of Fisher vector ??(?), we propose a natural measure which we call  Weighted Gradient Uncertainty (WGU) based on the facts explained in the previous paragraph:  ????(???)	[[128, 131]]	[[97, 126]]	['WGU']	['Weighted Gradient Uncertainty']
2733	 3. Transitional Phrases (TRP) We hypothesize that a more cohesive essay, being easier for a	[[26, 29]]	[[4, 24]]	['TRP']	['Transitional Phrases']
2734	Pos i t i ve  Recal l  (PR)  :  ? Pos i t ive  Prec is ion  (PP)  :  d ?	[[61, 63], [24, 26]]	[[34, 58], [0, 21]]	['PP', 'PR']	['Pos i t ive  Prec is ion', 'Pos i t i ve  Recal l']
2735	cal work is extensive. We draw on this work to design an Abstract Meaning Representation (AMR) appropriate for sembanking.	[[90, 93]]	[[57, 88]]	['AMR']	['Abstract Meaning Representation']
2736	We would expect this to be the case in general, but as always, cases exist where a conflict between a contrast (CoCo) and a change to a method (PModi) occur:	[[112, 116], [144, 149]]	[[83, 110]]	['CoCo', 'PModi']	['conflict between a contrast']
2737	End point SIP user agents: These are the SIP end points that exchange SIP signaling messages with the SIP Application server (AS) for call control.	[[126, 128], [41, 44], [70, 73], [102, 105]]	[[106, 124], [10, 13]]	['AS', 'SIP', 'SIP', 'SIP']	['Application server', 'SIP']
2738	1 Introduction  Scalability in dialog systems is, of course, not only a  matter of the natural language understanding (NLU)  component, but also of the NLG part of the system.2 We 	[[119, 122], [152, 155]]	[[87, 117]]	['NLU', 'NLG']	['natural language understanding']
2739	the divergence of their distributions in the targets and backgrounds. A support vector machine (SVM) was used to learn to classify between the targets and	[[96, 99]]	[[72, 94]]	['SVM']	['support vector machine']
2740	set of basically two algorithms. One algorithm is a  variant of Alignment Based Learning (ABL), as  described in Van Zaanen (2001).	[[90, 93]]	[[64, 88]]	['ABL']	['Alignment Based Learning']
2741	passenger vessel.  (5) Multiple mentions (MENTION): These alignments link one word to multiple occur-	[[42, 49]]	[[32, 40]]	['MENTION']	['mentions']
2742	 3.1 Identifying verbal blocks (Vbs) Verbal blocks are composed of a head (Vb-H) and possibly accompanying dependents (Vb-D).	[[75, 79], [32, 35], [119, 123]]	[[37, 73], [17, 30]]	['Vb-H', 'Vbs', 'Vb-D']	['Verbal blocks are composed of a head', 'verbal blocks']
2743	In Proceedings of the 2nd Workshop on Treebanks and Linguistic Theories (TLT), pages 217?220, Va?xjo?. 	[[73, 76]]	[[38, 71]]	['TLT']	['Treebanks and Linguistic Theories']
2744	Abbreviations: BI=Bioinformatician, NLP=Natural Language Processing researcher, ML=Machine Learning researcher, L=Linguist, Porter=Porter stemmer, McCCJ=McClosky-Charniak-Johnson parser, SD=Stanford Dependency conversion, Dict=Dictionary UTurku VIBGhent ConcordU HCMUS	[[187, 189], [15, 17], [36, 39], [80, 82], [124, 130], [147, 152], [222, 226]]	[[190, 209], [18, 34], [40, 67], [83, 99], [114, 122], [131, 137], [153, 178], [227, 237]]	['SD', 'BI', 'NLP', 'ML', 'Porter', 'McCCJ', 'Dict']	['Stanford Dependency', 'Bioinformatician', 'Natural Language Processing', 'Machine Learning', 'Linguist', 'Porter', 'McClosky-Charniak-Johnson', 'Dictionary']
2745	 We compared the resulting ranked lists of bigrams with a list of target MWEs extracted from the British National Corpus (BNC)3. The target list was pro-	[[122, 125], [73, 77]]	[[97, 120]]	['BNC', 'MWEs']	['British National Corpus']
2746	ing measure of the loss in modeling accuracy:     Probability Loss (PL):   )()()(),( vuvuvu +?	[[68, 70]]	[[50, 66]]	['PL']	['Probability Loss']
2747	 1 Introduction The task of Semantic Role Labeling (SRL) is to identify predicate-argument relationships in natural	[[52, 55]]	[[28, 50]]	['SRL']	['Semantic Role Labeling']
2748	rithms for learning neuropsychological and demographic data which are then used for the prediction of Clinical Dementia Rating (CDR) scores for different sub-types of Dementia and other cog-	[[128, 131]]	[[102, 126]]	['CDR']	['Clinical Dementia Rating']
2749	Conf.  Fifth Generation Computer Systems 1992 (FGCS'92),  pp.1133-1140, 1992.	[[47, 54]]	[[7, 45]]	"[""FGCS'92""]"	['Fifth Generation Computer Systems 1992']
2750	pairs. Transactions of the Association for Computational Linguistics (TACL), 2(10):377?392. 	[[70, 74]]	[[7, 68]]	['TACL']	['Transactions of the Association for Computational Linguistics']
2751	Lexical Entropy (LexH) -0.26 1.00 0.01 -0.40 0.43 -0.38 -0.03 0.02 0.11 -0.29 Syntactic Surprisal (SynS) 0.00 0.01 1.00 -0.12 0.08 0.18 0.77 0.21 0.38 -0.03 Lexical Surprisal (LexS) 0.24 -0.40 -0.12 1.00 -0.81 0.87 -0.10 -0.20 -0.35 0.64 Unigram Frequency (Freq) -0.24 0.43 0.08 -0.81 1.00 -0.69 0.02 0.18 0.31 -0.72	[[176, 180], [17, 21], [99, 103], [257, 261]]	[[157, 174], [0, 15], [78, 97], [246, 255]]	['LexS', 'LexH', 'SynS', 'Freq']	['Lexical Surprisal', 'Lexical Entropy', 'Syntactic Surprisal', 'Frequency']
2752	 Some implementation otes  The Carnegie Mellon Spoken Language Shell (CM-SLS)  was intentionally designed to have easily modifiable com- 	[[70, 76]]	[[31, 68]]	['CM-SLS']	['Carnegie Mellon Spoken Language Shell']
2753	 The actual performance of a system is measured in terms of detection error tradeoff (DET) curves and the minimal normalized cost.	[[86, 89]]	[[60, 84]]	['DET']	['detection error tradeoff']
2754	for evaluating the ASR,  2. Concept F-measure (ConF) ? the F-measure of 	[[47, 51], [19, 22]]	[[28, 37]]	['ConF', 'ASR']	['Concept F']
2755	Abstract  This paper proposes a novel reordering model  for statistical machine translation (SMT) by  means of modeling the translation orders of 	[[93, 96]]	[[60, 91]]	['SMT']	['statistical machine translation']
2756	is associated with the data sparseness problem. Most of the previously proposed methods to  extract compounds or to measure word association using mutual information (MI) either ignore  or penalize items with low co-occurrence ounts (Church and Hanks 1990, Su, Wu and Chang 	[[167, 169]]	[[147, 165]]	['MI']	['mutual information']
2757	 From the results shown in Table 3, we could find the proposed semantic word embedding (SWE) model can consistently achieve 0.8% (or more) ab-	[[88, 91]]	[[63, 86]]	['SWE']	['semantic word embedding']
2758	cal machine translation. The 41th Annual meeting of the Association for Computational Linguistics (ACL), 311-318.	[[99, 102]]	[[56, 97]]	['ACL']	['Association for Computational Linguistics']
2759	tences in the other part of the corpus. Therefore, we performed a language identification (LID)based filtering afterwards (performed only on the	[[91, 94]]	[[66, 89]]	['LID']	['language identification']
2760	 Reference:  MedLine sample # 6  Autonym:  decoy receptor 3 (DcR3)  Information a soluble decoy receptor  	[[61, 65]]	[[43, 59]]	['DcR3']	['decoy receptor 3']
2761	 We perform our analyses on data from the 20082011 Text Analysis Conference (TAC)1 organized by the National Institute of Standards and Technol-	[[77, 80]]	[[51, 75]]	['TAC']	['Text Analysis Conference']
2762	1978. Longman Dictionary of  Contemporary lCnglish (LI)OCE). Long\]nan, liar- 	[[51, 59]]	[[6, 50]]	['(LI)OCE)']	['Longman Dictionary of  Contemporary lCnglish']
2763	Harman D.K. 1983. Overview of the second Text Retrieval Conference (TREC-2). Information Processing	[[68, 74], [7, 10]]	[[34, 66]]	['TREC-2', 'D.K']	['second Text Retrieval Conference']
2764	parsing. In Tenth International Conference on Parsing Technologies (IWPT), pages 121?132, Prague, Czech Republic.	[[68, 72]]	[[18, 66]]	['IWPT']	['International Conference on Parsing Technologies']
2765	In Processdings of Sixth International Conference on  Language Resources and Evaluation (LREC),  pages 2961-2968, Marrakech, Morocco.	[[89, 93]]	[[54, 72]]	['LREC']	['Language Resources']
2766	 ? Because Dependency Grammar (DG) directly describes the functional relations between  words, and s dependency tree has not any non-terminal nodes, DG is suitable for our 	[[31, 33], [149, 151]]	[[11, 29]]	['DG', 'DG']	['Dependency Grammar']
2767	other animate and inanimate), which cover ten categories of noun phrases, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other	[[115, 119], [95, 98], [133, 136]]	[[121, 127], [100, 112], [150, 158]]	['ANIM', 'ORG', 'MAC']	['animal', 'organization', 'machines']
2768	51 Table 1: Semantic restrictions on Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model of Anatomy, other ontologies identified in the text.	[[61, 64], [88, 91]]	[[67, 86], [94, 123]]	['CCO', 'FMA']	['Cell Cycle Ontology', 'Foundational Model of Anatomy']
2769	ond accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal object, EP = expletive es 9AG = genitive adjunct	[[122, 124], [23, 25], [36, 38], [58, 60], [85, 87], [101, 103], [140, 143]]	[[127, 136], [28, 34], [41, 56], [63, 83], [90, 99], [106, 120], [146, 162]]	['EP', 'DA', 'OG', 'OP', 'PD', 'OC', '9AG']	['expletive', 'dative', 'genitive object', 'prepositional object', 'predicate', 'clausal object', 'genitive adjunct']
2770	Budanitsky and Hirst Lexical Semantic Relatedness Figure 5 Precision (PD), recall (RD), and F-measure (FD) for malapropism detection by measure and scope. 	[[70, 72], [83, 85], [103, 105]]	[[59, 68], [75, 81], [92, 101]]	['PD', 'RD', 'FD']	['Precision', 'recall', 'F-measure']
2771	For words which failed to be guessed by  tile guessing rules we applied the standard method  of classifying them as common nouns (NN) if they  are not capitalised inside a sentence and proper 	[[130, 132]]	[[123, 128]]	['NN']	['nouns']
2772	"f ~ r l r ~ f  TRANSFORIATICNS *llrllrt*  SCAN C A L L E D  AT 1 I  ANTEST CALLED FOR 1 '*REDVOW "" (AACC) ,SD= 2. RES= 6."	[[82, 85], [100, 104], [107, 109], [114, 117]]	[[68, 81]]	['FOR', 'AACC', 'SD', 'RES']	['ANTEST CALLED']
2773	Currently a large proportion of languageindependent MT approaches are based on the  statistical machine translation (SMT) paradigm  (Koehn, 2010).	[[117, 120], [52, 54]]	[[84, 115]]	['SMT', 'MT']	['statistical machine translation']
2774	By experiments, we show that the proposed model outperforms the bigram Hidden Markov model (HMM)based tagging model.	[[92, 95]]	[[71, 90]]	['HMM']	['Hidden Markov model']
2775	PROBING QUESTION (QP) Do you think that looks correct? 4.99% 4.76% 0.731 QUESTION PROMPT (QQ) Any questions? 2.49% 2.24% 0.978	[[18, 20], [90, 92]]	[[73, 88], [0, 16]]	['QP', 'QQ']	['QUESTION PROMPT', 'PROBING QUESTION']
2776	\[ Extra~:~nouns I IE~rac~'~ n?unsl i Calculating f requ~ vectors (FreqVa) I ICalculating frequency vectors (FreqVe)l  ._1 Calculating similarity I 	[[109, 115], [67, 73]]	[[90, 107], [50, 65]]	['FreqVe', 'FreqVa']	['frequency vectors', 'f requ~ vectors']
2777	In the next section we will explain the concep-  tual model of ILMs by means of the KADS Domain  Description Language (DDL) proposed in Schreiber  (Schreiber et al, 1993).	[[119, 122], [63, 67], [84, 88]]	[[97, 117]]	['DDL', 'ILMs', 'KADS']	['Description Language']
2778	In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD, October. 	[[117, 120], [134, 136]]	[[104, 115]]	['TMI', 'MD']	['Translation']
2779	sentences. The third, following (Yates et al, 2006), is maximum recall (MR). MR simply predicts that all	[[72, 74], [77, 79]]	[[56, 70]]	['MR', 'MR']	['maximum recall']
2780	Our relation extraction system is hierarchical (Bunescu and Mooney, 2005b; Sun et al, 2011) and apply maximum entropy (MaxEnt) in the MALLET	[[119, 125], [134, 140]]	[[102, 117]]	['MaxEnt', 'MALLET']	['maximum entropy']
2781	Processing, Hong Kong, Apr. HTK, 2004. Hidden Markov Model Toolkit (HTK) 3.2.	[[68, 71], [28, 31]]	[[39, 66]]	['HTK', 'HTK']	['Hidden Markov Model Toolkit']
2782	Task (Pradhan et al 2011), one text from each of the five represented genres: Broadcast Conversations (BC), Broadcast News (BN), Magazine (MZ), News Wire (NW) and Web Blogs and News Groups	[[124, 126], [139, 141]]	[[108, 122], [129, 137]]	['BN', 'MZ']	['Broadcast News', 'Magazine']
2783	"i' SRI - text extraction  ~"" TRW - document detection output  i' University of Massachusetts (UMass) -  document detection "	[[94, 99], [29, 32], [3, 6]]	[[65, 92]]	['UMass', 'TRW', 'SRI']	['University of Massachusetts']
2784	CTexT. 2011. Afrikaans WordNet. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa.	[[60, 65], [0, 5]]	[[32, 58]]	['CTexT', 'CTexT']	['Centre for Text Technology']
2785	The task of location normalization is to identify  the correct sense of a possibly ambiguous  location Named Entity (NE). Ambiguity is very 	[[117, 119]]	[[103, 115]]	['NE']	['Named Entity']
2786	 In Proceedings ofthe 12th International Conference on  Computational Linguistics (COLING), Budapest. 	[[83, 89]]	[[56, 81]]	['COLING']	['Computational Linguistics']
2787	Probabilistic topic models (PTM), such as probabilistic latent semantic indexing(PLSI) (Hofmann, 1999) and latent Dirichlet alocation(LDA) (Blei et al.,	[[134, 137], [28, 31], [81, 85]]	[[107, 132], [0, 26], [42, 80]]	['LDA', 'PTM', 'PLSI']	['latent Dirichlet alocatio', 'Probabilistic topic models', 'probabilistic latent semantic indexing']
2788	3.4 Algorithm The algorithm first splits the data into appropriate units (SL=source language, TL=target language): 1.	[[74, 76], [94, 96]]	[[77, 92], [97, 112]]	['SL', 'TL']	['source language', 'target language']
2789	4.3 Experiments with the QA data In the first set of experiments we focus on the Question Answering (QA) domain (CLEF corpus). 	[[101, 103], [25, 27], [113, 117]]	[[81, 99]]	['QA', 'QA', 'CLEF']	['Question Answering']
2790	This work investigated four well-known specifications created by four different organizations: Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (Beijing)	[[112, 114], [147, 152]]	[[95, 110], [117, 132]]	['AS', 'CITYU']	['Academia Sinica', 'City University']
2791	on the SMTnews dataset, with an increase in the Pearson correlation of over 0.10. MSRpar (MPar) is the only dataset in which TLsim (S?aric?	[[90, 94], [125, 130]]	[[82, 88], [7, 10]]	['MPar', 'TLsim']	['MSRpar', 'SMT']
2792	processing. This paper explores grammatical issues in Scottish Gaelic by means of dependency tagging and combinatory categorial grammar (CCG), which we see as complementary approaches. As such it	[[137, 140]]	[[105, 135]]	['CCG']	['combinatory categorial grammar']
2793	_ _eI$X=AI&HLH7K5HOGX5HOGPMLHLK ^ CWX=A$X=APH UI&K5XK5HOGX5H&GflMLHJaHLK5MOI5CEcCEG! 	[[44, 47]]	[[48, 90]]	['APH']	['U\x03I&K5X\x10K5HOG\x07X5H&GflMLHJa\x07HLK5MOI5CEc\x07CEG']
2794	ILP.  An integer linear program(ILP) is basically the same as a linear program.	[[32, 35], [0, 3]]	[[9, 30]]	['ILP', 'ILP']	['integer linear progra']
2795	than have them specified in a tag dictionary.  The Lexicon HMM (Lex-HMM) extends the Pitman-Yor HMM (PYP-HMM) described by	[[64, 71], [101, 108]]	[[51, 62], [85, 99]]	['Lex-HMM', 'PYP-HMM']	['Lexicon HMM', 'Pitman-Yor HMM']
2796	Cleveland Family study dceweb1.case.edu/ serc/collab/project_family.shtml), CHS (the Cardiovascular Heart Study www. 	[[76, 79]]	[[85, 111]]	['CHS']	['Cardiovascular Heart Study']
2797	editor, Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC?08), Marrakech, Morocco, may. European Language Resources Association (ELRA). http://www.lrec-	[[172, 176], [96, 103]]	[[131, 170], [61, 94]]	['ELRA', 'LREC?08']	['European Language Resources Association', 'Language Resources and Evaluation']
2798	of not requiring so much copying. On the con-  trary, constraint unification (CU) (Hasida 1986,  Tuda et al 1989), a disjunctive unification 	[[78, 80]]	[[54, 76]]	['CU']	['constraint unification']
2799	such as Declarative Sentence(SDEC),  Noun Phrase(NP), Inf init ive  Phrase(INF), and Verb Phrase(VP),  are big structures with some.k~y 	[[97, 99], [29, 33], [49, 51], [75, 78]]	[[85, 95], [8, 28], [37, 48], [54, 74]]	['VP', 'SDEC', 'NP', 'INF']	['Verb Phras', 'Declarative Sentence', 'Noun Phrase', 'Inf init ive  Phrase']
2800	of Translation Studies SI - 1000 Ljubljana, A?ker?eva 2 aljosav@gmail.com     Abstract  We report on a series of experiments aimed at improving the machine translation of ambig-uous lexical items by using wordnet-based unsupervised Word Sense Disambiguation (WSD) and comparing its results to three MT systems. Our experiments are performed for the English-Slovene language pair using UKB, a freely available graph-based word sense disambiguation system.	[[259, 262], [299, 301], [385, 388]]	[[232, 257]]	['WSD', 'MT', 'UKB']	['Word Sense Disambiguation']
2801	 4.3 MORPHOTACT1C MODEL  An associative Morphotactic Model (MTModel) is a pair  <{MRi},<*>, where {MRi} is a set of morphotactic rules 	[[60, 67], [82, 85], [99, 102]]	[[40, 58]]	['MTModel', 'MRi', 'MRi']	['Morphotactic Model']
2802	 In Section 3, we report on two Amazon Mechanical Turk (MTurk) experiments, which demonstrate that crowdsourcing is a feasible way	[[56, 61]]	[[39, 54]]	['MTurk']	['Mechanical Turk']
2803	overall 412 5298 1519 750 Table 1: Corpus statistics: number of sentences (S), words (W), frame elements (FE) and alignments.	[[106, 108]]	[[90, 104], [64, 73], [79, 84]]	['FE']	['frame elements', 'sentences', 'words']
2804	workbench (Hall et al, 2009). For SVM, we employ the radial basis function kernel (RBF) and we use the wrapper provided by Weka for	[[83, 86], [34, 37]]	[[53, 74]]	['RBF', 'SVM']	['radial basis function']
2805	3 Architecture of SCQA As shown in Figure 2, SCQA consists of a pair of deep convolutional neural networks (CNN) with convolution, max pooling and rectified lin-	[[108, 111], [18, 22], [45, 49]]	[[77, 106]]	['CNN', 'SCQA', 'SCQA']	['convolutional neural networks']
2806	  Abstract  Named entity recognition (NER) is nowadays an important task, which is responsi-	[[38, 41]]	[[12, 36]]	['NER']	['Named entity recognition']
2807	plementation of SVR, with tuned parameters.  Ranking: An SVM model for ranking (SVMRank) is trained using as ranking pairs all pairs of stu-	[[80, 87], [16, 19]]	[[57, 78]]	['SVMRank', 'SVR']	['SVM model for ranking']
2808	 110  ehange(CHA) communication(COMM)  cognition(COG) competition(COMP) 	[[32, 36], [13, 16], [49, 52], [66, 70]]	[[18, 30], [6, 12], [39, 48], [54, 65]]	['COMM', 'CHA', 'COG', 'COMP']	['communicatio', 'ehange', 'cognition', 'competition']
2809	Kearns (2002) distinguishes between two usages of light verbs in LVCs: what she calls a true light verb (TLV), as in give a groan, and what she calls a vague action verb (VAV), as in	[[105, 108], [65, 69], [171, 174]]	[[88, 103], [152, 169]]	['TLV', 'LVCs', 'VAV']	['true light verb', 'vague action verb']
2810	Abstract Verbal comprehension questions appear very frequently in Intelligence Quotient (IQ) tests, which measure human?s verbal ability includ-	[[89, 91]]	[[66, 87]]	['IQ']	['Intelligence Quotient']
2811	Pr(f |e) Pr(e) (2) where Pr(f |e) is the translation model and Pr(e) is the target language model (LM). This ap-	[[99, 101]]	[[83, 97]]	['LM']	['language model']
2812	by (Punyakanok et al, 2004). The process is formulated as an integer linear programming (ILP) problem that takes as inputs the confidences over each	[[89, 92]]	[[61, 87]]	['ILP']	['integer linear programming']
2813	tion. So we developed a method to optimize the CRFs towards the alignment error rate (AER) or the F-score with sure and possible links as introduced	[[86, 89]]	[[64, 84]]	['AER']	['alignment error rate']
2814	837 (a) (b) Figure 1: Deep recurrent neural network (DRNN) architectures: arrows represent connection matrices; white, black, and grey circles represent input frames, hidden states, and output frames, respectively; (a): L intermediate layer DRNN with recurrent connections	[[53, 57], [241, 245]]	[[22, 51]]	['DRNN', 'DRNN']	['Deep recurrent neural network']
2815	2.2 Hidden Markov Models One simple family of models for part-of-speech induction are the Hidden Markov Models (HMMs), in which there is a sequence of hidden state vari-	[[112, 116]]	[[90, 110]]	['HMMs']	['Hidden Markov Models']
2816	methods to identify such targets. The first method depends on identifying noun groups (NG). We con-	[[87, 89]]	[[74, 85]]	['NG']	['noun groups']
2817	We therefore chose to perform ASR using a statistical language model (LM) and employ CMU?s Sphinx to generate an n-best list of recogni-	[[70, 72], [30, 33], [85, 90]]	[[54, 68]]	['LM', 'ASR', 'CMU?s']	['language model']
2818	Table 2  Summary of error rates with the language model only (LM), the prosody model only (PM), the  combined ecision tree (CM-DT), and the combined HMM (CM-HMM). ( a) shows word-based 	[[124, 129], [154, 160], [62, 64], [91, 93]]	[[101, 122], [140, 152], [41, 55], [71, 84]]	['CM-DT', 'CM-HMM', 'LM', 'PM']	['combined ecision tree', 'combined HMM', 'language model', 'prosody model']
2819	n of  PSP Positive?sentence percentage (PSP) statistics  	[[40, 43], [6, 9]]	[[10, 38]]	['PSP', 'PSP']	['Positive?sentence percentage']
2820	Ensemble NN + LR (w/o alternate grammar) 54.38 41.90 Ensemble NN + LR (w/o synthetic data) 53.98 42.41 Table 1: Accuracy of the Neural Network (NN) and Logistic Regression (LR) implementations of our system with various configurations.	[[144, 146], [173, 175], [9, 16], [62, 69]]	[[128, 142], [152, 171]]	['NN', 'LR', 'NN + LR', 'NN + LR']	['Neural Network', 'Logistic Regression']
2821	(Bjo?rne et al 2011).  The Turku Event Extraction System (TEES)1 is an open source program for extracting events and re-	[[58, 62]]	[[27, 56]]	['TEES']	['Turku Event Extraction System']
2822	In  * This work has been sponsored by the Fonds zur  FSrderung der wissenschaftlichen Forschung (FWF),  Grant No.	[[97, 100]]	[[53, 95]]	['FWF']	['FSrderung der wissenschaftlichen Forschung']
2823	both in the form of documents and factual  databases. These knowledge sources (KSs) are  intrinsically heterogeneous and dynamic.	[[79, 82]]	[[60, 77]]	['KSs']	['knowledge sources']
2824	LA     =   The average length (ALen) of chunks for each  type is the average number of tokens in each chunk 	[[31, 35], [0, 2]]	[[15, 29]]	['ALen', 'LA']	['average length']
2825	We show each sentence to three unique workers on Amazon Mechanical Turk (MTurk) and ask each to judge how well the paraphrase retains the mean-	[[73, 78]]	[[56, 71]]	['MTurk']	['Mechanical Turk']
2826	 This  suspens ion takes place dur ing un i f icat ion  of  the Flat Concurrent  Pro log (FCP) pred icate   (see below), into which expert  rout ines are 	[[90, 93]]	[[64, 88]]	['FCP']	['Flat Concurrent  Pro log']
2827	The typical way to address these situations is to jointly model these relations, e.g., using Markov logic networks (MLN) (Poon and Vanderwende, 2010).	[[116, 119]]	[[93, 114]]	['MLN']	['Markov logic networks']
2828	In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA), Denver, Colorado. 	[[93, 97]]	[[22, 91]]	['AMTA']	['Conference of the Association for Machine Translation in the Americas']
2829	are also kernel methods that directly take into account the multiclass nature of the problem such as the kernel partial least squares regression (KPLS). 	[[146, 150]]	[[105, 144]]	['KPLS']	['kernel partial least squares regression']
2830	A model-theoretic coreference scoring scheme. In Proceedings of Message Understanding Conference 6 (MUC-6), pages 45?52. 	[[100, 105]]	[[64, 98]]	['MUC-6']	['Message Understanding Conference 6']
2831	"tially lexicalized) syntactic dependencies and patterns. The weight   is the Local Mutual Informa-tion (LMI) (Evert, 2005) computed on link type frequency (negative LMI values are raised to 0).3.1 Test set"	[[104, 107], [165, 168]]	[[77, 102]]	['LMI', 'LMI']	['Local Mutual Informa-tion']
2832	equivalent m Dutch For a sample of 59 Ital,an noun  s)nsets there is at least an overlap of 30% (20) with  Dutch Examples are Arbeltszeitverkurzung (DE)  = arbeidstijdverkortmg (NL) = (reduction of work- 	[[149, 151], [178, 180]]	[[107, 121]]	['DE', 'NL']	['Dutch Examples']
2833	"The second approach is based on statistical modeling. We adopted one typical  implementation called the ""vector space model"" (VSM) (Frakes and Baeza-Yates 1992;  Leacock, Towell, and Voorhees 1993; Salton and McGill 1983; Sch/itze 1992), which has "	[[126, 129]]	[[105, 123]]	['VSM']	['vector space model']
2834	2.1 The Machine Translation Performance Predictor (MTPP) In machine translation (MT), pairs of source and target sentences are used for training statistical	[[81, 83], [51, 55]]	[[60, 79], [8, 49]]	['MT', 'MTPP']	['machine translation', 'Machine Translation Performance Predictor']
2835	versational participants. This type of HMM is called a speaker HMM (SHMM) and has been successfully utilized to model two-party conversa-	[[68, 72], [39, 42]]	[[55, 66]]	['SHMM', 'HMM']	['speaker HMM']
2836	there are two ways of feeding the context vector into the main recurrent language model (RLM); (1) early fusion (EF) and (2) late fusion (LF), from Sec.	[[113, 115], [138, 140], [89, 92]]	[[99, 111], [125, 136], [63, 87]]	['EF', 'LF', 'RLM']	['early fusion', 'late fusion', 'recurrent language model']
2837	ADJ (adjectives), ADV (adverbs), CJ (conjunctions), CL (clitics), CN (common nouns), DA (definite articles), DEM (demonstratives), INF (infinitives), ITJ (interjections), NP (noun	[[85, 87], [109, 112], [0, 3], [18, 21], [33, 35], [52, 54], [66, 68], [131, 134], [150, 153], [171, 173]]	[[89, 106], [114, 128], [5, 15], [23, 30], [37, 49], [56, 63], [70, 83], [136, 147], [155, 168], [175, 179]]	['DA', 'DEM', 'ADJ', 'ADV', 'CJ', 'CL', 'CN', 'INF', 'ITJ', 'NP']	['definite articles', 'demonstratives', 'adjectives', 'adverbs', 'conjunctions', 'clitics', 'common nouns)', 'infinitives', 'interjections', 'noun']
2838	grammars is denoted CFGS.  In a linear indexed grammar (LIG),2 strings are derived from nonterminals with an associated	[[56, 59], [20, 24]]	[[32, 54]]	['LIG', 'CFGS']	['linear indexed grammar']
2839	1 Reinforcement Learning in Dialogue Machine Learning techniques, and particularly Reinforcement Learning (RL), have recently received great interest in research on dialogue man-	[[107, 109]]	[[83, 105]]	['RL']	['Reinforcement Learning']
2840	168 Figure 1: Plots of concreteness vs. imageability scores for literal vs. nonliteral words in the VUAMC (Conc=concreteness, Imag=imageability, NL=nonliteral, L=literal) concrete than the dependent/s; H	[[145, 147], [100, 105], [107, 111], [126, 130]]	[[148, 158], [112, 124], [131, 143], [162, 169]]	['NL', 'VUAMC', 'Conc', 'Imag']	['nonliteral', 'concreteness', 'imageability', 'literal']
2841	 For example, the surface subject (S-SBJ) of a passive verb is also the logical object (L-OBJ). These	[[88, 93], [35, 40]]	[[72, 86], [18, 33]]	['L-OBJ', 'S-SBJ']	['logical object', 'surface subject']
2842	"= Majority Class, Acc. = Accuracy, SE = Standard Error) ing on 5   with 20-fold cross-validation achieves an"	[[35, 37], [18, 21]]	[[40, 54], [25, 33]]	['SE', 'Acc']	['Standard Error', 'Accuracy']
2843	hypernym, hyponym, near-synonym, holonym, and mernoym are listed as below:  Hypernym(HYP) (a) IF x=ANT 	[[85, 88], [99, 102]]	[[76, 83]]	['HYP', 'ANT']	['Hyperny']
2844	chine learning models based on three different well known techniques, decision trees (C4.5), rule induction (RIPPER) and maximum entropy (MaxEnt), in order to find out which approach is the most suitable	[[138, 144], [109, 115]]	[[121, 136]]	['MaxEnt', 'RIPPER']	['maximum entropy']
2845	ABSTRACT  We present a progress report on our research  on nominal compounds (NC's). Recent approaches to 	[[78, 82]]	[[59, 76]]	"[""NC's""]"	['nominal compounds']
2846	that to go from the head of the chunk to the target in the dependency graph (Figure 3), you traverse a SUB (subject) link upwards. 	[[103, 106]]	[[108, 115]]	['SUB']	['subject']
2847	To our knowledge there exist two off the shelf English Arabic Machine Translation (MT) systems: Tarjim and Almisbar.3 We use both MT systems to translate	[[83, 85], [130, 132]]	[[62, 81]]	['MT', 'MT']	['Machine Translation']
2848	NNS?, in this paper; other work makes a distinction between ESL (English as a Second Language) speakers (who live and speak in a primarily English-speaking environment) or EFL	[[60, 63], [0, 4], [172, 175]]	[[65, 93]]	['ESL', 'NNS?', 'EFL']	['English as a Second Language']
2849	2 Preposition Semantic Role Disambiguation in Penn Treebank Significant numbers of prepositional phrases (PPs) in the Penn treebank [1] are tagged with their semantic role relative to the governing verb.	[[106, 109]]	[[83, 104]]	['PPs']	['prepositional phrases']
2850	competition submissions). Notice that most were using Support Vector Machine (SVM) with bagof-word features in a very small window, local col-	[[78, 81]]	[[54, 76]]	['SVM']	['Support Vector Machine']
2851	 3.2 Swedish Constructicon The Swedish Constructicon (SweCcn) 4	[[54, 60]]	[[31, 52]]	['SweCcn']	['Swedish Constructicon']
2852	Labels Base NP modifier NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR (proper noun), NT (temporal noun), JJ (other noun-modifier), or PU (punctuation) Base NP head NN (common noun), M (measure word), CD (cardinal number), OD (ordinal number), PN (pronoun), NR	[[184, 186]]	[[188, 199]]	['PU']	['punctuation']
2853	The pair of speakers KI-KA is in the top quin-tile (>13.6%). Based on this evidence we can conclude the following:   ExpDisagreementDRX (KI,KA, 5, dialogue-1) which may be read as follows: speakers KI and KA have the highest level of expressive disagreement in dialogue-1. This measure is complemented by a Cumulative Disagreement Index (CDX), which is computed for each speaker as a percentage of all Disagree-Reject utterances in the discourse that are made by this speaker. Unlike DRX, which is computed for pairs of speakers, the CDX values are as-signed to each group participant and indicate the degree of disagreement that each person generates.	[[338, 341], [21, 26], [198, 200], [205, 207], [484, 487], [534, 537]]	[[307, 336]]	['CDX', 'KI-KA', 'KI', 'KA', 'DRX', 'CDX']	['Cumulative Disagreement Index']
2854	This model is a multinomial DP model. Under the Chinese restaurant process (CRP) (Aldous, 1985) 394	[[76, 79], [28, 30]]	[[48, 74]]	['CRP', 'DP']	['Chinese restaurant process']
2855	      Input source sentence (ISS)    	[[29, 32]]	[[6, 27]]	['ISS']	['Input source sentence']
2856	learning this decision is learned automatically.  Reinforcement Learning (RL) has been successfully used for learning dialogue management	[[74, 76]]	[[50, 72]]	['RL']	['Reinforcement Learning']
2857	2.2 Graphical Representation Recently, Ding et al (2008) use skip-chain and 2D Conditional Random Fields (CRFs) (Lafferty et al, 2001) to perform the relational learning for	[[106, 110], [76, 78]]	[[79, 104]]	['CRFs', '2D']	['Conditional Random Fields']
2858	10-fold open test 62.80-58.54 59.15 66.46-65.55 65.55 65.55-64.63 Table 5: Comparison of Optimizers (Opinions in KNB Corpus) Batch Training (BFGS) Online Training (SD) Simulated parameter initialization chunked data selection Annealing	[[141, 145], [113, 116], [164, 166]]	[[125, 139]]	['BFGS', 'KNB', 'SD']	['Batch Training']
2859	2005; Wieling et al, 2007) for string similarity  estimation, and is based on the notion of string  Edit Distance (ED). String ED is defined here as 	[[115, 117], [127, 129]]	[[100, 113]]	['ED', 'ED']	['Edit Distance']
2860	tation for extracting entities from w. In our system, we let an extraction predicate be a simplified XML path (XPath) such as /html[1]/body[1]/table[2]/tr/td[1]	[[111, 116]]	[[101, 109]]	['XPath']	['XML path']
2861	maries that are too specific. In this paper, we propose a natural language generation (NLG) model for the automatic creation of indicative multidoc-	[[87, 90]]	[[58, 85]]	['NLG']	['natural language generation']
2862	*Event, *Mtrans-Action), and plans (i.e. *Pick-Up-  Gun). A hierarchy of Concept Class (CC) entities  stores knowledge both declaratively and procedurely 	[[88, 90]]	[[73, 86]]	['CC']	['Concept Class']
2863	discussed in section 2, are represented.  Evaluation of machine translation (MT) systems has to consider the pre-processing of input and	[[77, 79]]	[[56, 75]]	['MT']	['machine translation']
2864	Two-level rules are generally of the form CP OP LC RC where CP = Correspondence Part; OP = Operator; LC = Left Context; RC = Right Context	[[60, 62], [86, 88], [42, 44], [45, 47], [48, 50], [51, 53], [101, 103], [120, 122]]	[[65, 84], [91, 99], [106, 118], [125, 138]]	['CP', 'OP', 'CP', 'OP', 'LC', 'RC', 'LC', 'RC']	['Correspondence Part', 'Operator', 'Left Context', 'Right Context']
2865	Normalization (WCCN) (Dehak et al., 2011) and Eigen Factor Radial (EFR) (Bousquet et al., 2011).	[[67, 70], [15, 19]]	[[46, 65]]	['EFR', 'WCCN']	['Eigen Factor Radial']
2866	corpora for our experiments. The first is a new corpus of 70 articles from New York Times (NYT) LDC corpus, each describing one or more terrorist events	[[91, 94], [96, 99]]	[[75, 89]]	['NYT', 'LDC']	['New York Times']
2867	case where estimated user?s knowledge and preference are represented as discrete binary parameters instead of probability distributions (PDs). That is, the estimated	[[137, 140]]	[[110, 135]]	['PDs']	['probability distributions']
2868	full PTB, using 1st sense information. All results  are shown as labelled attachment score (LAS). 	[[92, 95], [5, 8]]	[[65, 90]]	['LAS', 'PTB']	['labelled attachment score']
2869	a. the 1000-headlines text (target domain) 1,181 40.2 32.1 35.7 b. the TEC (source domain) 32,954 29.9 26.1 27.9 c. the 1000-headlines text and the TEC (target and source) c.1.	[[148, 151], [71, 74]]	[[153, 170]]	['TEC', 'TEC']	['target and source']
2870	 1 In t roduct ion   For most natural language processing (NLP) systems,  thesauri comprise indispensable linguistic knowledge.	[[59, 62]]	[[30, 57]]	['NLP']	['natural language processing']
2871	words, from this the subscript b (bag-of-words).  Subtree Kernel (SbtK) is one of the simplest tree kernels as it only generates complete subtrees, i.e.,	[[66, 70]]	[[50, 64]]	['SbtK']	['Subtree Kernel']
2872	 The structure of a Concept is completed by its set of  Structural Descriptions (SD's). These express how the 	[[81, 85]]	[[56, 79]]	"[""SD's""]"	['Structural Descriptions']
2873	We then review some standard online learners (e.g. perceptron) before presenting the Bayes Point Machine (BPM) (Herbrich et al, 2001; Harrington et al, 2003).	[[106, 109]]	[[85, 104]]	['BPM']	['Bayes Point Machine']
2874	non-interactions   Y They are widely distributed and mediate all of the known biologic effects of  angiotensin II (AngII) through a variety of signal transduction systems, including activation of phospholipases C and A2, inhibition of adenylate cyc-	[[115, 120]]	[[99, 113]]	['AngII']	['angiotensin II']
2875	3.1 Vector SpaceModei for Text Catego-  r izat ion  The bulk of the VSM for Information Retrieval (IR) is  representing naturallanguage xpressions as term 	[[99, 101], [68, 71]]	[[76, 97]]	['IR', 'VSM']	['Information Retrieval']
2876	oracle? which determines the predominant sense, or most frequent sense (MFS), of each noun in our WSJ test data perfectly, and	[[72, 75], [98, 101]]	[[51, 70]]	['MFS', 'WSJ']	['most frequent sense']
2877	To further investigate the effectiveness of our  method, the third set of experiments evaluate the  negative transfer detection (NTD) compared to  co-training (CO) without negative transfer 	[[129, 132], [160, 162]]	[[100, 127], [147, 158]]	['NTD', 'CO']	['negative transfer detection', 'co-training']
2878	Overall recall and precision were 0.80 and 0.87 for drugs, and 0.56 and 0.85 for adverse events. 1 Introduction  It is well-known that adverse drug reactions (ADRs) are an important health problem. Indeed, ADRs are the 4th cause of death in hospitalized patients (Wester et al.,	[[159, 163], [206, 210]]	[[135, 157]]	['ADRs', 'ADRs']	['adverse drug reactions']
2879	More recently, (Areces et al, 2008) analysed GRE as a problem in Description Logic (DL), a formalism which, like Conceptual Graphs, is specifically designed for	[[84, 86], [45, 48]]	[[65, 82]]	['DL', 'GRE']	['Description Logic']
2880	Features. In Proceedings of the 21st Conference on Computational Linguistics (COLING). 	[[78, 84]]	[[51, 76]]	['COLING']	['Computational Linguistics']
2881	 ? Template Element (TE) -- Extract  basic information related to organization and 	[[21, 23]]	[[3, 19]]	['TE']	['Template Element']
2882	Table 6: GAP scores for compared methods.  UW = ukWaC; GW = Gigaword; WP = Wikipedia; WN = WordNet; BN = British National Corpus (Aston	[[43, 45], [55, 57], [70, 72], [9, 12], [86, 88], [100, 102]]	[[48, 53], [60, 68], [75, 84], [91, 98], [105, 121]]	['UW', 'GW', 'WP', 'GAP', 'WN', 'BN']	['ukWaC', 'Gigaword', 'Wikipedia', 'WordNet', 'British National']
2883	He con~iciers them foolish.  2) ASTG (adicctive string), including  adjectival Vens and Vings (see VENDADJ I found it well-dcsigned.	[[32, 36], [100, 106]]	[[38, 54]]	['ASTG', 'ENDADJ']	['adicctive string']
2884	acoustic ambiguity. We measure performance in  terms of character error rate (CER), which is the  number of characters wrongly converted from the 	[[78, 81]]	[[56, 76]]	['CER']	['character error rate']
2885	Computational Linguistics Volume 40, Number 1 of the 22nd International Conference on Computational Linguistics (COLING?08), pages 71?84, Manchester.	[[113, 122]]	[[86, 111]]	['COLING?08']	['Computational Linguistics']
2886	Given an occurrence of a word  in a natural language text, the task of word sense disambiguation (WSD) is to determine the correct sense of  in that context.	[[99, 102]]	[[72, 97]]	['WSD']	['word sense disambiguation']
2887	Their study with three different learners ? na??ve Bayes, maximum entropy (MaxEnt) and the support vector machine (SVM) ?	[[75, 81], [115, 118]]	[[58, 73], [91, 113]]	['MaxEnt', 'SVM']	['maximum entropy', 'support vector machine']
2888	on the base. ( Code-a-phone, 1989)  (2d) In the STBY (standby) position, the phone  will ring whether the handset .is on the base or 	[[48, 52]]	[[54, 61]]	['STBY']	['standby']
2889	Manning (2014) baseline. These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (?	[[71, 77], [90, 94], [117, 120]]	[[45, 69]]	['S-LSTM', 'LSTM', 'POS']	['stack LSTM parsing model']
2890	(CPs) increases the number of Complex Predicates (CPs) entries along with compound verbs  (CompVs) and conjunct verbs (ConjVs). The 	[[119, 125]]	[[103, 117]]	['ConjVs']	['conjunct verbs']
2891	actions and boolean b (> or ?) are used to ensure that unary reductions (RU) can only take place once after a SHIFT action.	[[73, 75], [110, 115]]	[[61, 71]]	['RU', 'SHIFT']	['reductions']
2892	?  Figure 2: Hierarchical Dirichlet Process (HDP) for WSI. 	[[45, 48], [54, 57]]	[[13, 43]]	['HDP', 'WSI']	['Hierarchical Dirichlet Process']
2893	 Preclinical data have supported the use of  fludarabine and cyclophosphamide (FC) in  combination for the treatment of indolent 	[[79, 81]]	[[45, 77]]	['FC']	['fludarabine and cyclophosphamide']
2894	as feature vectors. The model of our choice is the maximum entropy model (MaxEnt), also known as logistic regression (?).	[[74, 80]]	[[51, 66]]	['MaxEnt']	['maximum entropy']
2895	and stochastic optimization. In Proceedings of the Conference on Learning Theory (COLT), pages 257?269.	[[82, 86]]	[[51, 80]]	['COLT']	['Conference on Learning Theory']
2896	For example, NNS  (noun ? plural) became NN (noun). 	[[41, 43], [13, 16]]	[[45, 49], [19, 25]]	['NN', 'NNS']	['noun', 'noun ?']
2897	s), correlation with error counts (re and ? e), average precision (AP) and pairwise accuracy.	[[67, 69]]	[[48, 65]]	['AP']	['average precision']
2898	Dialogues were recorded and system and user behavior were logged automatically. The concept accuracy (CA) of each turn was manually labeled. If the	[[102, 104]]	[[84, 100]]	['CA']	['concept accuracy']
2899	We have proposed two independent evaluation measures: statistical analysis (SA) and classification accuracy (AC). 	[[109, 111], [76, 78]]	[[99, 107], [54, 74]]	['AC', 'SA']	['accuracy', 'statistical analysis']
2900	 1 Introduction Information Extraction (IE) refers to the problem of extracting structured information from unstructured	[[40, 42]]	[[16, 38]]	['IE']	['Information Extraction']
2901	In Proceedings of the 2012 ACM Interntional Conference on Intelligent User Interfaces (IUI), pages 189?198. 	[[87, 90], [27, 30]]	[[58, 85]]	['IUI', 'ACM']	['Intelligent User Interfaces']
2902	 3.2 Matching a review to an object Given the above review language model (RLM), we now state how to match a given review to an	[[75, 78]]	[[52, 73]]	['RLM']	['review language model']
2903	computation of distributional thesauri (Lin, 1998) has been around for decades, its full potential has yet to be utilized in Natural Language Processing (NLP) tasks and applications.	[[154, 157]]	[[125, 152]]	['NLP']	['Natural Language Processing']
2904	expert users in spoken dialogue systems. The key component of a spoken language understanding (SLU) system is the semantic parser, which translates the users?	[[95, 98]]	[[64, 93]]	['SLU']	['spoken language understanding']
2905	interface.  The PRIDES User Interface Layer (PUI) is  responsible for creating and managing the screen 	[[45, 48]]	[[16, 37]]	['PUI']	['PRIDES User Interface']
2906	data. In International Conference on Machine Learning (ICML). 	[[55, 59]]	[[9, 53]]	['ICML']	['International Conference on Machine Learning']
2907	tions of words. In Proceedings of the International Conference on Computational Linguistics (COLING), Bombay, India, December.	[[93, 99]]	[[66, 91]]	['COLING']	['Computational Linguistics']
2908	Maximum Entropy Markov Model (MEMM)-based word segmenter with Conditional Random Fields (CRF)based chunking; 3.	[[89, 92]]	[[62, 87]]	['CRF']	['Conditional Random Fields']
2909	z and a segmental grammar g, compute the sur-  face form y : g(z) of z.  The phonological recognition problem (PRP) is:  Given a (partially specified) surface form y, a dic- 	[[111, 114]]	[[77, 109]]	['PRP']	['phonological recognition problem']
2910	proposed two novel features, Intra-sentence positional term weighting (IPTW) and the Patched language model (PLM), and showed their effectiveness by conducting automatic	[[109, 112], [71, 75]]	[[85, 107], [29, 69]]	['PLM', 'IPTW']	['Patched language model', 'Intra-sentence positional term weighting']
2911	ically measuring the semantic similarity between two texts, which was the aim of the 2013 Semantic Textual Similarity (STS) task (Agirre et al 2013).	[[119, 122]]	[[90, 117]]	['STS']	['Semantic Textual Similarity']
2912	  *COMPLEXITY: avoid semantic complexity  BC (BE CONCRETE): have a concrete meaning   	[[42, 44]]	[[46, 57]]	['BC']	['BE CONCRETE']
2913	ported by Shared Annotated Resources (ShARe) project NIH 5R01GM090187 and Temporal Histories of Your Medical Events (THYME) project (NIH R01LM010090 and U54LM008748).	[[117, 122], [38, 43], [53, 56], [133, 136]]	[[74, 115], [10, 36]]	['THYME', 'ShARe', 'NIH', 'NIH']	['Temporal Histories of Your Medical Events', 'Shared Annotated Resources']
2914	 In order to rate models M1, M2, M3 in comparison to the vector space model (VS) using MSTs, STs and CTs as alternative hierarchi-	[[77, 79], [87, 91], [93, 96], [101, 104]]	[[57, 69]]	['VS', 'MSTs', 'STs', 'CTs']	['vector space']
2915	Spanish data set as Trec4S.  We used a Chinese-English lexicon from the  Linguistic Data Consortium (LDC). We pre- 	[[101, 104]]	[[73, 99]]	['LDC']	['Linguistic Data Consortium']
2916	opinions are about the 20 most popular Chicago hotels; deceptive opinions were generated using the Amazon Mechanical Turk (AMT)3, whereas ?	[[123, 126]]	[[99, 121]]	['AMT']	['Amazon Mechanical Turk']
2917	We further validate our approach on a large publicly available manipulation action dataset (MANIAC) from (Aksoy et al, 2014), achieving promising ex-	[[92, 98]]	[[63, 82]]	['MANIAC']	['manipulation action']
2918	and Rozovskaya and Roth (2011). The article system is trained using the Averaged Perceptron (AP) algorithm (Freund and Schapire, 1999), imple-	[[93, 95]]	[[72, 91]]	['AP']	['Averaged Perceptron']
2919	3.4 Innovative Features of SPTK The most similar kernel to SPTK is the Syntactic Semantic Tree Kernel (SSTK) proposed in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Mos-	[[103, 107], [27, 31], [59, 63]]	[[81, 101]]	['SSTK', 'SPTK', 'SPTK']	['Semantic Tree Kernel']
2920	ments. We experimented with several classifiers including: SVM, Logistic Regression (LR), and Naive Bayes.	[[85, 87], [59, 62]]	[[64, 83]]	['LR', 'SVM']	['Logistic Regression']
2921	3.3 Evaluation Metric We use both Root Mean Square (RMS) error and Correlation Coefficient (CRCoef) to evaluate our model, since the two metrics evaluate different as-	[[92, 98]]	[[67, 90]]	['CRCoef']	['Correlation Coefficient']
2922	Centro de Sondi E Imagen S.L. (Spain)  - Lead Industrial User  University of Sunderland (UK)  - Academic Research 	[[89, 91], [25, 28]]	[[63, 87], [31, 36]]	['UK', 'S.L']	['University of Sunderland', 'Spain']
2923	To create Bloom filter LMs we gathered n-gram counts from both the Europarl (EP) and the whole of the Gigaword Corpus (GW). Table 2 shows the	[[119, 121]]	[[102, 110]]	['GW']	['Gigaword']
2924	edge mining.  Biomedical NER (Bio-NER) tasks are, in general, more difficult than ones in the news domain.	[[30, 37]]	[[14, 28]]	['Bio-NER']	['Biomedical NER']
2925	Entity linking (EL) recognizes mentions in a text and associates them to their corresponding entries in a knowledge base (KB), for example, Wikipedia	[[122, 124], [16, 18]]	[[106, 120], [0, 14]]	['KB', 'EL']	['knowledge base', 'Entity linking']
2926	in the original English WordNet (Fellbaum, 1998) into lexicographer files. Arabic WordNet (AWN) (Elkateb et al 2006) allows us to recover super-	[[91, 94]]	[[75, 89]]	['AWN']	['Arabic WordNet']
2927	RH = Random House dictionary; WSJ = Wall Street Journal; BN = Broadcast News; SWB = Switchboard. 	[[78, 81], [0, 2], [30, 33], [57, 59]]	[[84, 95], [5, 17], [36, 55], [62, 76]]	['SWB', 'RH', 'WSJ', 'BN']	['Switchboard', 'Random House', 'Wall Street Journal', 'Broadcast News']
2928	 Another important similarity measure is cosine similarity of Personalized PageRank (PPR) vectors.	[[85, 88]]	[[62, 83]]	['PPR']	['Personalized PageRank']
2929	"The modifier*""its"" of TOK188 was converted to a modifier of the form  (POSSBY SCAFFOLD184), which was semantically processed to make TOK188 a  LOCPART (LOCationlPAIiT) SFRAME whwe SEMOBJ (SEMantic ODJect) is  SCAFFOLD1 84; idelltif'ication of the location referents of TOK 188 yieldad the two "	[[143, 150], [22, 28], [71, 77], [78, 89], [133, 139], [168, 174], [180, 186], [188, 196], [197, 203], [209, 218], [269, 272]]	[[152, 166]]	['LOCPART', 'TOK188', 'POSSBY', 'SCAFFOLD184', 'TOK188', 'SFRAME', 'SEMOBJ', 'SEMantic', 'ODJect', 'SCAFFOLD1', 'TOK']	['LOCationlPAIiT']
2930	 ? Research Question 1 (RQ1): How do we define suggestions in suggestion mining?	[[24, 27]]	[[3, 22]]	['RQ1']	['Research Question 1']
2931	precision, recall and f-measure. Precision measures the number of correct Named Entities(NEs) in the 107	[[89, 92]]	[[74, 87]]	['NEs']	['Named Entitie']
2932	our method in this domain. The analysis of variance (ANOVA) and Tukey?s honestly significant differences (HSD) tests on the classification accuracies	[[53, 58], [106, 109]]	[[31, 51], [72, 104]]	['ANOVA', 'HSD']	['analysis of variance', 'honestly significant differences']
2933	  The similarity of two words is the least common  ancestor information content(IC), and hence, the  higher the information content is, the more similar 	[[80, 82]]	[[60, 78]]	['IC']	['information conten']
2934	Research in molecular-biology field is discovering enormous amount of new facts, and thus there is an increasing need for information extraction (IE) technology to support database building and to find	[[146, 148]]	[[122, 144]]	['IE']	['information extraction']
2935	 i=1:n P (GR = gri|SCF = s) The three terms, given the hyper-parameters and	[[10, 12]]	[[15, 22]]	['GR']	['gri|SCF']
2936	ing to their different degree of specification. In the hierarchy of relations, Arguments (ARG) include Subject (SUBJ), Object (OBJ), Indirect	[[90, 93], [112, 116], [127, 130]]	[[79, 88], [103, 110], [119, 125]]	['ARG', 'SUBJ', 'OBJ']	['Arguments', 'Subject', 'Object']
2937	PCEDT 0.7681 0.7072 0.7364 0.0712 Average 0.8402 0.8090 0.8241 0.1397 Table 3: Labeled precision (LP), recall (LR), F 1	[[98, 100], [0, 5], [111, 113]]	[[79, 96]]	['LP', 'PCEDT', 'LR']	['Labeled precision']
2938	Three transliteration models have been used that  can generate the Hindi transliteration from an  English named entity (NE). An English NE is 	[[120, 122], [136, 138]]	[[106, 118]]	['NE', 'NE']	['named entity']
2939	the previous section. We contrast this metric with Normalized Pointwise Mutual Information (NPMI) which uses only the events A = X+a and B = X	[[92, 96]]	[[51, 90]]	['NPMI']	['Normalized Pointwise Mutual Information']
2940	Figure 1 shows the example of the input format of ACABIT in XML makes use of which conforms to Document Type Definition (DTD) in Figure 2.	[[121, 124], [50, 56], [60, 63]]	[[95, 119]]	['DTD', 'ACABIT', 'XML']	['Document Type Definition']
2941	Parsing. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pp.	[[101, 104]]	[[58, 99]]	['ACL']	['Association for Computational Linguistics']
2942	clude examples such as Facebook AI Research?s challenge problems for AI-complete QA (Weston et al, 2015) and the Allen Institute for AI?s (AI2) Aristo project (Clark, 2015) along with its recently	[[139, 142], [81, 83]]	[[113, 137]]	['AI2', 'QA']	['Allen Institute for AI?s']
2943	analysis. To include more of the corpus, parameters are relaxed: the high group (HH) includes anyone whose score is above .5 SD	[[81, 83], [125, 127]]	[[69, 73]]	['HH', 'SD']	['high']
2944	linear chain, CRFs make a first-order Markov independence assumption, and thus can be understood as conditionally-trained finite state machines(FSMs). 	[[144, 148], [14, 18]]	[[122, 142]]	['FSMs', 'CRFs']	['finite state machine']
2945	In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI?09), pages 1,058?1,064, Pasadena, CA.	[[81, 89], [121, 123]]	[[22, 79]]	['IJCAI?09', 'CA']	['International Joint Conference on Artificial Intelligence']
2946	NST = Noun Stem V-FLEX = Verbal Inflsxion  PRN = Pronoun A-FLEX = Adjectival Inflexion  CA = Case CL = Inflsxion Class GD = Gender MD = Mode  PF ~ Predicate Form PS = Person TN = Tense 	[[98, 100], [131, 133], [0, 3], [16, 22], [43, 46], [57, 63], [88, 90], [119, 121], [142, 144], [162, 164], [174, 176]]	[[93, 97], [136, 140], [6, 15], [25, 41], [49, 56], [66, 86], [113, 118], [124, 130], [147, 161], [167, 173], [179, 184]]	['CL', 'MD', 'NST', 'V-FLEX', 'PRN', 'A-FLEX', 'CA', 'GD', 'PF', 'PS', 'TN']	['Case', 'Mode', 'Noun Stem', 'Verbal Inflsxion', 'Pronoun', 'Adjectival Inflexion', 'Class', 'Gender', 'Predicate Form', 'Person', 'Tense']
2947	transcripts of user utterances, and included lexical, syntactic, numeric, and features from the output of Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al.,	[[141, 145]]	[[106, 139]]	['LIWC']	['Linguistic Inquiry and Word Count']
2948	Table 2: Parsing accuracy for 2-planar parser in comparison to MaltParser with (PP) and without (P) pseudo-projective transformations. LAS = labeled attachment score; UAS = unlabeled attachment score; NPP = precision on non-projective arcs; NPR = recall on non-projective arcs.	[[167, 170], [80, 82], [241, 244], [201, 204], [135, 138]]	[[173, 199], [141, 165], [257, 271]]	['UAS', 'PP', 'NPR', 'NPP', 'LAS']	['unlabeled attachment score', 'labeled attachment score', 'non-projective']
2949	3 Model We introduce a topic-model based approach to declarative knowledge (DK) acquisition and describe how this knowledge can be applied to two unsuper-	[[76, 78]]	[[53, 74]]	['DK']	['declarative knowledge']
2950	"For ex-  ample, temporal PPs, such as ""in 1959"", where  the prepositional object is tagged CD (cardi-  nal), favor attachment to the VP, because tile "	[[91, 93], [133, 135], [25, 28]]	[[95, 106]]	['CD', 'VP', 'PPs']	['cardi-  nal']
2951	The following are  typlcal relation names: NT (narrower term); PT (part); FUN (function);  SYN (syntax); EG (example). 	[[91, 94], [43, 45], [47, 60], [63, 65], [74, 77], [105, 107]]	[[96, 102], [67, 71], [79, 87], [109, 116]]	['SYN', 'NT', 'narrower term', 'PT', 'FUN', 'EG']	['syntax', 'part', 'function', 'example']
2952	we combine different perspectives, the performance improves and we use the L+S with SVR for run 1 (LSSVR), L+P+S with SVR for run 2 (LPSSVR), and L+P+S with SVR using transductive learning	[[99, 104], [84, 87], [133, 139], [157, 160]]	[[107, 129]]	['LSSVR', 'SVR', 'LPSSVR', 'SVR']	['L+P+S with SVR for run']
2953	 A natural solution would be to take advantage  of machine readable dictionaries (MILD's), such  as Longman's Dictionary of Contemporary En- 	[[82, 88]]	[[51, 80]]	"[""MILD's""]"	['machine readable dictionaries']
2954	3.2 Generation Performance We now compare the GYRO system with the Combinatory Categorial Grammar (CCG)-based system described in (Zhang et al.,	[[99, 102], [46, 50]]	[[67, 97]]	['CCG', 'GYRO']	['Combinatory Categorial Grammar']
2955	"common view of the semantics of time. Since the target application domain is an historical database, we  present the essential features of the Historical Relational Database Model (HRDM), an extension to the  relational model motivated by the desire to incorporate more ""real world"" semantics into a database at "	[[181, 185]]	[[143, 179]]	['HRDM']	['Historical Relational Database Model']
2956	.  3.4 Stochastic Gradient Descent (SGD) Training With the likelihood gradients, we apply L2-norm regularized SGD training to iteratively learn the feature	[[36, 39], [110, 113]]	[[7, 34]]	['SGD', 'SGD']	['Stochastic Gradient Descent']
2957	gramming. In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050?1055. 	[[73, 77]]	[[13, 71]]	['AAAI']	['Association for the Advancement of Artificial Intelligence']
2958	Hyungjong Noh* Jeong-Won Cha** Gary Geunbae Lee*  *Department of Computer Science and Engineering  Pohang University of Science & Technology (POSTECH)  San 31, Hyoja-Dong, Pohang, 790-784, Republic of Korea 	[[142, 149]]	[[99, 140]]	['POSTECH']	['Pohang University of Science & Technology']
2959	Second, we demonstrate correlation to a database of real-world international conflict events, the Militarized Interstate Dispute (MID) dataset (Jones et al, 1996).	[[130, 133]]	[[98, 128]]	['MID']	['Militarized Interstate Dispute']
2960	surveys of QA and DP data. The surveys are evaluated using nuggets drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).	[[97, 102], [11, 13], [18, 20], [119, 124], [150, 155]]	[[78, 95], [105, 117], [131, 148]]	['QA?CT', 'QA', 'DP', 'QA?AB', 'DP?CT']	['QA citation texts', 'QA abstracts', 'DP citation texts']
2961	category. The following transfer knowledge involves  sets of three common ouns (CNs):  3A' is the transferred expression of A 	[[80, 83]]	[[67, 78]]	['CNs']	['common ouns']
2962	In a spoken dialog system that can handle natural conversation between a human and a machine, spoken language understanding (SLU) is a crucial component aiming at capturing	[[125, 128]]	[[94, 123]]	['SLU']	['spoken language understanding']
2963	In addition, results from the machine learning based model are refined by a rule-based postprocessing, which is implemented using a finite state transducer (FST). The	[[157, 160]]	[[132, 155]]	['FST']	['finite state transducer']
2964	mark of language attainment at different stages of learning. The English Profile (EP)2 research programme aims to enhance the learning, teaching	[[82, 84]]	[[65, 80]]	['EP']	['English Profile']
2965	 1 Introduction Approaches to Machine Translation (MT) using Data-Oriented Parsing (DOP: (Bod, 1998; Bod et	[[51, 53], [84, 87]]	[[30, 49], [61, 82]]	['MT', 'DOP']	['Machine Translation', 'Data-Oriented Parsing']
2966	ALL X X X 0.614 0.186 0.706 0314 0.509 Table 2: Pearson?s ? for each feature set (FSet), as well as combinations of feature sets and adap-	[[82, 86]]	[[69, 80]]	['FSet']	['feature set']
2967	The computation of associative responses to multiword stimuli. In  Proceedings of the  Workshop on Cognitive Aspects of the Lexicon (COGALEX) at COLING-2008, p.102?109. Manchester, UK	[[133, 140], [181, 183]]	[[99, 131]]	['COGALEX', 'UK']	['Cognitive Aspects of the Lexicon']
2968	smoothness. Before creating a POMDP structure, we used the dynamic Bayesian network (DBN) structure (Fig.	[[85, 88], [30, 35]]	[[59, 83]]	['DBN', 'POMDP']	['dynamic Bayesian network']
2969	 minimizes the multiway normalized cut(MNCut): MNCut(I) = K ?	[[39, 44], [47, 52]]	[[15, 37]]	['MNCut', 'MNCut']	['multiway normalized cu']
2970	 In Proceedings of the Conference on Web Search and Web Data Mining (WSDM). 	[[69, 73]]	[[37, 67]]	['WSDM']	['Web Search and Web Data Mining']
2971	Abstract  We present a novel method for evaluating  the output of Machine Translation (MT),  based on comparing the dependency 	[[87, 89]]	[[66, 85]]	['MT']	['Machine Translation']
2972	Adding valency filtering to the setting in the preceding row. SUC = Subset of Stockholm-Umea? corpus of	[[62, 65]]	[[68, 87]]	['SUC']	['Subset of Stockholm']
2973	2008) and hierarchical log-bilinear embeddings (HLBL) (Mnih and Hinton, 2007) using the implementation in (Turian et al, 2010), Hellinger PCA (H-PCA) (Lebret and Collobert, 2014) and our connective-based representation (Bllip).	[[143, 148]]	[[128, 141]]	['H-PCA']	['Hellinger PCA']
2974	 1 Introduction Word Sense Disambiguation (WSD) is a difficult Natural Language Processing task which requires	[[43, 46]]	[[16, 41]]	['WSD']	['Word Sense Disambiguation']
2975	 1 Introduction Word sense disambiguation (WSD) is the task of assigning sense tags to ambiguous lexical items	[[43, 46]]	[[16, 41]]	['WSD']	['Word sense disambiguation']
2976	These features capture the context of the adverb and help in deciding the presence of the manner (MNR) component. 	[[98, 101]]	[[90, 96]]	['MNR']	['manner']
2977	end 1 <= V I ;   SPAN (SPANS, 'CONSTITUENTS ' ) = <TUP , CONSTITUENTS>;  TODO = TUP -t TODO;  r e t u r n ;  	[[80, 83], [51, 54]]	[[84, 86]]	['TUP', 'TUP']	['-t']
2978	"make assertions that personal pronouns like \she"" cannot co-refer with \company"".  In MUC-7, we developed a word sense disambiguation (WSD) module, which removes some of the implausible senses from the list of potential senses."	[[135, 138], [86, 91]]	[[108, 133]]	['WSD', 'MUC-7']	['word sense disambiguation']
2979	edges the support of Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract No.	[[132, 136], [64, 69]]	[[101, 130], [21, 62]]	['AFRL', 'DARPA']	['Air Force Research Laboratory', 'Defense Advanced Research Projects Agency']
2980	(Eisner, 1996). We define role value labeled precision (RLP) and role value labeled recall (RLR) on dependency links as follows:	[[92, 95], [56, 59]]	[[65, 90], [26, 54]]	['RLR', 'RLP']	['role value labeled recall', 'role value labeled precision']
2981	ratings of IP strategies. In the following we use a Reinforcement Learning (RL) as a statistical planning framework (Sutton and Barto,	[[76, 78], [11, 13]]	[[52, 74]]	['RL', 'IP']	['Reinforcement Learning']
2982	are defined:    (1)  VSM-based (Vector Space Model based)  trigger word similarity: the trigger words 	[[21, 30]]	[[32, 50]]	['VSM-based']	['Vector Space Model']
2983	 1 Introduction Statistical machine translation (SMT) relies on tokenization to split sentences into meaningful units	[[49, 52]]	[[16, 47]]	['SMT']	['Statistical machine translation']
2984	insertion operator for combining subtrees as in tree adjoining grammars (TAG) (Joshi, 1985) or tree insertion grammars (TIG) (Schabes and Waters, 1995).	[[120, 123], [73, 76]]	[[95, 118], [48, 70]]	['TIG', 'TAG']	['tree insertion grammars', 'tree adjoining grammar']
2985	predicate-only f-score, M = METEOR, WN = WordNet,  H_FL = human fluency score, H_AC = human accuracy  score, H_AVE = human average score.9   	[[109, 114], [36, 38], [51, 55], [79, 83]]	[[117, 130], [28, 34], [41, 48], [58, 71], [86, 100]]	['H_AVE', 'WN', 'H_FL', 'H_AC']	['human average', 'METEOR', 'WordNet', 'human fluency', 'human accuracy']
2986	2.2 Singular Value Decomposition Given any matrix S, its singular value decomposition (SVD) is S = U?V T . The matrix Sk =	[[87, 90], [99, 102]]	[[57, 85]]	['SVD', 'U?V']	['singular value decomposition']
2987	features: O-SEM ('ordinary semantics') and  I,'-SKEL (F-skeleton) of the type of a semantic ob-  ject, tile set-valued IS-CSTR (IS constraints) and  the binary MAX-F (for potential maximal focus).	[[119, 126], [10, 15], [44, 52], [160, 165]]	[[128, 142], [18, 36], [54, 64]]	"['IS-CSTR', 'O-SEM', ""I,'-SKEL"", 'MAX-F']"	['IS constraints', 'ordinary semantics', 'F-skeleton']
2988	 ? LEAD (lead-based): n% sentences are chosen from the beginning of the text.	[[3, 7]]	[[9, 13]]	['LEAD']	['lead']
2989	eigenvalues are not zero, then I? minimizes the multiway normalized cut(MNCut): MNCut(I) = K ??	[[72, 77], [80, 85]]	[[48, 71]]	['MNCut', 'MNCut']	['multiway normalized cut']
2990	 From the set of erroneous instances: True Positive (TP) ML class 6= student class False Negative (FN) ML class = student class	[[53, 55], [99, 101], [57, 59], [103, 105]]	[[38, 51], [83, 97]]	['TP', 'FN', 'ML', 'ML']	['True Positive', 'False Negative']
2991	Relative standard deviation of three intervals, left edge to anchor (LE-A), center to  anchor (CC-A), right edge to anchor (RE-A) calculated across productions of word sets by one 	[[124, 128], [69, 73], [95, 99]]	[[102, 122], [48, 67], [76, 93]]	['RE-A', 'LE-A', 'CC-A']	['right edge to anchor', 'left edge to anchor', 'center to  anchor']
2992	larger segments.  Speech Systems Incorporated (SSI) has been using a  version of this two-stage cascade of the MMI encoders in the 	[[47, 50], [111, 114]]	[[18, 45]]	['SSI', 'MMI']	['Speech Systems Incorporated']
2993	to cover terms frequently used by students, such as acronyms: E.L.C. (the English Language Center), R.O.C. (Republic of China), and so on. Other	[[100, 106], [62, 67]]	[[108, 125], [74, 97]]	['R.O.C.', 'E.L.C']	['Republic of China', 'English Language Center']
2994	In order to  make the notion of focusing more precise I will use the  notion of Reference Time (RT), adopted from Reichen-  bach 1947 but reinterpreted pragmatically: RT is to be 	[[96, 98], [167, 169]]	[[80, 94]]	['RT', 'RT']	['Reference Time']
2995	. However, a method based on singular value decomposition (SVD) provides an efficient and exact solution to this prob-	[[59, 62]]	[[29, 57]]	['SVD']	['singular value decomposition']
2996	source yes better Table 8: A comparison of syntactic SMT methods (part 1). FST=Finite State Transducer; SCFG=Synchronous Context-Free Grammar; TT=Tree Transducer.	[[75, 78], [53, 56], [104, 108], [143, 145]]	[[79, 102], [109, 141], [146, 161]]	['FST', 'SMT', 'SCFG', 'TT']	['Finite State Transducer', 'Synchronous Context-Free Grammar', 'Tree Transducer']
2997	based method is slightly better. The two systems share the same topic relevance score (REL) and sentiment score, but the sentence-ranking method	[[87, 90]]	[[70, 79]]	['REL']	['relevance']
2998	Minimum Sub-Structure (MSS) 87.95 87.88 Context-Sensitive MSS (CMSS) 89.11 89.01 Chunking Tree (CT) 86.17 86.21 Linear Features (Kl) 90.79 90.46	[[96, 98], [23, 26], [58, 61], [63, 67], [129, 131]]	[[81, 94], [0, 21]]	['CT', 'MSS', 'MSS', 'CMSS', 'Kl']	['Chunking Tree', 'Minimum Sub-Structure']
2999	3.2 Feature Space An essential aspect of our approach below is the word sense disambiguation (WSD) of the noun. Us-	[[94, 97]]	[[67, 92]]	['WSD']	['word sense disambiguation']
3000	1 will refer to pa~rs   primarily oriented towards the former goal as Practical Parsers (PP) and refer  to the others as Performance Model Parsers (PMP). With these distinctions 	[[148, 151], [89, 91]]	[[121, 146], [70, 86]]	['PMP', 'PP']	['Performance Model Parsers', 'Practical Parser']
3001	Head+Path 80.0 72.8 31.8 22.4 Path 80.0 72.7 31.6 22.0 Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled) Unlabeled Labeled	[[82, 84], [105, 107]]	[[87, 103], [110, 121], [127, 136], [142, 149]]	['AS', 'EM']	['attachment score', 'exact match', 'unlabeled', 'labeled']
3002	2009. Word sense disambiguation: A survey. ACM Computing Surveys (CSUR),41(2):10. 	[[66, 70], [43, 46]]	[[47, 64]]	['CSUR', 'ACM']	['Computing Surveys']
3003	guages: German, French and Italian, with German  usually serving as the source language (SL),  French and Italian as the target language (TL). 	[[138, 140], [89, 91]]	[[121, 136], [72, 87]]	['TL', 'SL']	['target language', 'source language']
3004	"1. Grider, T., Mosley, H., Snow, L, and Wilson, W., ""Users Manual for the  Dynamic Analytical Replanning Tool (DRAFT)"", prepared for BBN by  Systems Research and Applications Corporation, 9 November 1990."	[[111, 116]]	[[75, 109]]	['DRAFT']	['Dynamic Analytical Replanning Tool']
3005	tion system; then the output is classified into a  small number of domain-specific classes called  Domain Acts (DAs) that can indicate directly to  the dispatcher the general intended meaning of 	[[112, 115]]	[[99, 110]]	['DAs']	['Domain Acts']
3006	mentation in (Zhang et al, 2006) 2.2 OOV Recognition with Accessor Variety Accessor variety (AV) (Feng et al, 2004) is a simple and effective unsupervised method for extrac-	[[93, 95], [37, 40]]	[[75, 91]]	['AV', 'OOV']	['Accessor variety']
3007	word and sentence level QE. In this work we describe the Fondazione Bruno Kessler (FBK), Universitat Polit`ecnica de Val`encia (UPV) and Uni-	[[83, 86], [24, 26], [128, 131]]	[[57, 81], [89, 126]]	['FBK', 'QE', 'UPV']	['Fondazione Bruno Kessler', 'Universitat Polit`ecnica de Val`encia']
3008	ence.  Semantic Textual Similarity (STS) is the task of judging the similarity of two sentences on a scale	[[36, 39]]	[[7, 34]]	['STS']	['Semantic Textual Similarity']
3009	Specifically, we achieve a roughly 10% improvement in precision on text from the information technology (IT) business press via post hoc rule-based error reduction.	[[105, 107]]	[[81, 103]]	['IT']	['information technology']
3010	 1 Introduction Referring expressions (REs) are expressions intended by speakers to identify entities to hearers.	[[39, 42]]	[[16, 37]]	['REs']	['Referring expressions']
3011	cause de la limite des outils informatiques li?s ? son traitement  automatique,  ce  qui  rend  difcile  son  adh?sion  ?  ses  cons?urs  dans  le domaine des nouvelles technologies de l'information et de la communication (NTIC). Par cons?quent, un ensemble de recherches scientifques et linguistiques sont lanc?es pour rem?dier ?	[[223, 227]]	[[159, 221]]	['NTIC']	"[""nouvelles technologies de l'information et de la communication""]"
3012	Clark, 2010; Sun, 2011b; Li, 2011) are usually trained on human-annotated corpora such as the Penn Chinese Treebank (CTB) (Xue et al, 2005), and perform quite well on corresponding test sets.	[[117, 120]]	[[99, 115]]	['CTB']	['Chinese Treebank']
3013	Advanced Research and Development Activity (ARDA)?s Advanced Question Answering for Intelligence (AQUAINT) Program, a DOI grant under the Reflex	[[98, 105], [44, 48], [118, 121]]	[[0, 42], [52, 96]]	['AQUAINT', 'ARDA', 'DOI']	['Advanced Research and Development Activity', 'Advanced Question Answering for Intelligence']
3014	In Proc. 6th Canadian Conf on AI (CSCSI-86), pp. 78?83.	[[34, 42], [45, 47]]	[[13, 32]]	['CSCSI-86', 'pp']	['Canadian Conf on AI']
3015	Four training and testing corpora were used in the first bakeoff (Sproat and Emerson, 2003), including the Academia Sinica Corpus (AS), the Penn Chinese Treebank Corpus (CTB), the Hong Kong City Uni-	[[131, 133], [170, 173]]	[[107, 129], [145, 161]]	['AS', 'CTB']	['Academia Sinica Corpus', 'Chinese Treebank']
3016	context    Chinese Context(CC): ???????? 	[[27, 29]]	[[11, 25]]	['CC']	['Chinese Contex']
3017	1 Introduction Creating the annotated corpus needed for training a NER (named entity recognition) model is costly. 	[[67, 70]]	[[72, 96]]	['NER']	['named entity recognition']
3018	 We propose a new algorithm: collective iterative classification (CIC) to perform approximate inference to find the maximum a posteriori	[[66, 69]]	[[29, 64]]	['CIC']	['collective iterative classification']
3019	We suppose that the possessor of a noun phrase is  the subject or the noun phrase's nearest opic that  has a semantic mark,er HUM (human) or a seman-  tic marker AN I (animal).	[[126, 129], [162, 164]]	[[131, 136]]	['HUM', 'AN']	['human']
3020	As a starting point, the classes for complements  and features developed by the New York Univer-  sity Linguistic String Project (LSP) (Fitzpatrick,  1981), were selected sin(x, the coverage is very 	[[130, 133]]	[[103, 128]]	['LSP']	['Linguistic String Project']
3021	idea was an early version of branching entropy, one of the experts in VE, and they developed an algorithm called Phoneme to Morpheme (PtM) around it. 	[[134, 137], [70, 72]]	[[113, 132]]	['PtM', 'VE']	['Phoneme to Morpheme']
3022	system components goes through GDM, thereby insu-  lating parts from each other and providing a uniform  API (applications programmer interface) for manip-  ulating the data produced by the system.	[[105, 108], [31, 34]]	[[110, 143]]	['API', 'GDM']	['applications programmer interface']
3023	SUBS = substantif  compl~ment  VT  = verbe conjugu6  PROC = pronom compl~ment  SUSU = substantif  sujet 	[[53, 57], [0, 4], [31, 33], [79, 83]]	[[60, 72], [7, 17], [37, 42], [86, 103]]	['PROC', 'SUBS', 'VT', 'SUSU']	['pronom compl', 'substantif', 'verbe', 'substantif  sujet']
3024	This I will  claim to be in contrast with the ability of temporal  subordinate clauses and noun phrases (NPs) to direct the  listener to any position in the evolving structure.)	[[105, 108]]	[[91, 103]]	['NPs']	['noun phrases']
3025	formation of globally dispersed virtual communities, one of which is the very active and growing movement of Open Source Software (OSS) development.	[[131, 134]]	[[109, 129]]	['OSS']	['Open Source Software']
3026	chose a method from the second group. We use the Hierarchical Agglomerative Clustering (HAC) algorithm (Jain et al, 1999) for all experiments reported	[[88, 91]]	[[49, 86]]	['HAC']	['Hierarchical Agglomerative Clustering']
3027	 We first examined three randomly selected portions of the listing  in the American Heritage Word Frequency Book (AHWFB).side by side  with the corresponding entry lists of the American Heritage School Dic- 	[[114, 119]]	[[75, 112]]	['AHWFB']	['American Heritage Word Frequency Book']
3028	framework. This algorithm uses grammars in the Chomsky Normal Form (CNF) so we employed the open source Natural Language Toolkit2	[[68, 71]]	[[47, 66]]	['CNF']	['Chomsky Normal Form']
3029	5.1 Alternative Models To test LUX?s representations, we built a brute-force histogram model (HM) that discretizes HSV space and tracks frequency distributions of labels directly	[[94, 96], [31, 36], [115, 118]]	[[77, 92]]	['HM', 'LUX?s', 'HSV']	['histogram model']
3030	229  Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 19?26, Sofia, Bulgaria, August 9, 2013.	[[70, 77]]	[[36, 68]]	['DiscoMT']	['Discourse in Machine Translation']
3031	This data may be presented in various forms, e.g. as  dictionaries, transition networks for lexical analysis,  augmented transition networks (ATN) for syntactic analysis,  semantic networks,  re la t ions ,  end so on.	[[142, 145]]	[[111, 140]]	['ATN']	['augmented transition networks']
3032	For each  model sentence MSij,, the model builder selects  the Required Lexicon (RLijk), a set of the most  essential lexical entities required to appear in a 	[[81, 86], [25, 29]]	[[63, 79]]	['RLijk', 'MSij']	['Required Lexicon']
3033	ing to its current beliefs concerning the state of the dialogue. Reinforcement Learning (RL) has been more and more used for the optimisa-	[[89, 91]]	[[65, 87]]	['RL']	['Reinforcement Learning']
3034	transitions, depending on whether the backward-looking center of Ui?1 is maintained or not in Ui and on whether CB(Ui) is also the most highly ranked entity (CP) of Ui: Center Continuation (CON): CB(Ui) = CB(Ui?1), and CB(Ui) is the most highly ranked CF (CP) of Ui (i.e., CP(Ui) = CB(Ui))	[[190, 193], [112, 114], [94, 96], [65, 67], [158, 160], [196, 198], [252, 254], [256, 258], [282, 284], [273, 275]]	[[176, 188], [199, 201]]	['CON', 'CB', 'Ui', 'Ui', 'CP', 'CB', 'CF', 'CP', 'CB', 'CP']	['Continuation', 'Ui']
3035	After grammatical ambiguities are removed  by the stochastic parser, the phrase is divided  into noun phrases(NP) and verb phrases(VP),  giving, 	[[110, 112], [131, 133]]	[[97, 108], [118, 129]]	['NP', 'VP']	['noun phrase', 'verb phrase']
3036	that these heuristics have much effect not only in  the inductive inference (regular SVM) but also in  transductive inference (TSVM), especially when  the untagged data is large.	[[127, 131], [85, 88]]	[[103, 125]]	['TSVM', 'SVM']	['transductive inference']
3037	? ( PM), tree matching without the auxiliary patterns (TM), and tree matching with the auxiliary patterns	[[56, 58]]	[[49, 54]]	['TM']	['terns']
3038	 Figure 1. Reference answer representation revisions  Typical facets, as in (1a), are derived directly from a dependency parse, in this case retaining its dependency type label, NMod (noun modifier).  Other facets, such as (1b-e), are the result of com-bining multiple dependencies, VMod(stick, to) and PMod(to, nail) in the case of (1c).	[[178, 182], [283, 287], [303, 307]]	[[184, 197]]	['NMod', 'VMod', 'PMod']	['noun modifier']
3039	ellieioncy has been drastically improved for n)orpho-  logical analysis by representing large dictionaries with  Finite State Automata (FSA) and by representhig two-  level rnles and le?ical hlforination with finite-state 	[[136, 139]]	[[113, 134]]	['FSA']	['Finite State Automata']
3040	Kanayama et al99 Probabilistic model (ME + HPSG) EDR (192,778) 88.55 Haruno et al98 Probabilistic model (DT + Boosting) EDR (50,000) 85.03 Fujio et al98 Probabilistic model (ML) EDR (190,000) 86.67 Table 4: Comparison with the related work	[[174, 176], [38, 40], [43, 47], [49, 52], [105, 107], [120, 123], [178, 181]]	[[167, 172]]	['ML', 'ME', 'HPSG', 'EDR', 'DT', 'EDR', 'EDR']	['model']
3041	Table 3: Validation results for metaphor interpretation for English and Russian.  (ALL), or just two (TWO) validators. In most of	[[102, 105]]	[[97, 100]]	['TWO']	['two']
3042	The different resources used to build ArSenL.       The English WordNet (EWN) (Miller et al., 	[[73, 76]]	[[56, 71]]	['EWN']	['English WordNet']
3043	The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields (CRF). 	[[100, 103], [58, 61]]	[[73, 98]]	['CRF', 'CTB']	['conditional random fields']
3044	res. In Section 2 we present a structural reformulation of Support Vector Machines (SVMs) that can take similarities between different genres into ac-	[[84, 88]]	[[59, 82]]	['SVMs']	['Support Vector Machines']
3045	grammatically correct (readability). We engaged the services of Amazon Mechanical Turks (AMT) to judge the generated sentences based on a discrete	[[89, 92]]	[[64, 87]]	['AMT']	['Amazon Mechanical Turks']
3046	Schauder 91). The grammar is divided into an LD  (linear dominance) and an LP (linear precedence) part  so that the piecewise construction of syntactic 	[[75, 77], [45, 47]]	[[79, 96], [50, 67]]	['LP', 'LD']	['linear precedence', 'linear dominance)']
3047	Collaboratively constructed resources like Wiktionary (WKT) and OmegaWiki (OW) provide a viable option for such cases and seem especially suitable	[[75, 77]]	[[64, 73]]	['OW']	['OmegaWiki']
3048	726  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 657?669, October 25-29, 2014, Doha, Qatar.	[[93, 98]]	[[43, 91]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
3049	PP ??  ( VP ( VBP exercise ) ( NP ( JJ strict ) ( NN self-discipline ) ) ) ? ? ? ? 	[[9, 11], [0, 2], [31, 33], [50, 52]]	[[14, 17]]	['VP', 'PP', 'NP', 'NN']	['VBP']
3050	{tvu, aaiti, mzhang}@i2r.a-star.edu.sg  Abstract  Term Extraction (TE) is an important component of many NLP applications.	[[67, 69], [105, 108]]	[[50, 65]]	['TE', 'NLP']	['Term Extraction']
3051	siderable progress. The bakeoff series hosted by  the Chinese Information Processing Society (CIPS)  and ACL SIGHAN shows that an F measure of 	[[94, 98], [105, 108], [109, 115]]	[[54, 92]]	['CIPS', 'ACL', 'SIGHAN']	['Chinese Information Processing Society']
3052	To solve the ILP models we used lp solve, a highly efficient GNU-licence Mixed Integer Programming (MIP) solver11, that implements the Branch-and-Bound algorithm.	[[100, 103], [13, 16], [61, 72], [32, 34]]	[[73, 98]]	['MIP', 'ILP', 'GNU-licence', 'lp']	['Mixed Integer Programming']
3053	slightly higher F-measures than the UMass dictionary.  The error rates (ERR) for all three dictionaries were  identical.	[[72, 75]]	[[59, 70]]	['ERR']	['error rates']
3054	 3.1 Evaluation methods We use the Relative Utility (RU) method (Radev et al, 2000) to compare our various summaries.	[[53, 55]]	[[35, 51]]	['RU']	['Relative Utility']
3055	belief updating model. In Proc American Association for Artificial Intelligence (AAAI) Workshop on Statistical and Empirical Approaches	[[81, 85]]	[[40, 79]]	['AAAI']	['Association for Artificial Intelligence']
3056	2007; Noh and Pad?o, 2013).  2.2 Entailment Core (EC) The Entailment Core performs the actual entail-	[[50, 52]]	[[33, 48]]	['EC']	['Entailment Core']
3057	mentation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English,	[[139, 142], [163, 166], [172, 175]]	[[122, 130], [145, 154]]	['EGY', 'LEV', 'MSA']	['Egyptian', 'Levantine']
3058	lexical chaining.  4.1 LDA Mode Method (LDA-MM) The LDA-MM approach places all word tokens that	[[40, 46], [52, 58]]	[[23, 38]]	['LDA-MM', 'LDA-MM']	['LDA Mode Method']
3059	  We extracted bag-of-word features and trained a  Support Vector Machine (SVM) classifier (Burges, 1998) using the above dataset.	[[75, 78]]	[[51, 73]]	['SVM']	['Support Vector Machine']
3060	work first arose out of a broader family of approaches to pattern classifier design known as Generalized Probabilistic Descent (GPD) (Katagiri et al, 1991).	[[128, 131]]	[[93, 126]]	['GPD']	['Generalized Probabilistic Descent']
3061	ily' (lists truncated). Score = log-likelihood score; f = occurrence frequency of keyterm; NN = noun; VV = verb; AR =  article; AP = article+preposition; JJ = adjective; CC = con-	[[91, 93], [24, 29], [102, 104], [113, 115], [128, 130], [154, 156], [170, 172]]	[[96, 100], [69, 78], [47, 52], [107, 111], [119, 126], [133, 152], [159, 168]]	['NN', 'Score', 'VV', 'AR', 'AP', 'JJ', 'CC']	['noun', 'frequency', 'score', 'verb', 'article', 'article+preposition', 'adjective']
3062	Table 6 shows the number of true positive (TP), true negative (TN), false positive (FP) and false negative (FN) of models for the stocks. 	[[108, 110], [43, 45], [63, 65], [84, 86]]	[[92, 106], [33, 41], [48, 61], [68, 82]]	['FN', 'TP', 'TN', 'FP']	['false negative', 'positive', 'true negative', 'false positive']
3063	adapt the model to character or word level, or limit  the conversion target to only noun or expand it to  other Part of Speech (POS) tags, a series of  experiments has been performed.	[[128, 131]]	[[112, 126]]	['POS']	['Part of Speech']
3064	Transcutaneous Oxygen (TcPO2 ) = 9.5. Transcutaneous CO2 (TcPCO2) = 6.7. 	[[58, 64], [23, 28]]	[[38, 56], [0, 21]]	['TcPCO2', 'TcPO2']	['Transcutaneous CO2', 'Transcutaneous Oxygen']
3065	Gimenez and Marquez (2008) measure overlapping grammatical dependency relationships (DP), semantic roles (SR), and discourse representations (DR).	[[106, 108], [85, 87], [142, 144]]	[[90, 104], [59, 83], [115, 140]]	['SR', 'DP', 'DR']	['semantic roles', 'dependency relationships', 'discourse representations']
3066	1 Introduction  Syntax parsing is one of the most fundamental  tasks in natural language processing (NLP) and  has attracted extensive attention during the past 	[[101, 104]]	[[72, 99]]	['NLP']	['natural language processing']
3067	While much of the focus in developing a statistical machine translation (SMT) system revolves around the translation model (TM), most systems do not emphasize the role of the language model (LM).	[[124, 126], [73, 76], [191, 193]]	[[105, 122], [40, 71], [175, 189]]	['TM', 'SMT', 'LM']	['translation model', 'statistical machine translation', 'language model']
3068	ergistic working of several components: speech recognition (ASR), spoken language understanding (SLU), dialog management (DM), language generation (LG) and text-to-speech synthesis	[[122, 124], [60, 63], [97, 100], [148, 150]]	[[103, 120], [40, 58], [66, 95], [127, 146]]	['DM', 'ASR', 'SLU', 'LG']	['dialog management', 'speech recognition', 'spoken language understanding', 'language generation']
3069	 1 Introduction  Feature term formalisms (FTF) have proven extremely  useful for the declarative representation f linguistic 	[[42, 45]]	[[17, 40]]	['FTF']	['Feature term formalisms']
3070	true positive (TP) (i.e., a correct match), and an appropriate NNS triple not found in the gold standard set a false negative (FN) (i.e., an incorrect nonmatch), as shown in Table 4.	[[127, 129], [15, 17], [63, 66]]	[[111, 125], [0, 13]]	['FN', 'TP', 'NNS']	['false negative', 'true positive']
3071	the text. Our proposed metric is called Sentiment Annotation Complexity (SAC). 	[[73, 76]]	[[40, 71]]	['SAC']	['Sentiment Annotation Complexity']
3072	 2.5 Semantic Textual Similarity Semantic Textual Similarity (STS) is the task of judging the similarity of a pair of sentences on	[[62, 65]]	[[33, 60]]	['STS']	['Semantic Textual Similarity']
3073	 We  also  produced  an  upper  bound  using  Naive  Bayes  multinomial  (NBm)  and Support Vector Machine (SVM)6 classifiers  with the NTU Sentiment  Dictionary (Ku  et al, 	[[108, 111], [74, 77], [136, 139]]	[[84, 106], [46, 71]]	['SVM', 'NBm', 'NTU']	['Support Vector Machine', 'Naive  Bayes  multinomial']
3074	16 Production rule that expands n?s parent * * 17 Parse tree path from n to the nearest support verb * 18 Last part of speech (POS) subsumed by n * 19 Production rule that expands n?s left sibling *	[[127, 130]]	[[111, 125]]	['POS']	['part of speech']
3075	This work was partly supported by UK EPSRC project GR/N36462/93: ? Robust Accurate Statistical Parsing (RASP)?. 	[[104, 108], [34, 36], [37, 42]]	[[67, 102]]	['RASP', 'UK', 'EPSRC']	['Robust Accurate Statistical Parsing']
3076	the expected output.  formally a weighted finite state automation (FSA), where V is the set of nodes andE is the set of edges.	[[67, 70]]	[[42, 65], [120, 124]]	['FSA']	['finite state automation', 'edge']
3077	An effective semi-supervised extractor will have good performance over a range of extraction tasks and corpora. However, many of the learning procedures just cited have been tested on only one or two extraction tasks, so their generality is uncertain. To remedy this, we have tested learners based on both assumptions, targeting both a MUC (Message Understanding Conference) scenario and several ACE (Automatic Content Extraction) event types. We identify shortcomings of the prior bootstrapping methods, propose a more effective and stable ranking method, and consider the effect of different corpora and evaluation metrics.	[[336, 339], [396, 399]]	[[341, 373], [401, 429]]	['MUC', 'ACE']	['Message Understanding Conference', 'Automatic Content Extraction']
3078	The problem of evaluating machine translation output without reference translations is called quality estimation (QE) and has recently been the centre of attention (Bojar et al.,	[[114, 116]]	[[94, 112]]	['QE']	['quality estimation']
3079	Averaged Reca l l  (AR)  : 2  PP.~.NP ? Averaged Prec is ion  (AP)  : 2  (fl3-1-1)?PPxPR ?	[[63, 65], [20, 22], [30, 32], [35, 37], [83, 85], [86, 88]]	[[40, 60], [0, 17]]	['AP', 'AR', 'PP', 'NP', 'PP', 'PR']	['Averaged Prec is ion', 'Averaged Reca l l']
3080	To extract the verb?noun combinations that have been used by non-native speakers in practice, we use the Cambridge Learner Corpus (CLC), which is a 52.5 million-word corpus of learner En-	[[131, 134]]	[[105, 129]]	['CLC']	['Cambridge Learner Corpus']
3081	Query key words and concepts Token and concept Indexing Knowledge Source Adapters (KSAs)   integrate and deliver content from 	[[83, 87]]	[[56, 81]]	['KSAs']	['Knowledge Source Adapters']
3082	 Definition 1  A lexical conceptual structure (LCS) is a modified version of the representation proposed  by Jackendoff (1983, 1990) that conforms to the following structural form: 	[[47, 50]]	[[17, 45]]	['LCS']	['lexical conceptual structure']
3083	computational semantics. With the rise of massive and easily-accessible digital corpora, computation of co-occurrence statistics has enabled researchers in NLP to build distributional semantic models (DSMs) that have found relevance in many application areas.	[[201, 205], [156, 159]]	[[169, 199]]	['DSMs', 'NLP']	['distributional semantic models']
3084	139  Computational Linguistics Volume 18, Number 2  (18a) sense of JUMP = jumping. 	[[67, 71]]	[[74, 81]]	['JUMP']	['jumping']
3085	DC-10-30?s number 1 engine, a General Electric CF6-50C2, experienced a casing breach when the 2nd-stage low pressure turbine (LPT) anti-rotation nozzle locks failed.?	[[126, 129], [47, 55], [0, 8]]	[[104, 124]]	['LPT', 'CF6-50C2', 'DC-10-30']	['low pressure turbine']
3086	 The set of all homonyms built for a sentence is  called its morphological structure (MorphS). 	[[86, 92]]	[[61, 84]]	['MorphS']	['morphological structure']
3087	"C H A N G E .  H A V E  CSEXCH FOB BEPGFF IN 1 4   ANTEST CALLEC FOR 23""NASREL: "" (AACC) S D =  24, RES= 3. TOP= 1:s "	[[65, 68], [83, 87], [89, 92], [100, 103], [108, 111], [24, 30], [31, 34], [35, 41]]	[[51, 64]]	['FOR', 'AACC', 'S D', 'RES', 'TOP', 'CSEXCH', 'FOB', 'BEPGFF']	['ANTEST CALLEC']
3088	the problem as a multi-label classification task,  we trained a binary classification model for each  code using support vector machine (SVM) with  ten-fold cross-validation.	[[137, 140]]	[[113, 135]]	['SVM']	['support vector machine']
3089	of Petrov et al. ( 2012): NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners and articles), ADP	[[54, 57], [72, 75]]	[[59, 69], [77, 84]]	['ADJ', 'ADV']	['adjectives', 'adverbs']
3090	Soft cardinality has been shown to be a very strong text-overlapping baseline for the task of measuring semantic textual similarity (STS), obtaining 3rd place in SemEval-2012.	[[133, 136], [162, 174]]	[[104, 131]]	['STS', 'SemEval-2012']	['semantic textual similarity']
3091	The Meter Corpus chosen as the test data  is a collection of court reports from the British Press Association (PA) and some leading  British newspapers (Gaizauskas 2001; Clough 	[[111, 113]]	[[92, 109]]	['PA']	['Press Association']
3092	 1356 Batch Training (BFGS) Online Training (SD) Simulated parameter initialization chunked data selection Annealing	[[22, 26], [45, 47]]	[[6, 20]]	['BFGS', 'SD']	['Batch Training']
3093	mfly@sky.ru Abstract YARN (Yet Another RussNet) project started in 2013 aims at creating a large	[[21, 25]]	[[27, 46]]	['YARN']	['Yet Another RussNet']
3094	? airplane?. We will call this the GN (general noun) lexicon.	[[35, 37]]	[[39, 51]]	['GN']	['general noun']
3095	 Proceedings of 24th International Conference on Computational Linguistics (COLING): Posters. 	[[76, 82]]	[[49, 74]]	['COLING']	['Computational Linguistics']
3096	46 2 Related Work and Overview Although Spreading Activation (SA) is foremost a cognitive theory modelling semantic mem-	[[62, 64]]	[[40, 60]]	['SA']	['Spreading Activation']
3097	"(grammatical knowledge and lexical knowledge respectively). The insights gained from our work  with machine readable dictionaries (MRDs) the Longman Dictionary of Contemporary English,  henceforth LDOCE, and the Van Dale dictionary of contemporary Dutch ""Groot Woordenboek van "	[[131, 135], [197, 202]]	[[100, 129]]	['MRDs', 'LDOCE']	['machine readable dictionaries']
3098	Another dictionary device deals wlth  unrecognized elements - the so-called  transducing dictionary (TD). TD re- 	[[101, 103]]	[[77, 99]]	['TD']	['transducing dictionary']
3099	The model is presented below. 4 The Model This study employs feed-forward artificial neu-ral networks with a backpropagation algorithm as computational models for the analysis of un-accusative/unergative distinction in Turkish. 4.1 Artificial Neural Networks and Learn-ing Paradigms  An artificial neural network (ANN) is a compu-tational model that can be used as a non-linear statistical data modeling tool. ANNs are gener-ally used for deriving a function from observa-tions, in applications where the data are com-plex and it is difficult to devise a relationship 	[[314, 317]]	[[287, 312]]	['ANN']	['artificial neural network']
3100	related NIST metric (Doddington 2002), along with WER and PER, have been widely used by many machine translation researchers. The Translation Edit Rate (TER) (Snover et al 2006) and the CDER measure (Leusch, Ueffing, and Ney 2006) are based on the edit	[[153, 156], [8, 12], [50, 53], [58, 61], [186, 190]]	[[130, 151]]	['TER', 'NIST', 'WER', 'PER', 'CDER']	['Translation Edit Rate']
3101	experiments. For Hindi, we worked with the Hindi Dependency Treebank (HDT) released as part of Coling 2012 Shared Task (Bharati et	[[70, 73]]	[[43, 68]]	['HDT']	['Hindi Dependency Treebank']
3102	(equivalent to relation). Their relation instances are named entity(NE)-mention pairs conforming to a set of pre-specified rules.	[[68, 70]]	[[55, 67]]	['NE']	['named entity']
3103	 CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) administered by the Media Development Authority (MDA)	[[81, 84], [1, 6], [135, 138]]	[[51, 79], [106, 133]]	['NRF', 'CSIDM', 'MDA']	['National Research Foundation', 'Media Development Authority']
3104	boring dependency structures. CC = coordinate concatenate, LA = left adjoining, and RA = right adjoining.	[[59, 61]]	[[64, 78]]	['LA']	['left adjoining']
3105	3.2 Data Normalization The ACL shared task is very close in form and content to the Final Text Editions (FTE) task of the TCSTAR (TC-STAR, 2004) evaluation.	[[105, 108], [27, 30], [130, 137]]	[[84, 103], [122, 128]]	['FTE', 'ACL', 'TC-STAR']	['Final Text Editions', 'TCSTAR']
3106	Table 3: The effect of new features on the development set for English. UAS = unlabeled attachment score; UEM = unlabeled exact match.	[[72, 75], [106, 109]]	[[78, 104], [112, 133]]	['UAS', 'UEM']	['unlabeled attachment score', 'unlabeled exact match']
3107	interested in formalisms which are being  used or have applications in the domain  of machine translation (MT). These can 	[[107, 109]]	[[86, 105]]	['MT']	['machine translation']
3108	as ht, ct = LSTM(xt, ht?1, ct?1).  Residual Networks (ResNet) are among the pioneering works (Szegedy et al, 2015; Srivastava et	[[54, 60], [12, 16]]	[[35, 52]]	['ResNet', 'LSTM']	['Residual Networks']
3109	Semantic Specialization (ISS) (Girju, Badulescu,  and Moldovan, 2006), Na?ve Bayes (NB) 3  and  Maximum Entropy (ME)4.  	[[113, 115], [25, 28], [84, 86]]	[[96, 111], [0, 23], [71, 82]]	['ME', 'ISS', 'NB']	['Maximum Entropy', 'Semantic Specialization', 'Na?ve Bayes']
3110	We focused on syntax, esp. noun phrase (NP) syntax from the beginning.	[[40, 42]]	[[27, 38]]	['NP']	['noun phrase']
3111	blogs which accumulated a large number of posts during this period: Carpetbagger (CB),1 Daily Kos (DK),2 Matthew Yglesias (MY),3 Red State (RS),4 and Right Wing News (RWN).5 CB and MY ceased	[[123, 125], [140, 142], [82, 84], [167, 170], [181, 183], [174, 176]]	[[105, 121], [129, 138], [68, 80], [150, 164]]	['MY', 'RS', 'CB', 'RWN', 'MY', 'CB']	['Matthew Yglesias', 'Red State', 'Carpetbagger', 'Right Wing New']
3112	PIQ(Fy;R) or not.  z Predictive Information Redundancy(PIR) Based on the above two definitions, we can	[[55, 58], [0, 3], [4, 8]]	[[21, 53]]	['PIR', 'PIQ', 'Fy;R']	['Predictive Information Redundanc']
3113	 3.2 Structure Mapping Theory French (2002) cites Structure Mapping Theory (SMT) (Gentner 1983) and its implementation in the Structure Mapping Engine (SME) (Falkenhainer, Forbus, and Gentner	[[76, 79], [152, 155]]	[[50, 74], [126, 150]]	['SMT', 'SME']	['Structure Mapping Theory', 'Structure Mapping Engine']
3114	ing the most similar. This year we set up two tasks: (i) a core task (CORE), and (ii) a typed-similarity task (TYPED).	[[70, 74], [111, 116]]	[[59, 63], [88, 93]]	['CORE', 'TYPED']	['core', 'typed']
3115	Entailment [13] was organized by SemEval-2.  Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand 	[[76, 80], [96, 101]]	[[45, 74]]	['RITE', 'NTCIR']	['Recognizing Inference in Text']
3116	In this paper, we also implement type raising rules for compact representation of PP (prepositional phrase) and AP (adverbial phrase).	[[82, 84], [112, 114]]	[[86, 106], [116, 132]]	['PP', 'AP']	['prepositional phrase', 'adverbial phrase']
3117	Data Annotated. The data to be annotated in WSsim-1 were taken primarily from Semcor (Miller et al1993) and the Senseval-3 English lexical sample (SE-3) (Mihalcea, Chklovski, and Kilgarriff 2004).	[[147, 151]]	[[112, 122]]	['SE-3']	['Senseval-3']
3118	8  end if  Active characters are discussed in the section about identifying the SC (Section 8),  because the raison d'etre of the active-character component of an interpretation is that 	[[80, 82]]	[[84, 91]]	['SC']	['Section']
3119	Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al (2010), on this widely-used benchmark corpus are	[[108, 112], [55, 58]]	[[96, 106]]	['EMDL', 'MDL']	['EntropyMDL']
3120	volution model (DTCNN). Figure 1 illustrates an example from the Movie Reviews (MR) dataset (Pang and Lee, 2005).	[[80, 82], [16, 21]]	[[65, 78]]	['MR', 'DTCNN']	['Movie Reviews']
3121	3.5 Classification   For our task of classifying ALI patients, we  picked the Maximum Entropy (MaxEnt) algorithm due to its good performance in text classi-	[[95, 101], [49, 52]]	[[78, 93]]	['MaxEnt', 'ALI']	['Maximum Entropy']
3122	patterns allows a 4-way classification of slopes of lines: fast rising, rising, level, falling.  These are the 4 Fundamental Pattern Features (FPF). A combination of 2 or  3 (of the 4) 	[[143, 146]]	[[113, 141]]	['FPF']	['Fundamental Pattern Features']
3123	Abstract We present a system that automatically induces Selectional Preferences (SPs) for Latin verbs from two treebanks by using	[[81, 84]]	[[56, 79]]	['SPs']	['Selectional Preferences']
3124	scoring n-grams in the sentence.  Mean logprob (ML) This score is the logprob of the entire sentence divided by the length of the	[[48, 50]]	[[34, 46]]	['ML']	['Mean logprob']
3125	 First, we employ a multiple output GP based on the Intrinsic Coregionalization Model (ICM) (	[[87, 90], [36, 38]]	[[52, 85]]	['ICM', 'GP']	['Intrinsic Coregionalization Model']
3126	and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS). 	[[94, 98]]	[[55, 92]]	['NIPS']	['Neural Information Processing Systems']
3127	and intangible factors known to the engineer but unknown to the computer. For example, the engineer  may need to postpone the placement of equipment in a certain CSA (Carrier Serving Area) due to a fixed  cap on near-term expenditures, or she may decide to activate DLC (Digital Loop Carrier) equipment in 	[[162, 165]]	[[167, 187]]	['CSA']	['Carrier Serving Area']
3128	74.80 ?  Table 2: Parsing accuracy; AS = attachment score; ER = error reduction w.r.t. projective baseline (%)	[[36, 38], [59, 61]]	[[41, 57], [64, 79]]	['AS', 'ER']	['attachment score', 'error reduction']
3129	 3.2 Latent Structural SVM We employ the latent structural SVM (LS-SVM) model for learning the discriminative model of query	[[64, 70], [23, 26]]	[[41, 62]]	['LS-SVM', 'SVM']	['latent structural SVM']
3130	I  zwemmen  (17) is ttms obtained by setting VPo = VPh Zo = Yo,  attd Zl = Vl.	[[51, 54], [45, 48], [70, 72]]	[[55, 57]]	['VPh', 'VPo', 'Zl']	['Zo']
3131	Features representing syntax (a) Phrase structure (PS) rules (b) Grammatical relation (GR) distance measures	[[87, 89], [51, 53]]	[[65, 85], [33, 49]]	['GR', 'PS']	['Grammatical relation', 'Phrase structure']
3132	1 Introduction Semantic Parsing, the process of converting text into a formal meaning representation (MR), is one of the key challenges in natural language process-	[[102, 104]]	[[78, 100]]	['MR']	['meaning representation']
3133	In Proceedings of the Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 376?387.	[[95, 102]]	[[22, 93]]	['CICLing']	['Conference on Intelligent Text Processing and Computational Linguistics']
3134	"AFIPS Washington Off ice   GAO REPORTS ON FEDERAL MODELING  The General Accounting 0fEic.e (GAO') has released a repor t -  on ""Ways to  Improve  &mug-nt of Federally kcnded ~o@ute r i z ed  ~ o d e Z s I ~  (#-  enc luse $1.00) ."	[[92, 96], [0, 5], [27, 30]]	[[64, 90]]	"[""GAO'"", 'AFIPS', 'GAO']"	['General Accounting 0fEic.e']
3135	fier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring	[[95, 98]]	[[65, 93]]	['PMI']	['pointwise mutual information']
3136	et al, 1993). The learning algorithm was inspired  by several Inductive Logic Programming (ILP) sys-  tems and primarily consists of a specific-to-general 	[[91, 94]]	[[62, 89]]	['ILP']	['Inductive Logic Programming']
3137	dleware architecture (Scha?fer, 2006). It starts with sentence boundary detection (SBR) and regular expression-based tokenization using its built-in	[[83, 86]]	[[54, 71]]	['SBR']	['sentence boundary']
3138	120 compared to approximate maximum likelihood estimation (MLE). However, this method has not been	[[59, 62]]	[[28, 57]]	['MLE']	['maximum likelihood estimation']
3139	Such vectors can be used to perform all standard linear algebra operations applied in vector-based semantics: Measuring the cosine of the angle between vectors, applying singular value decomposition (SVD) to the whole matrix, and so on.	[[200, 203]]	[[170, 198]]	['SVD']	['singular value decomposition']
3140	It is encouraging that the results (after correcting the misaligned identifiers) for the patched system are approaching the Inter Tagger Agreement (ITA) level reported for OntoNotes sense tags by the task or-	[[148, 151]]	[[124, 146]]	['ITA']	['Inter Tagger Agreement']
3141	Winnows software package.  Maximum Entropy Model (MEM) is  especially suited for integrating evidences from 	[[50, 53]]	[[27, 48]]	['MEM']	['Maximum Entropy Model']
3142	 In this work, we explore n-gram models over Minimal Translation Units (MTUs) to explicitly capture contextual dependencies across	[[72, 76]]	[[45, 70]]	['MTUs']	['Minimal Translation Units']
3143	The Cell or Tissue Type category was split into two fine grained classes, CELL and CLNE (cell line). 	[[83, 87], [74, 78]]	[[89, 98]]	['CLNE', 'CELL']	['cell line']
3144	Additionally, we have acquired gazetteer lists for Hindi and used these gazetteers in the Maximum Entropy (MaxEnt) based Hindi NER system.	[[107, 113], [127, 130]]	[[90, 105]]	['MaxEnt', 'NER']	['Maximum Entropy']
3145	 The system determines the position of the main part of  the situation relative to the point of speech (PS). The 	[[104, 106]]	[[87, 102]]	['PS']	['point of speech']
3146	2 Conditional Random Fields 2.1 The model Conditional Random Fields(CRFs), a statistical sequence modeling framework, was first intro-	[[68, 72]]	[[42, 66]]	['CRFs']	['Conditional Random Field']
3147	lion tokens to 0.3832 for 237 million tokens. At such data sizes, Stupid Backoff (SB) with a constant backoff parameter ?	[[82, 84]]	[[66, 80]]	['SB']	['Stupid Backoff']
3148	640,000 non-empty abstracts were found.  Query Set (QSet): We download from PubMed the abstracts that mention a given gene name and its syn-	[[52, 56]]	[[41, 50]]	['QSet']	['Query Set']
3149	(3) range, e.g., (age(value(0 100))).  The notion of IMPORTANCE VALUES (IVs) are  introduced here and are used to numerically describe 	[[72, 75]]	[[53, 70]]	['IVs']	['IMPORTANCE VALUES']
3150	Morphological alterations of a search term have a negative impact on the recall performance of an information retrieval (IR) system (Choueka, 1990; Ja?ppinen and Niemisto?,	[[121, 123]]	[[98, 119]]	['IR']	['information retrieval']
3151	  Figure 1: Example of a BDT sentence in the CoNLL-X format  (V = main verb, AUXV = auxiliary verb, CONJ = conjunction, REL = subordinated clause, CMP = completive, ccomp_obj =  clausal complement object, ERG = ergative, SUBJ:3S: subject in 3rd person sing.,	[[77, 81], [100, 104], [147, 150]]	[[84, 98], [107, 118], [153, 163]]	['AUXV', 'CONJ', 'CMP']	['auxiliary verb', 'conjunction', 'completive']
3152	driver was measured in two ways. The driver performed a Tactile Detection Task (TDT) (van Winsum et al, 1999).	[[80, 83]]	[[56, 78]]	['TDT']	['Tactile Detection Task']
3153	terns for them. In the terminology of Frame Semantics, the roles are called frame elements (FEs), and the words which evoke the frame are referred	[[92, 95]]	[[76, 90]]	['FEs']	['frame elements']
3154	Table 7 shows the effects of merging the sub-  ject accessibility bias with both recency biases and  the restricted memory bias (RM). The results in 	[[129, 131]]	[[105, 122]]	['RM']	['restricted memory']
3155	Table 7: Feature template we use for our classification of event types. Feature examples are based on the sentence ta1 (he) bu4 (not) kan4 (read) zhen1tan4 (detective) xiao3shuo1 (novel) le0 (LE) ? he doesn?t	[[192, 194]]	[[187, 190]]	['LE']	['le0']
3156	ly corrected. An n-gram (n= 2 and 3) language model was then built from the Sinica corpus released  by the Association for Computational Linguistics and Chinese Language Processing (ACLCLP) using  the SRILM toolkit (Stolcke, 2002).	[[182, 188], [201, 206]]	[[107, 180]]	['ACLCLP', 'SRILM']	['Association for Computational Linguistics and Chinese Language Processing']
3157	ical analyzer for English (Sekine, 2001)are selected and translation candidates having POS tags other than NN (noun) are discarded. Selected translation	[[107, 109], [87, 90]]	[[111, 115]]	['NN', 'POS']	['noun']
3158	 The output is a prediction of whether the tweet is inside region (IR) or outside region (OR). We	[[67, 69], [90, 92]]	[[52, 65], [74, 88]]	['IR', 'OR']	['inside region', 'outside region']
3159	Building on the error analysis of the rule-based approach, we replace the rule-based component with support vector machine (SVM) classifiers trained on partial event annotation in the form of	[[124, 127]]	[[100, 122]]	['SVM']	['support vector machine']
3160	 The third thread is the sponsorship of the international  Message Understanding Conferences (MUC's) and Text  Retrieval Conferences (TREC's).	[[94, 99], [134, 140]]	[[59, 92], [105, 132]]	"[""MUC's"", ""TREC's""]"	['Message Understanding Conferences', 'Text  Retrieval Conferences']
3161	sists of given entities and their relation expression.  Here, we use a Salient Referent List (SRL) to obtain contextual structure.	[[94, 97]]	[[71, 92]]	['SRL']	['Salient Referent List']
3162	Frame representat ion ( for  mapping 4):  \[agent/an\] (i/PRP)  5PO$ abbreviations: PRP=personal pro-  noun, AUX=auxiliary verb, VB=main verb (non-inflected), 	[[84, 87], [109, 112], [129, 131]]	[[88, 101], [113, 122], [137, 141]]	['PRP', 'AUX', 'VB']	['personal pro-', 'auxiliary', 'verb']
3163	We have been concentrating on the Hong  Kong Hansard, which are the parliamentary pro-  ceedings of the Legislative Council (LegCo). Anal- 	[[125, 130]]	[[104, 123]]	['LegCo']	['Legislative Council']
3164	4. Crowdsourcing We used the Amazon Mechanical Turk (AMT) service to obtain annotations for different kinds of opposites.	[[53, 56]]	[[29, 51]]	['AMT']	['Amazon Mechanical Turk']
3165	= argmaxjP (zi = j|sk).  4.2 Lexical Chain Segmenter (LCSeg) Our second model is the lexical chain based seg-	[[54, 59]]	[[29, 52]]	['LCSeg']	['Lexical Chain Segmenter']
3166	modeled as distributed dense vectors of hidden layers. A recurrent neural network (RNN) is one such learner that can operate on sequential data of variable	[[83, 86]]	[[57, 81]]	['RNN']	['recurrent neural network']
3167	  Abstract  A Named Entity Recognizer (NER) generally  has worse performance on machine translated 	[[39, 42]]	[[14, 37]]	['NER']	['Named Entity Recognizer']
3168	 We apply both support vector machine (SVM)  and Maximum Entropy (ME) algorithms with the  help of the SVM-light4 and Mallet5 tools.	[[66, 68], [39, 42], [103, 106]]	[[49, 64], [15, 37]]	['ME', 'SVM', 'SVM']	['Maximum Entropy', 'support vector machine']
3169	e.g. Microsoft 2. Candidate Definition Features (CDs) : These consist of the two following feature classes.	[[49, 52]]	[[18, 47]]	['CDs']	['Candidate Definition Features']
3170	  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 787?798, October 25-29, 2014, Doha, Qatar.	[[90, 95]]	[[40, 88]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
3171	Fazly and Stevenson, 2007), our approach is to compare the context vector of a VNC with the composed vector of the verb and noun (V-N) component units of the VNC when they occur in iso-	[[130, 133]]	[[115, 128]]	['V-N']	['verb and noun']
3172	tions and keying in on student language to promote self-explanation of concepts, and its curriculum is based on the Full Option Science System (FOSS) 1 a proven system for inquiry based learning.	[[144, 148]]	[[116, 142]]	['FOSS']	['Full Option Science System']
3173	document-level CLOA tasks, respectively. The evaluations on simplified Chinese (SC) opinion analysis by using small SC training data and large	[[80, 82], [15, 19], [116, 118]]	[[60, 78]]	['SC', 'CLOA', 'SC']	['simplified Chinese']
3174	The variants of random walk topic models are compared against LDA and the relational topic model (RTM), each with 100 topics (Chang and Blei, 2010).	[[98, 101], [62, 65]]	[[74, 96]]	['RTM', 'LDA']	['relational topic model']
3175	ing is combined with dictionary-based (e.g.,  WordNet) reranking, which leads to a 25% increase in mean reciprocal rank (MRR). Xu et al 	[[121, 124]]	[[99, 119]]	['MRR']	['mean reciprocal rank']
3176	Specifically, the groups include children with ASD with language impairment (ALI); ASD with no language impairment (ALN); SLI alone; and typically developing (TD), which is	[[116, 119], [47, 50], [77, 80], [122, 125], [159, 161]]	[[83, 114], [56, 75], [137, 157]]	['ALN', 'ASD', 'ALI', 'SLI', 'TD']	['ASD with no language impairment', 'language impairment', 'typically developing']
3177	(NL) into formal meaning representations (MR), and are applied for both parsing of textual input and in Spoken Language Understanding (SLU). In data-	[[135, 138], [1, 3], [42, 44]]	[[104, 133], [17, 40]]	['SLU', 'NL', 'MR']	['Spoken Language Understanding', 'meaning representations']
3178	This representation, together with target language words, are fed to a deep neural network (DNN) to form a stronger NNJM.	[[92, 95], [116, 120]]	[[71, 90]]	['DNN', 'NNJM']	['deep neural network']
3179	| lexica: The relative frequency of categories, based on the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al, 2007).	[[96, 100]]	[[61, 94]]	['LIWC']	['Linguistic Inquiry and Word Count']
3180	latent topic space. Values within vectors are the TF-ITF (term frequency - inverse topic frequency) scores which are calculated in a completely ana-	[[50, 56]]	[[58, 98]]	['TF-ITF']	['term frequency - inverse topic frequency']
3181	These two representations are associated in an abstract type map, called AAM (Abstract Associative Map). This	[[73, 76]]	[[78, 102]]	['AAM']	['Abstract Associative Map']
3182	second verb in Japanese compound verbs. We  investigate Japanese compound verbs (JCVs) and  extract semantic constraints for the purpose of 	[[81, 85]]	[[56, 79]]	['JCVs']	['Japanese compound verbs']
3183	words, respectively. We include two versions of this general model; Continuous Bag of Words (CBOW) that predicts a word based on the context, and Skip-	[[93, 97]]	[[68, 91]]	['CBOW']	['Continuous Bag of Words']
3184	J~:u phom:,l:ical (graphemic) empressio~ in to  5 l \ [ ,+vei~  tecLogt+ah~illa~ieg (levm+l o+ US+st abbrev,  TR) + ,+:mr+ar:e  synkam (SR) + (neP'pfie/r+ic5 (HR) '+ phnnemics ,~nd phnnetit:ts  (g raphmaics} , ,  Eact+ ef the  ieve l~ i s  in terpreted  a~ a set, 	[[136, 138], [110, 112], [159, 161]]	[[128, 134]]	['SR', 'TR', 'HR']	['synkam']
3185	 The first principle of a search engine is based  on shallow Natural Language Processing (NLP)  techniques, for instance, string matching, while 	[[90, 93]]	[[61, 88]]	['NLP']	['Natural Language Processing']
3186	UAS = unlabeled attachment score; UEM = unlabeled exact match; LAS = labeled attachment score. 	[[63, 66], [0, 3], [34, 37]]	[[69, 93], [6, 32], [40, 61]]	['LAS', 'UAS', 'UEM']	['labeled attachment score', 'unlabeled attachment score', 'unlabeled exact match']
3187	Social media in general exhibit a rich variety of  information sources. Question answering (QA) has  been particularly amenable to social media, as it 	[[92, 94]]	[[72, 90]]	['QA']	['Question answering']
3188	340   Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 37?45, Gothenburg, Sweden, April 26-30 2014.	[[91, 96], [25, 29]]	[[47, 89]]	['TTNLS', 'EACL']	['Type Theory and Natural Language Semantics']
3189	 2003. Social Communication Questionnaire (SCQ). 	[[43, 46]]	[[7, 41]]	['SCQ']	['Social Communication Questionnaire']
3190	CHBWGE, H l V E  CSERCH FOR HERGEF IN 10  merge features  i n t o  node 10 (making it a schwa)  ANTEST CALLED FOB 211SCFIHDA (AACC) , SD= 3. RES= 7.	[[126, 130], [134, 136], [0, 6], [8, 15], [17, 23], [28, 34], [24, 27], [141, 144]]	[[96, 124]]	['AACC', 'SD', 'CHBWGE', 'H l V E', 'CSERCH', 'HERGEF', 'FOR', 'RES']	['ANTEST CALLED FOB 211SCFIHDA']
3191	EXPERIMENTS 3.1. Speaker Identification (SID) In order to investigate robust speaker identification under	[[41, 44]]	[[17, 39]]	['SID']	['Speaker Identification']
3192	Figure 8: Alternative English lexical entry for *WORK-  FUNCIONAR  (GENDER and NUMBER) count/mass (COUNT) and  a trinary distinction of ANIMACY (human, animal, 	[[79, 85], [56, 65], [99, 104], [136, 143], [68, 74]]	[[87, 92]]	['NUMBER', 'FUNCIONAR', 'COUNT', 'ANIMACY', 'GENDER']	['count']
3193	Yoram Bachrach is a researcher in the Online Services and Advertising group at Microsoft Research Cambridge UK. His research area is artificial intelligence (AI), focusing on multi-agent systems and computational game theory.	[[158, 160], [108, 110]]	[[133, 156]]	['AI', 'UK']	['artificial intelligence']
3194	Each extractor receives a sentence as input and determines which noun phrases (NPs) in the sentence are fillers for the event role.	[[79, 82]]	[[65, 77]]	['NPs']	['noun phrases']
3195	knowledge in building the target domain classifier, we propose a novel optimization method based on the Na??ve Bayesian (NB) framework and stochastic gradient descent.	[[121, 123]]	[[104, 119]]	['NB']	['Na??ve Bayesian']
3196	problem so that it includes finding the most probable corrections tags.  WDCLOREI arg max Pr( WDCLOREIIA ) WDCLOREI 	[[94, 104], [107, 115]]	[[73, 89]]	['WDCLOREIIA', 'WDCLOREI']	['WDCLOREI arg max']
3197	1 Introduction Recent research shows that it is possible, using current natural language processing (NLP) and machine learning technology, to automatically induce lex-	[[101, 104]]	[[72, 99]]	['NLP']	['natural language processing']
3198	crafted features, lexicons, and grammars.  Meanwhile, recurrent neural networks (RNNs) what are the major cities in utah ?	[[81, 85]]	[[54, 79]]	['RNNs']	['recurrent neural networks']
3199	gramming for probabilistic programming. In International Workshop on Statistical Relational AI (StarAI). 	[[96, 102]]	[[69, 94]]	['StarAI']	['Statistical Relational AI']
3200	In order to take the transitivity of outscoping relations into account, we use the transitive closure (TC) of DAGs. Let G+ =	[[103, 105], [110, 114]]	[[83, 101]]	['TC', 'DAGs']	['transitive closure']
3201	the  locat ions  shown in F ig .4 .  The s lo t  SH~P represent  whether  the  par t   cor respond ing  to  th i s  frame is  a reg ion(~EG)  or a branch(BR A) The  SU~P s lo t  records  i t s  subpar ts  and the i r  locat ions  of or re ia t ions  to 	[[154, 158], [49, 53], [136, 139], [165, 169]]	[[147, 152]]	['BR A', 'SH~P', '~EG', 'SU~P']	['branc']
3202	We labeled the Reddit dataset using crowdworkers on Amazon Mechanical Turk (AMT), creating the first public corpus annotated with levels of dogmatism.	[[76, 79]]	[[52, 74]]	['AMT']	['Amazon Mechanical Turk']
3203	ancient than transport 1.83E-102 old ancient 0.005 Table 8: Substitution Feature Table (SubFT) 3.3.2 Phrase Substitution	[[88, 93]]	[[60, 86]]	['SubFT']	['Substitution Feature Table']
3204	words in similar context have similar meanings ?  distributed semantic models (DSM)s build vector representations based on corpus-extracted context.	[[79, 82]]	[[50, 77]]	['DSM']	['distributed semantic models']
3205	by using RBM to implement the middle layers,  since RBM can be learned very quickly by the  Contrastive Divergence (CD) approach. 	[[116, 118], [9, 12], [52, 55]]	[[92, 114]]	['CD', 'RBM', 'RBM']	['Contrastive Divergence']
3206	topics vary over time, we aggregate the microblog posts published in a month as a document. Then, we use a Latent Dirichlet Allocation (LDA) to estimate their topics. Figure 1 illustrates an example, where	[[136, 139]]	[[107, 134]]	['LDA']	['Latent Dirichlet Allocation']
3207	We use three classes of features? Crowd Grades (CG), Force Alignment features (FA) and Natural Language Processing features (NLP).	[[79, 81], [48, 50], [125, 128]]	[[53, 68], [34, 46], [87, 114]]	['FA', 'CG', 'NLP']	['Force Alignment', 'Crowd Grades', 'Natural Language Processing']
3208	training set by cosine-similarity. Precision-recall (P-R) curves and mean average precision (MAP) are two metrics we used for evaluation.	[[93, 96], [53, 56]]	[[69, 91], [35, 51]]	['MAP', 'P-R']	['mean average precision', 'Precision-recall']
3209	2005a; Jeon et al, 2005b) compared four different  retrieval methods, i.e. vector space model, Okapi,  language model (LM), and translation-based model,  for automatically fixing the lexical chasm between 	[[119, 121]]	[[103, 117]]	['LM']	['language model']
3210	Table 2: Three kinds of preprocessing of a sentence in Japanese; N = noun, TOP = topic marker, ADV = adverbial particle, ADJ = adjective, COP = copula, EXCL = exclamation mark.	[[95, 98], [121, 124], [75, 78], [138, 141], [152, 156]]	[[101, 110], [127, 136], [69, 73], [81, 86], [144, 150], [159, 170]]	['ADV', 'ADJ', 'TOP', 'COP', 'EXCL']	['adverbial', 'adjective', 'noun', 'topic', 'copula', 'exclamation']
3211	Berwick and Weinberg (1982). Gazdar noted that if  transformational grammars (TG's) were stripped of  all their transformations, they became CFL- 	[[78, 82], [141, 144]]	[[51, 76]]	"[""TG's"", 'CFL']"	['transformational grammars']
3212	disasters or political crises in the media. There are two main approaches to audio hotspotting; one involves speech-to-text (STT), also known as large vocabulary continuous speech recognition (LVCSR),  and the other employs phonetic speech recognition.	[[193, 198], [125, 128]]	[[145, 191], [109, 123]]	['LVCSR', 'STT']	['large vocabulary continuous speech recognition', 'speech-to-text']
3213	196  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1779?1785, October 25-29, 2014, Doha, Qatar.	[[93, 98]]	[[43, 91]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
3214	cognitive system (Wilkes, 1997). We refer to this  subset of knowledge store (KS) in operation for and in  a given discourse as discourse model (DM) and hold 	[[78, 80], [145, 147]]	[[61, 76], [128, 143]]	['KS', 'DM']	['knowledge store', 'discourse model']
3215	tion algorithm. We use an existing unsupervised method, called Double Propagation (DP) (Qiu et al, 2011), for extraction.	[[83, 85]]	[[63, 81]]	['DP']	['Double Propagation']
3216	Some other related works should be mentioned. A notable method is Locality Sensitive Hashing (LSH) (Indyk et al, 1998).	[[94, 97]]	[[66, 92]]	['LSH']	['Locality Sensitive Hashing']
3217	 Take for example the word cliff which could be a  proper (NP) 1 or a common noun (NN) (ignoring ca-  pitalization of proper nouns for the moment).	[[83, 85], [59, 61]]	[[77, 81]]	['NN', 'NP']	['noun']
3218	 Issues, Tasks and Program Structures to Roadmap Research in Question & Answering (Q&A) http://www-nlpir.nist.gov/projrcts/	[[83, 86]]	[[61, 81]]	['Q&A']	['Question & Answering']
3219	 The specific case focused on in this paper is that of AL with SVMs (AL-SVM) for imbalanced ?	[[69, 75]]	[[55, 67]]	['AL-SVM']	['AL with SVMs']
3220	without any adaptation.  Laplacian SVM (L-SVM) This is a semisupervised learning method based on label	[[40, 45]]	[[25, 38]]	['L-SVM']	['Laplacian SVM']
3221	The dependency inductions were evaluated on 3 metrics: directed accuracy, undirected accuracy and Neutral Edge Detection (NED) (Schwartz et al.,	[[122, 125]]	[[98, 120]]	['NED']	['Neutral Edge Detection']
3222	and the parameters ? can be estimated using maximum likelihood estimation(MLE) on a training corpus(Och and Ney, 2003).	[[74, 77]]	[[44, 73]]	['MLE']	['maximum likelihood estimation']
3223	Xi, where Par(Xi) denotes the parents of Xi.  Conditional probability distributions (CPDs) can be defined in various ways, from look-up tables	[[85, 89], [10, 13]]	[[46, 83], [30, 37]]	['CPDs', 'Par']	['Conditional probability distributions', 'parents']
3224	(FW), SeekBw (BW), ScrollFw (FS), ScrollBw (BS), Ratechange Increase (RCI), Ratechange Decrease (RCD). 	[[97, 100], [1, 3], [14, 16], [29, 31], [44, 46], [70, 73]]	[[76, 95], [6, 12], [19, 27], [34, 42], [49, 68]]	['RCD', 'FW', 'BW', 'FS', 'BS', 'RCI']	['Ratechange Decrease', 'SeekBw', 'ScrollFw', 'ScrollBw', 'Ratechange Increase']
3225	3 Attribute Modeling based on LDA 3.1 Controled LDA This section introduces Controled LDA (C-LDA), a weakly supervised variant of LDA.	[[91, 96], [30, 33], [48, 51], [130, 133]]	[[76, 89]]	['C-LDA', 'LDA', 'LDA', 'LDA']	['Controled LDA']
3226	the percentage of words that were placed in a segment perfectly identical to that in the reference. The dialogue act based metric (DER) was proposed in Zimmermann et al (2005). In this metric a word is	[[131, 134]]	[[104, 129]]	['DER']	['dialogue act based metric']
3227	connective label?. The results are reported for Same Sentence (SS) and Previous Sentence (PS) models, and the joined results for each of the argu-	[[63, 65], [90, 92]]	[[48, 61], [71, 88]]	['SS', 'PS']	['Same Sentence', 'Previous Sentence']
3228	The TRIPS system here uses a wide range of statistically driven preprocessing, including part of speech tagging, constituent bracketing, inter-pretation of unknown words using WordNet, and named-entity recognition (Allen et al 2008). All these are generic off-the-shelf resources that ex-tend and help guide the deep parsing process.  The TRIPS LF (logical form) ontology1 is de-signed to be linguistically motivated and domain independent. The semantic types and selectional restrictions are driven by linguistic considera-tions rather than requirements from reasoning components in the system (Dzikovska et al 2003).	[[345, 347], [339, 344], [4, 9]]	[[349, 361]]	['LF', 'TRIPS', 'TRIPS']	['logical form']
3229	Chunking Tree (CT) 86.17 86.21 Linear Features (Kl) 90.79 90.46 Kl w/o using LM feature (Kl-LM) 84.24 84.06 Composite Kernel (Kc: MST+Kl) 92.98 92.67	[[89, 94], [15, 17], [48, 50], [130, 133], [134, 136]]	[[64, 79], [0, 13]]	['Kl-LM', 'CT', 'Kl', 'MST', 'Kl']	['Kl w/o using LM', 'Chunking Tree']
3230	the 8th International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, may.  European Language Resources Association (ELRA). 	[[148, 152], [71, 75]]	[[107, 146], [36, 53]]	['ELRA', 'LREC']	['European Language Resources Association', 'Language Resource']
3231	New York, N.Y. 10027  Introduction  COMET (COordinated Multimedia Explanation  Testbed) is an experimental system that generates inter- 	[[36, 41], [10, 14]]	[[43, 86], [0, 8]]	['COMET', 'N.Y.']	['COordinated Multimedia Explanation  Testbed', 'New York']
3232	and Darooneh, 2011).  Recurrent neural networks(RNNs) (Elman, 1990) has been applied to many sequential prediction	[[48, 52]]	[[22, 46]]	['RNNs']	['Recurrent neural network']
3233	2.2 Natural Language Processing  2.2.1 Woods' Augmented Transition Networks  Thc Augmented Transition Network (ATN) of Woods [Woods 501 is a  powerful formalisnl for representing grammars.	[[111, 114]]	[[81, 109]]	['ATN']	['Augmented Transition Network']
3234	"Phrase Extrac""on GIZA++ Figure 3: Building a twin phrase table (PT). First, sep-"	[[64, 66], [17, 23]]	[[50, 62]]	['PT', 'GIZA++']	['phrase table']
3235	  1 Introduction  Many Natural Language Processing (NLP)  applications need to recognize when the meaning 	[[52, 55]]	[[23, 50]]	['NLP']	['Natural Language Processing']
3236	ability integral transform to generate empirical cumulative density functions (ECDF): now instead of the probability density function (PDF) space, we are working in the ECDF space where the value of each	[[135, 138], [79, 83], [169, 173]]	[[105, 133], [39, 77]]	['PDF', 'ECDF', 'ECDF']	['probability density function', 'empirical cumulative density functions']
3237	  Abbreviation: ACK=Acknowledgment; COMP=Completion;  SNU=Signal Non Understanding; Sources abbreviated as: F  = Friendship; V = Visibility; Rte = Route 	[[54, 57], [16, 19], [36, 40], [141, 144]]	[[58, 82], [20, 33], [41, 51], [113, 123], [129, 139], [147, 152]]	['SNU', 'ACK', 'COMP', 'Rte']	['Signal Non Understanding', 'Acknowledgmen', 'Completion', 'Friendship', 'Visibility', 'Route']
3238	terial with relevant events will be done along the 1MUMIS is an on-going EU-funded project within the Information Society Program (IST) of the European Union, section Human Language Technology (HLT).	[[131, 134]]	[[102, 121]]	['IST']	['Information Society']
3239	Initiative (ECI) plans to distribute similar material in a variety of languages. Even  greater esources are expected from the Linguistic Data Consortium (LDC). And the 	[[154, 157], [12, 15]]	[[126, 152]]	['LDC', 'ECI']	['Linguistic Data Consortium']
3240	These features have been obtained using the knowledge contained into the Multilingual Central Repository (MCR) of the MEANING project3 (Atserias et al, 2004).	[[106, 109]]	[[73, 104]]	['MCR']	['Multilingual Central Repository']
3241	phrases (NPs) (representing 49.6% of the total number of phrases), verb phrases (VPs), prepositional phrases (PPs), adjectival phrases (ADJPs), and quantity phrases (QPs), representing 99.1% of	[[136, 141]]	[[116, 134]]	['ADJPs']	['adjectival phrases']
3242	cation), TMP (time), DIS (discourse connectives), PRP (purpose) or DIR (direction). Negations (NEG) and modals (MOD) are also marked.	[[95, 98], [9, 12], [21, 24], [50, 53], [67, 70], [112, 115]]	[[84, 93], [26, 35], [55, 62], [72, 81], [104, 110]]	['NEG', 'TMP', 'DIS', 'PRP', 'DIR', 'MOD']	['Negations', 'discourse', 'purpose', 'direction', 'modals']
3243	CIMA is an online information center maintained by the Spanish Agency for Medicines and Health Products (AEMPS). CIMA provides	[[105, 110], [0, 4], [113, 117]]	[[84, 103]]	['AEMPS', 'CIMA', 'CIMA']	['and Health Products']
3244	ture subset selection [2][4][8]. Yang and Pederson  found information gain (IG) and chi-square test (CHI)  most effective in aggressive term removal without los-	[[76, 78], [101, 104]]	[[58, 74], [84, 99]]	['IG', 'CHI']	['information gain', 'chi-square test']
3245	information nuggets?, called Summary Content Units (SCUs), which are short, atomic statements of facts con-	[[52, 56]]	[[29, 50]]	['SCUs']	['Summary Content Units']
3246	XC (compound),   NN (noun),   NNP (proper 	[[17, 19], [0, 2], [30, 33]]	[[21, 25], [4, 12]]	['NN', 'XC', 'NNP']	['noun', 'compound']
3247	"them as ""LIG-equivalent formalisms"". LIG is a vari-  ant of index grammar (IG) (Aho, 1968). Like CFG, IG "	[[75, 77], [37, 40], [9, 12], [97, 100], [102, 104]]	[[60, 73]]	['IG', 'LIG', 'LIG', 'CFG', 'IG']	['index grammar']
3248	semble learning. Ren et al (2011) explored the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a semi-supervised senti-	[[73, 75]]	[[54, 71]]	['LP']	['label propagation']
3249	Roget (RG) ablaze aglow alight argent auroral beaming blazing brilliant WordNet (WN) burnished sunny shiny lustrous undimmed sunshiny brilliant TransGraph (TG) nimble ringing fine aglow keen glad light picturesque Lin (LN) red yellow orange pink blue brilliant green white dark	[[156, 158], [219, 221], [81, 83], [7, 9]]	[[144, 154], [214, 217], [72, 79], [0, 5]]	['TG', 'LN', 'WN', 'RG']	['TransGraph', 'Lin', 'WordNet', 'Roget']
3250	to five possible values (rules have been presented with the sentence pairs from which they have been acquired): entailment=yes (YES), i.e. correctness of the rule; entailment=more-phenomena (+PHEN), i.e.	[[128, 131], [192, 196]]	[[123, 126], [180, 189]]	['YES', 'PHEN']	['yes', 'phenomena']
3251	form is  employed to denote funct ion  i t se l f .   b) Conceptual  Phrase St ructure  (CPS) is  a data  s t ruc ture  in which syntact i c  and semant ic  in fo rma-  	[[89, 92]]	[[57, 86]]	['CPS']	['Conceptual  Phrase St ructure']
3252	2 D IA        1 In order to test this hypothesis,            1   a double-stranded oligonucleotide probe that corresponds to bp +10 to +60 of the CCR3 gene was prepared, 3 0 NN            referred to as E1-FL (exon 1- full length, Figure 2A). 3 D A    1 1   This is the exact sequence 3 D N          that was deleted in the CCR3(-exon1).pGL3 plasmid 3 D N          that demonstrated decreased activity 3 D N      1   compared to the full length 1.6 kb construct [27].	[[203, 208], [146, 150], [174, 176], [324, 328]]	[[210, 229]]	['E1-FL', 'CCR3', 'NN', 'CCR3']	['exon 1- full length']
3253	VB (Verb) is a major source of confusion in automatic tagging.  It is collapsed with VB (Verb). 	[[85, 87], [0, 2]]	[[89, 93], [4, 8]]	['VB', 'VB']	['Verb', 'Verb']
3254	AB to letter l ji ? A where A is a regular letter alphabet and AB=A?{B} includes B as an abstract morpheme start symbol	[[63, 65]]	[[66, 70]]	['AB']	['A?{B']
3255	This paper describes a set of experiments on two sub-tasks of Quality Estimation of Machine Translation (MT) output.	[[105, 107]]	[[84, 103]]	['MT']	['Machine Translation']
3256	 1  0  20  40  60  80  100  120  140 information density (ID)Fisher information (FIR)query-by-committee (SVE)random FlySlip	[[58, 60], [81, 84], [105, 108], [116, 123]]	[[37, 56], [61, 67]]	['ID', 'FIR', 'SVE', 'FlySlip']	['information density', 'Fisher']
3257	schematic representations of situations involving  various participants, props, and other conceptual  roles, each of which is a frame element (FE). 	[[143, 145]]	[[128, 141]]	['FE']	['frame element']
3258	should not be allowed?.  Gay Marriage (GM) 5	[[39, 41]]	[[25, 37]]	['GM']	['Gay Marriage']
3259	and BLANC (Recasens and Hovy, 2011). We use the average scores (AVG) of these four metrics as the main comparison metric.10	[[64, 67], [4, 9]]	[[48, 55]]	['AVG', 'BLANC']	['average']
3260	(i) string kernels applied to sentences, or PTK applied to structural representations with and without embedded relational information (REL). This	[[136, 139], [44, 47]]	[[112, 122]]	['REL', 'PTK']	['relational']
3261	The scores are lowercased BLEU calculated on the held-out devtest set. NE = named entities. 	[[71, 73], [26, 30]]	[[76, 90]]	['NE', 'BLEU']	['named entities']
3262	 Because LCS is a simple procedure, a second baseline based on Greedy String Tiling (GST) (Wise, 1996) was added after the evaluation pe-	[[85, 88], [9, 12]]	[[63, 83]]	['GST', 'LCS']	['Greedy String Tiling']
3263	 2.2 Character-level text embeddings Simple Recurrent Networks (SRNs) were introduced by Elman (1990) as models of temporal, or	[[64, 68]]	[[37, 62]]	['SRNs']	['Simple Recurrent Networks']
3264	explanations of these metrics are described below.  Symptom name recognition rate (RRdet),  recognition error rate (RERdet) and recognition 	[[83, 88], [116, 122]]	[[65, 81], [92, 114]]	['RRdet', 'RERdet']	['recognition rate', 'recognition error rate']
3265	strategies for reducing ambiguity.  4.2.1 Longest ending filtering (LEF) The first approach to ambiguity reduction is based	[[68, 71]]	[[42, 66]]	['LEF']	['Longest ending filtering']
3266	TV programs they watch.  Collaborative filtering (CF) (Resnick et al, 1994; Breese et al, 1998) and content-based (or	[[50, 52], [0, 2]]	[[25, 48]]	['CF', 'TV']	['Collaborative filtering']
3267	(perhaps maller than traditional linguistic units) out of  component words: 1. noun group (NG), which consists  of a noun and its immediately preceding words (e.g., 	[[91, 93]]	[[79, 89]]	['NG']	['noun group']
3268	 The non-standard words in text are referred to as Out of Vocabulary (OOV) words. The nor-	[[70, 73]]	[[51, 68]]	['OOV']	['Out of Vocabulary']
3269	matic speech recognizers in Speech Normalized Orthographic Representation (SNOR) format, or from optical character recognition (OCR) output. For the	[[128, 131], [75, 79]]	[[97, 126], [28, 73]]	['OCR', 'SNOR']	['optical character recognition', 'Speech Normalized Orthographic Representation']
3270	web. We require parallel data to build a statistical machine translation (SMT) system that translates from German into Sim-	[[74, 77]]	[[41, 72]]	['SMT']	['statistical machine translation']
3271	creation (CR)  emotion (EM)  motion (MO)  perception (PC) 	[[37, 39], [10, 12], [24, 26], [54, 56]]	[[29, 35], [0, 8], [15, 22], [42, 52]]	['MO', 'CR', 'EM', 'PC']	['motion', 'creation', 'emotion', 'perception']
3272	structure come in two different types: ? Grammatical Functions (GFs) indicate the relationship between the predicate and depen-	[[64, 67]]	[[41, 62]]	['GFs']	['Grammatical Functions']
3273	Taalkommissie van die Suid-Afrikaanse Akademie vir Wetenskap en Kuns. Centre for Text Technology (CTexT), North-West University, Potchefstroom, South Africa. 	[[98, 103]]	[[70, 96]]	['CTexT']	['Centre for Text Technology']
3274	run in parallel. This kind of parallelism is a good fit for the Same Instruction Multiple Thread (SIMT) hardware paradigm implemented by modern GPUs.	[[98, 102], [144, 148]]	[[64, 96]]	['SIMT', 'GPUs']	['Same Instruction Multiple Thread']
3275	 Shirai, K., T. Tokunaga, and H. Tanaka: Automatic Extraction of Japanese Grammar f om  a Bracketed Corpus, in Natural Language Processing Pacific Rim Symposium(NLPRS'gs),  pp.	[[161, 169], [173, 175]]	[[111, 160]]	"[""NLPRS'gs"", 'pp']"	['Natural Language Processing Pacific Rim Symposium']
3276	 1 Introduction Multiword Expressions (MWEs) are commonly defined as ?	[[39, 43]]	[[16, 37]]	['MWEs']	['Multiword Expressions']
3277	8 Fully Supervised Model 53.7 70.8 61.1 53.0 70.3 60.4 52.0 69.6 59.6 53.1 70.5 60.6 Table 4: Results obtained using the MUC scoring program for the Broadcast News and Newswire data sets Broadcast News (BNEWS) Newswire (NWIRE) True Mentions System Mentions True Mentions System Mentions	[[203, 208], [220, 225], [121, 124]]	[[187, 201], [210, 218]]	['BNEWS', 'NWIRE', 'MUC']	['Broadcast News', 'Newswire']
3278	dominated by different Root Nodes. In this table, for each  possible Root Node category (RN), its corresponding Head  Node (HN), Dependent Nods/8 (DN) and Control Features (CF), 	[[89, 91], [124, 126], [147, 149], [173, 175]]	[[69, 78], [112, 122], [129, 145], [155, 171]]	['RN', 'HN', 'DN', 'CF']	['Root Node', 'Head  Node', 'Dependent Nods/8', 'Control Features']
3279	They used a statistical finite-state transducer (SFST) as a generative model and a support vector machine (SVM) and conditional random fields (CRF) as discrim-	[[107, 110], [49, 53], [143, 146]]	[[83, 105], [12, 47], [116, 141]]	['SVM', 'SFST', 'CRF']	['support vector machine', 'statistical finite-state transducer', 'conditional random fields']
3280	 Acknowledgements  This work was supported by the Social Sciences and Humanities Research Council (SSHRC) of Canada. We 	[[99, 104]]	[[50, 97]]	['SSHRC']	['Social Sciences and Humanities Research Council']
3281	1992. 100 million words of  English: the British National Corpus (BNC). 	[[66, 69]]	[[41, 64]]	['BNC']	['British National Corpus']
3282	1969)).  4 Ia the following \] will use the term Unification Grammar (UG) aa hyperonym for  GPSG, LFG, FUG, IIPSG etc.,	[[70, 72], [92, 96], [98, 101], [103, 106], [108, 113]]	[[49, 68]]	['UG', 'GPSG', 'LFG', 'FUG', 'IIPSG']	['Unification Grammar']
3283	lated by the original Penn Treebank grammar to total PCFG surprisal calculated by the Nguyen et al (2012) Generalized Categorial Grammar (GCG). 	[[138, 141], [53, 57]]	[[106, 136]]	['GCG', 'PCFG']	['Generalized Categorial Grammar']
3284	1. Introduction Semantic role labeling (SRL) is the task of identifying arguments for a predicate and assigning semantically meaningful labels to them.	[[40, 43]]	[[16, 38]]	['SRL']	['Semantic role labeling']
3285	special case describing whether/t /  is glottalized. A context other than preceding phoneme and following  phoneme is incorporated; the first split (nodes 0 and 10) in this tree is on syllable boundary (SYLL-BDRY),  indicating that when / t /  is glottalized it is generally in syllable-final position.	[[203, 212]]	[[184, 201]]	['SYLL-BDRY']	['syllable boundary']
3286	 8SB = subject, OA = accusative object, OA2 = second accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal ob-	[[72, 74], [1, 4], [16, 18], [40, 43], [85, 87], [107, 109], [134, 136], [150, 152]]	[[77, 83], [7, 14], [21, 38], [46, 70], [90, 105], [112, 132], [139, 148], [155, 165]]	['DA', '8SB', 'OA', 'OA2', 'OG', 'OP', 'PD', 'OC']	['dative', 'subject', 'accusative object', 'second accusative object', 'genitive object', 'prepositional object', 'predicate', 'clausal ob']
3287	notator. For brevity, we only considered PubMed as the source DB, and named entity recognition (NER)type annotations, which may be simply represented	[[96, 99]]	[[70, 94]]	['NER']	['named entity recognition']
3288	2 Textual Entailment for MT Evaluation 2.1 Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (	[[100, 102], [25, 27], [66, 68]]	[[80, 98]]	['TE', 'MT', 'MT']	['Textual entailment']
3289	? This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No.	[[96, 99], [62, 66]]	[[67, 94]]	['NSF', 'U.S.']	['National Science Foundation']
3290	4.1 Preprocessing  HTML Page Parsing  The Document Object Model (DOM) is an application programming interface used for parsing 	[[65, 68], [19, 23]]	[[42, 63]]	['DOM', 'HTML']	['Document Object Model']
3291	ments were annotated with word-level labels by professional translators using the core categories in MQM (Multidimensional Quality Metrics) 13	[[101, 104]]	[[106, 138]]	['MQM']	['Multidimensional Quality Metrics']
3292	distinct verbs, not occurrences. The seventh column shows the number of verbs that  were misclassified (MC)--the sum of false positives and false negatives. The eighth 	[[104, 106]]	[[89, 102]]	['MC']	['misclassified']
3293	ARG1, ARG2, MOD-BENEFICIARY, and MOD-TIME. To identify which slot has the most similarity  among its elements, we calculate the number of distinct elements (NDE) in each slot across the  propositions.	[[157, 160], [0, 4], [6, 10], [12, 27], [33, 41]]	[[128, 155]]	['NDE', 'ARG1', 'ARG2', 'MOD-BENEFICIARY', 'MOD-TIME']	['number of distinct elements']
3294	5, we discuss our approach to increase the coverage of the model by using synset ID?s from the English WordNet (EWN). Section 6 describes our	[[112, 115], [81, 85]]	[[95, 110]]	['EWN', 'ID?s']	['English WordNet']
3295	This paper discusses the automatic labeling of semantic relations in nominalized noun phrases (NPs) using a support vector machines learning algorithm.	[[95, 98]]	[[81, 93]]	['NPs']	['noun phrases']
3296	"H A V E  CSEXCH F O R  MOVEF IN 11  C H A N G E ,  CALL EL3MOP P O R  E R A S E  0 I 2   ANTEST CALLED FOR 14'IGLOT "" (AACC) S D =  15. RES= '3."	[[103, 106], [119, 123], [125, 128], [9, 15], [16, 21], [23, 28], [136, 139]]	[[89, 102], [107, 115]]	['FOR', 'AACC', 'S D', 'CSEXCH', 'F O R', 'MOVEF', 'RES']	"['ANTEST CALLED', ""14'IGLOT""]"
3297	presented at the 16th International Conference on Computational Linguistics, Copenhagen. Gupta, D., Saul, M., & Gilbertson, J. (2004). Evaluation of a deidentification (DE-ID) software engine to share pathology reports and clinical documents for research. Am J Clin Pathol, 121(2), 176-186.	[[169, 174]]	[[151, 167]]	['DE-ID']	['deidentification']
3298	come. 2008. Deciding strictly local (SL) languages.	[[37, 39]]	[[21, 35]]	['SL']	['strictly local']
3299	 More recently, i2b2 organizers also reported a  Maximum Entropy (ME) based approach for the  2009 challenge (Halgrim, Xia, Solti, Cadag, & 	[[66, 68], [16, 20]]	[[49, 64]]	['ME', 'i2b2']	['Maximum Entropy']
3300	Abstract In this work we present results from using Amazon?s Mechanical Turk (MTurk) to annotate translation lexicons between English	[[78, 83]]	[[61, 76]]	['MTurk']	['Mechanical Turk']
3301	inference procedures?Markov Chain Monte Carlo (MCMC) (Johnson et al 2012), Expectation Maximization (EM), and Variational Bayes (VB)10?as well as the discourse-level model described above.	[[129, 131], [47, 51], [101, 103]]	[[110, 127], [21, 45], [75, 99]]	['VB', 'MCMC', 'EM']	['Variational Bayes', 'Markov Chain Monte Carlo', 'Expectation Maximization']
3302	of binary relations that vary in length and since a  question representation (SRq) can be answered by a  sentence candidate (SRc) that includes more information  than the question specified, the Arity constraint i~ revised 	[[125, 128], [78, 81]]	[[105, 123], [53, 76]]	['SRc', 'SRq']	['sentence candidate', 'question representation']
3303	the percentage of questions with a correct answer at rank 1, Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP). The reported	[[117, 120], [83, 86]]	[[93, 115], [61, 81]]	['MAP', 'MRR']	['Mean Average Precision', 'Mean Reciprocal Rank']
3304	The major categories of components included in these capabilities include: Sentence Splitter, Phrase Chunker, Tokenizer, Part-of-Speech (POS) Tagger, Shallow Parser, Name Entity Recognizer (NER), Coreference Solution, etc.	[[190, 193], [137, 140]]	[[166, 188], [121, 135]]	['NER', 'POS']	['Name Entity Recognizer', 'Part-of-Speech']
3305	PCFG score (SP ) 49.5 50.0 TSG score (ST ) 49.5 49.7 Charniak score (SC) 50.0 50.0 l + S3 61.0 64.3	[[69, 71], [0, 4], [12, 14], [27, 30], [38, 40]]	[[62, 67]]	['SC', 'PCFG', 'SP', 'TSG', 'ST']	['score']
3306	In order to solve idiomatic expressions as well as  collocations and frozen compound nouns, we  have developed the compound unit(CU)  recognizer (Jung et.	[[129, 131]]	[[115, 127]]	['CU']	['compound uni']
3307	Results on the dev set using two metrics: instance classification accuracy (CA), and soundbite name  recognition accuracy (RA). The oracle RA is 79.1%.	[[123, 125], [76, 78], [139, 141]]	[[101, 121], [51, 74]]	['RA', 'CA', 'RA']	['recognition accuracy', 'classification accuracy']
3308	The representative vectors for each phoneme category consist of the mean vectors of the Gaussian Mixture Model (GMM). 	[[112, 115]]	[[88, 110]]	['GMM']	['Gaussian Mixture Model']
3309	tion of them is also given: WH-Subject (WHS), WH-Object (WHO), passive Subject (PSubj), control Subject (CSubj), and the anaphor of the relative clause pronoun (RclSubjA).	[[105, 110], [40, 43], [57, 60], [80, 85], [161, 169]]	[[88, 103], [28, 38], [46, 55], [63, 78], [121, 159]]	['CSubj', 'WHS', 'WHO', 'PSubj', 'RclSubjA']	['control Subject', 'WH-Subject', 'WH-Object', 'passive Subject', 'anaphor of the relative clause pronoun']
3310	representing natural language in a traversable graph, composed of propositions and their semantic interrelations ? A Propositional Knowledge Graph (PKG). The resulting structure provides a representation	[[148, 151]]	[[117, 146]]	['PKG']	['Propositional Knowledge Graph']
3311	 2.3 Markov Logic Network Markov Logic Networks (MLN) (Richardson and Domingos, 2006) are a framework for probabilis-	[[49, 52]]	[[26, 47]]	['MLN']	['Markov Logic Networks']
3312	of a sentence or phrase, in turn, is necessary for automatic acquisition of important lexical knowledge, such as subcategorization frames and argument structures, which is used in several natural language processing (NLP) tasks and applications, such as parsing, machine translation, and information extraction (Srinivas and Joshi 1999;	[[217, 220]]	[[188, 215]]	['NLP']	['natural language processing']
3313	used as the seed to build a reliable hedge cue set. Maximum Entropy (MaxEnt) model is used as the learning technique to	[[69, 75]]	[[52, 67]]	['MaxEnt']	['Maximum Entropy']
3314	Tokens (T) .72 / .77 .83 / .84 .72 / .76 .85 / .84 .82 / .84 .88 / .86 Named Entities (NE) .75 / .80 .84 / .79 .75 / .77 .85 / .78 .89 / .78 .89 / .73 NE targeted (NE-T) .54 / .55 .49 / .47 .66 / .64 .60 / .57 .64 / .64 .57 / .58 Host (H) .72 / .57 .64 / .48 .67 / .51 .58 / .41 .67 / .63 .59 / .55	[[164, 168]]	[[151, 162]]	['NE-T']	['NE targeted']
3315	Abstract This paper suggests two ways of improving semantic role labeling (SRL). First, we intro-	[[75, 78]]	[[51, 73]]	['SRL']	['semantic role labeling']
3316	studied three activation functions: 1. Rectified linear units (ReLUs) (Nair and Hinton, 2010):	[[63, 68]]	[[39, 61]]	['ReLUs']	['Rectified linear units']
3317	 One of the most competitive summarization methods is based on Integer Linear Programming (ILP). 	[[91, 94]]	[[63, 89]]	['ILP']	['Integer Linear Programming']
3318	2 Classifiers 2.1 NB Naive Bayes(NB) probabilistic classifiers are commonly studied in machine learning(Mitchell, 1996).	[[33, 35]]	[[21, 31]]	['NB']	['Naive Baye']
3319	 To adapt for Chinese phonetic rule, we divide the  continuous CLs into independent CLs(IC) and  divide structure of CL+VL+CL into CL+VL and 	[[88, 90], [63, 66], [117, 119], [120, 122], [123, 125], [131, 133], [134, 136]]	[[72, 86]]	['IC', 'CLs', 'CL', 'VL', 'CL', 'CL', 'VL']	['independent CL']
3320	against the human annotation.  2.3 Distributed Tree Kernel (DTK) Distributed Tree Kernel (DTK) (Zanzotto and	[[60, 63], [90, 93]]	[[35, 58], [65, 88]]	['DTK', 'DTK']	['Distributed Tree Kernel', 'Distributed Tree Kernel']
3321	and reranking for the statistical parsing of spanish. In Proceedings of Human Language Technology (HLT) and the Conference on Empirical Methods in Natural	[[99, 102]]	[[72, 97]]	['HLT']	['Human Language Technology']
3322	3.1 Data We use the TiGer treebank release 2.2 (TIGER), and the NeGra treebank (NEGRA). For TIGER,	[[80, 85], [48, 53], [92, 97]]	[[64, 78], [20, 34]]	['NEGRA', 'TIGER', 'TIGER']	['NeGra treebank', 'TiGer treebank']
3323	 As an example, consider a user who is looking for information on digital video recorders (DVR), in particular, on how she can use a DVR with a	[[91, 94], [133, 136]]	[[66, 89]]	['DVR', 'DVR']	['digital video recorders']
3324	Section 2 discusses related work. Section 3 introduces the Condition Random Fields(CRFs)  and the defined Long-Dependency CRFs 	[[83, 87], [122, 126]]	[[59, 82]]	['CRFs', 'CRFs']	['Condition Random Fields']
3325	 2 American National Corpus The American National Corpus (ANC) project (Ide and Macleod, 2001; Ide and Suderman, 2004) has	[[58, 61]]	[[32, 56]]	['ANC']	['American National Corpus']
3326	dhi/NEP road/NEL. The structure of the tagged  element using the Shakti Standard Format (SSF)5  will be as follows: 	[[89, 92], [0, 7], [8, 16]]	[[65, 87]]	['SSF', 'dhi/NEP', 'road/NEL']	['Shakti Standard Format']
3327	 We observe the following: First, pre-enrollment reviews have noun phrases(NP) that contain fewer leaf nodes than in the post-enrollment reviews.	[[75, 77]]	[[62, 73]]	['NP']	['noun phrase']
3328	sity is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000).	[[123, 128], [52, 56]]	[[95, 121]]	['LinGO', 'HPSG']	['Linguistic Grammars Online']
3329	as director of Pentagon telecom~~nicntions and commc~nd :~nd control systcms.  Requests for coments on a proposed Federal information processing -t.lndard (PII'S)  for the National Communications System have been requested by .Innuor).	[[156, 161]]	[[105, 144]]	"[""PII'S""]"	['proposed Federal information processing']
3330	CN Bank(CNB): 200,000 samples  ? TFN Bank(TFNB): 38,769 samples  CPN Bank(CPNB): 17,637 samples 	[[42, 46], [8, 11], [74, 78]]	[[33, 40], [0, 7], [65, 73]]	['TFNB', 'CNB', 'CPNB']	['TFN Ban', 'CN Bank', 'CPN Bank']
3331	Sagae and Tsujii (2008) present a pruning algorithm in their paper on transition-based parsing of directed acyclic graphs (DAGs), which discards the edges of longest span entering nodes.	[[123, 127]]	[[98, 121]]	['DAGs']	['directed acyclic graphs']
3332	 ? Target types per Source type (TpS), i.e. the number of target types a specific source type	[[33, 36]]	[[10, 26]]	['TpS']	['types per Source']
3333	init ial ly be described. I wil l  c laim that  such an initial descr ipt ion (ID) is  cr it ical  to both model synthesis and 	[[79, 81]]	[[56, 69]]	['ID']	['initial descr']
3334	 The implemented system has three main modules: the Feature Extractor (FE), the Generalized Iterative Scaling (GIS), and the Classifica-	[[71, 73], [111, 114]]	[[52, 69], [80, 109]]	['FE', 'GIS']	['Feature Extractor', 'Generalized Iterative Scaling']
3335	 Figure 2 shows the learning curve for pseudoprojective parsing (P-Proj), compared to using only projectivized training data (Proj), measured as error	[[65, 71], [126, 130]]	[[45, 63]]	['P-Proj', 'Proj']	['projective parsing']
3336	1(c)) for each k. 3.3.2 PLEB PLEB (Point Location in Equal Balls) was first proposed by Indyk and Motwani (1998) and further	[[29, 33], [24, 28]]	[[35, 64]]	['PLEB', 'PLEB']	['Point Location in Equal Balls']
3337	CoTrain vs. LEX(CN) 9.53E-10 7.15E-11 1.17E-09 CoTrain vs. LEX(EN) 1.87E-05 1.64E-05 8.92E-07 CoTrain vs. SVM(CN) 5.7E-08 2.91E-09 2.27E-11 CoTrain vs. SVM(EN) 3.74E-15 5.77E-17 1.18E-20	[[110, 112], [12, 15], [16, 18], [59, 62], [63, 65], [152, 155], [156, 158]]	[[94, 101]]	['CN', 'LEX', 'CN', 'LEX', 'EN', 'SVM', 'EN']	['CoTrain']
3338	 66 adopt a minimum Bayes risk (MBR) approach, with the goal of finding the graph with the lowest	[[32, 35]]	[[12, 30]]	['MBR']	['minimum Bayes risk']
3339	Abstract Data sparsity is one of the main factors that make word sense disambiguation (WSD) difficult.	[[87, 90]]	[[60, 85]]	['WSD']	['word sense disambiguation']
3340	aKemp|es  de  t. raL~uct lon  du  russe  eD f ranca ls  rea l  i s le  par   le  G~TA a Grenob le ,   MUTS-CLES:  TAO ( t raduct ion  ass i s t ,  co  par  o rd lnateur )e   ana lyse  morpno l  og ique~ t rans fer t  lex lca l~ 0enerat  1o~ 	[[114, 117], [81, 85], [102, 111]]	[[120, 168]]	['TAO', 'G~TA', 'MUTS-CLES']	['t raduct ion  ass i s t ,  co  par  o rd lnateur']
3341	Adjoining Grammars as the compilation of a  more abstract and modular layer of linguistic  description : the  metagrammar (MG). MG 	[[123, 125], [128, 130]]	[[110, 121]]	['MG', 'MG']	['metagrammar']
3342	This paper presents a web application and a web service for the diagnostic evaluation of Machine Translation (MT). These web-based	[[110, 112]]	[[89, 108]]	['MT']	['Machine Translation']
3343	require. We solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a PAS.	[[78, 82]]	[[48, 76]]	['SSTK']	['Shallow Semantic Tree Kernel']
3344	In (Frasconi et al., 2004) we introduced declarative kernels (DK) as a set of kernels working on mereotopological	[[62, 64]]	[[41, 60]]	['DK']	['declarative kernels']
3345	ing default parameters. Error analysis was done by means of Mean Squared Error estimate (MSE). 	[[89, 92]]	[[60, 78]]	['MSE']	['Mean Squared Error']
3346	 5 Conclusions and Future Work Distributed Smoothed Trees (DST) are a novel class of Compositional Distributional Semantics Models (CDSM) that effectively encode structural information and distributional semantics in tractable 2-	[[59, 62], [132, 136]]	[[31, 57], [85, 129]]	['DST', 'CDSM']	['Distributed Smoothed Trees', 'Compositional Distributional Semantics Model']
3347	 MUC marks locations (LOC), organisations (ORG) and personal names (PER), in addition to numerical and time information.	[[68, 71], [1, 4], [22, 25], [43, 46]]	[[52, 60], [11, 20], [28, 41]]	['PER', 'MUC', 'LOC', 'ORG']	['personal', 'locations', 'organisations']
3348	Keyword 0.168 0.168 0.157 6.3 Table 3: Title quality as compared to the reference for the hierarchical discriminative (HD), flat discriminative (FD), hierarchical generative (HG), flat	[[119, 121], [145, 147], [175, 177]]	[[90, 117], [124, 143], [150, 173]]	['HD', 'FD', 'HG']	['hierarchical discriminative', 'flat discriminative', 'hierarchical generative']
3349	 5.4 Nonshared Concept Activation with  No Identif ication Intention (NSNI). 	[[70, 74]]	[[40, 68]]	['NSNI']	['No Identif ication Intention']
3350	The first step is the identification of the presence of a graphical image in the web page by a  Browser Helper Object (BHO) (Elzer et al., 2007).	[[119, 122]]	[[96, 117]]	['BHO']	['Browser Helper Object']
3351	to efficiently implement their computation.  In Natural Language Processing (NLP), the typical dimensionality of databases, which are made	[[77, 80]]	[[48, 75]]	['NLP']	['Natural Language Processing']
3352	? iyi = 0 This is a quadratic programming (QP) problem and we can always find the global maximum of	[[43, 45]]	[[20, 41]]	['QP']	['quadratic programming']
3353	result. We have used the symbol Comp in that case (e.g., if  ANTI (A)=B and CONV(B)=C, then the relation result-  ing from the composition is simply ANTI(CONV(A))----C).	[[61, 65], [149, 153], [76, 80]]	[]	['ANTI', 'ANTI', 'CONV']	[]
3354	Sofa. The viewer makes use the open-source library Java Speech Toolkit (JSTK)5. 	[[72, 76]]	[[51, 70]]	['JSTK']	['Java Speech Toolkit']
3355	scoring the candidate set of which class of anaphoric expression (DNOM = definite  NP, PER{I,2,3} = first/second/third person pronouns, POS{1,2,3} = first/second/third  person possessives, RELA = relative pronouns, REFL = reflexive/reciprocal pronouns). 	[[189, 193], [215, 219], [66, 70], [83, 85], [87, 90], [136, 139]]	[[196, 204], [222, 231], [73, 81]]	['RELA', 'REFL', 'DNOM', 'NP', 'PER', 'POS']	['relative', 'reflexive', 'definite']
3356	provided for the slots over the course of the dialog.  These are our String Consistency (SC) features. 	[[89, 91]]	[[69, 87]]	['SC']	['String Consistency']
3357	even for very high-dimensional data.  4.2 Open Directory Project (ODP) Open Directory Project (ODP)7 is a multilingual	[[66, 69], [95, 98]]	[[42, 64], [71, 93]]	['ODP', 'ODP']	['Open Directory Project', 'Open Directory Project']
3358	2007. CRFsuite: a fast implementation of conditional random fields (CRFs). 	[[68, 72], [6, 14]]	[[41, 66]]	['CRFs', 'CRFsuite']	['conditional random fields']
3359	4.1 English training sets To train an SVO metaphor classifier, we employ the TroFi (Trope Finder) dataset. 	[[77, 82], [38, 41]]	[[84, 96]]	['TroFi', 'SVO']	['Trope Finder']
3360	Step 1  Tagging using Global Distribution (NEIG) Trained Model Statistical System (MEMM) Step 2 MEMM Based Statistical System(S-MEMM)Final Tagged DataSet Added    as a feature	[[126, 132], [43, 47], [83, 87], [146, 153]]	[[89, 100]]	['S-MEMM', 'NEIG', 'MEMM', 'DataSet']	['Step 2 MEMM']
3361	eal papers. Ill PTvcccdings of thc Pac'lific Sym-  posium on Biocomp'uting'98 (PSB'98), .Jan-  1uAYy.	[[79, 85], [16, 27]]	[[51, 77]]	"[""PSB'98"", 'PTvcccdings']"	"[""posium on Biocomp'uting'98""]"
3362	Preliminary experimental data indicates that the approach is likely to be successful. The major benefit of the approach is that it makes extending pronunciation lexi-cons accessible to average users. 1 Introduction Speech recognition (SR) systems use pronuncia-tion lexicons to map words into the phoneme-like units used for acoustic modeling. Text-to-speech (TTS) systems also make use of pronunciation lexicons, both internally and as ?	[[235, 237], [360, 363]]	[[215, 233], [344, 358]]	['SR', 'TTS']	['Speech recognition', 'Text-to-speech']
3363	? All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).	[[60, 64], [92, 95]]	[[50, 58], [82, 90]]	['PART', 'Pos']	['particle', 'positive']
3364	Head and Obj3 and the counts f(gf, fe) of occurrences of the grammatical functions together with the roles DEGREE (DEG), THEME (THM), DEPICTIVE (DEP) and LOCATION (LOC).	[[128, 131], [9, 13], [115, 118], [145, 148], [164, 167]]	[[121, 126], [107, 113], [134, 143], [154, 162]]	['THM', 'Obj3', 'DEG', 'DEP', 'LOC']	['THEME', 'DEGREE', 'DEPICTIVE', 'LOCATION']
3365	1.1 A System Exhibiting Reinforcement Learning The central motivation for building this dialogue system is as a platform for Reinforcement Learning (RL) experiments.	[[149, 151]]	[[125, 147]]	['RL']	['Reinforcement Learning']
3366	 In Proceedings of the 17th International Conference on Computational Linguistics (COLING), Montreal, Canada.	[[83, 89]]	[[56, 81]]	['COLING']	['Computational Linguistics']
3367	4.4.2 Optimization To maximize the objective in (6), we employ a stochastic gradient descent (SGD) algorithm (Rendle et al, 2009).	[[94, 97]]	[[65, 92]]	['SGD']	['stochastic gradient descent']
3368	226  Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304?313, Seoul, South Korea, 5-6 July 2012.	[[101, 108]]	[[51, 99]]	['SIGDIAL']	['Special Interest Group on Discourse and Dialogue']
3369	 ? True Positive (TP), the predicted e was correctly referred to by s.   	[[18, 20]]	[[3, 16]]	['TP']	['True Positive']
3370	Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti,  Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on  Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.  	[[210, 214], [147, 151]]	[[166, 208]]	['CIDM', 'IEEE']	['Computational Intelligence and Data Mining']
3371	Probable PR+ (probable) PR? ( not probable) [NA] Possible PS+ (possible) PS? ( not certain) [NA]	[[58, 61]]	[[63, 71]]	['PS+']	['possible']
3372	numeroinen se on (UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance).	[[63, 65], [18, 20], [107, 109]]	[[68, 77], [23, 32], [112, 128]]	['UR', 'UT', 'SU']	['Utterance', 'Utterance', 'System utterance']
3373	review  ? Product feature level (PFL)  ?	[[33, 36]]	[[10, 31]]	['PFL']	['Product feature level']
3374	mathematical models from experimental data. The  algorithm is similar to Genetic Programming (GP),  but uses fixed-length character strings (called chro-	[[94, 96]]	[[73, 92]]	['GP']	['Genetic Programming']
3375	similar to how Iida et al (2011) computed features to present to their classifier: namely Ling (linguistic features), TaskSp (task specific features), and Gaze (from SV only).	[[118, 124], [166, 168]]	[[126, 139]]	['TaskSp', 'SV']	['task specific']
3376	compared is a familiar problem from the fields of information retrieval (IR), text mining (TM), textual data analysis (TDA) and natural language processing (NLP) (Lebart and Rajman, 2000).	[[119, 122], [73, 75], [91, 93], [157, 160]]	[[96, 117], [50, 71], [78, 89], [128, 155]]	['TDA', 'IR', 'TM', 'NLP']	['textual data analysis', 'information retrieval', 'text mining', 'natural language processing']
3377	 Examples of lexical and contextual rules learned by  the Brill tagger.  NNP = proper noun, CD = cardinal number,  CC = coordinating conjunction, JJ = adjective, VBG = verb, 	[[92, 94], [73, 76], [115, 117], [146, 148], [162, 165]]	[[97, 105], [79, 90], [120, 144], [151, 160], [168, 172]]	['CD', 'NNP', 'CC', 'JJ', 'VBG']	['cardinal', 'proper noun', 'coordinating conjunction', 'adjective', 'verb']
3378	tice. They found that a domain-specific corpus performs better than a Wall Street Journal (WSJ) corpus for the trigram LM.	[[91, 94]]	[[70, 89]]	['WSJ']	['Wall Street Journal']
3379	the parameters. We trained our network with stochastic gradient descent (SGD), mini-batches and adagrad updates (Duchi et al, 2011), using	[[73, 76]]	[[44, 71]]	['SGD']	['stochastic gradient descent']
3380	 4.4 SLU Features The SLU (Spoken Language Understanding) features are used to resolve implicit and explicit REs.	[[22, 25], [5, 8], [109, 112]]	[[27, 56]]	['SLU', 'SLU', 'REs']	['Spoken Language Understanding']
3381	ment. In Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci), Sapporo. 	[[82, 88]]	[[55, 80]]	['CogSci']	['Cognitive Science Society']
3382	carded in LSI-based approaches. We dub our model ONETA (OrthoNormal Explicit Topic Analysis) and empirically show that on a cross-lingual retrieval	[[49, 54], [10, 13]]	[[56, 91]]	['ONETA', 'LSI']	['OrthoNormal Explicit Topic Analysis']
3383	 ? Lexical Overlap and Length (LO): This set of features represents the lexical overlap between	[[31, 33]]	[[3, 18]]	['LO']	['Lexical Overlap']
3384	 1997) has led us to employ, among other param-  eters, mutual information (MI) bits of individ-  ual characters derived from large hierarchically 	[[76, 78]]	[[56, 74]]	['MI']	['mutual information']
3385	 ? Shift(SH): Push NEXT onto the stack. 	[[9, 11], [19, 23]]	[[3, 8]]	['SH', 'NEXT']	['Shift']
3386	categories of both test texts  PoS tags  DET (Determiner)  NM (Pronoun) 	[[41, 44], [59, 61]]	[[46, 56]]	['DET', 'NM']	['Determiner']
3387	Pro = percent of the words as pronominals. WPS = Words per sentence. 6LTR = percent of words that are longer than 6 letters.	[[43, 46], [70, 73]]	[[49, 67]]	['WPS', 'LTR']	['Words per sentence']
3388	BLEU-4 (Papineni et al, 2002) used in the two  experiments, we design another evaluation metrics Reordering Accuracy (RAcc) for forced decoding evaluation.	[[118, 122], [0, 6]]	[[97, 116]]	['RAcc', 'BLEU-4']	['Reordering Accuracy']
3389	71 tweets, called T-NER, is presented which employs Conditional Random Fields (CRF) for named entity segmentation and labelled topic modelling for	[[79, 82], [18, 23]]	[[52, 77]]	['CRF', 'T-NER']	['Conditional Random Fields']
3390	cal search. We examine this by using three different parsers in phase 0: (i) MS (Marecek and Straka, 657	[[77, 79]]	[[81, 99]]	['MS']	['Marecek and Straka']
3391	that are of interest o specific users. An example of IE is the  Named Ent i ty  (NE) task, which has become established  as the important first step in many other IE tasks, provid- 	[[81, 83], [53, 55], [163, 165]]	[[64, 78]]	['NE', 'IE', 'IE']	['Named Ent i ty']
3392	State (STT) 42.85 76.93 Gender (GEN) 67.42 84.17 Determiner (DET) 59.71 85.41 Number (NUM) 70.61 87.31	[[61, 64]]	[[49, 59]]	['DET']	['Determiner']
3393	To model the relations between objects and verbs, we follow the data preparation in (Le et al 2013), using the British National Corpus (BNC) which has been preprocessed and parsed using TreeTagger and	[[136, 139]]	[[111, 134]]	['BNC']	['British National Corpus']
3394	larities of MSA. ARET has two subparts tools : the  Arabic Reading Facilitation Tool (ARFT) and the  Arabic Reading Assessment Tool (ARAT).	[[86, 90], [17, 21], [12, 15], [133, 137]]	[[52, 84], [101, 131]]	['ARFT', 'ARET', 'MSA', 'ARAT']	['Arabic Reading Facilitation Tool', 'Arabic Reading Assessment Tool']
3395	"The rules, which may be different for each  dictionary, are written using a formalism in the spirit of  the ""definite clause grammars"" (DCG's) of Pereira and  Warren (1980) and ""modular logic grammars"" (MLG's) "	[[136, 141], [203, 208]]	[[109, 133], [178, 200]]	"[""DCG's"", ""MLG's""]"	['definite clause grammars', 'modular logic grammars']
3396	al., 2005) to quickly find possible answers, given the relational conjunction (RC) of the question. 	[[79, 81]]	[[55, 77]]	['RC']	['relational conjunction']
3397	11 Figure 4: Variation of Average Information Processing Indices(IPI) for Video 4-6 Figure 5: Variation of Average Information Processing Indices(IPI) for the full course the last week, i.e, students who do not finish the	[[146, 149], [65, 68]]	[[115, 144], [34, 64]]	['IPI', 'IPI']	['Information Processing Indice', 'Information Processing Indices']
3398	also included as features.  Concept Unique Identifiers (CUIs): We follow the approach presented by McInnes et al (2007) to	[[56, 60]]	[[28, 54]]	['CUIs']	['Concept Unique Identifiers']
3399	****I TRANSFORPlATICNS **SIW:  S C A N  C-ALLED AT 1 I  ANTFST CALLED FOR SrqSYLLA B (AACC) ,SD= 6 .  RES= 1 1 .	[[70, 73], [93, 95], [86, 90]]	[[56, 69]]	['FOR', 'SD', 'AACC']	['ANTFST CALLED']
3400	Thus, it is necessary to define a proper ECS for each  language: Japanese ECS (J-ECS) \[6\] for Japanese language and  English ECS (E-ECS) \[7\] for English language. In the translation 	[[132, 137], [41, 44], [79, 84]]	[[119, 130], [65, 77]]	['E-ECS', 'ECS', 'J-ECS']	['English ECS', 'Japanese ECS']
3401	line debates. In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM)@ EACL, pages 35?43.	[[88, 92], [95, 99]]	[[52, 86]]	['LASM', 'EACL']	['Language Analysis for Social Media']
3402	The prefer-  ence score (PS) for a pair is determined by the ratio  of its local dominance count (LDC)--the total num-  ber of cases in which the pair is locally dominant--to 	[[98, 101], [25, 27]]	[[75, 96], [4, 23]]	['LDC', 'PS']	['local dominance count', 'prefer-  ence score']
3403	the parameters ? = (s,W,b,x) via backpropagation with stochastic gradient descent (SGD). 	[[83, 86]]	[[54, 81]]	['SGD']	['stochastic gradient descent']
3404	mapping all non-core argument labels in the guessed and correct labelings to NONE.  Coarse Modifier Argument Measures (COARSEARGM). Sometimes it is sufficient to	[[119, 129], [77, 81]]	[[84, 117]]	['COARSEARGM', 'NONE']	['Coarse Modifier Argument Measures']
3405	137 pare our system with a non-sequential classifier,  a support vector machine (SVM), with the same  settings as those described above.	[[81, 84]]	[[57, 79]]	['SVM']	['support vector machine']
3406	grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) Figure 3: Plots of imageability scores for literal vs. nonliteral/metaphorical words in the VUAMC, grouped by parts of speech (L=literal, NL=nonliteral, N=noun, V=verb, P=preposition) clearest distinction between literal vs. nonliteral items.	[[223, 225], [39, 41], [177, 182]]	[[226, 236], [30, 37], [42, 52], [56, 60], [64, 68], [72, 83], [214, 221], [240, 244], [248, 252], [256, 267]]	['NL', 'NL', 'VUAMC']	['nonliteral', 'literal', 'nonliteral', 'noun', 'verb', 'preposition', 'literal', 'noun', 'verb', 'preposition']
3407	email: elenimi@linc.cis.upenn.edu Jerry R. Hobbs University of Southern California (USA) email: hobbs@isi.edu	[[84, 87]]	[[49, 82]]	['USA']	['University of Southern California']
3408	Re-moving PHI from these free text portions requires application of techniques from natural language processing that are capable of identifying phrases of specific types based on the lexical content (the words that make up the phrases) and the surround-ing words.  3 Current Methods and Metrics Fortunately, the problem of identifying types of information in free text is a well-studied problem in the natural language processing community. We can leverage several decades of research on infor-mation extraction and the named entity identifica-tion problem in particular, including multiple community evaluations such as the Message Un-derstanding Conferences (MUC) (Grishman & Sundheim, 1996) and the subsequent Automated Content Extraction (ACE) evaluations1 ? both fo-cused on extraction from newswire -- as well as evaluations of biomedical entity extraction from the published literature e.g., in the BioCreative evaluations (Krallinger, et al, 2008).	[[743, 746]]	[[713, 741]]	['ACE']	['Automated Content Extraction']
3409	Electronic Text Encoding and Interchange were first published in April 1994 and were initially based on Standard Generalized Markup Language (SGML).  The	[[142, 146]]	[[104, 140]]	['SGML']	['Standard Generalized Markup Language']
3410	Semantic Computing Group Cognitive Interaction Technology ? Center of Excellence (CITEC), Bielefeld University, Germany	[[82, 87]]	[[60, 80]]	['CITEC']	['Center of Excellence']
3411	C STORE SPEECH WAVE POINT  C  NBUF (NPT) =IFIX(YN)  750 CONTINUE 	[[36, 39], [47, 49]]	[[30, 34]]	['NPT', 'YN']	['NBUF']
3412	exist solely to guide processing. These words, known  as function words (FW's), are quite common, and  include articles, prepositions, and auxiliary verbs.	[[73, 77]]	[[57, 71]]	"[""FW's""]"	['function words']
3413	Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein,  1998), latent semantic analysis (LSA) (Gong and  Liu, 2001).	[[115, 118], [50, 53]]	[[89, 113], [22, 48]]	['LSA', 'MMR']	['latent semantic analysis', 'maximal marginal relevance']
3414	 - 19  -  Both L I and L 2 are CSL's (Context sensitive languages). They 	[[31, 36]]	[[38, 65]]	"[""CSL's""]"	['Context sensitive languages']
3415	In order to make it man-  ageable and intuitive, it employs yntactic constructs  called Typed Feature Structures (TFSs). The ,~vocab- 	[[114, 118]]	[[88, 112]]	['TFSs']	['Typed Feature Structures']
3416	 4.1 Representing Transformations as FSTs Finite State Transducers (FSTs) provide a natural formalism for representing output transformations.	[[68, 72], [37, 41]]	[[42, 66]]	['FSTs', 'FSTs']	['Finite State Transducers']
3417	following formulas.  PPe = Pe(Se)? 	[[21, 24]]	[[27, 32]]	['PPe']	['Pe(Se']
3418	"For each disjunetiort in indef.""  Let compatible-disjuncts = CHECK-DISJ (disjunction, cond). "	[[61, 71]]	[[73, 84]]	['CHECK-DISJ']	['disjunction']
3419	al., 2005) filters and summarizes the OmniPage output into Intermediate XML (IXML), as well as correcting certain characteristic errors from that stage.	[[77, 81]]	[[59, 75]]	['IXML']	['Intermediate XML']
3420	and only these. The possible binary tree structures of ABCD are covered by  ABCD = A(BCD) U (AB)(CD) U (ABC)D.  Since we are to handle binary concatenations only, we consider two concatenations 	[[76, 80], [55, 59]]	[[83, 88]]	['ABCD', 'ABCD']	['A(BCD']
3421	selection (FS). For word-level prediction, generalised linear models (GLM) (Collins, 2002) and GLM with dynamic learning	[[70, 73], [11, 13], [95, 98]]	[[43, 68]]	['GLM', 'FS', 'GLM']	['generalised linear models']
3422	knowledge powered model to several baselines.  Random Guess Model (RG). Random guess is	[[67, 69]]	[[47, 59]]	['RG']	['Random Guess']
3423	the natural (CC natural) and strong (CC strong)  levels; and (b) advanced level texts from a popular  science magazine called Ci?ncia Hoje (CH). Table 	[[140, 142], [13, 15], [37, 39]]	[[126, 138]]	['CH', 'CC', 'CC']	['Ci?ncia Hoje']
3424	puts to a system and the outputs it is intended to produce. In Machine Translation (MT), such resources take the form of sentence-aligned parallel corpora of	[[84, 86]]	[[63, 82]]	['MT']	['Machine Translation']
3425	tleneck has been addressed through gathering annotations using many untrained workers on platforms such as Amazon Mechanical Turk (MTurk), a task commonly referred to as crowdsourcing.	[[131, 136]]	[[114, 129]]	['MTurk']	['Mechanical Turk']
3426	Valerio Basile and Johan Bos and Kilian Evang and Noortje Venhuizen {v.basile,johan.bos,k.evang,n.j.venhuizen}@rug.nl Center for Language and Cognition Groningen (CLCG) University of Groningen, The Netherlands	[[163, 167]]	[[118, 161]]	['CLCG']	['Center for Language and Cognition Groningen']
3427	BP = log (Prob(N2 | N1)) (2) ? Pointwise Mutual Information (PMI): Defined as a measure of how collocated two words are,	[[61, 64], [0, 2]]	[[31, 59], [5, 28]]	['PMI', 'BP']	['Pointwise Mutual Information', 'log (Prob(N2 | N1)) (2)']
3428	 1 Introduction The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), based on the Rhetorical Struc-	[[44, 50]]	[[20, 42]]	['RST-DT']	['RST Discourse Treebank']
3429	This paper is an at tempt  to provide part  of the basis for a general theory of robust  process ing in Machine Trans lat ion  (MT) wi th  relevance to other areas of Natural  Language 	[[128, 130]]	[[104, 125]]	['MT']	['Machine Trans lat ion']
3430	Diacritization evaluation of our experiments is  reported in terms of word error rate (WER), and  diacritization error rate (DER)5. 	[[125, 128], [87, 90]]	[[98, 123], [70, 85]]	['DER', 'WER']	['diacritization error rate', 'word error rate']
3431	Other functions such as textual element (ET), sentence adjunct (AO), negation (NEG), vocative (VOC) and verb modifiers (MOD) were tagged, but did not receive	[[79, 82], [95, 98], [41, 43], [64, 66], [120, 123]]	[[69, 77], [85, 93], [32, 39], [109, 117], [46, 62]]	['NEG', 'VOC', 'ET', 'AO', 'MOD']	['negation', 'vocative', 'element', 'modifier', 'sentence adjunct']
3432	Parse tree of tagged sentence in Box 1  3 Geographic Information Retrieval  3.1 Propositional logic of context (PLC)  As previously discussed, candidate named entities 	[[112, 115]]	[[80, 110]]	['PLC']	['Propositional logic of context']
3433	tiveness of rule-based (Zilka et al., 2013), maximum entropy (MaxEnt) (Lee and Eskenazi, 2013) and deep neural network (DNN) (Henderson et al.,	[[62, 68]]	[[49, 60]]	['MaxEnt']	['mum entropy']
3434	 This is reflected in the query focused tasks run in the Document Understanding Conference (DUC) and Text Analysis Conference (TAC) over the past	[[92, 95], [127, 130]]	[[57, 90], [101, 125]]	['DUC', 'TAC']	['Document Understanding Conference', 'Text Analysis Conference']
3435	2 Background  2.1 Gene Expression Programming  Gene Expression Programming (GEP), first introduced by (Ferreira 2001), is an evolutionary algo-	[[76, 79]]	[[47, 74]]	['GEP']	['Gene Expression Programming']
3436	of Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC). The data	[[132, 136], [71, 74], [87, 90], [107, 110]]	[[117, 130], [63, 69], [77, 85], [93, 105]]	['MISC', 'PER', 'LOC', 'ORG']	['miscellaneous', 'person', 'location', 'organization']
3437	types through various features based on e.g., partof-speech tagging (POS) and named entity recognition (NER). 	[[104, 107], [69, 72]]	[[78, 102], [46, 59]]	['NER', 'POS']	['named entity recognition', 'partof-speech']
3438	cies and percentages of which are given in Table 1 (where the letters in example (3)  correspond to the letters in the table). Example (3a) uses a to infinitive form (TNF). 	[[167, 170]]	[[147, 165]]	['TNF']	['to infinitive form']
3439	five tasks) representing fine-grained bio-IE.  2.1 Genia task (GE) The GE task (Kim et al, 2011) preserves the task	[[63, 65], [71, 73], [38, 45]]	[[51, 56]]	['GE', 'GE', 'bio-IE.']	['Genia']
3440	evaluation metrics. Legend: d = dependency f-score, _pr =  predicate-only f-score, M = METEOR, WN = WordNet,  H_FL = human fluency score, H_AC = human accuracy 	[[95, 97], [53, 55], [110, 114], [138, 142]]	[[100, 107], [32, 42], [59, 73], [87, 93], [117, 136], [145, 159]]	['WN', 'pr', 'H_FL', 'H_AC']	['WordNet', 'dependency', 'predicate-only', 'METEOR', 'human fluency score', 'human accuracy']
3441	neighboring phrases serially or inversely. They built a maximum entropy (MaxEnt) classifier based on boundary words to predict the order of neighboring	[[73, 79]]	[[56, 71]]	['MaxEnt']	['maximum entropy']
3442	guese to Swedish via different pivot language.  RW=Random Walk. * indicates the results are signifi-	[[48, 50]]	[[51, 62]]	['RW']	['Random Walk']
3443	Data. We evaluate our model on predicting paraphrases from the Lexical Substitution (LexSub) dataset (McCarthy and Navigli, 2009).	[[85, 91]]	[[63, 83]]	['LexSub']	['Lexical Substitution']
3444	Abbreviations: CoreNLP=Stanford CoreNLP, Porter=Porter stemmer, BLem=BioLemmatizer, Snowball=Snowball stemmer, McCCJ=McClosky-Charniak-Johnson parser, Charniak=Charniak parser, SD=Stanford Dependency conversion annotation process introduced by Pyysalo et al	[[177, 179], [15, 22], [41, 47], [64, 68], [84, 92], [111, 116], [151, 159]]	[[180, 199], [32, 39], [48, 54], [69, 82], [93, 101], [117, 142], [160, 168]]	['SD', 'CoreNLP', 'Porter', 'BLem', 'Snowball', 'McCCJ', 'Charniak']	['Stanford Dependency', 'CoreNLP', 'Porter', 'BioLemmatizer', 'Snowball', 'McClosky-Charniak-Johnson', 'Charniak']
3445	nextpos part of speech of next word in the sentence determiner if the word has a determiner prepgoverning if the word is governed by a prepositional phrase (PP), we extract the preposition insidequotes if the word is inside quotes	[[157, 159]]	[[135, 155]]	['PP']	['prepositional phrase']
3446	in a Swedish Clinical Corpus Hercules Dalianis, Maria Skeppstedt Department of Computer and Systems Sciences (DSV) Stockholm University	[[110, 113]]	[[65, 108]]	['DSV']	['Department of Computer and Systems Sciences']
3447	  In the late 1970s a research team at USCInformation Sciences Institute (ISI) studied natural  dialogues with particular interest in applying the 	[[74, 77], [39, 42]]	[[42, 72]]	['ISI', 'USC']	['Information Sciences Institute']
3448	 2 Hidden Markov Models Hidden Markov models (HMMs) are commonly used to represent a wide range of linguistic phe-	[[46, 50]]	[[24, 44]]	['HMMs']	['Hidden Markov models']
3449	domains: ? Artificial Intelligence (AI) domain: 4,119 papers extracted from the IJCAI proceedings	[[36, 38], [80, 85]]	[[11, 34]]	['AI', 'IJCAI']	['Artificial Intelligence']
3450	Our corpus for this task is collected from Facebook and contains instances of Bengali(BN)English(EN)-Hindi(HI) code mixing. 	[[97, 99], [107, 109], [86, 88]]	[[89, 95], [101, 106], [78, 85]]	['EN', 'HI', 'BN']	['Englis', 'Hindi', 'Bengali']
3451	1 Introduction A fairly novel area of retrieval called topic detection and tracking (TDT) attempts to design methods to automatically (1) spot new, previously unreported events, and (2)	[[85, 88]]	[[55, 83]]	['TDT']	['topic detection and tracking']
3452	Program. It is intended that this interface be coupled to the battle management system being  developed under DARPA's Fleet Command Center Battle Management Program(FCCBMP). In the 	[[165, 171], [110, 115]]	[[118, 163]]	['FCCBMP', 'DARPA']	['Fleet Command Center Battle Management Progra']
3453	a first-order statistical language model can reduce perplex-  ity by at least a factor of 10 with little computation, while  applying complete natural language (NL) models of syn-  tax and semantics to all partial hypotheses typically requires 	[[161, 163]]	[[143, 159]]	['NL']	['natural language']
3454	translation architecture, was developed under sponsorship from  Swedish Telecom by a collaboration between SRI International,  the Swedish Institute of Computer Science (SICS), and Telia  Research.	[[170, 174], [107, 110]]	[[131, 168]]	['SICS', 'SRI']	['Swedish Institute of Computer Science']
3455	 Undoubtedly, Web 2.0 and the constant increase of User Generated Content (UGC) lead to a  higher demand for translation.	[[75, 78]]	[[51, 73]]	['UGC']	['User Generated Content']
3456	 1 In t roduct ion   Language Understanding (LU) has been the focus of much research work in the last twenty ears. 	[[45, 47]]	[[21, 43]]	['LU']	['Language Understanding']
3457	"1 In t roduct ion   For some NLP applications, it is important o  identify, ""named entities"" (NE), such as person  names, organization ames, time, date, or money "	[[94, 96]]	[[77, 91]]	['NE']	['named entities']
3458	Figure 1: Overall architecture of Sentiment Classifier when a word is used with highly positive (HP), positive (P), highly negative (HN), negative (N) or objective (O) meaning based on a sentiment sense inven-	[[133, 135], [97, 99]]	[[116, 131], [80, 95], [102, 110], [138, 146], [154, 163]]	['HN', 'HP']	['highly negative', 'highly positive', 'positive', 'negative', 'objective']
3459	systems. This shared task was partially supported by Shared Annotated Resources (ShARe) project NIH 5R01GM090187 and Temporal His-	[[81, 86], [96, 99]]	[[53, 79]]	['ShARe', 'NIH']	['Shared Annotated Resources']
3460	124   Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 202?203, Vancouver, October 2005.	[[79, 83]]	[[31, 77]]	['IWPT']	['International Workshop on Parsing Technologies']
3461	represents the editing cycle. Given a  Semantic Network (SN) in a knowledge base (KB),  the system generates a description of the SN in the 	[[57, 59], [82, 84], [130, 132]]	[[39, 55], [66, 80]]	['SN', 'KB', 'SN']	['Semantic Network', 'knowledge base']
3462	Measuring speech quality for text-to-speech systems: Development and assessment of a modified mean opinion score (MOS) scale. Computer Speech	[[114, 117]]	[[94, 112]]	['MOS']	['mean opinion score']
3463	has been discussed extensively in various formulations in the NLP literature, notably in PP attachment, semantic role labeling (SRL) and subcategorization acquisition.	[[128, 131], [62, 65], [89, 91]]	[[104, 126]]	['SRL', 'NLP', 'PP']	['semantic role labeling']
3464	The focus of the robust CSR techniques  on SLS applications is being facilitated by development and implementation of a well-structured  interface between a CSR and a natural language processor (NLP), allowing collaboration with other  groups developing NLPs for SLS applications.	[[195, 198], [24, 27], [43, 46], [157, 160], [254, 258], [263, 266]]	[[167, 193]]	['NLP', 'CSR', 'SLS', 'CSR', 'NLPs', 'SLS']	['natural language processor']
3465	 3.3 Prosodic Model Training We choose to use a support vector machine (SVM) classifier1 for the prosodic model based on previous	[[72, 75]]	[[48, 70]]	['SVM']	['support vector machine']
3466	compiled two datasets consisting of research papers from two top-tier machine learning conferences: World Wide Web (WWW) and Knowledge Discovery and Data Mining (KDD).	[[116, 119], [162, 165]]	[[100, 114], [125, 153]]	['WWW', 'KDD']	['World Wide Web', 'Knowledge Discovery and Data']
3467	 In Proceedings of Natural Language Processing in Biomedicine (BioNLP) NAACL 2009 Workshop, pages 1?9.	[[63, 69], [71, 76]]	[[50, 61]]	['BioNLP', 'NAACL']	['Biomedicine']
3468	boundaries. Additionally, a local character-based joint segmentation and tagging solver (SegTagL) is used to provide word boundaries as well as inaccu-	[[89, 96]]	[[56, 87]]	['SegTagL']	['segmentation and tagging solver']
3469	2 Background 2.1 Surface Realization with Combinatory Categorial Grammar (CCG) CCG (Steedman, 2000) is a unification-based cat-	[[74, 77], [79, 82]]	[[42, 72]]	['CCG', 'CCG']	['Combinatory Categorial Grammar']
3470	For aviation incidents, the advantage of the proposed prior is reflected in the location (LO) and country (CO) slots, which may confuse the various models as they both belong to the entity type loca-	[[107, 109], [90, 92]]	[[98, 105], [80, 88]]	['CO', 'LO']	['country', 'location']
3471	"to say"" and the ""what to say"" is still an open  question. RAGS (rags, 1999) proposes astandard  architecture for the data, but leaves the "	[[58, 62]]	[[64, 68]]	['RAGS']	['rags']
3472	Some researchers rely on generic planners (e.g., (Dale, 1988)) for this task, while others use plans based on Rhetorical Structure Theory (RST) (e.g., (Bouayad-Aga et al, 2000; Moore and Paris,	[[139, 142]]	[[110, 137]]	['RST']	['Rhetorical Structure Theory']
3473	280  Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1203?1209, October 25-29, 2014, Doha, Qatar.	[[93, 98]]	[[43, 91]]	['EMNLP']	['Empirical Methods in Natural Language Processing']
3474	irrespective of word order.  Longest Common Substring (LCS): This measures the longest sequence of words shared between	[[55, 58]]	[[29, 53]]	['LCS']	['Longest Common Substring']
3475	In this paper, we explore the benefits of semantic features and in particular, evaluate the application of semantic role labeling (SRL) to Open IE. 	[[131, 134], [144, 146]]	[[107, 129]]	['SRL', 'IE']	['semantic role labeling']
3476	alignment. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 215?222.	[[103, 106]]	[[60, 101]]	['ACL']	['Association for Computational Linguistics']
3477	Texts The performance of punctuation prediction on both Chinese (CN) and English (EN) texts in the correctly recognized output of the BTEC and CT datasets are	[[65, 67], [82, 84], [134, 138], [143, 145]]	[[56, 63], [73, 80]]	['CN', 'EN', 'BTEC', 'CT']	['Chinese', 'English']
3478	A Machine Learning based Approach to Evaluating Retrieval Systems Huyen-Trang Vu and Patrick Gallinari Laboratory of Computer Science (LIP6) University of Pierre and Marie Curie	[[135, 139]]	[[103, 133]]	['LIP6']	['Laboratory of Computer Science']
3479	employing them simultaneously. We also include the oracle word error rate (WER) of the WCNs and lattices for each ASR configuration.	[[75, 78], [87, 91], [114, 117]]	[[58, 73]]	['WER', 'WCNs', 'ASR']	['word error rate']
3480	tribution from most features but C1 seems to benefit more from GENIA named entity tagging, Human Phenotype Ontology (HPO), Foundation Model of Anatomy (FMA) and	[[117, 120], [33, 35], [63, 68], [152, 155]]	[[91, 115], [123, 150]]	['HPO', 'C1', 'GENIA', 'FMA']	['Human Phenotype Ontology', 'Foundation Model of Anatomy']
3481	 1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful ap-	[[37, 39]]	[[16, 35]]	['MT']	['Machine translation']
3482	face form via the application of a set of grammar rules based on particular linguistic theories, e.g. Lexical Functional Grammar (LFG), HeadDriven Phrase Structure Grammar (HPSG), Com-	[[130, 133], [173, 177]]	[[102, 128], [136, 171]]	['LFG', 'HPSG']	['Lexical Functional Grammar', 'HeadDriven Phrase Structure Grammar']
3483	 We trained a dialog independent (DI) class based LM and dialog dependent (DD) grammar based LM. In all LMs n is set to 3.	[[75, 77], [34, 36], [93, 95], [104, 107]]	[[64, 73], [14, 32]]	['DD', 'DI', 'LM', 'LMs']	['dependent', 'dialog independent']
3484	(ST), i.e. any node of a tree along with all its descendants. A subset tree (SST) exploited by the SubSetTreeKernel is a more	[[77, 80], [1, 3]]	[[64, 75], [99, 109]]	['SST', 'ST']	['subset tree', 'SubSetTree']
3485	gree (TTCD) 1, tongue body constriction location (TBCL) and degree (TBCD), lower tooth height (LTH), and glottal vibration (GLO). For example,	[[124, 127], [6, 10], [50, 54], [68, 72], [95, 98]]	[[105, 122], [15, 48], [75, 93]]	['GLO', 'TTCD', 'TBCL', 'TBCD', 'LTH']	['glottal vibration', 'tongue body constriction location', 'lower tooth height']
3486	telling. In Computers in Entertainment (CIE), Association for Computing Machinery (ACM), volume 5.	[[83, 86], [40, 43]]	[[46, 81], [12, 38]]	['ACM', 'CIE']	['Association for Computing Machinery', 'Computers in Entertainment']
3487	object (J), which is bad for the Jews (the  event is marked by minus), and after that (>) a  Jewish bearer of Sacred Power (J*SP) miracu-  lously (MIR) protects the (above) Jewish object, 	[[124, 128], [147, 150]]	[[93, 122], [130, 145]]	['J*SP', 'MIR']	['Jewish bearer of Sacred Power', 'miracu-  lously']
3488	 ? Reduce Left - X (RL) : Pops the top two nodes from the stack, combines them into a new node	[[20, 22]]	[[3, 14]]	['RL']	['Reduce Left']
3489	In Proc. of IEEE/ACL workshop on Spoken Language Technology (SLT). 	[[61, 64], [12, 20]]	[[33, 59]]	['SLT', 'IEEE/ACL']	['Spoken Language Technology']
3490	NN = Noun, NN-PL = Plural Noun  DET = Determiner, PREP = Preposition  POS = Possessive, J J = Adjective  Table h Patterns for partOf(basement,building) 	[[70, 73], [0, 2], [11, 16], [32, 35], [50, 54], [88, 91]]	[[76, 86], [5, 9], [19, 30], [38, 48], [57, 68], [94, 103]]	['POS', 'NN', 'NN-PL', 'DET', 'PREP', 'J J']	['Possessive', 'Noun', 'Plural Noun', 'Determiner', 'Preposition', 'Adjective']
3491	........................................ LTH  ........... Link between STH and LTHs  TLink (Translation Link) between language LTHs  Figure 2: Example of an STH linked to a Fragment 	[[85, 90], [71, 74], [79, 83], [127, 131], [157, 160], [41, 44]]	[[92, 108]]	['TLink', 'STH', 'LTHs', 'LTHs', 'STH', 'LTH']	['Translation Link']
3492	Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracle F-scores on CCGbank Section 00. The table shows the labelled F-score (LF), precision (LP) and recall (LR) and the the number of lexical categories per word used (from first to last parsing attempt).	[[186, 188], [46, 49], [128, 135], [202, 204], [218, 220]]	[[168, 178], [23, 44]]	['LF', 'AST', 'CCGbank', 'LP', 'LR']	['labelled F', 'adaptive supertagging']
3493	We review the hierarchical Dirichlet document (HDD) model in section 2, and present our proposed hierarchical Dirichlet tree (HDT) document model in section 3.	[[126, 129], [47, 50]]	[[97, 124], [14, 45]]	['HDT', 'HDD']	['hierarchical Dirichlet tree', 'hierarchical Dirichlet document']
3494	ditto Shi-fen three-hours ten-minute, i.e. 'three-  ten'), post-position phrases (GPs), preposition  phrases (PPs), br adverbs (ADVs). They all share 	[[128, 132], [82, 85], [110, 113]]	[[119, 126], [88, 108], [59, 80]]	['ADVs', 'GPs', 'PPs']	['adverbs', 'preposition  phrases', 'post-position phrases']
3495	emes are represented, and not function words.  Conceptual representations (ConcSs) used by  PRESENTOR are inspired by the characteristics 	[[75, 81], [92, 101]]	[[47, 73]]	['ConcSs', 'PRESENTOR']	['Conceptual representations']
3496	1.10% 0.20% 18.86% 0    We define Simple MNP (SMNP) whose  length is less than 5 words and Complete MNP 	[[46, 50], [100, 103]]	[[34, 44]]	['SMNP', 'MNP']	['Simple MNP']
3497	also recorded in the miscellaneous information field of the lexeme. Similarly, Gene Ontology (GO) (Consortium.,	[[94, 96]]	[[79, 92]]	['GO']	['Gene Ontology']
3498	 3 Corpus description The British National Corpus (BNC) (Leech, 1992) is annotated with POS tags, using the CLAWS-4	[[51, 54], [88, 91], [108, 115]]	[[26, 49]]	['BNC', 'POS', 'CLAWS-4']	['British National Corpus']
3499	(Marcus et al., 1993) whereas IN = preposition or conjunction, subordinating; CC = Coordinating Conjunction; VBN = Verb, past participle; VBG = verb, gerund or present partici-	[[78, 80], [30, 32], [109, 112], [138, 141]]	[[83, 107], [115, 136], [144, 175]]	['CC', 'IN', 'VBN', 'VBG']	['Coordinating Conjunction', 'Verb, past participle', 'verb, gerund or present partici']
3500	Abstract The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate.	[[72, 74]]	[[51, 70]]	['MT']	['machine translation']
3501	to be relevant: 1. Subject (SS) 2.	[[28, 30]]	[[19, 26]]	['SS']	['Subject']
3502	PARSEVAL scores for our parse results. We focus on the Labeled Precision (LP) and Labeled Recall (LR) scores only in this paper, as these are	[[74, 76], [0, 8], [98, 100]]	[[55, 72], [82, 96]]	['LP', 'PARSEVAL', 'LR']	['Labeled Precision', 'Labeled Recall']
3503	In this paper we show how the extraction process can be scaled to the complete Wall Street Journal (WSJ) section of the Penn-II treebank, with about 1 mil-	[[100, 103]]	[[79, 98]]	['WSJ']	['Wall Street Journal']
3504	The knowledge about actions and plans is stored in  a plan library structured on the basis of two main hier-  archies: the Decomposition Hierarchy (DH) and the  Generalization Hierarchy (GH) \[Kautz and Allen, 86\].	[[148, 150], [187, 189]]	[[123, 146], [161, 185]]	['DH', 'GH']	['Decomposition Hierarchy', 'Generalization Hierarchy']
3505	autism. In Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR), pages 131?140, Gothenburg, Sweden, April. Association for Computational Linguistics.	[[119, 123]]	[[46, 117]]	['PITR']	['Predicting and Improving Text Readability for Target Reader Populations']
3506	CoTrain vs. LEX(CN) 6.09E-29 3.72E-21 1.61E-24 CoTrain vs. LEX(EN) 0.0018 0.0276 0.00329 CoTrain vs. SVM(CN) 1.26E-13 6.45E-10 2.7E-14 CoTrain vs. SVM(EN) 2.15E-18 1.1E-17 3.46E-13	[[105, 107]]	[[89, 96]]	['CN']	['CoTrain']
3507	Another important note is that, although the audio sets consist of both broadcast news (BN) and broadcast conversations (BC), we did not perform BN or BC-specific tuning.	[[121, 123], [88, 90], [145, 147], [151, 153]]	[[96, 119], [72, 86]]	['BC', 'BN', 'BN', 'BC']	['broadcast conversations', 'broadcast news']
3508	4} is not a most frequent token  and will reach at bi-gram queue manager only  after passing  through all forms generator (AFG).  	[[123, 126]]	[[102, 121]]	['AFG']	['all forms generator']
3509	placed in a transcript. Here, we focus on the syndrome of primary progressive aphasia (PPA). PPA	[[87, 90], [93, 96]]	[[58, 85]]	['PPA', 'PPA']	['primary progressive aphasia']
3510	of-the-art dependency parser, the 2014 online version. 4) PPAD Naive Bayes(NB) is the same as PPAD but uses a generative model, as opposed to	[[75, 77], [58, 62], [94, 98]]	[[63, 73]]	['NB', 'PPAD', 'PPAD']	['Naive Baye']
3511	BP as a computational expression graph ? Automatic differentiation (AD) 7.	[[68, 70], [0, 2]]	[[41, 66]]	['AD', 'BP']	['Automatic differentiation']
3512	marn et al 2007). The training procedure usually employs an expectation maximization (EM) procedure and the resulting transducer can be used to	[[86, 88]]	[[60, 84]]	['EM']	['expectation maximization']
3513	where C is the concept that subsumes both C1 and C2 and has the highest information content (i.e., it is the least common subsumer (LCS)). 	[[132, 135]]	[[109, 130]]	['LCS']	['least common subsumer']
3514	grammatical person and number (1PS, 1PP, 2P, 3PS, 3PP), the quantified pronouns (QUANT), and a group including all other expressions (OTHER). 	[[134, 139], [81, 86]]	[[115, 132], [60, 79]]	['OTHER', 'QUANT']	['other expressions', 'quantified pronouns']
3515	in table 4. As we can see a small improvement is obtained for the interpretation error rate (IER) with the integrated strategy (strat2).	[[93, 96]]	[[66, 91]]	['IER']	['interpretation error rate']
3516	event coreference resolver has to rely on the noisy outputs produced by its upstream components in the standard information extraction (IE) pipeline. 	[[136, 138]]	[[112, 134]]	['IE']	['information extraction']
3517	 The practical application of flame-based knowledge-based systems, such as in expert systems, requires the maintenance of  potentially very large amounts of declarative knowledge stored in their knowledge bases (KBs). As a KB grows in size and 	[[212, 215], [223, 225]]	[[195, 210]]	['KBs', 'KB']	['knowledge bases']
3518	 1 In t roduct ion   Word Sense Disambiguation (WSD) is an open prob-  lem in Natural Language Processing.	[[48, 51]]	[[21, 46]]	['WSD']	['Word Sense Disambiguation']
3519	difficult to disambiguate.  Preposition sense disambiguation (PSD) has many potential uses.	[[62, 65]]	[[28, 60]]	['PSD']	['Preposition sense disambiguation']
3520	the source and target sides are lexicons (terminals) 2) Unlexicalized (ULex): all leaf nodes in both the 46	[[71, 75]]	[[56, 69]]	['ULex']	['Unlexicalized']
3521	end returnmodels [] Algorithm 1: Positive Diversity Tuning (PDT) lectively produce very diverse translations.3	[[60, 63]]	[[33, 58]]	['PDT']	['Positive Diversity Tuning']
3522	2 GT  and  Move-c~  The central operations of the Minimalist Program  are Generalized Transformation (GT) and Move-  ~. GT is a structure-building operation that builds 	[[102, 104], [2, 4], [120, 122]]	[[74, 100]]	['GT', 'GT', 'GT']	['Generalized Transformation']
3523	"ANTEST R E T U R N S  ** 1**  CHANGT, HAVE CSEXCE1 FOR HESGEF IN  ARTEST CALLEC FOR 18""REGVO I C E u  (AACC)  ANTEST E T U E N S  ** l** "	[[103, 107], [43, 50], [51, 54], [55, 61], [0, 6]]	[[66, 96]]	['AACC', 'CSEXCE1', 'FOR', 'HESGEF', 'ANTEST']	"['ARTEST CALLEC FOR 18""REGVO I C']"
3524	 1 Introduction In the semantic dependency parsing (SDP) task of SemEval 2014, the meaning of a sentence is repre-	[[52, 55]]	[[23, 50]]	['SDP']	['semantic dependency parsing']
3525	questions evaluated in TREC1. For example questions 1The Text REtrieval Conferences (TREC) are evaluation workshops in which Information Retrieval tasks are annually	[[85, 89], [23, 27]]	[[57, 83]]	['TREC', 'TREC']	['Text REtrieval Conferences']
3526	tic attachment. Eight categories of syntactic onstituent were used: sentence (S), noun  phrase (NP), verb phrase (VP), prepositional phrase (PP), wh-noun phrase (WHNP),  adjective or adverbial phrase (AP), any other constituent (O), and both words in the 	[[114, 116], [141, 143], [96, 98], [162, 166], [201, 203]]	[[101, 112], [119, 139], [68, 76], [82, 94], [146, 160], [170, 199], [206, 227]]	['VP', 'PP', 'NP', 'WHNP', 'AP']	['verb phrase', 'prepositional phrase', 'sentence', 'noun  phrase', 'wh-noun phrase', 'adjective or adverbial phrase', 'any other constituent']
3527	The basic aim of Acquilex is the  development of techniques and methods in order to use  Machine Readable Dictionaries (MRD) * for building lexical  components for Natural Language Processing Systems.	[[120, 123]]	[[89, 118]]	['MRD']	['Machine Readable Dictionaries']
3528	While initial-state ECR provides a measure of the likelihood of a favorable outcome, it does not address how well a particular state representation captures key decision points. That is, it does not directly represent the extent to which each deci-sion along the path to a successful outcome con-tributed to that outcome, or whether the second-best decision in a particular state would have been equally useful. In order to measure this dif-ference, we introduce the Separation Ratio (SR), which represents how much better a particular policy is compared to its alternatives. SR for a state is calculated by taking the absolute differ-ence between the estimated values of two actions in that state and dividing by the mean of the two values.	[[485, 487], [576, 578], [20, 23]]	[[467, 483]]	['SR', 'SR', 'ECR']	['Separation Ratio']
3529	document is different from the question. Also, in  Information Extraction (IE), in which the system  tries to extract elements of some events (e.g. 	[[75, 77]]	[[51, 73]]	['IE']	['Information Extraction']
3530	 LTP. LTP (Language Technology Platform  developed by HIT) is a package of tools to 	[[6, 9], [1, 4], [54, 57]]	[[11, 39]]	['LTP', 'LTP', 'HIT']	['Language Technology Platform']
3531	 1 Introduction The goal of statistical machine translation (SMT) is to produce a target sentence e from a source sen-	[[61, 64]]	[[28, 59]]	['SMT']	['statistical machine translation']
3532	SK(d1,d1)?SK(d2,d2) 4 Experiments The use of WordNet (WN) in the term similarity function introduces a prior knowledge whose impact	[[54, 56], [0, 2], [10, 12]]	[[45, 52]]	['WN', 'SK', 'SK']	['WordNet']
3533	In my semantic role labeling research, I used the Tilburg Memory Learner (TiMBL) for this purpose.	[[74, 79]]	[[50, 72]]	['TiMBL']	['Tilburg Memory Learner']
3534	An entity has  three types of mention: NAM (proper name), NOM  (nominal) or PRO (pronoun). A relation was 	[[76, 79], [39, 42], [58, 61]]	[[81, 88], [51, 55], [64, 71]]	['PRO', 'NAM', 'NOM']	['pronoun', 'name', 'nominal']
3535	In this part, we introduce how to make use of the  original and opposite training/test data together  for dual training and dual prediction (DTDP). 	[[141, 145]]	[[106, 139]]	['DTDP']	['dual training and dual prediction']
3536	?? ( sE) one or more times are considered to be locative MWEs (LOC). In contrast, bigrams	[[63, 66], [57, 61]]	[[48, 56]]	['LOC', 'MWEs']	['locative']
3537	2.3.1 KiF (Knowledge in Frame) The whole system, training and prediction, has been implemented in KiF (Knowledge in Frame), a script language that has been implemented into	[[98, 101], [6, 9]]	[[103, 121], [11, 29]]	['KiF', 'KiF']	['Knowledge in Frame', 'Knowledge in Frame']
3538	A simple solution to this problem is  to compute the probability of words in the target  language as maximum likelihood estimates (MLE)  over a large corpus and reformulate the general 	[[131, 134]]	[[101, 129]]	['MLE']	['maximum likelihood estimates']
3539	+), and (?) denote Run1, Run2, and Run3, respectively, our submissions to the shared task; FL=Flagging, FS=Feature stacking, DS=Domain stacking. ? denotes the difference in system	[[91, 93], [104, 106], [125, 127]]	[[94, 102], [107, 123], [128, 143]]	['FL', 'FS', 'DS']	['Flagging', 'Feature stacking', 'Domain stacking']
3540	GaChalign (Crosslingual): Gale-Church Sentence-level Aligner with variable parameters ? Indotag (Indonesian): Conditional Random Field (CRF) POS tagger. 	[[136, 139], [141, 144]]	[[110, 134]]	['CRF', 'POS']	['Conditional Random Field']
3541	these 153,014 verb-noun collocations.  We used 'Bunrui Goi Hyou'(BGH) (NLRI, 1993)  as the Japanese thesaurus.	[[65, 68], [71, 75]]	[[48, 63]]	['BGH', 'NLRI']	['Bunrui Goi Hyou']
3542	symbols as well as full names. Groups such as the  Human Genome Organisation (HUGO), Mouse Genome  Institute (MGI), UniProt, and the National Center for 	[[78, 82], [110, 113]]	[[51, 76], [85, 108]]	['HUGO', 'MGI']	['Human Genome Organisation', 'Mouse Genome  Institute']
3543	 The algorithm was first proposed by the Institute for Computer Science  and Technology (ICST) of the National Bureau of Standards (NBS') in 1973. 	[[132, 136], [89, 93]]	[[102, 130], [41, 87]]	"[""NBS'"", 'ICST']"	['National Bureau of Standards', 'Institute for Computer Science  and Technology']
3544	answers accordingly. Specially, we develop a supervised Maximum Entropy (MaxEnt) based model to rescore the answers from the pipelines,	[[73, 79]]	[[56, 71]]	['MaxEnt']	['Maximum Entropy']
3545	We show how these strategies are captured in a grammar developed in the Grammatical Framework (GF).1 We evaluated our method by experimenting	[[95, 97]]	[[72, 93]]	['GF']	['Grammatical Framework']
3546	slightly simplified version of Webber's original rule  schema, says that for any formula that meets the  structural description (SD), a discourse ntity identified  by the ID formula is to be constructed.	[[129, 131], [171, 173]]	[[105, 127]]	['SD', 'ID']	['structural description']
3547	event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. 	[[110, 113]]	[[81, 108]]	['NLP']	['natural language processing']
3548	For comparison, the graphs in Figures 1 and 2 also show the curves corresponding to the evaluation of Pointwise Mutual Information (PMI).8 The cooccurrence statistics of the expressions in Disco-En-	[[132, 135]]	[[102, 130]]	['PMI']	['Pointwise Mutual Information']
3549	ate the surface form in the opposite direction.  Amazon?s Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources	[[75, 80]]	[[58, 73]]	['MTurk']	['Mechanical Turk']
3550	niscent of balanced-tree structures using left and right  rotations. A left rotation changes a (A(BC)) structure to  a ((AB)C) structure, and vice versa for a right rotation.	[[96, 100], [120, 126]]	[[69, 92]]	['A(BC', '(AB)C)']	['A left rotation changes']
3551	ing decisions in multi-party discussions.  Several types of dialogue act (DA) are distinguished on the basis of their roles in	[[74, 76]]	[[60, 72]]	['DA']	['dialogue act']
3552	written japanese. In Proceedings of the 6th  Workshop on Asian Language Resources (ALR),  pages 101?102.	[[83, 86]]	[[57, 81]]	['ALR']	['Asian Language Resources']
3553	 Type Short time members Long time members Abbreviations Husband My DD (Dear Daughter), Your PS (Plastic Surgeon) Social networks Facebook, fb	[[68, 70], [93, 95]]	[[72, 85], [97, 112]]	['DD', 'PS']	['Dear Daughter', 'Plastic Surgeon']
3554	 In addition, the tense,                                                             1 S=Subject; IO=Indirect Object; DO=Direct Object;  V=Verb; ERG=Ergative; DAT=Dative 	[[98, 100], [118, 120], [145, 148], [159, 162]]	[[101, 116], [121, 134], [89, 96], [139, 143], [149, 157], [163, 169]]	['IO', 'DO', 'ERG', 'DAT']	['Indirect Object', 'Direct Object', 'Subject', 'Verb', 'Ergative', 'Dative']
3555	for the different settings. GM = Google Maps, CI = Campus Indoor, CO = Campus Outdoor. 	[[66, 68], [28, 30], [46, 48]]	[[71, 85], [33, 44], [51, 64]]	['CO', 'GM', 'CI']	['Campus Outdoor', 'Google Maps', 'Campus Indoor']
3556	of General Linguistics  MAINE, JUNE '94  SEMANTIC SYNTAX (SeSyn) is a direct continuation of work done in the '60s and '70s under the name of GENERATIVE  SEMANTICS.	[[58, 63]]	[[41, 56]]	['SeSyn']	['SEMANTIC SYNTAX']
3557	model organism databases (e.g., for mouse3 and  yeast4) as well as various protein databases (e.g.,  Protein Information Resource5 (PIR) or SWISS-                                                                                           tor), a model organism for genetics research: 	[[132, 135]]	[[101, 130]]	['PIR']	['Protein Information Resource5']
3558	Table 4: Entailment judgment in closed test  of mutual information (T=True, F=False,  MI=mutual information). 	[[86, 88]]	[[89, 107], [70, 74], [78, 83]]	['MI']	['mutual information', 'True', 'False']
3559	 ? Unaligned word penalty feature (UWP): hUWP (Q,S,A), which is defined as the ratio	[[35, 38], [41, 45]]	[[3, 25]]	['UWP', 'hUWP']	['Unaligned word penalty']
3560	weiwei@cs.columbia.edu Abstract In Semantic Textual Similarity (STS), systems rate the degree of semantic equivalence,	[[64, 67]]	[[35, 62]]	['STS']	['Semantic Textual Similarity']
3561	dependency tree.  3.1 Naive Approach (NA)  In this approach we first run a parser on the input 	[[38, 40]]	[[22, 36]]	['NA']	['Naive Approach']
3562	ity. We presented an evaluation of these parameters for preposition sense disambiguation (PSD). 	[[90, 93]]	[[56, 88]]	['PSD']	['preposition sense disambiguation']
3563	 B. MTE features We use the following MTE metrics (MTFEATS), which compare the similarity between the question and a candidate answer:	[[51, 58], [4, 7], [38, 41]]	[[42, 49]]	['MTFEATS', 'MTE', 'MTE']	['metrics']
3564	that have to be defined in a theory-specific way.  Thus a document representation (DocRep) has two components, a DocAttr and a DocRepSeq.	[[83, 89], [113, 119], [127, 136]]	[[58, 81]]	['DocRep', 'DocAtt', 'DocRepSeq']	['document representation']
3565	 2 Swedish FrameNet Swedish FrameNet (SweFN), which began in 2011, is part of Swedish FrameNet++ (Borin et al.,	[[38, 43]]	[[20, 36]]	['SweFN']	['Swedish FrameNet']
3566	i. The Eng!\[sh t_qMal_a~franslationSsSSSSSSSS_2~trm  Baak~reusd  Computer Aided T~anslation (CAT) research at Universiti  Sa~m MalsysL~ (USM) began in 1976 as an individual research 	[[94, 97], [138, 141]]	[[66, 82], [111, 136]]	['CAT', 'USM']	['Computer Aided T', 'Universiti  Sa~m MalsysL~']
3567	 NOM (nominative), ACC (accusative), DAT (dative), ABL (ablative), CMI (comitative), GEN (genitive) and TOP (topic marker).	[[51, 54], [67, 70], [85, 88], [1, 4], [19, 22], [37, 40], [104, 107]]	[[56, 64], [72, 82], [90, 98], [6, 16], [24, 34], [42, 48], [109, 121]]	['ABL', 'CMI', 'GEN', 'NOM', 'ACC', 'DAT', 'TOP']	['ablative', 'comitative', 'genitive', 'nominative', 'accusative', 'dative', 'topic marker']
3568	ings. They evaluate their work against the SENSEVAL 2 AW test data (SV2AW). They tune the	[[68, 73]]	[[43, 56]]	['SV2AW']	['SENSEVAL 2 AW']
3569	In t roduct ion   This paper discusses the relationstfip between Tree Adjoin-  ing Grammars (TAG's) and :Head Grammars (HG's). TAG's 	[[120, 124], [93, 98], [127, 132]]	[[105, 118], [65, 91]]	"[""HG's"", ""TAG's"", ""TAG's""]"	['Head Grammars', 'Tree Adjoin-  ing Grammars']
3570	it is the in f in i t ive  form of a verb. If SO, it is to be attached to the  parsing tree, and given the additionql feature MVB (main verb). The current 	[[126, 129], [46, 48]]	[[131, 140]]	['MVB', 'SO']	['main verb']
3571	ponent Analysis (PCA). We then compare our embeddings with the CW (Collobert and Weston, 2008), Turian (Turian et al.,	[[63, 65]]	[[67, 87]]	['CW']	['Collobert and Weston']
3572	     elements; ? |? is used for alternating elements; TOP = topic marker. 	[[54, 57]]	[[60, 65]]	['TOP']	['topic']
3573	 2.3 Parsing and DSyntS conversion We adopt Deep Syntactic Structures (DSyntSs) as a format for syntactic structures because they can	[[71, 78], [17, 23]]	[[44, 69]]	['DSyntSs', 'DSyntS']	['Deep Syntactic Structures']
3574	 1 Introduction Information extraction (IE) systems recover structured information from text.	[[40, 42]]	[[16, 38]]	['IE']	['Information extraction']
3575	cation and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks.	[[116, 119]]	[[87, 114]]	['NLP']	['natural language processing']
3576	ical functions. Apart from commonly accepted grammatical functions, such as SB (subject) or OA (accusative object), Negra grammatical func-	[[76, 78], [92, 94]]	[[80, 87], [96, 113]]	['SB', 'OA']	['subject', 'accusative object']
3577	SEPA parameters are S = 13, 000, N = 20. In both rows, SEPA results for the in-domain (left) and adaptation (middle) scenarios are compared to the confidence (CB) and minimum length (ML) baselines. The	[[183, 185], [0, 4], [55, 59], [159, 161]]	[[167, 181]]	['ML', 'SEPA', 'SEPA', 'CB']	['minimum length']
3578	4 Discussion We provide a brief introduction to the framework of Linear Categorial Grammar (LinCG). One of	[[92, 97]]	[[65, 90]]	['LinCG']	['Linear Categorial Grammar']
3579	pathway representation formats: Systems Biology Markup Language (SBML)3 (Hucka et al, 2003) and Biological Pathway Exchange (BioPAX)4 (Demir et al, 2010).	[[125, 131], [65, 69]]	[[96, 123], [32, 63]]	['BioPAX', 'SBML']	['Biological Pathway Exchange', 'Systems Biology Markup Language']
3580	ing mobile devices. Such an application typically uses a speech recognizer (ASR) for transforming the user?s speech input to text and a search component	[[76, 79]]	[[55, 74]]	['ASR']	['a speech recognizer']
3581	ple sentences. Such knowledge is also important for Textual Entailment (TE), a generic framework for modeling semantic inference.	[[72, 74]]	[[52, 70]]	['TE']	['Textual Entailment']
3582	 As an example consider the multiple alignments in Figure 4, with the gold standard alignment (GS) on the left and the generated alignment (GA) on	[[95, 97]]	[[70, 83]]	['GS']	['gold standard']
3583	pendencies is only context-free (section 2.1). Our argument is based on our desire to use a discourse grammar in natural language generation (NLG). It is well-known that	[[142, 145]]	[[113, 140]]	['NLG']	['natural language generation']
3584	But still we can say that even in languages with  that kind of structural properties like Slavic languages have, named entities (NE) form a subset of  natural language expressions that demonstrates 	[[129, 131]]	[[113, 127]]	['NE']	['named entities']
3585	semantic processing component. A detailed descrip-  tion of the lexical conccplual structure (LCS) which  serves as the interlingua is not given here, but see 	[[94, 97]]	[[64, 92]]	['LCS']	['lexical conccplual structure']
3586	five COMPARE3, and five RECOMMEND CPs.  Each of the 112 textplans (TPs) that produced 4	[[67, 70], [34, 37]]	[[56, 65]]	['TPs', 'CPs']	['textplans']
3587	monly used tasks: small vocabulary recognition (TI-digits), read and spontaneous text dictation (WSJ), and goal-oriented spoken dialog (ATIS). The broadcast news task is quite general, covering a	[[136, 140], [48, 57], [97, 100]]	[]	['ATIS', 'TI-digits', 'WSJ']	[]
3588	(Joshi, Vijay?Shanker, and Weir, 1989; Weir and Joshi,  1988) have shown that LIGs, Combinatory Categorial  Grammars (CCG), Tree Adjoinig Grammars (TAGs),  and Head Grammars (HGs) are weakly equivalent.	[[148, 152], [78, 82], [118, 121], [175, 178]]	[[124, 146], [84, 116], [160, 173]]	['TAGs', 'LIGs', 'CCG', 'HGs']	['Tree Adjoinig Grammars', 'Combinatory Categorial  Grammars', 'Head Grammars']
3589	The training set consisted of 12,927 texts: News reports (News) about Education (Edu), Editorials (Edit.) about Defense (Def), and Letters to the Editor (LttE) about Medicine (Med). 	[[154, 158], [81, 84], [99, 103], [121, 124], [176, 179]]	[[131, 152], [70, 79], [87, 97], [112, 119], [166, 174]]	['LttE', 'Edu', 'Edit', 'Def', 'Med']	['Letters to the Editor', 'Education', 'Editorials', 'Defense', 'Medicine']
3590	3.3 Sentiment Model The design of the sentiment model used in our system was based on the assumption that the opinions expressed would be highly subjective and contextualized.  Therefore, for generating data for model training and testing, we used a crowd-sourcing approach to do sentiment annotation on in-domain political data. To create a baseline sentiment model, we used Amazon Mechanical Turk (AMT) to get as varied a population of annotators as possible. We designed an interface that allowed annotators to perform the annotations outside of AMT so that they could participate anonymously.	[[400, 403]]	[[376, 398]]	['AMT']	['Amazon Mechanical Turk']
3591	The types of links  traversed in the search (in the first case) or the checked slots (in the second case) are a  function of the semantic lass (SEM-C) of the first constituent. This function assigns to each 	[[144, 149]]	[[129, 137]]	['SEM-C']	['semantic']
3592	a PP (Mirroshandel and Ghassem-Sani, 2011), the prepositional lexeme of the PP if e2 is governed by a PP, the POS of the head of the verb phrase (VP) if e1 is governed by a VP, the POS of the head of the	[[146, 148], [2, 4], [76, 78], [102, 104], [110, 113], [173, 175], [181, 184]]	[[133, 144]]	['VP', 'PP', 'PP', 'PP', 'POS', 'VP', 'POS']	['verb phrase']
3593	segmentable candidates, and picks a correct segmentation candidate from the list by using a value of LEF (Likelihood Evaluation Function, Section 2.1) and so on.	[[101, 104]]	[[106, 136]]	['LEF']	['Likelihood Evaluation Function']
3594	152 Kuhn A Survey and Classification of Controlled Natural Languages Controlled Language for Crisis Management (CLCM) (Temnikova 2010, 2011, 2012) is a language for writing instructions about how to deal with crisis situations.	[[112, 116]]	[[69, 110]]	['CLCM']	['Controlled Language for Crisis Management']
3595	1 Introduction ? Language Model (LM) Growing? refers to adding	[[33, 35]]	[[17, 31]]	['LM']	['Language Model']
3596	   Figure 2. Tutor's Priming Ratio aggregated by task set (TS = Task Set)     Figure 3.	[[59, 61]]	[[64, 72]]	['TS']	['Task Set']
3597	pense of introducing noise.  We propose Embedded base phrase(EBP) detection as algorithm.2.	[[61, 64]]	[[40, 60]]	['EBP']	['Embedded base phrase']
3598	induction algorithm (Bisk and Hockenmaier, 2012), which induces a language-specific Combinatory Categorial grammar (CCG) and lexicon based on a small number of linguistic	[[116, 119]]	[[84, 114]]	['CCG']	['Combinatory Categorial grammar']
3599	labeled as negative; otherwise, the review is labeled as positive.  3.2 Lexicon-Based Method in Chinese Language: LEX(CN) This method first translates English sentiment lexica into Chinese lexica, and then	[[118, 120], [114, 117]]	[[96, 112]]	['CN', 'LEX']	['Chinese Language']
3600	Two types of initial parameter configurations were tried for BFGS; initial parameters have the same fixed values, or were chosen randomly. Steepest Descent (SD) was used as online training where some portion (i.e. chunk) of the training data were used during an iteration.	[[157, 159], [61, 65]]	[[139, 155]]	['SD', 'BFGS']	['Steepest Descent']
3601	is different from the annotation scheme (Abbas, 2012; Abbas, 2014) of phrase structure (PS) and the hyper dependency structure (HDS) of the URDU.KON-TB treebank along with the different	[[128, 131], [88, 90], [140, 151]]	[[100, 126], [70, 86]]	['HDS', 'PS', 'URDU.KON-TB']	['hyper dependency structure', 'phrase structure']
3602	 Table 5: EPPS task: translation quality and time for different input conditions (CN=confusion network, time in seconds per sentence).	[[82, 84], [10, 14]]	[[85, 102]]	['CN', 'EPPS']	['confusion network']
3603	learning community: ? Rectified Linear Unit (ReLU) (Nair and Hinton, 2010): max(0, x);	[[45, 49]]	[[22, 43]]	['ReLU']	['Rectified Linear Unit']
3604	Japanese  (Jap). Germanic (Get), and Southern Romance (SRom). Only the 	[[55, 59], [11, 14], [27, 30]]	[[37, 53], [0, 8], [17, 25]]	['SRom', 'Jap', 'Get']	['Southern Romance', 'Japanese', 'Germanic']
3605	prim. = primary source; C06?C09 = CoNLL 2006?2009; I10 = ICON 2010; SM = shared modifier; CJ = conjunct; Nested CS = portion of CSs participating in nested CSs (both as the inner and outer CS); RT UAS = unlabeled attachment score of the roundtrip	[[68, 70], [90, 92], [112, 114], [128, 131], [156, 159], [189, 191], [194, 196], [197, 200]]	[[73, 88], [95, 103]]	['SM', 'CJ', 'CS', 'CSs', 'CSs', 'CS', 'RT', 'UAS']	['shared modifier', 'conjunct']
3606	 Acknowledgments This work has been funded in part by a research grant from Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289 (INSIGHT) and by the EU FP7 program in the context of the project LIDER	[[104, 107], [209, 214], [164, 166], [167, 170]]	[[76, 102]]	['SFI', 'LIDER', 'EU', 'FP7']	['Science Foundation Ireland']
3607	ran, and the historical ancestor of the other varieties. Modern Standard Arabic (MSA) is the modern  version of CA and is, broadly speaking, the univer-	[[81, 84]]	[[57, 79]]	['MSA']	['Modern Standard Arabic']
3608	5http://wordnet.princeton.edu/. 58 that, as in Informtion Retrieval (IR), multiple occurrences in the same document count as just one	[[69, 71]]	[[47, 67]]	['IR']	['Informtion Retrieval']
3609	EVG smoothed-skip-head 65.0 (5.7) L-EVG smoothed 68.8 (4.5) Table 1: Directed accuracy (DA) for WSJ10, section 23. 	[[88, 90], [0, 3], [34, 39]]	[[69, 86]]	['DA', 'EVG', 'L-EVG']	['Directed accuracy']
3610	been semi-automatically detected in human reference and machine translations from English (EN) to French (FR) and German (DE) (Section 3). 	[[106, 108], [91, 93], [122, 124]]	[[98, 104], [82, 89], [114, 120]]	['FR', 'EN', 'DE']	['French', 'English', 'German']
3611	occurs from different production sources, we  propose an extension to this genre of technique  in the form of a Group Sparse Model (GSM)  which enforces sparsity with a L2,1 norm instead 	[[132, 135]]	[[112, 130]]	['GSM']	['Group Sparse Model']
3612	 (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.	[[81, 85], [38, 44], [2, 5]]	[[50, 79], [21, 36]]	['MEMM', 'MaxEnt', 'MAP']	['maximum entropy Markov models', 'maximum entropy']
3613	query, some form of translation is required. One might conjecture that a combination of two existing fields, IR and machine translation (MT), would be satisfactory for accomplishing the combined translation and retrieval task.	[[137, 139]]	[[116, 135]]	['MT']	['machine translation']
3614	are usually followed by a number n ? 0. DU = discourse unit, ce = conversational event, K = DRS, u = utterance, sem	[[40, 42], [92, 95]]	[[45, 54], [101, 110]]	['DU', 'DRS']	['discourse', 'utterance']
3615	approach to structuring knowledge is based  on:  z automatic term recognition (ATR)  z automatic term clustering (ATC) as an 	[[79, 82], [114, 117]]	[[51, 77], [87, 112]]	['ATR', 'ATC']	['automatic term recognition', 'automatic term clustering']
3616	paring average precision values obtained 1) against the original, manual chronologies (APC), and 2) against the expert assessment (APE). These values	[[131, 134], [87, 90]]	[[100, 129]]	['APE', 'APC']	['against the expert assessment']
3617	 Finally there is a set of measures relating to the receiver operating characteristic (ROC) curves, which measure the discrimination of the scores for	[[87, 90]]	[[52, 85]]	['ROC']	['receiver operating characteristic']
3618	"performance"" (MOPs) presented here with  results we still need from system-external  ""measures of effectiveness"" (MOEs)25 MOE-  based methods evaluate (i) baseline unaided "	[[114, 118], [14, 18], [122, 125]]	[[86, 111]]	['MOEs', 'MOPs', 'MOE']	['measures of effectiveness']
3619	Table 1: Summary of distance values between the 380 observed A-N pairs and the predictions from each model (ADD=additive, MUL=multiplicative, PLSR=Partial Least Squares Regression).	[[108, 111], [122, 125], [61, 64], [142, 146]]	[[112, 120], [126, 140], [147, 179]]	['ADD', 'MUL', 'A-N', 'PLSR']	['additive', 'multiplicative', 'Partial Least Squares Regression']
3620	Abstract  There has been a long-standing methodology for  evaluating work in speech recognition (SR), but until  recently no community-wide methodology existed for either 	[[97, 99]]	[[77, 95]]	['SR']	['speech recognition']
3621	on a Wikipedia corpus instead of the ukWaC corpus. The University of Sussex (UoS) team submitted two WSI systems that use dependency-parsed	[[77, 80], [37, 42], [101, 104]]	[[55, 75]]	['UoS', 'ukWaC', 'WSI']	['University of Sussex']
3622	 The third is the confusion between adjective  (JJ) and noun (NN), when the word in question  modifies a noun that immediately follows.	[[62, 64], [48, 50]]	[[56, 60], [36, 45]]	['NN', 'JJ']	['noun', 'adjective']
3623	restrictive relative clauses, and epithets ? trigger conventional implicatures (CI) whose truth is necessarily presupposed, even if the truth conditions	[[80, 82]]	[[53, 78]]	['CI']	['conventional implicatures']
3624	nique was used in The MAYO Clinic Vocabulary  Server (MCVS)5, which encodes clinical expressions into medical ontology (SNOMED-CT) and  identifies whether the event is positive or negative.	[[120, 129], [22, 26], [54, 58]]	[]	['SNOMED-CT', 'MAYO', 'MCVS']	[]
3625	 2007. The Syntax Augmented MT (SAMT) system at the Shared Task for the 2007 ACL Workshop on	[[32, 36], [77, 80]]	[[11, 30]]	['SAMT', 'ACL']	['Syntax Augmented MT']
3626	tion, Generation, Question Answering (QA), etc.  STS is related to both Textual Entailment (TE) and Paraphrasing, but differs in a number of ways	[[92, 94], [38, 40], [49, 52]]	[[72, 90], [18, 36]]	['TE', 'QA', 'STS']	['Textual Entailment', 'Question Answering']
3627	 The LRS representation of (1) is shown in Figure 1, where INCONT (INTERNAL CONTENT) encodes the core semantic contribution of the head, EXCONT	[[59, 65], [5, 8], [137, 143]]	[[67, 83]]	['INCONT', 'LRS', 'EXCONT']	['INTERNAL CONTENT']
3628	It is a comprehensive study but not directly related to ours, as we model our problem with Markov Decision Processes (MDP) and evaluate model-based and model-free algorithms on a	[[118, 121]]	[[91, 116]]	['MDP']	['Markov Decision Processes']
3629	2 Dialogue data The dialogue corpus used to perform the experiments is the Switchboard database (SWBD). It	[[97, 101]]	[[75, 95]]	['SWBD']	['Switchboard database']
3630	Instead of using words directly, it is possible to employ a (much smaller) controlled vocabulary: Medical Subject Headings (MeSH), consisting of 22,500 codes, are (mostly) manually assigned to	[[124, 128]]	[[98, 122]]	['MeSH']	['Medical Subject Headings']
3631	 4.3 Spanish?English (ES?EN), French?English (FR?EN) In Table 3, we see that on the ES?EN and	[[46, 51]]	[[30, 44]]	['FR?EN']	['French?English']
3632	 4 DOM Tree Alignment Model  The Document Object Model (DOM) is an application programming interface for valid HTML 	[[56, 59], [3, 6], [111, 115]]	[[33, 54]]	['DOM', 'DOM', 'HTML']	['Document Object Model']
3633	egorization. ACM Transactions on Information Systems (TOIS), 12(3):233?251. 	[[54, 58], [13, 16]]	[[17, 52]]	['TOIS', 'ACM']	['Transactions on Information Systems']
3634	Answer Pinpointing. In Proceedings of the DARPA Human Language Technology Conference (HLT). 	[[86, 89], [42, 47]]	[[48, 73]]	['HLT', 'DARPA']	['Human Language Technology']
3635	ysis. The Chinese comma is viewed as a delimiter of elementary discourse units (EDUs), in the sense of the Rhetorical Structure Theory (Carlson et al,	[[80, 84]]	[[52, 78]]	['EDUs']	['elementary discourse units']
3636	dictionary (viz. /hu:pana-taNa/ and /Fi:tiki-RaNa/), I also searched the Ma?ori Broadcast Corpus (MBC) for words ending as if they had gerundial suffixes	[[98, 101]]	[[73, 96]]	['MBC']	['Ma?ori Broadcast Corpus']
3637	future work.1 1 Introduction and Motivation Typically, Information Extraction (IE) systems learn an extractor for each target relation from la-	[[79, 81]]	[[55, 77]]	['IE']	['Information Extraction']
3638	3.1 Graph-based parsing model The graph-based parsing model aims to search for the maximum spanning tree (MST) in a graph (McDonald et al, 2005).	[[106, 109]]	[[83, 104]]	['MST']	['maximum spanning tree']
3639	 School of Information Science Japan Advanced Institute of Science and Technology (JAIST), Japan nthnhung@jaist.ac.jp	[[83, 88]]	[[31, 81]]	['JAIST']	['Japan Advanced Institute of Science and Technology']
3640	sLDA is a model that is an extension of Latent Dirichlet Allocation (LDA) (Blei et al, 2003) that models each document as having an output variable in addition to	[[69, 72], [0, 4]]	[[40, 67]]	['LDA', 'sLDA']	['Latent Dirichlet Allocation']
3641	In Proceedings of the First International Conference on Language Resources and Evaluation (LREC), pages 581?588, Granada.	[[91, 95]]	[[56, 74]]	['LREC']	['Language Resources']
3642	(Para), POS, syntactic dependency tree (DT), syntactic constituent tree (CT), named entities (NE), WordNet Relations (WNR), WordNet supersenses (WNSS), semantic role labeling (SRL), causal relations (CR), query classes (QC), query-log co-ocurrences (QLCoOcc).Models: bag-of-words scoring (BOW), tree matching (TreeMatch), linear (LM), log-linear (LLM), statistical learning	[[220, 222], [8, 11], [40, 42], [73, 75], [94, 96], [118, 121], [145, 149], [176, 179], [200, 202], [250, 257], [289, 292], [310, 319], [330, 332], [347, 350]]	[[205, 218], [23, 38], [55, 71], [78, 92], [99, 116], [124, 143], [152, 174], [182, 198], [225, 248], [267, 279], [295, 308], [322, 328], [335, 345]]	['QC', 'POS', 'DT', 'CT', 'NE', 'WNR', 'WNSS', 'SRL', 'CR', 'QLCoOcc', 'BOW', 'TreeMatch', 'LM', 'LLM']	['query classes', 'dependency tree', 'constituent tree', 'named entities', 'WordNet Relations', 'WordNet supersenses', 'semantic role labeling', 'causal relations', 'query-log co-ocurrences', 'bag-of-words', 'tree matching', 'linear', 'log-linear']
3643	According to the fifth rule, the Arabic letter Z may match an empty string on the English side, if there is an English consonant (EC) in the right context of the English side.	[[130, 132]]	[[111, 128]]	['EC']	['English consonant']
3644	KEY: Number of discussions and posts on the topic (Discs, Posts).  Number of authors (NumA). Posts per author (P/A).	[[86, 90], [111, 114]]	[[67, 84], [93, 109]]	['NumA', 'P/A']	['Number of authors', 'Posts per author']
3645	To address this problem, Xiong et al (2006) enhance the BTG with a maximum entropy (MaxEnt) based reordering model which uses boundary words of bilingual	[[84, 90], [56, 59]]	[[67, 82]]	['MaxEnt', 'BTG']	['maximum entropy']
3646	Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), page 89, Beijing, August 2010 Multi-Word Expressions as Discourse Relation Markers (DRMs) Aravind K. Joshi	[[166, 170], [71, 74]]	[[138, 164], [19, 39], [112, 133]]	['DRMs', 'MWE']	['Discourse Relation Markers', 'Multiword Expression', 'Multi-Word Expression']
3647	ters instead of Y or N as labels. The character-level machine translation (MT) approach (Pennell and Liu, 2011) was modified in (Li and Liu, 2012a)	[[75, 77]]	[[54, 73]]	['MT']	['machine translation']
3648	of FIS model used to resolve each expression.  4.1 Similarity Features (SIM) Similarity features represent the lexical overlap	[[72, 75], [3, 6]]	[[51, 61]]	['SIM', 'FIS']	['Similarity']
3649	phases that are performed sequentially without feedback: Question Processing (QP), Passage Retrieval (PR) and Answer Extraction (AE). More	[[129, 131], [78, 80], [102, 104]]	[[110, 127], [57, 76], [83, 100]]	['AE', 'QP', 'PR']	['Answer Extraction', 'Question Processing', 'Passage Retrieval']
3650	nodes  where  the re lat ion cor responds  to a verb .   Verb  nodes  (VERBSTR)  conta in  a po in ter  to the  RE-  LATION represented  by  the  verb ?	[[71, 78]]	[[57, 61]]	['VERBSTR']	['Verb']
3651	represents the UTB statistics. For Telugu, the Telugu Treebank (TTB) released for ICON 2010 Shared Task (Husain et al. ( 2010)) was used for evaluation.	[[64, 67], [15, 18], [82, 86]]	[[54, 62]]	['TTB', 'UTB', 'ICON']	['Treebank']
3652	No 0 12 23 292 Table 4: Confusion matrix (SVM) for argument component classification (MC = Major Claim; Cl = Claim; Pr = Premise; No = None)	[[86, 88], [42, 46], [104, 106], [116, 118], [130, 132]]	[[91, 102], [109, 114], [121, 128], [135, 139]]	['MC', 'SVM)', 'Cl', 'Pr', 'No']	['Major Claim', 'Claim', 'Premise', 'None']
3653	For each dataset, we report Pearson?s correlation r with human judgments on pairs that are found in both resources (BOTH). Otherwise, the re-	[[116, 120]]	[[100, 104]]	['BOTH']	['both']
3654	understood as follows: ? if the POS-tag of current word  is VB (Verb) and  its word-form  is ? can?	[[60, 62], [32, 39]]	[[64, 68]]	['VB', 'POS-tag']	['Verb']
3655	4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments. STS is related to both Recognizing Textual En-tailment (RTE) and Paraphrase Recognition, but  54	[[371, 374], [12, 14], [36, 38], [132, 135], [153, 156], [315, 318]]	[[338, 369], [98, 126]]	['RTE', 'MT', 'MT', 'STS', 'STS', 'STS']	['Recognizing Textual En-tailment', 'Semantic Textual Similari-ty']
3656	all levels of syntactic represent-  ation, name\].y, D-structure,  S-structure, and LF (logical form). 	[[84, 86]]	[[88, 100], [55, 64], [69, 78]]	['LF']	['logical form', 'structure', 'structure']
3657	demanding.  Latent Semantic Analysis (LSA) (Deerwester et al.,	[[38, 41]]	[[12, 36]]	['LSA']	['Latent Semantic Analysis']
3658	Abstract We describe a machine learning approach, a Random Forest (RF) classifier, that is used to automatically compile bilingual	[[67, 69]]	[[52, 65]]	['RF']	['Random Forest']
3659	of MT and HT in terms of the following two ratios, LC = lexical cohesion devices / content words, RC = repetition / content words. 	[[98, 100], [3, 5], [10, 12], [51, 53]]	[[103, 123], [56, 72]]	['RC', 'MT', 'HT', 'LC']	['repetition / content', 'lexical cohesion']
3660	tation from a user?s speech. That is, it consists of automatic speech recognition (ASR) and language understanding (LU).	[[83, 86], [116, 118]]	[[53, 81], [92, 114]]	['ASR', 'LU']	['automatic speech recognition', 'language understanding']
3661	   Additionally, Mean Reciprocal Rank (MRR) is  also reported.	[[39, 42]]	[[17, 37]]	['MRR']	['Mean Reciprocal Rank']
3662	transmission envelope; the German and Portuguese services  are sent in the transmission enveloped esigned by the  International Press Telecommunications Council (IPTC). 	[[162, 166]]	[[114, 160]]	['IPTC']	['International Press Telecommunications Council']
3663	navigation3  SCAN was developed initially for the TREC-6  Spoken Document Retrieval (SDR) task, which em-  ploys the NIST/DARPA HUB4 Broadcast News cor- 	[[85, 88], [117, 127], [128, 132]]	[[58, 83], [50, 54]]	['SDR', 'NIST/DARPA', 'HUB4']	['Spoken Document Retrieval', 'TREC']
3664	Finite State Automata (FSA) have traditionally been used in speech processing, but they are clearly  inappropriate for spoken language systems. In this section, we contrast unification grammars (UGs) with  Context-Free grammars (CFGs) and discuss extensions needed for spoken language systems.	[[195, 198], [23, 26], [229, 233]]	[[173, 193], [0, 21], [206, 227]]	['UGs', 'FSA', 'CFGs']	['unification grammars', 'Finite State Automata', 'Context-Free grammars']
3665	Then, a supervised machine learning algorithm (e.g., Support Vector Machines  (SVM), na?ve Bayesian classifier (NB)) is applied  to the training examples to build a classifier that is 	[[79, 82], [112, 114]]	[[85, 99], [53, 76]]	['SVM', 'NB']	['na?ve Bayesian', 'Support Vector Machines']
3666	~968)). Le SN d~fini  es tmarqu~ par la presence du d~ter-  minati f  d~fini  (DEF), lequel peut ~tre anaphorique (ANAPH),  ou d~monstrat i f  (DEM), selon qu'i l  se r~f~re ~ l 'envi- 	[[115, 120], [11, 13], [79, 82], [144, 147]]	[[102, 113], [49, 76], [127, 141]]	['ANAPH', 'SN', 'DEF', 'DEM']	['anaphorique', 'du d~ter-  minati f  d~fini', 'd~monstrat i f']
3667	The  suggestions made here for the organization of space are only a working  set for which l i t t le  justification can be offered: location (LOC)--a  neutral statement of position, contact--in physical contact, and -* near 	[[143, 146]]	[[133, 141]]	['LOC']	['location']
3668	In the cross-validation process, Multinomial Naive Bayes (MNB) has shown better results than Support Vector Machines (SVM) as a component for AdaBoost.	[[118, 121], [58, 61]]	[[93, 116], [33, 56]]	['SVM', 'MNB']	['Support Vector Machines', 'Multinomial Naive Bayes']
3669	? ( S (NP (DT the) (NN man)) (VP (VBZ plays) (NP (DT the) (NN piano)))	[[34, 37], [7, 9], [11, 13], [20, 22], [30, 32], [46, 48], [50, 52], [59, 61]]	[[38, 43]]	['VBZ', 'NP', 'DT', 'NN', 'VP', 'NP', 'DT', 'NN']	['plays']
3670	la AR  TD  par PREP (prEposition)  main SUBS (substamif)  REC8 (rEcursif simple) 	[[40, 44], [3, 5], [7, 9], [15, 18], [58, 62]]	[[46, 55], [21, 32], [64, 72]]	['SUBS', 'AR', 'TD', 'PRE', 'REC8']	['substamif', 'prEposition', 'rEcursif']
3671	 1 Introduction Most Open Information Extraction (Open-IE) systems (Banko et al, 2007) extract textual relational	[[50, 57]]	[[21, 48]]	['Open-IE']	['Open Information Extraction']
3672	1 In t roduct ion   The main goal of the proposed project is to develop  a language model(LM) that uses syntactic structure. 	[[90, 92]]	[[75, 88]]	['LM']	['language mode']
3673	 The growing need to  manage and search large video collections presents  a challenge to traditional information retrieval (IR)  technologies.	[[124, 126]]	[[101, 122]]	['IR']	['information retrieval']
3674	 The second set of experiments was run on the Chinese Treebank (CTB) data sets from Bakeoff-3 (Levow, 2006), which contains a training and a test	[[64, 67]]	[[46, 62]]	['CTB']	['Chinese Treebank']
3675	Translation. In Proceedings of the 13th International  Conference on Computational Linguistics (COLING-90),  Vol.	[[96, 105]]	[[55, 94]]	['COLING-90']	['Conference on Computational Linguistics']
3676	3.4 Example  of  in tegrat ion   Figure 3 shows the starting point of an integra-  tion process with the trigger word (TW) lelter, its  definition, its temporary graph (TG), the concept 	[[119, 121], [169, 171]]	[[105, 117], [152, 167]]	['TW', 'TG']	['trigger word', 'temporary graph']
3677	~ ~ . ~  5  Object S t r i n g  (OBJL1,ST) Refercn'ce Guide. .............. 9 	[[33, 41]]	[[12, 30]]	['OBJL1,ST']	['Object S t r i n g']
3678	Nevertheless, LSI has known a resurging interest. Supervised Semantic Indexing (SSI) (Bai et al.,	[[80, 83], [14, 17]]	[[50, 78]]	['SSI', 'LSI']	['Supervised Semantic Indexing']
3679	we used two other error-annotated learner corpora.  The NUS Corpus of Learner English (NUCLE) contains one million words of academic writing	[[87, 92]]	[[56, 85]]	['NUCLE']	['NUS Corpus of Learner English']
3680	We evaluated our parsers using standard labeled accuracy scores (LAS) and unlabeled accuracy scores (UAS) excluding punctuation.	[[101, 104], [64, 68]]	[[74, 99], [40, 63]]	['UAS', '(LAS']	['unlabeled accuracy scores', 'labeled accuracy scores']
3681	MOR = morphological features (set) ? LEM = lemma ?	[[37, 40], [0, 3]]	[[43, 48], [6, 19]]	['LEM', 'MOR']	['lemma', 'morphological']
3682	proaches, namely Bisecting K-Means (Steinbach et al.,  2000), and Latent Semantic Analysis (LSA)-based document clustering (for short, LSA).	[[92, 95], [135, 138]]	[[66, 90]]	['LSA', 'LSA']	['Latent Semantic Analysis']
3683	produced by a transliteration system 1. Word Accuracy in Top-1 (ACC) Also known as Word Error Rate, it measures correct-	[[64, 67]]	[[45, 53]]	['ACC']	['Accuracy']
3684	sisted of 2000 features, representing the most frequent (head, subject) and (head, object) pairs in the British National Corpus (BNC). The feature-	[[129, 132]]	[[104, 127]]	['BNC']	['British National Corpus']
3685	tics.  Linguistic Data Consortium (LDC). 2013.	[[35, 38]]	[[7, 33]]	['LDC']	['Linguistic Data Consortium']
3686	NP NP\NP (S\NP )/NP NP< >NP S\NP <S Fruit flies like bananas NP S\NP (S\S)/NP NP< >S S\S <S (b) The search space in this work, with one node for each	[[64, 68], [0, 2], [3, 8], [61, 63], [25, 27], [28, 32]]	[[70, 77]]	['S\\NP', 'NP', 'NP\\NP', 'NP', 'NP', 'S\\NP']	['S\\S)/NP']
3687	For the bilingual corpus, we  used the BTEC and PIVOT data of IWSLT 2008,  HIT corpus 5  and other Chinese LDC (CLDC)                                                            	[[112, 116], [39, 43], [48, 53], [62, 67], [75, 78]]	[[99, 110]]	['CLDC', 'BTEC', 'PIVOT', 'IWSLT', 'HIT']	['Chinese LDC']
3688	entire sentence length.  SyntaxBased (SyntB): contextual features have been computed according to the ?	[[38, 43]]	[[25, 36]]	['SyntB']	['SyntaxBased']
3689	In l~,ooth's approach, the FSV is detined by re-  (:ursion on the truth conditional structure which  is itself derived from LF (i.e. Logical Form, the  Government and Binding level of semantic rep- 	[[124, 126], [27, 30]]	[[133, 145]]	['LF', 'FSV']	['Logical Form']
3690	SemEval 2012 competition for evaluating Natural  Language Processing (NLP) systems presents a  new task called Semantic Textual Similarity (STS)  (Agirre et al, 2012).	[[140, 143], [70, 73]]	[[111, 138], [40, 68]]	['STS', 'NLP']	['Semantic Textual Similarity', 'Natural  Language Processing']
3691	2. Definition of Stochastic Context-Free Grammars  We will now define stochastic ontext free grammars (SCFGs) and establish some  notation.	[[103, 108]]	[[70, 101]]	['SCFGs']	['stochastic ontext free grammars']
3692	 1 Introduction Verb Phrase Ellipsis (VPE) is an anaphoric construction in which a verbal constituent has been omitted.	[[38, 41]]	[[16, 36]]	['VPE']	['Verb Phrase Ellipsis']
3693	 In Proceedings of the International Conference on Language Resources and Evaluation (LREC). 	[[86, 90]]	[[51, 69]]	['LREC']	['Language Resources']
3694	and a segment relation are identified.  Topic break index (TBI) takes the value of 1 or 2: the boundary with TBI=2 is less con-	[[59, 62], [109, 112]]	[[40, 57]]	['TBI', 'TBI']	['Topic break index']
3695	rate words (PERFECT). Second, we let the parser introduce the EEs itself (INSERT). 	[[74, 80], [12, 19]]	[[48, 72]]	['INSERT', 'PERFECT']	['introduce the EEs itself']
3696	Final step in treebuilding.  The English Destressin8 Rule (EDR) is used to  determ/ne which vowels should be reduced.	[[59, 62]]	[[33, 57]]	['EDR']	['English Destressin8 Rule']
3697	 University of Texas at Austin Word sense disambiguation (WSD) is an old and important task in computational linguistics that still remains challenging, to machines as well as to human annotators.	[[58, 61]]	[[31, 56]]	['WSD']	['Word sense disambiguation']
3698	 2.3 Linear Programming A Linear Program (LP) is an optimization problem where a linear objective function is minimized (or maximized) under linear constraints.	[[42, 44]]	[[26, 40]]	['LP']	['Linear Program']
3699	229 2 Participation in STS-SEM2013  The Semantic Textual Similarity (STS) task consists of estimated the value of semantic similarity 	[[69, 72], [23, 34]]	[[40, 67]]	['STS', 'STS-SEM2013']	['Semantic Textual Similarity']
3700	report false positives. This can be quantified by the positive predictive value (PPV), or probability that a research finding is true.	[[81, 84]]	[[54, 79]]	['PPV']	['positive predictive value']
3701	man::n a::d Figure 2: Lexical Only Centered Tree (LOCT) be::v	[[50, 54]]	[[22, 48]]	['LOCT']	['Lexical Only Centered Tree']
3702	since it appears in the corpus with the six differ-  ent tags: CD (cardinal), DT (determiner), JJ (ad-  jective), NN (noun). NNP (proper noun) and VBP 	[[114, 116], [63, 65], [78, 80], [95, 97], [125, 128], [147, 150]]	[[118, 122], [67, 75], [82, 92], [104, 111], [130, 141]]	['NN', 'CD', 'DT', 'JJ', 'NNP', 'VBP']	['noun', 'cardinal', 'determiner', 'jective', 'proper noun']
3703	three things:  (1) a new formalism for logic grammars, which we  call modifier structure grammars (MSGs),  (2) an interpreter (or parser) for MSGs that takes all 	[[99, 103], [142, 146]]	[[70, 97]]	['MSGs', 'MSGs']	['modifier structure grammars']
3704	 A stack based extraction Algorithm 1 was designed to extract a context free grammar (CFG) from the URDU.KON-TB treebank.	[[86, 89], [100, 111]]	[[64, 84]]	['CFG', 'URDU.KON-TB']	['context free grammar']
3705	Lack of Orientation (LO). If there is at least one obstacle of the former, more serious kind, we will speak of Dead End (DE). For example, in the case of the DE Exam-	[[121, 123], [20, 23], [158, 160]]	[[111, 119], [0, 19]]	['DE', '(LO', 'DE']	['Dead End', 'Lack of Orientation']
3706	1 Introduction We extend a popular model, latent Dirichlet al location (LDA), to unbounded streams of documents.	[[72, 75]]	[[42, 70]]	['LDA']	['latent Dirichlet al location']
3707	rejected by the space-saving algorithm.  Least Recently Used (LRU) When the volume of flow in a text stream rapidly increases, it is likely to relate to a burst of a certain topic.	[[62, 65]]	[[41, 60]]	['LRU']	['Least Recently Used']
3708	RERANKED 56.2 13.5 57.3 12.7 ORACLE 85.0 70.3 80.4 60.0 Table 2: Word accuracies and error rate reductions (ERR) in percentages for English-to-Japanese MTL augmented	[[108, 111], [152, 155]]	[[85, 106]]	['ERR', 'MTL']	['error rate reductions']
3709	4 Hierarchic Autoepistemic  Logic  Autoepistemic (AE) logic was developed by Moore  \[I0\] as a reconstruction of McDermott's nonmono- 	[[50, 52]]	[[35, 48]]	['AE']	['Autoepistemic']
3710	minimal human SO annotation Table 2: Key details of semantic orientation (SO) lexicons. ASL = affix seeds lexicon, GI = General Inquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN =	[[88, 91], [74, 76], [115, 117], [138, 142], [185, 188], [218, 221]]	[[94, 113], [52, 72], [120, 136], [145, 183], [191, 216]]	['ASL', 'SO', 'GI', 'MSOL', 'PSL', 'SWN']	['affix seeds lexicon', 'semantic orientation', 'General Inquirer', 'Macquarie semantic orientation lexicon', 'Pitt subjectivity lexicon']
3711	This representation uses the logical formulation of feature structures  as given by Kasper and Rounds (1986) and Johnson (1988) and is similar in approach  to the logical formulation of Functional Unification Grammar (FUG) given by Rounds  and Manaster-Ramer (1987).	[[218, 221]]	[[186, 216]]	['FUG']	['Functional Unification Grammar']
3712	 2 Forced Derivation Tree for SMT A forced derivation tree (FDT) of a sentence pair {f, e} can be defined as a pair G =< D,A >:	[[60, 63], [30, 33]]	[[36, 58]]	['FDT', 'SMT']	['forced derivation tree']
3713	4 Variational vs. Min-Risk Decoding In place of the MAP decoding, another commonly used decision rule is minimum Bayes risk (MBR): y?	[[125, 128], [18, 26], [52, 55]]	[[105, 123]]	['MBR', 'Min-Risk', 'MAP']	['minimum Bayes risk']
3714	ed AIC   a) Dialogue ActsOnly (DAONLY)    N (number of hidden states) 	[[31, 37], [3, 6]]	[[12, 29]]	['DAONLY', 'AIC']	['Dialogue ActsOnly']
3715	To v e r i f y  th'e r e l a t i o n s h i p s  between the s t a t i s t i c a l  models of word  importance and t h s  vector space model, dcsument c o l l e c t i o n s  are used i n  three  d i f f e r e n t  subjec t  a reas ,  including aerodynamics (cRAN),  medicine (MED) and  world a f f a i r s  (TIME).	[[275, 278], [257, 261]]	[[265, 273]]	['MED', 'cRAN']	['medicine']
3716	 An alternative paradigm is to view error correction as a statistical machine translation (SMT) problem from ?	[[91, 94]]	[[58, 89]]	['SMT']	['statistical machine translation']
3717	DO: parent:number := node:number; parent:gender := node:gender; 3.1.5 Preposition without children (PrepNoCh) In our dependency trees, the preposition is the	[[100, 108], [0, 2]]	[[70, 98]]	['PrepNoCh', 'DO']	['Preposition without children']
3718	 1 Introduction Tree substitution grammar (TSG) is a promising formalism for modeling language data.	[[43, 46]]	[[16, 41]]	['TSG']	['Tree substitution grammar']
3719	X:?  TypeChanging (TCR) X:? ? Y:?(?)	[[19, 22]]	[[5, 17]]	['TCR']	['TypeChanging']
3720	 2 HMM transitions can be modeled using Weighted Finite State Automata (WFSAs), corresponding to regular expressions.	[[72, 77], [3, 6]]	[[40, 70]]	['WFSAs', 'HMM']	['Weighted Finite State Automata']
3721	S i 3.3.3 Method 3: TrueSkill (TS) TrueSkill is an adaptive, online system that em-	[[31, 33]]	[[20, 29]]	['TS']	['TrueSkill']
3722	"  OBST(obstacle): The boy stumbled over a stumb.    INTT (intent): He came there to look for Jane. "	[[52, 56], [2, 6]]	[[58, 64], [7, 15]]	['INTT', 'OBST']	['intent', 'obstacle']
3723	Classification, Word Sense Disambiguation, etc. In  Natural Language Processing (NLP), one of the  most used resources for WSD and other tasks is 	[[81, 84], [123, 126]]	[[52, 79]]	['NLP', 'WSD']	['Natural Language Processing']
3724	II: irrelevant input due to ASR errors or noise.  We adopt logistic regression (LR)-based dialogue act tagging approach (Tur et al.,	[[80, 82], [28, 31]]	[[59, 78]]	['LR', 'ASR']	['logistic regression']
3725	ysis incorporating social networks. In Proceedings of Knowledge Discovery and Data Mining (KDD). 	[[91, 94]]	[[54, 82]]	['KDD']	['Knowledge Discovery and Data']
3726	means, correct  sentences could then be  computat iona l ly  generated from the lo-  g ical  patters  ment ioned  in (II). 	[]	[]	[]	[]
3727	with their scope and corresponding negated events is an important task that could benefit other natural language processing (NLP) tasks such as extraction of factual information	[[125, 128]]	[[96, 123]]	['NLP']	['natural language processing']
3728	jective U-shaped is an example of gesture enriching an adjectival meaning through the interface default Adjective meaning extended (AdjMExt) AdjMExt: Adjective(u), sem(u) is ?	[[132, 139], [141, 148]]	[[104, 130]]	['AdjMExt', 'AdjMExt']	['Adjective meaning extended']
3729	2. We propose a new topic model, Topic Sentiment Latent Dirichlet Allocation (TSLDA), which can capture the topic and sentiment si-	[[78, 83]]	[[33, 76]]	['TSLDA']	['Topic Sentiment Latent Dirichlet Allocation']
3730	minimal set of defaults.  A Preferential Default Description Logic (PDDL)  based on weigthed defaults has been developed in 	[[68, 72]]	[[28, 66]]	['PDDL']	['Preferential Default Description Logic']
3731	attempts to use synchronous parsing to produce the  tree structure of both source language (SL) and  target language (TL) simultaneously, most SSMT  approaches make use of monolingual parser to 	[[118, 120], [92, 94], [143, 147]]	[[101, 116], [75, 90]]	['TL', 'SL', 'SSMT']	['target language', 'source language']
3732	In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), pages 1262?1267, Genoa, Italy, May.	[[89, 93]]	[[54, 87]]	['LREC']	['Language Resources and Evaluation']
3733	vector built from training set. However, this  could lead to many out of vocabulary (OOV)  cases, in addition to long vector.	[[85, 88]]	[[66, 83]]	['OOV']	['out of vocabulary']
3734	to such an extent that?)  Multiword interjections (MWI) are a small category with expressions such as mille sabords (?	[[51, 54]]	[[26, 49]]	['MWI']	['Multiword interjections']
3735	1 Int roduct ion   In a recent paper Boguraev and Levin (1990) point out inadequacies in common concep-  tions of what a Lexical Knowledge Base (LKB) should be, inadequacies which stem from  the assumption that a machine-readable dictionary (MRD) is not only the right source for 	[[145, 148]]	[[121, 143]]	['LKB']	['Lexical Knowledge Base']
3736	 It is a Neo-Reichenbachian representation (Reichenbach,  1966) in that its s imple tense s t ructures  (STSs) re-  late the following three entities: the time of the event 	[[105, 109]]	[[76, 102]]	['STSs']	['s imple tense s t ructures']
3737	 c?2015 Association for Computational Linguistics Building a Scientific Concept Hierarchy Database (SCHBASE) Eytan Adar	[[100, 107]]	[[61, 98]]	['SCHBASE']	['Scientific Concept Hierarchy Database']
3738	for more general audience of users.  Helping our own (HOO) is an initiative that could in future spark a new interest in the re-	[[54, 57]]	[[37, 52]]	['HOO']	['Helping our own']
3739	 In a second experiment, we used the original TGRD corpus but added the language variety (LV) (i.e., MSA and DA) features.	[[90, 92], [46, 50], [101, 104], [109, 111]]	[[72, 88]]	['LV', 'TGRD', 'MSA', 'DA']	['language variety']
3740	Abstract   In recent years, statistical approaches on  ATR (Automatic Term Recognition) have  achieved good results.	[[55, 58]]	[[60, 86]]	['ATR']	['Automatic Term Recognition']
3741	wk} be its vocabulary. In the monolingual settings, the Vector Space Model (VSM) is a k-dimensional space Rk, in which the text tj ?	[[76, 79], [106, 108]]	[[56, 74]]	['VSM', 'Rk']	['Vector Space Model']
3742	The starting point for the approach followed here was a dissatisfaction with certain  aspects of the theory of quasi-logical form as described in Alshawi (1990, 1992), and  implemented in SRI's Core Language Engine (CLE). In the CLE-QLF approach, as ra- 	[[216, 219], [188, 192], [229, 236]]	[[194, 214]]	"['CLE', ""SRI'"", 'CLE-QLF']"	['Core Language Engine']
3743	, VP ...... Others (OTHER): The remaining cases of comma receive the OTHER label, indicating they do	[[20, 25]]	[[12, 18]]	['OTHER']	['Others']
3744	2014).  The RST Discourse Treebank (RST-DT) (Carlson et al.,	[[36, 42]]	[[12, 34]]	['RST-DT']	['RST Discourse Treebank']
3745	resent horizontal movement of the eyebrows.  2.2 Continuous Profile Models (CPM) Continuous Profile Model (CPM) aligns a set	[[76, 79], [107, 110]]	[[49, 74], [81, 105]]	['CPM', 'CPM']	['Continuous Profile Models', 'Continuous Profile Model']
3746	annotations of sentiment values for individual syntactic phrases in a binarized tree, and an approach based on recursive neural tensor networks (RNTN) which yields significant improvements over the ear-	[[145, 149]]	[[111, 143]]	['RNTN']	['recursive neural tensor networks']
3747	used throughout the paper. The notation is that  used in the Core Language Engine (CLE) devel-  oped by SKI's Cambridge Computer Science Re- 	[[83, 86], [104, 107]]	[[61, 81]]	['CLE', 'SKI']	['Core Language Engine']
3748	We show that the best  prediction of translation complexity is given by the  average number of syllables per word (ASW). The 	[[115, 118]]	[[77, 113]]	['ASW']	['average number of syllables per word']
3749	5 Conclusion In this pilot experiment, we explore the possibility of using Amazon Mechanical Turk (MTurk) to collect bilingual word alignment data to assist automatic word align-	[[99, 104]]	[[82, 97]]	['MTurk']	['Mechanical Turk']
3750	representation. Thus, it is necessary to define a proper ECS for each  language: Japanese ECS (J-ECS) \[6\] for Japanese language and  English ECS (E-ECS) \[7\] for English language.	[[95, 100], [57, 60], [148, 153]]	[[81, 93], [135, 146]]	['J-ECS', 'ECS', 'E-ECS']	['Japanese ECS', 'English ECS']
3751	   In the SUM (Summarization) setting, the  entailment pairs were generated using two proce-	[[10, 13]]	[[15, 28]]	['SUM']	['Summarization']
3752	ability and to give analytical insights into the  features. Classification Accuracy (CAcc), the  percentage of the correctly labeled instances over 	[[85, 89]]	[[60, 83]]	['CAcc']	['Classification Accuracy']
3753	 3.1 Underspecified domains An underspecified domain (UD) represents a partially specified reference domain corresponding to the	[[54, 56]]	[[31, 52]]	['UD']	['underspecified domain']
3754	ers. In Proceedings of the 19th International Conference on Compuatational Linguistics (COLING),  pages 556?562.	[[88, 94]]	[[60, 86]]	['COLING']	['Compuatational Linguistics']
3755	Due to the existence of CTB-I, we were able to train new automatic Chinese language processing (CLP) tools, which crucially use annotated corpora as training	[[96, 99], [24, 29]]	[[67, 94]]	['CLP', 'CTB-I']	['Chinese language processing']
3756	explicitly addresses the language ambiguity issue. Key to our approach is the use of Word Sense Induction (WSI), that is, techniques aimed at automatically discovering the different meanings of a given term (i.e., query).	[[107, 110]]	[[85, 105]]	['WSI']	['Word Sense Induction']
3757	NE = Named Entity CE = Correlated Entity EP = Entity Profile SVO = Subject-Verb-Object	[[41, 43], [0, 2], [18, 20], [61, 64]]	[[46, 60], [5, 17], [23, 40], [67, 86]]	['EP', 'NE', 'CE', 'SVO']	['Entity Profile', 'Named Entity', 'Correlated Entity', 'Subject-Verb-Object']
3758	2-21. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conver-	[[69, 73]]	[[47, 67]]	['CTB4']	['Chinese Treebank 4.0']
3759	verbs. They adopted the Nearest Neighbor (NN) and Information Bottleneck (IB) methods for clustering.	[[74, 76], [42, 44]]	[[50, 72], [24, 40]]	['IB', 'NN']	['Information Bottleneck', 'Nearest Neighbor']
3760	though domain sublanguages are characterized by specific vocabularies, a well-defined border between specific sublanguages (SLs) and general language (GL) vocabularies is difficult to establish	[[124, 127], [151, 153]]	[[110, 122], [133, 149]]	['SLs', 'GL']	['sublanguages', 'general language']
3761	 1 Introduction The Semantic Textual Similarity (STS) shared task consists of several data sets of paired passages of	[[49, 52]]	[[20, 47]]	['STS']	['Semantic Textual Similarity']
3762	1 Introduction Kernel methods are considered the most effective techniques for various relation extraction (RE) tasks on both general (e.g. newspaper text) and	[[108, 110]]	[[87, 106]]	['RE']	['relation extraction']
3763	Haile) and (?????, Selassie). TM has been shown to be effective in several Information Retrieval (IR) and Natural Language Processing (NLP) applications. For example, in cross language IR, TM was used to handle out-of-vocabulary query words by mining transliterations between words in queries and top n retrieved documents and then using transliterations to expand queries (Udupa et al, 2009a).	[[98, 100], [135, 138], [185, 188], [189, 191], [30, 32]]	[[75, 96], [106, 133]]	['IR', 'NLP', 'IR,', 'TM', 'TM']	['Information Retrieval', 'Natural Language Processing']
3764	referred to as Maximum Mutual Information Estimation (MMIE) and the second component Maximum Likelihood Estimation (MLE), therefore in this paper we use a brief notation for (1) just for conve-	[[116, 119]]	[[89, 114]]	['MLE']	['mum Likelihood Estimation']
3765	include: 1) candidate frequency and its distribution in different Web pages, 2) length ratio between source terms and target candidates (S-T), 3)  distance between S-T, and 4) keywords, key 	[[137, 140], [164, 167]]	[[101, 135]]	['S-T', 'S-T']	['source terms and target candidates']
3766	process (PR)  quantity (QU)  relation (RE)  shape (SH) 	[[39, 41], [9, 11], [24, 26], [51, 53]]	[[29, 37], [0, 7], [14, 22], [44, 49]]	['RE', 'PR', 'QU', 'SH']	['relation', 'process', 'quantity', 'shape']
3767	As a result, a text to text similarity measure is built based on word to word similarity. It is as-sumed that combining word to word similarity metrics to text level would be a good indicator of text to text similarity. 4 Sagan for MT evaluation Sagan for MT evaluation is based on a core devel-opment to approach the Semantic Textual Similari-ty task(STS). The pilot task STS was recently defined in Semeval 2012 (Aguirre et al, 2012) and has as main objective measuring the degree of semantic equivalence between two text fragments.	[[352, 355], [232, 234], [256, 258], [373, 376]]	[[318, 346]]	['STS', 'MT', 'MT', 'STS']	['Semantic Textual Similari-ty']
3768	There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG). Paraphrase	[[116, 118]]	[[93, 114]]	['PG']	['paraphrase generation']
3769	 2 Tutorial Dialogue Setting and Data My Science Tutor (MyST) (Ward et al, 2010) is a conversational virtual tutor designed to improve science learning and understanding for students in grades 3-5.	[[56, 60]]	[[38, 54]]	['MyST']	['My Science Tutor']
3770	tion). All markables have named entity types such as FACILITY, GPE (geopolitical entity), PERSON, LOCATION, ORGANIZATION, PERSON, VEHI-	[[63, 66]]	[[68, 87]]	['GPE']	['geopolitical entity']
3771	of money to perform tasks that are simple for humans but difficult for computers. Examples of these Human Intelligence Tasks (HITs) range from labeling images to moderating blog comments to providing feedback on the relevance of results for a search query.	[[126, 130]]	[[100, 124]]	['HITs']	['Human Intelligence Tasks']
3772	 1 In t roduct ion :  a problem  Grammar development environments (GDE's) for  analysis and for generation have not yet come to- 	[[67, 72]]	[[33, 65]]	"[""GDE's""]"	['Grammar development environments']
3773	natural language utterances. It accesses a database typi-  cal for the information retrieval task (CORDIS). 	[[99, 105]]	[[59, 97]]	['CORDIS']	['cal for the information retrieval task']
3774	as automatic speech recognition (ASR), natural language understanding (NLU), dialogue management (DM), natural language generation (NLG), and speech synthesis (TTS).	[[132, 135], [33, 36], [71, 74], [98, 100], [160, 163]]	[[103, 130], [3, 31], [39, 69], [77, 96]]	['NLG', 'ASR', 'NLU', 'DM', 'TTS']	['natural language generation', 'automatic speech recognition', 'natural language understanding', 'dialogue management']
3775	posterior distribution.  We utilize maximum entropy (MaxEnt) model  (Berger et al, 1996) to design the basic classifier 	[[53, 59]]	[[36, 51]]	['MaxEnt']	['maximum entropy']
3776	 Below we focus on a special case of the latter problem: noun compound (NC) coordination. Con-	[[72, 74]]	[[57, 70]]	['NC']	['noun compound']
3777	 [and, therefore, so]  Contrastive Connectives (CC)  men den ?	[[48, 50]]	[[23, 46]]	['CC']	['Contrastive Connectives']
3778	Note that the proponents of the BootCaT method seem to acknowledge this evolution, see for example Marco Baroni?s talk at this year?s BootCaTters of the world unite (BOTWU) workshop: ?	[[166, 171], [32, 39]]	[[134, 164]]	['BOTWU', 'BootCaT']	['BootCaTters of the world unite']
3779	Method (METH) the methods used Result (RES) the results achieved Conclusion (CON) the authors? conclusions	[[77, 80], [8, 12], [39, 42]]	[[65, 75], [0, 6], [31, 37]]	['CON', 'METH', 'RES']	['Conclusion', 'Method', 'Result']
3780	294  The surface phonetic tones are:  LC = low constant (in Baule, only initial)  HC = high constant (only initial) 	[[38, 40], [82, 84]]	[[43, 55], [87, 100]]	['LC', 'HC']	['low constant', 'high constant']
3781	many competing approaches to tagging problems including Hidden Markov Models (HMMs), Maximum Entropy Markov Models (MEMMs) and Conditional Random Fields (CRFs).	[[116, 121], [78, 82], [154, 158]]	[[89, 114], [56, 76], [127, 152]]	['MEMMs', 'HMMs', 'CRFs']	['mum Entropy Markov Models', 'Hidden Markov Models', 'Conditional Random Fields']
3782	cation which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).	[[92, 94], [124, 128]]	[[80, 90], [100, 123]]	['NB', 'SVMs']	['Naive Baye', 'Support Vector Machines']
3783	The approach involved training a standard Hidden Markov Model (HMM) using the Expectation Maximization (EM) algorithm (Dempster et al.,	[[104, 106], [63, 66]]	[[78, 102], [42, 61]]	['EM', 'HMM']	['Expectation Maximization', 'Hidden Markov Model']
3784	NIST-06. The bilingual training corpus comes from Linguistic Data Consortium (LDC)6, which consists of 3.4M sentence pairs with 64M/70M Chi-	[[78, 81], [0, 7]]	[[50, 76]]	['LDC', 'NIST-06']	['Linguistic Data Consortium']
3785	 1 Introduction Quality Estimation (QE) for Machine Translation (MT) involves judging the correctness of the output of an MT system given an input and no reference translation (Blatz et al.,	[[36, 38], [65, 67], [122, 124]]	[[16, 34], [44, 63]]	['QE', 'MT', 'MT']	['Quality Estimation', 'Machine Translation']
3786	pondence between proofs and dependency structures.  Dependency grammar (DG) takes as fundamental  ~This approach of 'normal form parsing' has been 	[[72, 74]]	[[52, 70]]	['DG']	['Dependency grammar']
3787	"@""' itself is counted. Another way is to count how  many parts of words (PWs) are eontMnmd in the  SA."	[[73, 76], [99, 101]]	[[57, 71]]	['PWs', 'SA']	['parts of words']
3788	 CTexT. 2005. CKarma (C5 KompositumAnaliseerder vir Robuuste Morfologiese Analise). [ C5 Compound	[[14, 20], [1, 6]]	[[22, 47]]	['CKarma', 'CTexT']	['C5 KompositumAnaliseerder']
3789	Source Material (SMaterial) e.g., As, InGaAs   ? Source Material Characteristic  (SMChar) e.g. ,(111)B  	[[82, 88], [17, 26], [38, 44]]	[[49, 79], [0, 15]]	['SMChar', 'SMaterial', 'InGaAs']	['Source Material Characteristic', 'Source Material']
3790	namely person name, location name, organization name and miscellaneous name to apply  Support Vector Machine (SVM) based machine  learning technique.	[[110, 113]]	[[86, 108]]	['SVM']	['Support Vector Machine']
3791	standing that shares tasks with OIE. AMR parsing (Banarescu et al, ), semantic role labeling (SRL) (Toutanova et al, 2008; Punyakanok et al, 2008)	[[94, 97], [37, 40], [32, 35]]	[[70, 92]]	['SRL', 'AMR', 'OIE']	['semantic role labeling']
3792	It  is at this critical point, when care is being trans-  ferred from the Operating Room (OR) to the ICU  and monitoring is at a minimum, that the pa- 	[[90, 92], [101, 104]]	[[74, 88]]	['OR', 'ICU']	['Operating Room']
3793	As the entire sentence is informative to determine the proper conjunction of all roles, a Smoothed Partial Tree Kernel (SPTK) within the classifier that enhances both syntactic and lexical in-	[[120, 124]]	[[90, 118]]	['SPTK']	['Smoothed Partial Tree Kernel']
3794	The input to the first block are the words of the TEXTCHUNK, represented by CW (Collobert and Weston, 2008) embeddings.	[[76, 78], [50, 59]]	[[80, 100]]	['CW', 'TEXTCHUNK']	['Collobert and Weston']
3795	conjuncts depend on it. Nilsson et al (2007)  advocate the Mel?cuk style (MS) for parsing  Czech, taking the first conjunct as the head, 	[[74, 76]]	[[59, 72]]	['MS']	['Mel?cuk style']
3796	The data are packaged as the payload of incremental units (IU) which are passed between modules. The IUs themselves are also interconnected via so-called same level links (SLL) and groundedin links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that	[[172, 175], [59, 61], [101, 104], [199, 203], [241, 244]]	[[154, 170], [40, 57], [181, 197]]	['SLL', 'IU', 'IUs', 'GRIN', 'IUs']	['same level links', 'incremental units', 'groundedin links']
3797	142 ? National University of Mongolia (NUM),  Mongolia 	[[39, 42]]	[[6, 37]]	['NUM']	['National University of Mongolia']
3798	LPM output using application knowledge  ? Function Generator Module (FGM) converting  SAM output into executable function calls 	[[69, 72], [86, 89], [0, 3]]	[[42, 67]]	['FGM', 'SAM', 'LPM']	['Function Generator Module']
3799	respect to phonology, morphology, syntax and the lexicon. Linguistic resources (lexica, corpora) and natural language processing (NLP) tools for such dialects (parsers) are very rare. 	[[130, 133]]	[[101, 128]]	['NLP']	['natural language processing']
3800	Lexical features show a much more mixed result. Type?Token Ratio (TTR) is only important for document classification, whereas most of the	[[66, 69]]	[[48, 64]]	['TTR']	['Type?Token Ratio']
3801	AVERAGE 70.8% 70.1% Table 2: Ten-fold cross-validation classification results, using a Na??ve Bayes (NB) or Support Vector Machines (SVM) classifier	[[101, 103], [133, 136]]	[[87, 99], [108, 131]]	['NB', 'SVM']	['Na??ve Bayes', 'Support Vector Machines']
3802	Ravi and Knight (2008) solve 1:1 substitution ciphers optimally by formulating the decipherment problem as an integer linear program (ILP) while Corlett and Penn (2010) solve the problem using	[[134, 137]]	[[110, 132]]	['ILP']	['integer linear program']
3803	is re-written as NN, and NNPS as NNP.  Parent (PP) The category of the parent node of the NP. 	[[47, 49], [17, 19], [25, 29], [33, 36], [90, 92]]	[[39, 45]]	['PP', 'NN', 'NNPS', 'NNP', 'NP']	['Parent']
3804	subset by eliminating the redundant features.  In this paper, Rough Set Theory (RST) based  feature selection method is applied for sen-	[[80, 83]]	[[62, 78]]	['RST']	['Rough Set Theory']
3805	Model level, there is an intermediate stage of analysis , characterized by a  formal language , resp r  - The Engliaboriented Formal Language ( EFL) , which containa  constant^ that  correspond to the terms of English, This language is wed to represent the 	[[144, 147]]	[[110, 141]]	['EFL']	['Engliaboriented Formal Language']
3806	This  was a motivating factor for the establishment of the  Common Pattern Specification Language (CPSL)  Working Group devoted to formulating a CPSL in 	[[99, 103], [145, 149]]	[[60, 97]]	['CPSL', 'CPSL']	['Common Pattern Specification Language']
3807	CFG. The proof is based on a lexicalization procedure related to the lexicalization  procedure used to create Greibach normal form (GNF) as presented in Harrison 1978. 	[[132, 135], [0, 3]]	[[110, 130]]	['GNF', 'CFG']	['Greibach normal form']
3808	kitchen cabinet and will hardly be able to win the elections]. The parse tree contains phrase labels NP (Noun Phrase), PP (Prepositional Phrase), VP (Verb Phrase), S (Sentence), and CS (Coordinated Sentence).	[[119, 121], [146, 148]]	[[123, 143], [150, 161]]	['PP', 'VP']	['Prepositional Phrase', 'Verb Phrase']
3809	inference of lexical semantic roles After the training phase, a testing procedure using the Markov Chain Monte Carlo (MCMC) inference engine can be used to infer role labels.	[[118, 122]]	[[92, 116]]	['MCMC']	['Markov Chain Monte Carlo']
3810	segmentation While the model structure is reminiscent of a factorial hidden Markov model (HMM), there are important differences that prevent the direct application of	[[90, 93]]	[[69, 88]]	['HMM']	['hidden Markov model']
3811	KEA: Practical automatic keyphrase  extraction. Proceedings of Digital Libraries 99 (DL'99), pp. 	[[85, 90], [0, 3], [93, 95]]	[[63, 83]]	"[""DL'99"", 'KEA', 'pp']"	['Digital Libraries 99']
3812	and Chinese (ZH). The second section gives the result for the English (EN) test set, PTB Section 23. 	[[71, 73], [13, 15], [85, 88]]	[[62, 69], [4, 11]]	['EN', 'ZH', 'PTB']	['English', 'Chinese']
3813	figure 5).  Base clauses (BC)  are subclauses of type sub-  junctive and subordinate.	[[26, 28]]	[[12, 24]]	['BC']	['Base clauses']
3814	 enard Centre de Recherche Informatique de Montr?eal (CRIM) Montr?eal, QC, Canada	[[54, 58], [71, 73]]	[[7, 52]]	['CRIM', 'QC']	['Centre de Recherche Informatique de Montr?eal']
3815	 Table 10: Average value of the mutual infor-  mation (MI) of compound noun seeds  .Number of elements \[ 2 I 3 	[[55, 57]]	[[47, 53]]	['MI']	['mation']
3816	(iv) Embedded appositional phrases.  (2) Very long PE's (Phrasal Elements) appear occasionally. ( eg.	[[51, 55]]	[[57, 73]]	"[""PE's""]"	['Phrasal Elements']
3817	The next four columns show the number of true positives (TP)--verbs judged +S both  by machine and by hand; false positives (FP)--verbs judged +S by machine, -S  by  hand; true negatives (TN)--verbs judged -S  both by machine and by hand; and false  negatives (FN)--verbs judged -S  by machine, +S by hand.	[[188, 190], [57, 59], [125, 127], [261, 263]]	[[172, 186], [41, 54], [108, 122], [243, 258]]	['TN', 'TP', 'FP', 'FN']	['true negatives', 'true positive', 'false positive', 'false  negative']
3818	prior polarity of verb, verb score (V_score).  Verb-PP (prepositional phrase) rules:  1.	[[52, 54]]	[[56, 76]]	['PP']	['prepositional phrase']
3819	In: Ernst Buch-berger (ed.): Tagungsband der 7. Konferenz zur Verarbeitung nat?rlicher Sprache (KONVENS), Universit?t Wien, 161?168. Strube, Gerhard (1984).	[[96, 103]]	[[48, 94]]	['KONVENS']	['Konferenz zur Verarbeitung nat?rlicher Sprache']
3820	  2 Dimensionality Reduction   VSM (Vector Space Model) is a basic technique  to transform text documents to numeric vectors.	[[31, 34]]	[[36, 54]]	['VSM']	['Vector Space Model']
3821	words. In Proceedings of the International Conference on Computational Linguistics (COLING). 	[[84, 90]]	[[57, 82]]	['COLING']	['Computational Linguistics']
3822	CC = coordinating conjunction; CD = cardinal number; JJ = adjective; MD = modal; NN = singular noun; NNP = proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB = base form verb; VBD = past tense verb; VBZ = third-person singular present verb.	[[179, 181], [0, 2], [31, 33], [53, 55], [69, 71], [81, 83], [101, 104], [120, 124], [147, 150], [166, 168], [188, 190], [209, 212], [232, 235]]	[[184, 186], [5, 29], [36, 44], [58, 67], [74, 79], [95, 99], [107, 118], [127, 145], [153, 164], [171, 177], [203, 207], [215, 230], [238, 273]]	['TO', 'CC', 'CD', 'JJ', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'RB', 'VB', 'VBD', 'VBZ']	['to', 'coordinating conjunction', 'cardinal', 'adjective', 'modal', 'noun', 'proper noun', 'plural proper noun', 'plural noun', 'adverb', 'verb', 'past tense verb', 'third-person singular present verb.']
3823	2003). Recently, Huang et al (2015) showed that building a conditional random field (CRF) layer on top of bidirectional LSTM-RNNs performs com-	[[85, 88], [120, 129]]	[[59, 83]]	['CRF', 'LSTM-RNNs']	['conditional random field']
3824	 1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly	[[47, 49]]	[[27, 45]]	['QA']	['Question Answering']
3825	is a very laborious and costly process. Silver standard corpus (SSC) annotation is a very recent direction of corpus development which	[[64, 67]]	[[40, 62]]	['SSC']	['Silver standard corpus']
3826	the boys) or part of a complex coordinating conjunction (both boys and girls), the Penn  Treebank tags both differently in each of these syntactic ontexts--as PDT (predeter-  miner), RB (adverb), NNS (plural common noun) and coordinating conjunction (CC),  respectively.	[[251, 253], [159, 162], [183, 185], [196, 199]]	[[225, 249], [164, 173]]	['CC', 'PDT', 'RB', 'NNS']	['coordinating conjunction', 'predeter-']
3827	noun modification? which generally is shown in the form of a Noun phrase (NP) [A DE B]. A in-	[[74, 76]]	[[61, 72]]	['NP']	['Noun phrase']
3828	tion), and thus mitigating the overfitting problem.  A Dirichlet process (DP) prior is typically used to achieve this interplay.	[[74, 76]]	[[55, 72]]	['DP']	['Dirichlet process']
3829	We performed a 10-fold cross-validation on each dataset and experimented with three feature sets by using a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995).	[[132, 135]]	[[108, 130]]	['SVM']	['Support Vector Machine']
3830	potential can be fully exploited by our convolution partial tree kernel: - Dependency Words (DW) tree is similar to PET adapted for dependency tree constituted	[[93, 95], [116, 119]]	[[75, 91]]	['DW', 'PET']	['Dependency Words']
3831	 1 Introduction Currently the Machine Translation (MT) research community attempts to seamlessly integrate both	[[51, 53]]	[[30, 49]]	['MT']	['Machine Translation']
3832	+ b) (4) where f is a non-linear activation function such as rectified linear unit (ReLu) or sigmoid function. 	[[84, 88]]	[[61, 82]]	['ReLu']	['rectified linear unit']
3833	PCFG = Probabilistic  Context-Free Grammar, LM = Bigram Model with Witten-Bell smoothing,  PM = Priority Model. 	[[91, 93], [0, 4], [44, 46]]	[[96, 110], [7, 42], [49, 61]]	['PM', 'PCFG', 'LM']	['Priority Model', 'Probabilistic  Context-Free Grammar', 'Bigram Model']
3834	4.1 The NIST evaluation scheme The National Institute of Science and Technology (NIST) proposed an evaluation scheme that looks at the following properties when	[[81, 85], [8, 12]]	[[35, 79]]	['NIST', 'NIST']	['National Institute of Science and Technology']
3835	 6. Decision Tree (DT) - with 12,782 MWEs of D5.	[[19, 21]]	[[4, 17]]	['DT']	['Decision Tree']
3836	 4.4 Tokenizing Multiword Expressions      Multiword Expressions (MWEs) are two or  more words that behave like a single word syntac-	[[66, 70]]	[[43, 64]]	['MWEs']	['Multiword Expressions']
3837	VBL (Light Verb) is used in complex predicates (Butt 1995), but its syntactic similarity with  VB (Verb) is a major source of confusion in automatic tagging.	[[95, 97], [0, 3]]	[[99, 103], [5, 15]]	['VB', 'VBL']	['Verb', 'Light Verb']
3838	 Since we planned to eventually test our algorithms in  word recognition on the Resource Management (RM)  database, our phone classification experiments were also 	[[101, 103]]	[[80, 99]]	['RM']	['Resource Management']
3839	We present an open source, freely available Java implementation of Align, Disambiguate, and Walk (ADW), a state-of-the-art approach for measuring semantic similarity based on	[[98, 101]]	[[67, 96]]	['ADW']	['Align, Disambiguate, and Walk']
3840	suffix and prefix information, as well as information about the sorrounding words and their tags are used to develop a Maximum Entropy (MaxEnt) based Hindi NER system.	[[136, 142], [156, 159]]	[[119, 134]]	['MaxEnt', 'NER']	['Maximum Entropy']
3841	which fixes its results after a given time ? and report the corresponding word error rate (WER). This	[[91, 94]]	[[74, 89]]	['WER']	['word error rate']
3842	set of pivots from a given pair of domains such as the minimum frequency of occurrence of a feature in the two domains, mutual information (MI), and the entropy of the feature distribution over the	[[140, 142]]	[[120, 138]]	['MI']	['mutual information']
3843	Abstract  Since statistical machine translation (SMT)  and translation memory (TM) complement  each other in matched and unmatched regions, 	[[79, 81], [49, 52]]	[[59, 77], [16, 47]]	['TM', 'SMT']	['translation memory', 'statistical machine translation']
3844	 In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain.	[[74, 77], [85, 87]]	[[53, 72]]	['HMM', 'NE']	['Hidden Markov Model']
3845	Senior Researcher and Lecturer Knowledge Management Group Applied Computer Science Institute (AIFB) University of Karlsruhe, Germany	[[94, 98]]	[[58, 92]]	['AIFB']	['Applied Computer Science Institute']
3846	We gratefully acknowledge the support of Turkish  Scientific and  Technological Research Council of  Turkey  (TUBITAK)  and  METU  Scientific  Research  Fund  (no.	[[110, 117], [125, 129]]	[]	['TUBITAK', 'METU']	[]
3847	ing at combining the strengths of different grammars, we describes a synthetic synchronous grammar (SSG), which tentatively in this paper, integrates a syn-	[[100, 103]]	[[69, 98]]	['SSG']	['synthetic synchronous grammar']
3848	sided brevity penalty (C = 0.01). Table 2 shows the average compression rates (CompR) for McDonald (2006) and our model (STSG) as well as their perfor-	[[79, 84], [121, 125]]	[[60, 77]]	['CompR', 'STSG']	['compression rates']
3849	sentence are not participate in evaluation. Exact  match rate(EMR), violate match rate(VMR), and  inside match rate(IMR) denote the ratio of three 	[[87, 90], [62, 65], [116, 119]]	[[68, 85], [44, 61], [98, 115]]	['VMR', 'EMR', 'IMR']	['violate match rat', 'Exact  match rate', 'inside match rate']
3850	extract phrasal translations or transliterations of  phrase based on machine learning, or more  specifically the conditional random fields (CRF)  model.	[[140, 143]]	[[113, 138]]	['CRF']	['conditional random fields']
3851	unigram model (BoW)+HI, where in addition to representing what words occur in a text, we also represent what Harvard Inquirer (HI)3 word classes occur in it.	[[127, 129], [15, 18], [20, 23]]	[[109, 125]]	['HI', 'BoW', 'HI,']	['Harvard Inquirer']
3852	10-best(+AG) 4.35 90.0 Table 1: Task completion rate according to using the AG (Agenda Graph) and n-best hypotheses for n=1 and n=10.	[[76, 78], [9, 11]]	[[80, 92]]	['AG', 'AG']	['Agenda Graph']
3853	Total 2,910 1,086 3,996 Table 1: Number of annotated elements per category in our gold standard (CR=controlled requirements, UR=uncontrolled requirements)	[[97, 99], [125, 127]]	[[100, 123], [128, 140]]	['CR', 'UR']	['controlled requirements', 'uncontrolled']
3854	lem of selecting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that crosslingual word embeddings will allow for accurate	[[128, 130]]	[[102, 126]]	['EM']	['expectation maximisation']
3855	~ '1  procedural component  Q data structure  SSP = SemanticJSyntacticJPhonological  Figl~e 1: The SYNPHONICS Formulator 	[[46, 49], [99, 109]]	[[52, 83]]	['SSP', 'SYNPHONICS']	['SemanticJSyntacticJPhonological']
3856	proper textual evidences. We formulate this task as an Integer Linear Programming (ILP). Instead of	[[83, 86]]	[[55, 81]]	['ILP']	['Integer Linear Programming']
3857	8. Strong forms of pronouns not preceded by preposition (unless they carry IC) t Table 1: Annotation guidelines; IC = Intonation Center 4.2 Evaluation framework	[[113, 115], [75, 77]]	[[118, 135]]	['IC', 'IC']	['Intonation Center']
3858	 scores cw(ei) are combined: MEAN CM (cM (eI1)) is computed as the geometric mean of the confidence scores of the	[[34, 36]]	[[38, 40]]	['CM']	['cM']
3859	probabilistic Earley?s, and minimum edit distance algorithms). Dynamic programming (DP) involves solving certain kinds of recursive equations	[[84, 86]]	[[63, 82]]	['DP']	['Dynamic programming']
3860	Our model can now be represented like this:  241  Database (DB)  Facts about hotels 	[[60, 62]]	[[50, 58]]	['DB']	['Database']
3861	sible transliteration candidates. We measured performance using the Mean Reciprocal Rank (MRR) measure.	[[90, 93]]	[[68, 88]]	['MRR']	['Mean Reciprocal Rank']
3862	Traditional readability measures for L1 Swedish at the text level include LIX (L?asbarthetsindex, ? Readability index?)	[[74, 77], [37, 39]]	[[79, 96]]	['LIX', 'L1']	['L?asbarthetsindex']
3863	 1 Introduction  Statistical Machine Translation(SMT) is currently the state of the art solution to the machine 	[[49, 52]]	[[17, 47]]	['SMT']	['Statistical Machine Translatio']
3864	email: mal@aber.ac.uk Stephen Pulman University of Oxford (UK) email: sgp@clg.ox.ac.uk	[[59, 61]]	[[37, 57]]	['UK']	['University of Oxford']
3865	The parser uses a semantic grammar with approx-  imately 1000 rules which maps the input sentence  onto an interlingua representation (ILT) which rep-  resents the meaning of the sentence in a language- 	[[135, 138]]	[[107, 133]]	['ILT']	['interlingua representation']
3866	? SRI has developed the DECIPHER speaker-independent speech recognition system, a  hidden Markov model (HMM)-based system that achieves tate-of-the-art recognition  performance through accurate modeling of phonetic and phonological detail.	[[104, 107], [2, 5], [24, 32]]	[[83, 102]]	['HMM', 'SRI', 'DECIPHER']	['hidden Markov model']
3867	8. THE SAIL INTERFACING SYSTEM  The SAIL Interfacing System (S.I.S.) is the f ramework   where a user  can interact with SAIL in developing NL 	[[61, 67], [140, 142], [121, 125]]	[[36, 59]]	['S.I.S.', 'NL', 'SAIL']	['SAIL Interfacing System']
3868	? ? ? ?  Figure 2: Laten Event Model (LEM). 	[[38, 41]]	[[19, 36]]	['LEM']	['Laten Event Model']
3869	ing different methods: The methods respectively without prediction(NP), with prediction(P),  with prediction and feedback(PF) only using term  frequency (TM), and with prediction and feed-	[[122, 124], [67, 69], [154, 156]]	[[98, 120], [77, 87], [137, 152]]	['PF', 'NP', 'TM']	['prediction and feedbac', 'prediction', 'term  frequency']
3870	Language Weaver, Inc. This article shows that the structure of bilingual material from standard parsing and alignment tools is not optimal for training syntax-based statistical machine translation (SMT) systems. 	[[198, 201]]	[[165, 196]]	['SMT']	['statistical machine translation']
3871	construction relies on existing natural language processing tools, e.g., syntactic parsers (Wiebe, 2000), information extraction (IE) tools (Riloff and Wiebe, 2003) or rich lexical resources such	[[130, 132]]	[[106, 128]]	['IE']	['information extraction']
3872	 To combat this inefficiency, after every state transition we estimate the effective sample size (ESS) of the particle weights as ??	[[98, 101]]	[[75, 96]]	['ESS']	['effective sample size']
3873	 (UT = Utterance as actually made by the user, UR = Utterance as recognized by the system, SU = System utterance).	[[47, 49], [2, 4], [91, 93]]	[[52, 61], [7, 16], [96, 112]]	['UR', 'UT', 'SU']	['Utterance', 'Utterance', 'System utterance']
3874	(Vehicle). Each mention of an entity has a mention  type: NAM (proper name), NOM (nominal) or                                                            	[[77, 80]]	[[82, 89]]	['NOM']	['nominal']
3875	4.1 Selection of PPs in the Lexicon Our parser makes use of the computational lexicon HaGenLex (Hagen German Lexicon, see (Hartrumpf et al, 2003)), which is a general do-	[[86, 94], [17, 20]]	[[96, 116]]	['HaGenLex', 'PPs']	['Hagen German Lexicon']
3876	Abstract This work looks at a temporal aspect of multiword expressions (MWEs), namely that the behaviour of a given n-gram and	[[72, 76]]	[[49, 70]]	['MWEs']	['multiword expressions']
3877	 Association for Computational Linguistics.                       ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,                   Unsupervised Lexical Acquisition: Proceedings of the Workshop of the	[[109, 115], [66, 69]]	[[70, 107]]	['SIGLEX', 'ACL']	['Special Interest Group on the Lexicon']
3878	also accessible through a phrase internal reordering. A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li	[[94, 96]]	[[80, 92]]	['SO']	['source order']
3879	collocations in each sentence.  Perplexity from Language Model (PLM) Perplexity measures are extracted from a trigram language	[[64, 67]]	[[32, 62]]	['PLM']	['Perplexity from Language Model']
3880	Scores for PK    Because Academia Sinica corpora (AS) are  provided by us, we are not allowed to participate 	[[50, 52], [11, 13]]	[[25, 40]]	['AS', 'PK']	['Academia Sinica']
3881	tive standard deviation of three intervals, left  edge to anchor (LE-A), center to anchor (CC-A),  right edge to anchor (RE-A), calculated across  productions of pot, sot, spot, lot, plot, and splot 	[[121, 125], [91, 95], [66, 70]]	[[99, 119], [73, 89], [44, 64]]	['RE-A', 'CC-A', 'LE-A']	['right edge to anchor', 'center to anchor', 'left  edge to anchor']
3882	School of Computer Science, University of Manchester, UK ? National Centre for Text Mining (NaCTeM), UK ?	[[92, 98], [54, 56], [101, 103]]	[[59, 90]]	['NaCTeM', 'UK', 'UK']	['National Centre for Text Mining']
3883	the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-)divergence: KL^M_NQPR`NQS S54%67	[[118, 121], [146, 149], [134, 143]]	[[100, 116]]	['KL-', 'NQS', 'K\x02L^M_NQP']	['Kullback Leibler']
3884	Table 4. WSD precision recall and F-measure for  the algorithm based on aligned wordnets (AWN),  for AWN with clustering (AWN+C) and for 	[[90, 93], [9, 12], [122, 127], [101, 104]]	[[72, 88]]	['AWN', 'WSD', 'AWN+C', 'AWN']	['aligned wordnets']
3885	Visweswariah et al (2011) regarded the preordering problem as a Traveling Salesman Problem (TSP) and applied TSP solvers for obtaining reordered words.	[[92, 95], [109, 112]]	[[64, 90]]	['TSP', 'TSP']	['Traveling Salesman Problem']
3886	In practice we can find approximate solution using such algorithms as: Loopy Belief Propagation (BP), Mean Field (MF), Gibbs Sampling (Gibbs). 	[[114, 116], [97, 99], [135, 140]]	[[102, 112], [77, 95], [119, 124]]	['MF', 'BP', 'Gibbs']	['Mean Field', 'Belief Propagation', 'Gibbs']
3887	I.e., we consider three increasingly constrained conditions: (1) substitution according only to the form constraints (FORM), (2) substitution according to both form and taboo	[[118, 122]]	[[100, 104]]	['FORM']	['form']
3888	value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment task and 64.1 (AUTO), 79.3 (TABLETS) for	[[70, 74], [125, 129], [138, 145], [83, 90]]	[]	['AUTO', 'AUTO', 'TABLETS', 'TABLETS']	[]
3889	Verb classification performance (precision, recall, and F for MS are macro-averaged). Global accuracy supplemented by 95% binomial confidence intervals (CI). 	[[153, 155], [62, 64]]	[[131, 151]]	['CI', 'MS']	['confidence intervals']
3890	cerd,jurafsky,manning@stanford.edu Abstract Minimum error rate training (MERT) is a widely used learning procedure for statistical	[[73, 77]]	[[44, 71]]	['MERT']	['Minimum error rate training']
3891	We present experiments using counts of three types of ngrams: lemma ngrams (LN), POS ngrams (PN) and mixed ngrams (MN).2 Mixed ngram is a restricted formulation of lemma ngram where open-	[[115, 117], [76, 78], [93, 95]]	[[101, 113], [62, 74], [81, 91]]	['MN', 'LN', 'PN']	['mixed ngrams', 'lemma ngrams', 'POS ngrams']
3892	Direction, Manner, and Purpose are Prop-Bank adjunctive argument labels (Palmer et al, 2005). Quantifier, Means, Cause-to-Know and copulas were added to the preceding roles. Finally, anything that did not fit into the above categories retained its dependency parse type: VMod (Verb Modifier), NMod (Noun Modifier), AMod (Adjec-tive or Adverb Modifier), and Root (Root was used when a single word in the answer, typically yes, no, agree, disagree, A-D, etc., stood alone without a significant relation to the remainder of the refer-ence answer; this occurred only 21 times, account-ing for fewer than 1% of the reference answer facets).	[[271, 275], [293, 297], [315, 319]]	[[277, 290], [299, 312], [321, 350]]	['VMod', 'NMod', 'AMod']	['Verb Modifier', 'Noun Modifier', 'Adjec-tive or Adverb Modifier']
3893	In  Proceedings of the 16th International Conference on  Computational Linguistics (COLING-96), Copenhagen,  Denmark, pp 459-465.	[[84, 93]]	[[57, 82]]	['COLING-96']	['Computational Linguistics']
3894	 3.1 Ng and Cardie (2002a) Ng and Cardie (N&C) do not attempt to improve PA, simply using the anaphoricity information it pro-	[[42, 45], [73, 75]]	[[27, 40]]	['N&C', 'PA']	['Ng and Cardie']
3895	on the labels from DSlabels+MinCut: (4) MaxEnt with named entities (NE); (5) MaxEnt with NE and semantic (SEM) features; (6) CRF with NE; (7) MaxEnt with NE and sequential (SQ) features;	[[106, 109], [68, 70], [89, 91], [125, 128], [134, 136], [142, 148], [154, 156], [173, 175], [40, 46]]	[[96, 104], [52, 66]]	['SEM', 'NE', 'NE', 'CRF', 'NE', 'MaxEnt', 'NE', 'SQ', 'MaxEnt']	['semantic', 'named entities']
3896	also republish the baseline results of Schnabel and Sch?utze (2014) using the Stanford POS Tagger, a maximum entropy Markov model (MEMM) tagger. 	[[131, 135], [87, 90]]	[[101, 129]]	['MEMM', 'POS']	['maximum entropy Markov model']
3897	ferent sources. The first feature source comes  from our DSSMs (DSSM and DSSM_BOW) using the output layers as feature generators as de-	[[57, 62]]	[[64, 81]]	['DSSMs']	['DSSM and DSSM_BOW']
3898	every node be covered by some lexeme.  Partial SemSpec (PSemSpec): The contribution that the lexeme can  make to a sentence SemSpec.	[[56, 64]]	[[39, 54]]	['PSemSpec']	['Partial SemSpec']
3899	not be effective due to the brevity of contributions.  Barzilay and Lee?s algorithm (B&L) did not  generalize well to either dialogue corpus.	[[85, 88]]	[[55, 83]]	['B&L']	['Barzilay and Lee?s algorithm']
3900	 2.3 Perceptron Sequential Tagger This system uses a Global Linear Model (GLM), a sequential tagger using the perceptron algorithm	[[74, 77]]	[[53, 72]]	['GLM']	['Global Linear Model']
3901	In Proceedings of ACM 12th International Conference on Intelligent User interfaces (IUI). 	[[84, 87], [18, 21]]	[[55, 82]]	['IUI', 'ACM']	['Intelligent User interfaces']
3902	Since all covariance matrices are positive semi-definite, the quadratic program (QP) remains convex in w?, 	[[81, 83]]	[[62, 79]]	['QP']	['quadratic program']
3903	Figure 2: Translation examples (SRC = source, BASE = baseline system, BACKOFF = backoff system, REF = reference). OOVs and their trans-	[[96, 99], [32, 35], [46, 50], [70, 77], [114, 118]]	[[102, 111], [38, 44], [53, 61], [80, 87]]	['REF', 'SRC', 'BASE', 'BACKOFF', 'OOVs']	['reference', 'source', 'baseline', 'backoff']
3904	 Results (in percentages) are for per-logical-predication (PR) and per-whole-graph (GRPH) tagging accurcies. 	[[84, 88], [59, 61]]	[[77, 82], [46, 57]]	['GRPH', 'PR']	['graph', 'predication']
3905	In our development of 60 Japanese predicates (verb and verbal noun) frequently appearing in Kyoto University Text Corpus (KTC) (Kurohashi and Nagao, 1997) , 37.6% of the frames included	[[122, 125]]	[[92, 120]]	['KTC']	['Kyoto University Text Corpus']
3906	(disharmonic) combinators to increase the expressive power of the model.   KZGS10 (Kwiatkowski et al2010) uses a restricted higher-order unification procedure, which iteratively breaks up a logical form into	[[76, 82]]	[[84, 105]]	['KZGS10']	['Kwiatkowski et al2010']
3907	"	 		 consists of a directed acyclic graph (DAG) that encodes a set of conditional independence assertions about vari-"	[[64, 67]]	[[40, 62]]	['DAG']	['directed acyclic graph']
3908	one of German, English or Japanese. The system has been designed  around the task of conference r gistration (CR). It has initially been 	[[110, 112]]	[[85, 108]]	['CR']	['conference r gistration']
3909	For Task 2-2, we design two kinds of evaluation metrics:  1) POS accuracy (POS-A)  This index is used to evaluate the performance 	[[75, 80]]	[[61, 73]]	['POS-A']	['POS accuracy']
3910	On the standard view in transformational theory (Chomsky, 1981) both subject raising and object raising, or Exceptional Case Marking (ECM), cases are explained by the same principles.	[[134, 137]]	[[108, 132]]	['ECM']	['Exceptional Case Marking']
3911	Mean values (with standard deviations) of each of the first eight features on each sub-corpus are displayed in Table 4. The number of unusual punctuation marks (UnPunc) is the only feature whose value does not differ significantly between the original and simplified versions of the texts in any of the four	[[161, 167]]	[[134, 153]]	['UnPunc']	['unusual punctuation']
3912	Here the construction of the Chinese VP involves joining a prepositional phrase (PP) and a smaller verbal phrase (VP-A), with the preposition at the beginning as a PP marker.	[[114, 118], [37, 39], [81, 83], [164, 166]]	[[99, 112], [59, 79]]	['VP-A', 'VP', 'PP', 'PP']	['verbal phrase', 'prepositional phrase']
3913	Universitat Polit`ecnica de Catalunya (UPC), Barcelona 2 Centro de Investigaci?on en Computaci?on (CIC), Instituto Polit?ecnico Nacional (IPN), Mexico 1	[[99, 102], [138, 141], [39, 42]]	[[57, 97], [105, 136], [0, 37]]	['CIC', 'IPN', 'UPC']	['Centro de Investigaci?on en Computaci?on', 'Instituto Polit?ecnico Nacional', 'Universitat Polit`ecnica de Catalunya']
3914	proposed by (Jia and Zhao, 2013). We will mainly consider MIU accuracy (MIU-Acc) which is the ratio of the number of completely corrected gen-	[[72, 79]]	[[58, 70]]	['MIU-Acc']	['MIU accuracy']
3915	(41a) PP = ~ Cat i (PN)  i i>0  (41b) NP = N ~ Cati(P N) i  1 	[[38, 40], [6, 8], [20, 22]]	[[43, 53]]	['NP', 'PP', 'PN']	['N ~ Cati(P']
3916	In  Proc. of Intelligent Tutoring Systems (ITS). 	[[43, 46]]	[[13, 41]]	['ITS']	['Intelligent Tutoring Systems']
3917	model, it then shows how meaning specificity affects the linguistic behavior and semantic content of Chinese resultative verb compounds (RVCs). 	[[137, 141]]	[[109, 135]]	['RVCs']	['resultative verb compounds']
3918	 ? National Drug File7 (NDF): this ontology  contains information about a comprehensive 	[[24, 27]]	[[3, 22]]	['NDF']	['National Drug File7']
3919	"The/AT table\]NN is/BEZ ready/J/./.  (PPS = subject pronoun; MD = modal; V'B =  verb (no inflection); AT = article; NN = noun;  BEZ ffi present 3rd sg form of ""to be""; Jl = "	[[102, 104], [116, 118], [38, 41], [61, 63], [73, 76]]	[[107, 114], [121, 125], [66, 71], [80, 84], [44, 59]]	"['AT', 'NN', 'PPS', 'MD', ""V'B""]"	['article', 'noun', 'modal', 'verb', 'subject pronoun']
3920	 4.2 Data  We used the Wall Street Journal (WSJ) of the years  88-89.	[[44, 47]]	[[23, 42]]	['WSJ']	['Wall Street Journal']
3921	transcripts. The print news consisted of 22 New York Times (NYT) articles from January 1998.	[[60, 63]]	[[44, 58]]	['NYT']	['New York Times']
3922	5 5 of such properties is acyclicity, as in Hidden Markov Models (HMMs). For	[[66, 70]]	[[44, 64]]	['HMMs']	['Hidden Markov Models']
3923	 3.3 Cascaded ATN Grammars A Cascaded ATN Grammars (CATN) (Woods 1980) is a cooperating sequence of ATN transducers, each feeding its output to the next stage.	[[52, 56], [14, 17], [100, 103]]	[[29, 41]]	['CATN', 'ATN', 'ATN']	['Cascaded ATN']
3924	2.1 Description of the procedure Two specialized topics In this study MA student interpreters were invited to prepare for simultaneous interpreting tasks on two specialised topics: fast reactors (FR) and Seabed minerals (SM). They	[[196, 198], [221, 223], [70, 72]]	[[181, 194], [204, 219]]	['FR', 'SM', 'MA']	['fast reactors', 'Seabed minerals']
3925	5) is blocked by (7), because of the  passinginto the subject'a story about es'; i.e., the specifier of  INFL (in the transformational ccount) or of VP (in theories  like GPSG, etc.).	[[105, 109], [149, 151], [171, 175]]	[[111, 134]]	['INFL', 'VP', 'GPSG']	['in the transformational']
3926	In order to normalize Thai input character  sequences to a canonical Unicode form, we developed a finite state transducer (FST) which  detects and repairs a number of sequencing er-	[[123, 126]]	[[98, 121]]	['FST']	['finite state transducer']
3927	ically used as the target. For example, the NIST Open Machine Translation Evaluation (OpenMT) 2009 (Garofolo, 2009) constrained Arabic-English	[[86, 92], [44, 48]]	[[49, 84]]	['OpenMT', 'NIST']	['Open Machine Translation Evaluation']
3928	1. Construct word representation model for  corpus in the base time, D(TB), and in the  target time, D(TT). (	[[71, 73], [103, 105]]	[[54, 62], [88, 99]]	['TB', 'TT']	['the base', 'target time']
3929	5 Learning Algorithms We used two non-parametric learning approaches, Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and Gaussian Processes (GPs)	[[95, 99], [162, 165]]	[[70, 93], [142, 160]]	['SVMs', 'GPs']	['Support Vector Machines', 'Gaussian Processes']
3930	score word pairs for relatedness (on a scale of 0 to 10), which is in contrast to the similarity judgments requested of the Miller and Charles (MC) and Rubenstein and Goodenough (RG) participants.	[[144, 146], [179, 181]]	[[124, 142], [152, 177]]	['MC', 'RG']	['Miller and Charles', 'Rubenstein and Goodenough']
3931	 Recently researchers have been investigating Amazon Mechanical Turk (MTurk) as a source of non-expert natural language annotation, which is a	[[70, 75]]	[[53, 68]]	['MTurk']	['Mechanical Turk']
3932	 1 Introduction Weighted Context Free Grammars (WCFG) define an important class of languages.	[[48, 52]]	[[16, 46]]	['WCFG']	['Weighted Context Free Grammars']
3933	BC = Broadcast Conversations; BN = Broadcast News; CTS = Conversational Telephone Speech; NW = Newswire; UN = Usenet Newsgroups; and WL = Weblogs. 	[[105, 107], [133, 135], [0, 2], [30, 32], [51, 54], [90, 92]]	[[110, 127], [138, 145], [5, 28], [35, 49], [57, 88], [95, 103]]	['UN', 'WL', 'BC', 'BN', 'CTS', 'NW']	['Usenet Newsgroups', 'Weblogs', 'Broadcast Conversations', 'Broadcast News', 'Conversational Telephone Speech', 'Newswire']
3934	based Translation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING?90), pages 247?252.	[[101, 110]]	[[60, 99]]	['COLING?90']	['Conference on Computational Linguistics']
3935	5 23   Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69, Sofia, Bulgaria, August 9, 2013.	[[72, 79]]	[[38, 70]]	['DiscoMT']	['Discourse in Machine Translation']
3936	 In Proceedings of the 22nd International Conference on Computational Linguistics (COLING), pages 865? 	[[83, 89]]	[[56, 81]]	['COLING']	['Computational Linguistics']
3937	(V = main verb, N = noun, AUXV = auxiliary verb, COMPL = completive, ccomp_obj = clausal complement object, ERG =  ergative, S: singular, auxmod = auxiliary, ncsubj = non-clausal subject, B-NP = beginning of NP, I-NP = inside an NP,  &MAINV = main verb, &<AUXMOD = verbal auxiliary modifier). 	[[235, 240], [26, 30], [49, 54], [108, 111], [69, 78], [138, 144], [158, 164], [188, 192], [212, 216], [256, 262]]	[[243, 252], [5, 14], [20, 24], [33, 47], [57, 67], [115, 123], [81, 107], [128, 136], [147, 156], [167, 186], [195, 210], [219, 231], [265, 292]]	['MAINV', 'AUXV', 'COMPL', 'ERG', 'ccomp_obj', 'auxmod', 'ncsubj', 'B-NP', 'I-NP', 'AUXMOD']	['main verb', 'main verb', 'noun', 'auxiliary verb', 'completive', 'ergative', 'clausal complement object,', 'singular', 'auxiliary', 'non-clausal subject', 'beginning of NP', 'inside an NP', 'verbal auxiliary modifier).']
3938	tell verb base (VB), tell VB tell told verb past tense (VBD), tell VBD,VBN tell verb past participle (VBN), tell tells verb present 3rd person sing (VBZ), tell VBZ tell	[[102, 105], [16, 18], [26, 28], [56, 59], [67, 70], [71, 74], [149, 152], [160, 163]]	[[80, 100], [5, 14], [39, 54], [119, 142]]	['VBN', 'VB', 'VB', 'VBD', 'VBD', 'VBN', 'VBZ', 'VBZ']	['verb past participle', 'verb base', 'verb past tense', 'verb present 3rd person']
3939	"semaatic grounds, new referent objects must be created. The number of objects to be  created is set equal to the QTY (quantity) attribute of the noun phrase if specified (as in  ""two boys"" (P20)), to two if the noun phrase is plural and not compound, to the number "	[[113, 116]]	[[118, 126]]	['QTY']	['quantity']
3940	 We ran three parsing experiments: (i) replacing the value of the surface form (FORM) of pronominal prepositions with their lemma form (LEMMA), for	[[80, 84], [136, 141]]	[[74, 78], [124, 129]]	['FORM', 'LEMMA']	['form', 'lemma']
3941	data. The QA surveys are evaluated using nuggets drawn from citation texts (CT), or abstracts (AB), and DP surveys are evaluated using nuggets from	[[76, 78], [95, 97], [10, 12], [104, 106]]	[[60, 74], [84, 93]]	['CT', 'AB', 'QA', 'DP']	['citation texts', 'abstracts']
3942	topic distributions for all phrase pairs in the phrase table in an unsupervised fashion, using a variant of Latent Dirichlet Allocation (LDA). The underly-	[[137, 140]]	[[108, 135]]	['LDA']	['Latent Dirichlet Allocation']
3943	translations: ovc~iflow in the Data Processing  category (DPR,) and out-o\[-./lushnc.ss in the Air-  m'M't Structure category (STR). As shown in the 	[[127, 130], [58, 61]]	[[107, 125], [31, 56]]	['STR', 'DPR']	['Structure category', 'Data Processing  category']
3944	Following the ideas of (Collobert et al, 2011), Zeng et al (2014) first solve relation classification using convolutional neural network (CNN). The	[[138, 141]]	[[108, 136]]	['CNN']	['convolutional neural network']
3945	1. Introduction  A verb phrase ellipsis (VPE) exists when a  sentence has an auxiliary verb but no verb phrase 	[[41, 44]]	[[19, 39]]	['VPE']	['verb phrase ellipsis']
3946	have a tea and read a good criminal book) and we cannot forget that entertainment is also one of this Embodied Conversational Agent (ECA)?s goal. 	[[133, 136]]	[[102, 131]]	['ECA']	['Embodied Conversational Agent']
3947	uation contained in the RST Discourse Treebank (RST-DTB)(Carlson et al 2001) distributed by the Linguistic Data Consortium (LDC)3. The RST-DTB	[[124, 127], [48, 55], [135, 142]]	[[96, 122], [24, 46]]	['LDC', 'RST-DTB', 'RST-DTB']	['Linguistic Data Consortium', 'RST Discourse Treebank']
3948	3 The  S imulat ion  Mode l   The computational simulation supports the evolu-  tion of a population of Language Agents (LAgts),  similar to Holland's (1993) Echo agents.	[[121, 126]]	[[104, 119]]	['LAgts']	['Language Agents']
3949	data are classified manually (Human) into three  stability classes. Decision Tree (DT) automatic  algorithm C4.5 (Quinlan, 1993; Weiss & Kulikowski, 	[[83, 85]]	[[68, 81]]	['DT']	['Decision Tree']
3950	the conjunction but. Turney and Littman (2003) use pointwise mutual information (PMI) (Church and Hanks, 1990) and latent semantic analysis	[[81, 84]]	[[51, 79]]	['PMI']	['pointwise mutual information']
3951	It is particularly interesting to see that when hypotheses selection is applied, oracle error rate (OER) drops of 2% points from an already accurate OER	[[100, 103], [149, 152]]	[[81, 98]]	['OER', 'OER']	['oracle error rate']
3952	 1 Introduction Named entity recognition (NER) is the task of identifying and classifying phrases that denote certain types of named	[[42, 45]]	[[16, 40]]	['NER']	['Named entity recognition']
3953	meaning of the other. Examples are:  * Senior research assistant at the Belgian National Fund for Scientific Research (F.N.R.S.). 	[[119, 127]]	[[89, 117]]	['F.N.R.S.']	['Fund for Scientific Research']
3954	ACC mPUR + ACC The random baseline(BL) is calculated as follows: BL = 1/number of classes	[[35, 37], [0, 3], [4, 8], [11, 14], [65, 67]]	[[26, 33]]	['BL', 'ACC', 'mPUR', 'ACC', 'BL']	['baselin']
3955	Several different learning algorithms have been explored for text classification (Dumais et al 1998) and support vector machines (SVMs) (Vapnik, 1995) were found to be the most computationally ef-	[[130, 134]]	[[105, 128]]	['SVMs']	['support vector machines']
3956	extrinsic and language independent features.  The student response analysis (SRA) task (Dzikovska et al 2013) addresses the fol-	[[77, 80]]	[[50, 75]]	['SRA']	['student response analysis']
3957	knowledge. The use of Proposition Stores as  Background Knowledge Bases (BKB) have been  argued to be useful for improving parsing, co-	[[73, 76]]	[[45, 71]]	['BKB']	['Background Knowledge Bases']
3958	give an indication as to what the vibhakti/TAM are.  Words with PSP (postposition) and NST (noun with spatial and temporal properties) tags are generally	[[64, 67], [34, 46], [87, 90]]	[[69, 81]]	['PSP', 'vibhakti/TAM', 'NST']	['postposition']
3959	The GALE manual WA corpus and the Chinese to English corpus from the shared task of the NIST open machine translation (OpenMT) 2006 evaluation 6 were employed as the experimental corpus	[[119, 125], [4, 8], [16, 18], [88, 92]]	[[93, 117]]	['OpenMT', 'GALE', 'WA', 'NIST']	['open machine translation']
3960	It is well known that for English, the automatic conversion of a constituency parser?s output to dependency format can achieve competitive unlabeled attachment scores (ULA) to a dependency parser?s output trained on automatically converted trees	[[168, 171]]	[[139, 159]]	['ULA']	['unlabeled attachment']
3961	` and survival variables S`. Languages shown are Latin (LA), Vulgar Latin (VL), Proto-Iberian (PI), Italian (IT), Portuguese (PT), and Spanish (ES). Note that only modern	[[75, 77], [126, 128], [56, 58], [95, 97], [109, 111], [144, 146]]	[[61, 73], [114, 124], [49, 54], [80, 93], [100, 107], [135, 142], [6, 24]]	['VL', 'PT', 'LA', 'PI', 'IT', 'ES']	['Vulgar Latin', 'Portuguese', 'Latin', 'Proto-Iberian', 'Italian', 'Spanish', 'survival variables']
3962	The work described in this paper is based on the output of Inputlog3, but it can also be applied to the output of other keystroke-logging programs.  To promote more linguistically-oriented writing process research, Inputlog aggregates the logged process data from the character level (keystroke) to the word level.  In a subsequent step, we use various Natural Language Processing (NLP) tools to further annotate the logged process data with different kinds of linguistic information: part-of-speech tags, lemmata, chunk boundaries, syllable boundaries, and word frequency.  The remainder of this paper is structured as follows.	[[382, 385]]	[[353, 380]]	['NLP']	['Natural Language Processing']
3963	hierarchical phrase-based model, but constrained so that the English part of the right-hand side is restricted to a Greibach Normal Form (GNF)like structure: A contiguous sequence of termi-	[[138, 141]]	[[116, 136]]	['GNF']	['Greibach Normal Form']
3964	We introduce a  exible history reference mechanism called an ACT (arboreal context tree; an extension of the context tree to tree-shaped his-	[[61, 64]]	[[66, 87]]	['ACT']	['arboreal context tree']
3965	the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING) 1998, pages 41?47, Montre?al.	[[116, 122], [47, 50]]	[[89, 114], [4, 45]]	['COLING', 'ACL']	['Computational Linguistics', 'Association for Computational Linguistics']
3966	research relies is that the failures of current automatic metrics are not algorithmic: BLEU, Meteor, TER (Translation Edit Rate), and other metrics efficiently and correctly compute informative distance	[[101, 104], [87, 91]]	[[106, 127]]	['TER', 'BLEU']	['Translation Edit Rate']
3967	for each word in the DAL. ( e.g., the verb definition for LOL (laugh out loud) in Wiktionary is ? To laugh	[[58, 61], [21, 24]]	[[63, 77]]	['LOL', 'DAL']	['laugh out loud']
3968	The primary purpose of the toolkit is to allow students to concentrate on building natural language processing (NLP) systems.	[[112, 115]]	[[83, 110]]	['NLP']	['natural language processing']
3969	hdaume,marcu  @isi.edu Abstract Entity detection and tracking (EDT) is the task of identifying textual mentions	[[64, 67]]	[[33, 62]]	['EDT']	['Entity detection and tracking']
3970	trol agreement principle.  Consider first the foot feature principle (FFP). 	[[70, 73]]	[[46, 68]]	['FFP']	['foot feature principle']
3971	Here we perform a set of experiments where we investigate the potential of multi-source transfer for NER, in German (DE), English (EN), Spanish (ES) and Dutch (NL), using cross-lingual	[[131, 133], [101, 104], [117, 119], [145, 147], [160, 162]]	[[122, 129], [109, 115], [136, 143], [153, 158]]	['EN', 'NER', 'DE', 'ES', 'NL']	['English', 'German', 'Spanish', 'Dutch']
3972	tures. Since the Parallel Alignment TreeBank is a subset of the Chinese TreeBank (CTB) 8.0, we automatically parsed the CTB 8.0 by doing a 10-	[[82, 85], [120, 123]]	[[64, 80]]	['CTB', 'CTB']	['Chinese TreeBank']
3973	 ? Bigram Predictability (BP): Defined as the predictability of a word given a previous word, it	[[26, 28]]	[[3, 24]]	['BP']	['Bigram Predictability']
3974	We use two different windows to define a triggering environment: one for morpheme and another for its part of speech (POS) tag. Figure 2 shows 	[[118, 121]]	[[102, 116]]	['POS']	['part of speech']
3975	linguistic resources  ? Semantic Analysis Module (SAM) interpreting  LPM output using application knowledge 	[[50, 53], [69, 72]]	[[24, 48]]	['SAM', 'LPM']	['Semantic Analysis Module']
3976	take et al (2005)) for the representation of meaning.  LXGram is developed in the Linguistic Knowledge Builder (LKB) system (Copestake, 2002), a development environment for constraint-based grammars.	[[112, 115], [55, 61]]	[[82, 110]]	['LKB', 'LXGram']	['Linguistic Knowledge Builder']
3977	clickthrough data. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD), pages 133?142.	[[84, 87], [22, 25], [26, 32]]	[[47, 75]]	['KDD', 'ACM', 'SIGKDD']	['Knowledge Discovery and Data']
3978	S2LS. The best parameters were then used on the Senseval-3 English Lexical Sample task (S3LS), where a similar semi-supervised method was used	[[88, 92], [0, 4]]	[[48, 86]]	['S3LS', 'S2LS']	['Senseval-3 English Lexical Sample task']
3979	C-NOUN: Nouns with POS NN that are not marked O-NOUN or PER-NOUN.  Verbs Only ING-VERBs (VBG) and ED-VERBs (VBN and VBD) are needed for this task (other verbs trigger state O).	[[89, 92], [0, 6], [19, 22], [23, 25], [46, 52], [56, 64], [98, 106], [108, 111], [116, 119]]	[[67, 81]]	['VBG', 'C-NOUN', 'POS', 'NN', 'O-NOUN', 'PER-NOUN', 'ED-VERBs', 'VBN', 'VBD']	['Verbs Only ING']
3980	text corpus, to be made available without royalties for scientific research. The text will  be formatted using SGML (the Standard Generalized Markup Language). To date 	[[111, 115]]	[[121, 157]]	['SGML']	['Standard Generalized Markup Language']
