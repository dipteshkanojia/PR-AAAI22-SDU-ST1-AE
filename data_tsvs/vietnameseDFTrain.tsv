ID	text	acronyms	long-forms	acronyms-text	long-forms-text
1	• Đổi chỗ 𝜇𝑘 và 𝑥𝑛 , gán lại cụm cho các quan sát . Tính giá trị hàm lỗi L • Nếu hàm lỗi không giảm thì hủy bỏ thao tác đổi chỗ .	[]	[]	[]	[]
2	Tổng quan về SSL Khái niêm Trước tiên , đồ án sẽ trình bày một số khái niệm về SSL ở trên mạng rồi mới phân	[[13, 16], [79, 82]]	[]	['SSL', 'SSL']	[]
3	Đầu vào : Dòng dữ liệu với các minibatch { D1 , D2 , ... , Dt , ... } , các tham số drop rate dr , phương sai σ 2 Đầu ra : Biến toàn cục β	[]	[]	[]	[]
4	Bayes , trong đó mỗi node trong mạng đại diện cho từng cấu trúc ngữ nghĩa ẩn của tập văn bản được sinh ra đó . Cụ thể LDA giả sử rằng mỗi văn bản d là	[[118, 121]]	[]	['LDA']	[]
5	LDA có thể không hoạt động tốt đối với các văn bản ngắn do sự thưa thớt dữ liệu . Mô hình chủ đề Biterm ( BTM ) học các chủ đề bằng cách mô hình hóa các cặp từ	[[0, 3], [106, 109]]	[]	['LDA', 'BTM']	[]
6	cho 𝜎𝑖 . Hard Attention to the Task ( HAT ) ( Joan Serra , 2018 ) xác định độ quan trọng của nơron thông qua cơ chế hard attention trên nơ-ron sau mỗi tầng . Tại mỗi tầng của mạng sẽ thêm vào một tầng task - embedding , được huấn luyện cùng với mạng nhờ sự hỗ trợ của một kỹ	[[38, 41]]	[]	['HAT']	[]
7	Chú ý rằng khi suy diễn , X l được nhân với µl . X̃i = Xi ∗ µi ∗ I [ pi < thresh ] ,	[]	[]	[]	[]
8	( MLM ) đều được huấn luyện không giám sát , chỉ yêu cầu dữ liệu đơn ngữ và không tận dụng được dữ liệu song ngữ có sẵn . Mô hình ngôn ngữ dịch ( TLM ) giúp cải	[[2, 5], [146, 149]]	[[122, 143]]	['MLM', 'TLM']	['Mô hình ngôn ngữ dịch']
9	Công thức tường minh của đại lượng KL trong 3.14 khi xét các wij độc lập nhau :  L	[[35, 37]]	[]	['KL']	[]
10	bag-of-words : wd = { wd1 , wd2 , ... , wdNd } trong đó wdn là từ thứ n trong dãy các từ của văn bản d . • Tập văn bản bao gồm M văn bản được ký hiệu bởi D = { w1 , w2 , ... , wM }	[]	[]	[]	[]
11	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 49 in short	[[61, 70]]	[]	['CNTT – TT']	[]
12	là việc biểu diễn ẩn đấy còn giữ được một số thông tin nhất định từ đầu vào , hay nói cách khác , nếu ta xem X là dữ liệu , Y là một biểu diễn ẩn nào đó mà ta tìm được , thì việc tìm ra một ánh xạ q tốt từ X đến Y được đưa về nghiệm của bài	[]	[]	[]	[]
13	Mô hình đề xuất ITE - onehot . . . . .	[[16, 19]]	[]	['ITE']	[]
14	Điểm BLEU của các mô hình học đối ngẫu sử dụng nhiễu . . . . . . . . . . 44	[[5, 9]]	[]	['BLEU']	[]
15	• So sánh các mô hình qua các vòng lặp với các k khác nhau ( a ) HR @ 10	[[65, 67]]	[]	['HR']	[]
16	Tối ưu hàm mất mát : Khi cố định P , việc tối ưu Q được đưa về bài toán tối ưu hàm	[]	[]	[]	[]
17	Tóm lại , bài toán học mô hình LDA chính là việc ước lượng các biến ẩn khi đã biết các từ của các văn bản . Dưới lý thuyết Bayesian , công việc này chính là	[[31, 34]]	[]	['LDA']	[]
18	Tài liệu tham khảo Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 66	[[79, 90]]	[]	['KSTN - CNTT']	[]
19	i=1 ( 37 ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[71, 82]]	[]	['KSTN - CNTT']	[]
20	với phân phối tiên nghiệm sẽ được lựa chọn . Trong phương pháp đề xuất 𝑠𝑡 , 𝑚𝑑 có thể tùy biến lựa chọn từ các phân phối dạng khác chứ không hạn chế như trong VD	[[159, 161]]	[]	['VD']	[]
21	M Sỗ lượng người dùng N	[]	[]	[]	[]
22	False Negative ( FN ) : Không phát hiện được ground truth True Negative ( TN ) : Không được sử dụng trong bài toán này . Trong nhiệm vụ	[[17, 19], [74, 76]]	[]	['FN', 'TN']	[]
23	sao cho : θM LE = argmaxθ p ( D | θ ) . ( 2.9 )	[]	[]	[]	[]
24	Long short-term memory Long short-term memory ( LSTM ) được giới thiệu lần đầu tiên bởi Hochreiter và Schmidhuber ( 1997 ) [ 12 ] , và được tinh chỉnh cũng như phổ biến rộng rãi bởi cộng đồng nghiên cứu . LSTM được thiết kế theo cách đặc biệt với memory cell ( cell state ) để giải quyết vấn đề phụ	[[48, 52], [205, 209]]	[]	['LSTM', 'LSTM']	[]
25	Layout LM sử dụng 2 chiến lược huấn luyện : • Mask Visual-language Model ( MVLM ) • Multi-label Document Classification ( MDC )	[[75, 79], [122, 125], [7, 9]]	[]	['MVLM', 'MDC', 'LM']	[]
26	Để quyết định nơ-ron nào được giữ lại , VBD học ra một tỉ lệ drop cho mỗi nơ-ron . Tỉ lệ này mang thông tin về độ quan trọng của nơ-ron đó đối với tác vụ	[[40, 43]]	[]	['VBD']	[]
27	Như vậy , có vẻ nhóm tác giả không quan tâm tới độ tin cậy của dữ liệu trong Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 27	[[140, 151]]	[]	['KSTN - CNTT']	[]
28	Hình 21 : So sánh các mô hình trên bộ Retailrocket qua từng vòng lặp với k = 8 ( a ) HR @ 10	[[85, 87]]	[]	['HR']	[]
29	Mỗi tác 31 vụ chỉ cần tốn O ( M + N ) đơn vị bộ nhớ để lưu lại độ quan trọng cho nơ-ron , thay	[]	[]	[]	[]
30	PD cập PNnhật d	[[0, 2]]	[]	['PD']	[]
31	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 40 Câu 1 : viewers will be given a 900 number to call .	[[61, 70]]	[]	['CNTT – TT']	[]
32	• Các phép DA liên quan đến màu sắc : độ sáng , độ tương phản , giảm màu , độ bão hòa Các phép DA được sử dụng kết hợp với nhau tạo thành một tập hợp và mỗi	[[11, 13], [95, 97]]	[]	['DA', 'DA']	[]
33	thời gian ( item trong hệ thống này là các banner quảng cáo ) . Ở đây , view là một hành vi tiềm ẩn , nó xảy ra khi người dùng lướt các trang web có hiển thị quảng	[]	[]	[]	[]
34	cứu đã khai thác thông tin từ nối trên chính bộ dữ liệu PDTB này ( Liu et al. , 2016 ; Wu et al. , 2016 ; Lan et al. , 2017 ; Bai and Zhao , 2018 ) . Đó cũng chính là hướng tiếp cận thứ	[[56, 60]]	[]	['PDTB']	[]
35	Chúng tôi sử dụng mô hình LDA để phân tích chủ đề của 7 bộ dữ liệu văn bản phổ biến , trong đó có : • 6 bộ không có nhãn thời gian bao gồm 3 bộ gồm các văn bản dài là { Grolier	[[26, 29]]	[]	['LDA']	[]
36	này khác với MLE hay MAP bởi hai ước lượng này chỉ tìm được một giá trị của tham số , hay còn gọi là ước lượng điểm ( Point estimate ) . Trong rất nhiều trường	[[13, 16], [21, 24]]	[]	['MLE', 'MAP']	[]
37	Như đã đề cập lúc đầu , mỗi từ nối có thể kết hợp với một hoặc nhiều mối quan hệ . Ví dụ , trong bộ dữ liệu PDTB , em phát hiện ra khoảng 53 % từ nối chỉ kết hợp với một	[[108, 112]]	[]	['PDTB']	[]
38	3.4 . 2 Mutil - Layer Perceptron ( MLP ) Hình 19 . Mô hình Mutil - Layer Perceptron [ 3 ]	[[35, 38]]	[]	['MLP']	[]
39	mạng RNN cơ bản được mô tả như trên Hình 1 . 5 . Mạng nơ-ron hồi quy cho phép ta mô hình hóa các sự phụ thuộc dài hạn về thông tin vì nó có khả năng cho phép thông tin được lan	[[5, 8]]	[]	['RNN']	[]
40	National Institute of Information and Communications Technology , Japan ( NICT ) khởi xướng vào năm 2014 . Quá trình xây	[[74, 78]]	[]	['NICT']	[]
41	mỗi tầng nhiễu l này bao gồm hai biến ngẫu nhiên { µl , σ l } . X 2 = ( X 1 E 1 ) W 2 .	[]	[]	[]	[]
42	CSS CSDL URL	[]	[]	[]	[]
43	truyền qua hoặc bị chặn lại ở đầu vào hoặc đầu ra của LSTM . Giá trị các phần tử của it và ut luôn nằm trong khoảng từ 0 đến 1 .	[[54, 58]]	[]	['LSTM']	[]
44	và chuyển nó xuống mạng . Ở đây sử dụng cổng update gate . Nó quyết định thông tin	[]	[]	[]	[]
45	tiếp liên quan đến sự kiện chính Anecdotal Event ( D2 ) : là các sự kiện khó xác minh , các tình huống hư cấu hoặc	[]	[]	[]	[]
46	Đầu ra : • Y : ma trận dữ liệu đã được giảm chiều , với chiều m × d . Mỗi hàng là một điểm	[]	[]	[]	[]
47	( 10 ) ( u , i ) ∈Y∪Y − Hàm mất mát là độ lệch bình phương sẽ phù hợp hơn khi các tập các giá trị	[]	[]	[]	[]
48	việc sử dụng và không sử dụng Dropout . Từ đó , ta có thể thấy rằng ALV có khả năng giúp mô hình cân bằng tốt giữa hai yếu tố là tính mềm dẻo và ổn định trong	[[68, 71]]	[]	['ALV']	[]
49	pháp học dựa trên ràng buộc trọng số đã có . 3 . Các nhiệm vụ cụ thể của ĐATN - Tìm hiểu các vấn đề và giải pháp trong lĩnh vực Học liên tục .	[[73, 77]]	[]	['ĐATN']	[]
50	sử dụng Dropout và ALV đem lại hiệu quả thực sự đáng kể cho EWC , sự cải thiện này rõ rệt hơn rất nhiều so thử nghiệm trong hai phương pháp UCL và VCL . Điều này có thể giải thích là vì UCL và VCL sử dụng kiến trúc mạng BNN để biểu diễn	[[19, 22], [60, 63], [140, 143], [147, 150], [186, 189], [193, 196], [220, 223]]	[]	['ALV', 'EWC', 'UCL', 'VCL', 'UCL', 'VCL', 'BNN']	[]
51	giá biểu diễn ẩn , thì thu được kết quả không có một phép DA nào cho biểu diễn ẩn quá tốt dù cho mô hình có thể giải quyết pretext task rất tốt . Mặt khác , việc	[[58, 60]]	[]	['DA']	[]
52	véc - tơ 𝑎 , ⨀ là phép nhân Hadamard . Công thức PT 3 . 5 thể hiện rằng biến hỗ trợ ( 𝑙 )	[[49, 51]]	[]	['PT']	[]
53	4 CÁC MÔ HÌNH ĐỀ XUẤT  onehot 	[]	[]	[]	[]
54	j − ( Eξ [ A(s̃k ) ] − A ( sk ) ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[94, 105]]	[]	['KSTN - CNTT']	[]
55	t chúng tôi sẽ chọn f là hàm softmax ) . Khi đó , Θ̃ sẽ là một biến ẩn toàn cục mới ,	[]	[]	[]	[]
56	EWC , VCL và UCL . Trục hoành trên các đồ thị này thể hiện tác vụ tương ứng sau khi học xong và trục tung thể hiện độ chính xác của mô hình trên thử nghiệm với	[[0, 3], [6, 9], [13, 16]]	[]	['EWC', 'VCL', 'UCL']	[]
57	Cụ thể , chúng ta xét họ biến phân mean-field , trong đó giả sử các biến ẩn Z là đôi một độc lập dưới từng tham số biến phân riêng rẽ , tức	[]	[]	[]	[]
58	Số lượng tham số khi áp dụng ALV được tổng hợp sau khi đã học xong toàn bộ các tác vụ trong kịch bản thử nghiệm tương ứng . Bảng 4 . 5 cho biết thông tin về số lượng tham số trên các thử nghiệm ứng với	[[29, 32]]	[]	['ALV']	[]
59	được phép truy cập tới ( hoặc chỉ một phần nhỏ ) dữ liệu ở các tác vụ trước đó , nên hàm mục tiêu được viết lại thành : R = RT ( θ ) +	[]	[]	[]	[]
60	2 KIẾN THỨC CƠ SỞ Hình 5 : Neural Collaborative Filtering năng của mô hình dưới góc độ thuần lọc cộng tác .	[]	[]	[]	[]
61	chiều của từ điển và số chiều của biểu diễn từ . Tầng thứ 2 là tầng CNN ( convolutional neural network ) [ 10 ] . Tầng CNN s có	[[68, 71], [119, 122]]	[]	['CNN', 'CNN']	[]
62	7 2 . 2 Variational Autoencoders ( VAE ) 2 . 2 . 1 VAE là gì ?	[[35, 38], [51, 54]]	[]	['VAE', 'VAE']	[]
63	Ở mức khái quát , có thể coi mạng nơ-ron lan truyền tiến như là một chuỗi các hàm phi tuyến : hL = g 1 ◦ g 2 ◦ . . . g L	[]	[]	[]	[]
64	xuất phương pháp Group Lasso ( GL ) [ 11 ] để khắc phục điểm yếu của Lasso . Thay vì sử dụng chuẩn L1 làm đại lượng hiệu chỉnh thì Group Lasso sử dụng chuẩn L2 ( khác với hồi	[]	[]	[]	[]
65	• Độ sâu lớn nhất của cây = 4 . SVM : C = 100 Hồi quy Logistic : không có	[[32, 35]]	[]	['SVM']	[]
66	Cuối cùng lớp đầu ra sẽ đưa ra giá trị nhãn dự đoán của kiểu dữ liệu tương ứng . các ma trận trọng số và bias được ký hiệu là W và b . Khi đó , bộ tham số cần tối ưu của	[]	[]	[]	[]
67	Xem các bài đã đăng tuyển NTD NTD chọn 1 mục dưới mục quản lý đăng tuyển	[[26, 29], [30, 33]]	[]	['NTD', 'NTD']	[]
68	tác vụ tiêu đề sau khi đã học xong tác vụ tương ứng trên trục hoành . 35 Hình 4 . 10 Độ chính xác trên từng tác vụ trong thử nghiệm với EWC	[[136, 139]]	[]	['EWC']	[]
69	2 . Phép đánh giá P : là thước đo năng lực của một thuật toán học máy . Thông thường ,	[]	[]	[]	[]
70	bào liên kết với mạch đơn của DNA theo nguyên tắc bổ sung tương tự khi DNA nhân đôi , tuy nhiên base Thymine bị thay thế bằng base Uracil . Nguyên tắc bổ sung lúc này sẽ	[[30, 33], [71, 74]]	[]	['DNA', 'DNA']	[]
71	trong đó I [ n = nd ] là hàm indicator , bằng 1 nếu n = nd và bằng 0 nếu ngược lại . FD là một phân phối population trên tập D văn bản đã cho . Như vậy để sinh ra	[]	[]	[]	[]
72	Dưới đây là hình vẽ mô tả mô hình của bài toán : - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03	[[316, 325]]	[]	['CNTT – TT']	[]
73	Keith đã sử dụng hai phương pháp khác nhau để thu được bộ trích xuất đặc trưng : phương pháp thứ nhất là sử dụng các đặc trưng được xây dựng thủ công với hồi quy logistic ( soft-LR ( EM ) ) , phương pháp thứ hai là sử dụng một mạng nơ-ron tích chập ( soft-CNN ( EM ) ) để tự	[[262, 264], [256, 259], [178, 180], [183, 185]]	[[227, 248], [154, 170]]	['EM', 'CNN', 'LR', 'EM']	['mạng nơ-ron tích chập', 'hồi quy logistic']
74	CHƯƠNG 1 . GIỚI THIỆU ĐỀ TÀI Trong lĩnh vực Trí tuệ nhân tạo nói chung và Học máy , Học sâu nói riêng , mạng	[]	[]	[]	[]
75	Giảng viên hướng dẫn : ThS. Ngô Văn Linh Chữ ký của GVHD	[[23, 27], [52, 56]]	[]	['ThS.', 'GVHD']	[]
76	Hình 25 : So sánh các mô hình trên bộ Recobell qua từng vòng lặp với k = 16 ( a ) HR @ 10	[[82, 84]]	[]	['HR']	[]
77	Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 43 Thuật toán 4 Phương pháp học PVB cho LDA	[[60, 71], [104, 107], [112, 115]]	[]	['KSTN - CNTT', 'PVB', 'LDA']	[]
78	Số chiều ra của GRU là 100 chiều cho chuỗi mã hóa từ , câu . Vec - tơ ngữ cảnh từ / câu cũng có số chiều là 100 và được khởi tạo ngẫu nhiên .	[[16, 19]]	[]	['GRU']	[]
79	Variational Bayesian By Backprop ( BBB ) Xét tập các biến quan sát x ∈ RN và w là các biến ẩn . Mạng nơ ron chính là một mô hình đồ thi	[[35, 38]]	[]	['BBB']	[]
80	• Constant learning rate là learning rate schedule mặc định trong thuật toán tối ưu hóa SGD . Mặc đinh , ta cài đặt momentum =0 và decay rate =0 , chọn	[[88, 91]]	[]	['SGD']	[]
81	1 . Các thuật toán hồi quy : Hồi quy tuyến tính , Hồi quy logistic , Stepwise regression , . . . 2 . Các thuật toán phân loại : Phân loại tuyên tính , Máy hỗ trợ vector ( Support Vector Machine - SVM ) , Kernel SVM , Sparse Representation-based classification ( SRC ) , . . .	[[211, 214], [262, 265]]	[]	['SVM', 'SRC']	[]
82	Ngược lại , khi tham số này lớn , mô hình sẽ tập trung nhiều hơn vào việc tiếp nhận thông tin từ dữ liệu mới đến . Không như iDropout , SVB và	[[136, 139]]	[]	['SVB']	[]
83	Hàm lỗi của K-medoids tương tự K-means : 𝑁	[]	[]	[]	[]
84	Hình 23 : Kết quả của các cách tính w khác nhau với temperature = 0.1 Hai hình vẽ trên thể hiện sự so sánh giữa SimCLR gốc và phương pháp LCL với 3 view , ta nhận thấy , phương pháp LCL có sự hội tụ nhanh hơn trong quá trình	[[112, 118], [138, 141], [182, 185]]	[]	['SimCLR', 'LCL', 'LCL']	[]
85	24 CHƯƠNG 4 . THỬ NGHIỆM VÀ ĐÁNH GIÁ	[]	[]	[]	[]
86	Trong phần thử nghiệm , CIFAR100 được chia thành 10 tác vụ , với mỗi tác vụ yêu cầu phân loại 10 vật thể . Split CIFAR10 - 100 : Tập dữ liệu gốc CIFAR10 [ 37 ] tương tự như CIFAR100	[[24, 32], [113, 126], [145, 152], [173, 181]]	[]	['CIFAR100', 'CIFAR10 - 100', 'CIFAR10', 'CIFAR100']	[]
87	của dữ liệu . Đây cũng chính là hướng nghiên cứu chính của em trong đồ án tốt nghiệp này - Self-supervised learning ( SSL ) , hay nói theo Tiếng Việt là học tự giám	[[118, 121]]	[[153, 164]]	['SSL']	['học tự giám']
88	vào các phương pháp hiện tại , em cảm thấy có hứng thú với nhánh Contrastive Learning ( CL ) , một nhánh nhỏ của SSL nhưng đang phát triển một cách vượt bậc . Dựa trên sự gợi ý và sự hướng dẫn của thầy , em nhận thấy mình có thể khai thác	[[88, 90], [113, 116]]	[]	['CL', 'SSL']	[]
89	được học đồng thời trong suốt quá trình huấn luyện . 𝛼𝑖𝑡 = 𝑇	[]	[]	[]	[]
90	"đủ ( sufficient statistics ), h(x) là một hàm cho trước và A ( θ ) là đại lượng chuẩn hóa . Những mô hình với giả sử này có tính chất "" conjugate exponential "" ."	[]	[]	[]	[]
91	diện cho trạng thái ẩn của RNN phụ di chuyển tiến và lùi theo thời gian . Điều này cho phép các đơn vị đầu ra 𝑜 ( 𝑡 ) tính toán một đại diện phụ thuộc vào cả quá khứ và	[[27, 30]]	[]	['RNN']	[]
92	Ngoài ra để đánh giá mô hình trong quá trình huấn luyện , ma trận độ chính xác [ R ] T ×T được lưu lại , với T là tổng số tác vụ . Ri , j là độ chính	[]	[]	[]	[]
93	tầng ẩn thứ 𝑙 , 𝐻ℎ( 𝑙)( 𝑥) ( 𝐿̅) là ma trận Hessian của 𝐿̅ tương ứng với ℎ( 𝑙) ( 𝑥) , 〈 . , . 〉 là phép nhân tích vô hướng . Tuy nhiên [ 14 ] chỉ đưa ra phân tích trên phân phối	[]	[]	[]	[]
94	Ta kí hiệu , 𝐹 là hàm đại diện cho mô hình mạng nơ-ron , ℎ ( 𝑙 ) là tầng ẩn thứ 𝑙 trong mạng , 𝐹 ( 𝑙 ) là hàm dựa trên phần kiến trúc mạng từ tầng 𝑙 cho đến tầng đầu ra ( nghĩa là 𝐹 (𝑥) = 𝐹 (𝑙) ( ℎ(𝑙) (𝑥)) ) .	[]	[]	[]	[]
95	42 5 THỬ NGHIỆM VÀ ĐÁNH GIÁ	[]	[]	[]	[]
96	𝑐 với 𝑐 là một hằng số PT 3.4 Khi đó thành phần KL sẽ không có dạng tường minh , nhưng có thể xấp xỉ bằng	[[23, 25], [48, 50]]	[]	['PT', 'KL']	[]
97	Có hai cách để xây dựng word2vec : - Sử dụng các từ ngữ cảnh để dự đoán tự mục tiêu ( CBOW – Continuous Bag-of-word ) - Sử dụng một từ để dự đoán ra ngữ cảnh mục tiêu ( Skip-gram )	[[86, 90]]	[[37, 83]]	['CBOW']	['Sử dụng các từ ngữ cảnh để dự đoán tự mục tiêu']
98	Các tác vụ trong cùng một bộ dữ liệu sẽ có số lượng nhãn cần phân loại là như nhau . Thêm vào đó kiến trúc CNN cũng sẽ được sử dụng để đánh giá	[[107, 110]]	[]	['CNN']	[]
99	_ alpha = 0.5 , KL - weight = 0.0001 VCL • w/o Dropout : Không cần	[[16, 18], [37, 40], [43, 46]]	[]	['KL', 'VCL', 'w/o']	[]
100	Hệ đào tạo : Đại học chính quy Đồ án tốt nghiệp ( ĐATN ) được thực hiện tại : Trường Đại học	[[50, 54]]	[[31, 47]]	['ĐATN']	['Đồ án tốt nghiệp']
101	HAT_1000_0.5 VBD - CL chính xác trung bình	[[13, 21]]	[]	['VBD - CL']	[]
102	log 𝑝 ( 𝐷| 𝑤 ) + 𝐾𝐿 ( 𝑞𝜃 , 𝛼 ( 𝑤 ) | |𝑝(𝑤 ) ) PT 3.3 Tuy nhiên trong công thức PT 3.3 , ta thấy rằng đại lượng KL vẫn phụ thuộc	[[46, 48], [79, 81], [111, 113]]	[]	['PT', 'PT', 'KL']	[]
103	liệu . Giả sứ không gian gốc có m điểm dữ liệu xi với 1 ≤ i ≤ m , thuật toán LLE sẽ tìm k lân cận gần nhất với điểm dữ liệu xi và tái thiết lập xi như là một hàm	[[77, 80]]	[]	['LLE']	[]
104	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT ( 40 ) 40	[[63, 74]]	[]	['KSTN - CNTT']	[]
105	trọng số được giới hạn tùy theo giá trị độ quan trọng của hai nơ -ron ở hai đầu , để can thiệp vào sự thay đổi của trọng số đó . Nhưng HAT chưa hiệu quả trong việc	[[135, 138]]	[]	['HAT']	[]
106	3 . 1 . 3 . Ứng dụng của GCN trong xử lý ngôn ngữ tự nhiên ( NLP ) Phân loại văn bản làm một trong những bài toán phổ biến trong NLP . Với bài	[[25, 28], [61, 64], [129, 132]]	[[35, 58]]	['GCN', 'NLP', 'NLP']	['xử lý ngôn ngữ tự nhiên']
107	Như vậy , bài toán mới của chúng ta là xấp xỉ phân phối hậu nghiệm population trên một dòng dữ liệu đến liên tục từ phân phối population Fα . Giống như VB	[[152, 154]]	[]	['VB']	[]
108	Chú ý rằng dưới đây sẽ trình bày VBD dưới góc nhìn của việc nén mạng có cấu trúc nên một số định nghĩa sẽ khác so với quy ước trong phần tóm tắt về VD .	[[33, 36], [148, 150]]	[]	['VBD', 'VD']	[]
109	𝐶× 𝑊×𝐻 với 𝐶 , 𝑊 , 𝐻 lần lượt là chiều sâu , chiều ngang và chiều dọc của ma trận . Xét tầng 𝑙 có kênh đầu ra là K chiều khi đó số	[]	[]	[]	[]
110	Với 𝐾𝐿 ( 𝑞𝑡 ( 𝜃 ) | | 𝑞𝑡−1 ( 𝜃 ) ) 𝐿 2	[]	[]	[]	[]
111	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 35	[[267, 276]]	[]	['CNTT – TT']	[]
112	giống như trong các phương pháp cơ sở đã trình bày . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 46	[[113, 124]]	[]	['KSTN - CNTT']	[]
113	Vì vậy kiến trúc của mô hình học HAN cũng được thiết kế theo mô hình phân cấp gồm 2 cấp : cấp từ , cấp câu ( được thể hiện rõ ở hình vẽ phía dưới ) . Hình 7 Mạng tập trung phân cấp ( Hierarchical Attention Network )	[[33, 36]]	[[157, 180]]	['HAN']	['Mạng tập trung phân cấp']
114	Một phiên bản ’ cứng ’ của VBD - CL , ký hiệu là HardVBD - CL , được cài đặt khi toàn bộ trọng số nối với cả hai nơ-ron quan trọng được giữ nguyên khi học các tác vụ tiếp theo . Điều này giúp mô hình giữ nguyên tri thức	[[27, 35], [49, 61]]	[]	['VBD - CL', 'HardVBD - CL']	[]
115	Hình 2 : Ví dụ về phân lớp ảnh Trong mô hình CNN có hai đặc tính đó là là tính bất biến và tính kết hợp . Ví dụ trong	[[45, 48]]	[]	['CNN']	[]
116	v2iT ∈ ℝn x n trong đó FFN là một mạng feed forward được sử dụng cho chiều cuối cùng của từ . w2i = softmax ( Mi ) v2i ∈ ℝn x de	[[23, 26]]	[]	['FFN']	[]
117	Giải pháp cho học liên tục mà đồ án đưa ra có tên là Structure Compression - based Continual Learning ( SCCL ) , học liên tục dựa trên nén cấu trúc mạng ; được xây dựng để giải quyết các vấn đề về bộ nhớ sử dụng , mở rộng mạng của nhóm phương pháp dựa trên kiến	[[104, 108]]	[[113, 152]]	['SCCL']	['học liên tục dựa trên nén cấu trúc mạng']
118	• Xác suất mỗi quan sát được sinh bởi từng thành phần 𝛾𝑛𝑘 , 𝑘 = 1 , 2 , … , 𝐾 , n = 1, 2, … , N. 1	[]	[]	[]	[]
119	( 1 , nếu như có click nằm trong top K . HR@K =	[[41, 45]]	[]	['HR@K']	[]
120	quan hệ thứ tự như đã nói , ngoài ra còn có thêm thông tin mô tả về người dùng hay item . Nhóm thứ hai gồm ba mô hình ITE - 2 , ITE - 3 và ITE - 4 , được xây dựng	[[118, 125], [128, 135], [139, 146]]	[]	['ITE - 2', 'ITE - 3', 'ITE - 4']	[]
121	số chủ đề của tập văn bản V	[]	[]	[]	[]
122	• w/o Dropout : Không cần • Dropout : droprate = 0.5 • ALV : init _ alpha = 0.5 , KL - weight = 0.01	[[2, 5], [82, 84], [55, 58]]	[]	['w/o', 'KL', 'ALV']	[]
123	Với hi vọng có thể cải thiện hướng nghiên cứu CL nói chung , LCL ra đời và mong muốn của em cùng với nhóm nghiên cứu là có thể đưa được ý tưởng này kết hợp vào rất nhiều phương pháp khác nhau trong hướng nghiên cứu CL như SimCLR ,	[[46, 48], [61, 64], [215, 217], [222, 228]]	[]	['CL', 'LCL', 'CL', 'SimCLR']	[]
124	sát ( supervised learning ) , học không giám sát ( unsupervised learning ) , học tăng cường ( reinforcement Learning ) . Có thể nói , việc học kinh nghiệm E chính là việc học các tham số mô hình ( model	[]	[]	[]	[]
125	49.6 49.7 Bảng 2 : Kết quả của biểu diễn ẩn khi thay đổi các yếu tổ ảnh hưởng đến tập hoán vị ( Nguồn : [ NF16 ] )	[]	[]	[]	[]
126	tự bài toán phân loại cho K + 1 lớp . Hàm mất mát được tính như sau : 𝑃	[]	[]	[]	[]
127	vụ cũ 5 . 2 . Từ quan sát trên hai tập dữ liệu , VBD - CL , UCL , AGS - CL cân bằng tính ổn định và	[[49, 57], [60, 63], [66, 74]]	[]	['VBD - CL', 'UCL', 'AGS - CL']	[]
128	2 . 3 . Mô hình chủ đề ẩn ( Latent Dirichlet Allocation ) 2 . 3 . 1 . Ý tưởng Mô hình Latent Dirichlet Allocation ( LDA ) là mô hình sinh xác xuất cho tập	[[116, 119]]	[[8, 25]]	['LDA']	['Mô hình chủ đề ẩn']
129	Hình 6 : Hiệu suất LPP của các phương pháp trên mô hình LDA với 6 bộ dữ liệu không có nhãn thời gian Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[19, 22], [56, 59], [161, 172]]	[]	['LPP', 'LDA', 'KSTN - CNTT']	[]
130	− → EFα [ log p( X ] = EFα [ KL ( q ( z , Θ ) | | p ( z , Θ | X ) ) ] + EFα [ Eq [ p ( z , Θ , X ) − q ( z , Θ ) ] ] Theo bất đẳng thức Jensen đối với hàm lõm log ta có :	[]	[]	[]	[]
131	câu nhiều nhất trong một văn bản là MAX _ SENTS , ( ii ) số từ nhiều nhất trong một câu MAX _ SENT _ LENGTH , ( iii ) số bộ trong tập dữ liệu là TRAIN _ SET _ LENGTH .	[]	[]	[]	[]
132	EWC có thể học trong môi trường liên tục nhiều tác vụ , tuy nhiên tác giả diễn giải trên hai tác vụ cho đơn giản và dễ hình dung . Tham số cho việc học tác	[[0, 3]]	[]	['EWC']	[]
133	‖ 22 là chuẩn Frobenius của một ma trận . Ta thấy thành phần ( 𝑎 ) trong công thức PT 2. 17 đóng vai trò như một đại lượng ( 𝑙 )	[[83, 85]]	[]	['PT']	[]
134	Chữ ký của GVHD Bộ môn : Viện :	[[11, 15]]	[]	['GVHD']	[]
135	Một thuật toán A được gọi là mạnh mẽ - ( K , ) , với ( · ) : X m → R nếu với mọi tập học S ∈ X m , ∀s ∈ S , ∀z ∈ X , ∀i ∈ { 1 , ... , K} : s , z ∈ Xi = ⇒ | L ( A ( S) , s ) − L ( A ( S ) , z ) | ≤ ( S )	[]	[]	[]	[]
136	2 . Tác tử thứ hai , tương ứng với ngôn ngữ B , nhận được thông điệp được gửi từ tác tử thứ nhất sang dưới dạng ngôn ngữ B và kiểm tra tính hợp lý của đoạn thông điệp đó	[]	[]	[]	[]
137	EWC HAT Tác v 11	[[4, 7], [0, 3]]	[]	['HAT', 'EWC']	[]
138	qi Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 8	[[66, 77]]	[]	['KSTN - CNTT']	[]
139	L1 làm đại lượng hiệu chỉnh : 𝑁 ∗	[]	[]	[]	[]
140	𝐿𝐷 ( 𝛽 ) trong công thức ( 3 ) . Bài toán hồi quy Logistic cho Group Lasso [ 46 ] được viết dưới dạng :	[[0, 2]]	[]	['𝐿𝐷']	[]
141	( DVAE ) [ 3 ] có thực hiện một ý tưởng đó là sử dụng thành phàn nhiễu ² để thêm vào bức ảnh gốc ban đầu . Với mục tiêu đặt ra là mặc dù ta đưa bức ảnh	[[2, 6]]	[]	['DVAE']	[]
142	thay đổi C thì kết quả không có quá nhiều chênh lệch nên có thể cách làm này cũng không quá cải thiện được kết quả cuối cùng của mô hình . Song , nó có thể	[]	[]	[]	[]
143	Thông số thực nghiệm chung : • Optimizer : Adam với β1 = 0.5 , β2 = 0.999 • Nhiễu z : gồm 100 chiều tuân theo phân phối chuẩn N ( 0, I )	[]	[]	[]	[]
144	bộ các thử nghiệm . - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03	[[287, 296]]	[]	['CNTT – TT']	[]
145	Quá trình học ra biểu diễn văn bản với đầu vào là TF - IDF của văn bản đó , cố định tham số 𝛽̅ đã học và tối ưu tham số 𝜃 : 24	[[50, 58]]	[]	['TF - IDF']	[]
146	Networks ( CNN ) còn có nhiều các mạng nơ-ron kết cấu phức tạp khác như Recurrent Neural Networks ( RNN ) ( LR Medsker , 2001 ) , Transformer ( Ashish Vaswani , 2017 ) , … Trong đồ án này sử dụng hai mạng nơ-ron là MLP và CNN cho bài toán phân loại ảnh để thử nghiệm	[[11, 14], [100, 103], [215, 218], [222, 225]]	[]	['CNN', 'RNN', 'MLP', 'CNN']	[]
147	Hình 24 : Kết quả khi so sánh các cách tính w khác nhau với số view là 5 Thêm một thí nghiệm nữa thì số view đã được tăng lên là 5 . Ở thí nghiệm này em	[]	[]	[]	[]
148	Hình 7 : Hiệu suất NPMI của các phương pháp trên mô hình LDA với 6 bộ dữ liệu không có nhãn thời gian Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[19, 23], [57, 60], [162, 173]]	[]	['NPMI', 'LDA', 'KSTN - CNTT']	[]
149	PT 3.1 Graph Convolutional Networks ( GCN ) Graph Convolutional Networks ( GCN ) là một mạng neural đa lớp hoạt	[[38, 41], [0, 2], [75, 78]]	[]	['GCN', 'PT', 'GCN']	[]
150	cũng là một mô hình sinh rất hiệu quả . Phần tiếp theo sẽ trình bày chi tiết cơ sở lý thuyết và phương pháp học cho mô hình LDA để làm rõ những điều này .	[[124, 127]]	[]	['LDA']	[]
151	không có nhiều cải thiện so với mô hình ITE - item _ pcat , đôi khi còn thiếu tính ổn định . Nguyên nhân có thể là do cách xây dựng biểu diễn pcat của	[[40, 57]]	[]	['ITE - item _ pcat']	[]
152	TN : True Negative Độ đo MRR Độ đo MRR có ý nghĩa đánh giá hệ thống gợi ý dựa vào thứ tự sắp xếp các vị trí	[[25, 28], [35, 38], [0, 2]]	[]	['MRR', 'MRR', 'TN']	[]
153	0.65 Tác v 9 EWC	[[13, 16]]	[]	['EWC']	[]
154	để đưa ra đầu ra có cỡ 4 × 4 × K . Tiếp đó , ma trận trọng số ứng W l được tính bằng công thức : ∂L ∂z l	[]	[]	[]	[]
155	Hệ đào tạo : Đại học chính quy Đồ án tốt nghiệp ( ĐATN ) được thực hiện tại : Trường Đại học Bách Khoa	[[50, 54]]	[[31, 47]]	['ĐATN']	['Đồ án tốt nghiệp']
156	1825K Tuy nhiên giả định trong môi trường học liên tục là số lượng tác vụ có thể vô cùng lớn hay thậm chí là vô hạn , nên đây vẫn là một vấn đề cần phải giải quyết	[]	[]	[]	[]
157	42 7 CÁC CHỮ VIẾT TẮT	[]	[]	[]	[]
158	Hình 5 . 7 : Độ chính xác trung bình khi kết thúc mỗi tác vụ của VBD - CL và HardVBD - CL . 44 HAT_800_0.1	[[65, 73], [77, 89]]	[]	['VBD - CL', 'HardVBD - CL']	[]
159	Giả sử phân phối xấp xỉ hậu nghiệm trên 𝜃 là GMF , khi đó ta có 2 ) 𝑞𝜙 ( 𝜃𝑑ℎ ) = 𝑁( 𝜇𝑑ℎ , 𝜎𝑑ℎ	[[45, 48]]	[]	['GMF']	[]
160	Cụ thể , vector đầu vào ở lớp thấp nhất sẽ có dạng xi = embeding + P E ( i ) với i là vị trí của từ tương ứng . Cụ thể như sau :	[]	[]	[]	[]
161	5.4.3 Thử nghiệm với mạng CNN Split CIFAR100 và Split CIFAR10 - 100 : VBD - CL cũng thể hiện hiệu quả vượt	[[26, 29], [36, 43], [54, 67], [70, 78]]	[]	['CNN', 'CIFAR10', 'CIFAR10 - 100', 'VBD - CL']	[]
162	đảm bảo hội tụ đến điểm tối ưu . Từ đây chúng ta có phương pháp học PVB cho mô hình LDA được trình bày chi tiết dưới giải thuật sau :	[[68, 71], [84, 87]]	[]	['PVB', 'LDA']	[]
163	 ( 6 ) Trong các hệ thống thực tế , số lượng các người dùng M và số lượng các item	[]	[]	[]	[]
164	tham số tối ưu 𝜃𝐵 ∗ sao cho nằm trong miền tối ưu của 𝜃𝐴 ∗ . EWC cho rằng tồn tại một miền giao thoa giữa các tham số tối ưu của từng tác vụ và mong muốn tìm ra	[[61, 64]]	[]	['EWC']	[]
165	sự hướng dẫn của PGS. TS. Đỗ Phan Thuận . Các kết quả trong đồ án là trung thực , không phải sao chép toàn văn của bất kì công trình nào khác .	[[17, 21], [22, 25]]	[]	['PGS.', 'TS.']	[]
166	Giá trị 𝑚=1 𝐿 𝑀	[]	[]	[]	[]
167	𝐴𝑟𝑔𝑚𝑖𝑛𝜃 , 𝛽 𝐿 = ∑ ( 𝛾̅𝑑𝑣 − 𝜃𝑑𝑘 𝑣 Sau quá trình huấn luyện , tham số 𝛽 được cố định và sử dụng cho quá trình suy	[]	[]	[]	[]
168	Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 67 Tài liệu tham khảo	[[60, 71]]	[]	['KSTN - CNTT']	[]
169	Các phương pháp được huấn luyện với 100 vòng lặp , riêng VBD - CL là 150 . 5.4 5.4.1	[[57, 65]]	[]	['VBD - CL']	[]
170	liệu VCCorp được cho ở Bảng 2 . Bộ dữ liệu này được sử dụng để so sánh , đánh giá giữa ba mô hình ITE - 2 , ITE - 3 và ITE - 4 .	[[98, 105], [108, 115], [119, 126]]	[]	['ITE - 2', 'ITE - 3', 'ITE - 4']	[]
171	Tuy nhiên , nếu so sánh với mô hình NAML và NRMS đã trình bày ở phần trên thì mô hình LSTUR có kết quả tốt hơn NRMS nhưng kém hơn so với NAML . Từ đó có thể thấy , cách tiếp cận của mô hình LSTUR là hợp lý nhưng chưa khai thác	[[36, 40], [44, 48], [86, 91], [111, 115], [137, 141], [190, 195]]	[]	['NAML', 'NRMS', 'LSTUR', 'NRMS', 'NAML', 'LSTUR']	[]
172	j−k/2 , . . . , hj+k/2 ] + bw ) + dj với hij là đầu ra của kernel j trong lớp i . Ở vị trí encoder , các đầu ra của lớp trước được đảm bảo có số chiều tương ứng để trở	[]	[]	[]	[]
173	43 Bảng 3 . 2 miêu tả kết quả BLEU của các thử nghiệm áp dụng nhiễu với 3 mô hình ngôn ngữ , có thể nhận thấy mô hình dịch sử dụng nhiều kênh nhiễu có kết quả gần tương đương	[[30, 34]]	[]	['BLEU']	[]
174	NDCG @ 10 Hình 19 : So sánh các mô hình trên bộ Retailrocket qua từng vòng lặp với k = 64 Hình 19 và Hình 20 là kết quả độ đo trên tập test của các mô hình qua	[[0, 4]]	[]	['NDCG']	[]
175	với 𝑠𝑖 ∈ 𝑠 sẽ được biểu diễn bằng nhiều phân phối Gauss đơn biến với tỉ lệ trộn khác nhau : 𝑀	[]	[]	[]	[]
176	5 . 3 . 4 Kết quả thử nghiệm 0.80 SVB	[[34, 37]]	[]	['SVB']	[]
177	50 % mạng là có thể học xong 5 tác vụ của Split MNIST . Permuted MNIST : Ở tập dữ liệu này , VBD - CL vẫn giữ được sự ổn định của độ chính xác trung bình trong quá trình học và từ tác vụ thứ hai trở đi đều cao hơn các	[[48, 53], [65, 70], [93, 101]]	[]	['MNIST', 'MNIST', 'VBD - CL']	[]
178	khác nhau , độ chính xác được tính trên tập test với thang đo F1 . Mô hình học Bộ dữ liệu Facebook	[]	[]	[]	[]
179	Với M = 3 , đồ thị mà mô hình dự đoán khá giống với đồ thị thực tế . Tuy nhiên , khi bậc của mô hình dự đoán tăng lên	[]	[]	[]	[]
180	CƠ SỞ LÝ THUYẾT Các khái niệm cơ bản Feature extractor ( FE ) - trích xuất đặc trưng là một quá trình nhằm biến dữ	[[57, 59]]	[[64, 84]]	['FE']	['trích xuất đặc trưng']
181	nữa VCL cần hai tham số ( kỳ vọng và độ lệch chuẩn ) để biểu diễn cho mỗi trọng số của mạng . Do vậy số lượng tham số sẽ gấp đôi số lượng trọng số gốc của mạng ,	[[4, 7]]	[]	['VCL']	[]
182	Do đó việc sử dụng một phân phối đơn giản hơn để xấp xỉ 𝑝 ( 𝜃 | 𝐷 ) là cần thiết . Từ những nhận định đó , Suy diễn biến phân ( VI ) là một phương pháp đã	[[128, 130]]	[[107, 125]]	['VI']	['Suy diễn biến phân']
183	Bảng 5 . 7 : Phần trăm không gian mạng CNN sử dụng qua các tác vụ trên tập Split Omniglot Tác vụ	[[39, 42]]	[]	['CNN']	[]
184	AB Tính stochastic gradient của ΘBA : K	[]	[]	[]	[]
185	( 2005 ) [ 64 ] ; Langfelder và cộng sự ( 2008 ) [ 65 ] giới thiệu WGCNA , một công cụ phân nhóm gene đồng biểu hiện dựa trên HAC , hỗ trợ việc chọn lựa cách tính khoảng cách	[[67, 72], [126, 129]]	[]	['WGCNA', 'HAC']	[]
186	hiệu quả trong bài toán phân loại cảm xúc văn bản chỉ đạt 78 % , 80 % tương ứng với Naïve Bayes , SVM học trên tập dữ liệu Facebook , và 77 % , 81 % tương ứng với Naïve 25	[[98, 101]]	[]	['SVM']	[]
187	34 Bảng 2 . 5 : Điểm BLEU của các thí nghiệm sử dụng mô hình ConvS2S , b là beam size khi thực hiện beam search sử dụng hàm phạt mặc định	[[21, 25], [61, 68]]	[]	['BLEU', 'ConvS2S']	[]
188	Mạng nơ-ron tích chập ( convolutional neural network , CNN ) [ 12 ] , [ 15 ] là một mạng nơ-ron truyền tiến đặc biệt và áp dụng chủ yếu vào lĩnh vực xử lý ảnh . Đầu vào cũng như biểu diễn	[[55, 58]]	[[0, 21]]	['CNN']	['Mạng nơ-ron tích chập']
189	hàng xóm của người dùng u thông thường sẽ là những người dùng v mà có mức Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 13	[[137, 148]]	[]	['KSTN - CNTT']	[]
190	𝟒1. 𝟑7 ± 0. 𝟑6 Bảng 2 . 3 Kết quả mô hình LSTUR	[[42, 47]]	[]	['LSTUR']	[]
191	Các siêu tham số đó là : λ của EWC , { smax , c } của HAT , { β } của UCL , { λ , µ , ρ } của AGS - CL , { thresh , s } của VBD - CL 35	[[31, 34], [54, 57], [70, 73], [94, 102], [124, 132]]	[]	['EWC', 'HAT', 'UCL', 'AGS - CL', 'VBD - CL']	[]
192	Từ các tính chất trên chúng ta có nhận xét rằng : Không giống như VB cổ điển , trong đó việc cực đại hoá hàm ELBO này tương đương với cực tiểu khoảng cách	[[66, 68], [109, 113]]	[]	['VB', 'ELBO']	[]
193	Hà Nội , ngày 10 tháng 6 năm 2021 Tác giả ĐATN Vũ Hồng Phúc	[[42, 46]]	[]	['ĐATN']	[]
194	liên quan đến mô hình LDA đã được trình bày ở phần 2 . 1 . Tuy nhiên có một điều khác biệt là trong các phương pháp học dòng , quá trình sinh được định	[[22, 25]]	[]	['LDA']	[]
195	nói một cách khác , việc tối ưu hàm loss NCE đồng nghĩa với việc ta đi cực đại hóa MI giữa các view khác nhau của cùng một bức ảnh . Tuy nhiên , trong phạm	[[41, 44], [83, 85]]	[]	['NCE', 'MI']	[]
196	nhau tùy thuộc vào từng miền và lĩnh vực mà bài toán gợi ý đang áp dụng . Để học các tham số của mô hình , có thể sử dụng kĩ thuật stochastic gradient descent ( SGD ) truyền thống .	[[161, 164]]	[]	['SGD']	[]
197	Hà Nội , ngày 22 tháng 05 năm 2019 Giảng viên hướng dẫn PGS. TS. Đỗ Phan Thuận	[[56, 60], [61, 64]]	[]	['PGS.', 'TS.']	[]
198	sẽ được học còn Q là tập các mã được tính theo mọt bài toán tối ưu khác không liên quan đến bước học . Sau khi có đủ ba thành phần z , Q , C , sẽ khởi	[]	[]	[]	[]
199	Mặt khác , trong bài báo gốc , các tác giả cũng so sánh HPP với SVB - PP khi tinh chỉnh cẩn thận tham số ρ .	[[56, 59], [64, 72]]	[]	['HPP', 'SVB - PP']	[]
200	biến phân cũng cần mang tri thức tiên nghiệm . Kỹ thuật Reparameterization : Để có thể tối ưu ELBO với tham số φ , cần phải	[[94, 98]]	[]	['ELBO']	[]
201	. . . . . . . . . . . . . . . . 31 5 . 1 Độ chính xác trung bình khi kết thúc mỗi tác vụ trên tập Split MNIST	[[104, 109]]	[]	['MNIST']	[]
202	Dropout biến phân cụ thể được cài đặt , đó là Dropout Bayes biến phân ( Variational Bayesian Dropout , VBD ) [ 1 ] . Các kết quả thử nghiệm tiến hành trên một số bộ dữ	[[103, 106]]	[[46, 69]]	['VBD']	['Dropout Bayes biến phân']
203	21 2 KIẾN THỨC CƠ SỞ • Hàm sigmoid với công thức	[]	[]	[]	[]
204	không thay đổi quá nhiều theo thành phần ( a ) . UCL thay đổi hai thành phần này dựa trên các nhận xét vừa rồi để xây dựng độ không chắc chắn cho nơ-ron , từ đó	[[49, 52]]	[]	['UCL']	[]
205	quá khứ và tính toán xấp xỉ được phân phối hậu nghiệm trên dữ liệu này , tức là p ( Θ | D1 , D2 , ... , Dt−1 , η ) . Khi đó chúng ta có thể tính toán phân phối hậu nghiệm	[]	[]	[]	[]
206	Nói cách khác , mỗi giá trị trong ma trận tương tác R sẽ có công thức rui ≈ pTu qi . Giá trị biểu thức pTu qi sẽ cao nếu như hệ số của	[]	[]	[]	[]
207	Xét mô hình tổng quát B ( Θ , z , x ) trong đó x1 : N là các biến dữ liệu quan sát được , Θ là biến ẩn toàn cục đặc trưng cho mô hình , z1 : N là các biến ẩn cục bộ mà mỗi zi đặc trưng tương ứng với một xi .	[]	[]	[]	[]
208	∗ ) ℒ 𝑡 ( 𝜃 ) = ℒ ( 𝐷𝑡 , 𝜃 ) + 𝑅( 𝜃 , 𝜃𝑡−1 với R là hàm ràng buộc	[]	[]	[]	[]
209	( a ) Cấu trúc Transformer ( b ) Kiến trúc mô-đun User Encoder Hình 3 . 4 Mô hình đề xuất dựa trên BERT	[[99, 103]]	[]	['BERT']	[]
210	28.58 31.87 Bảng 2 . 6 : Điểm BLEU của các thí nghiệm sử dụng mô hình Transformer , b là beam size khi thực	[[30, 34]]	[]	['BLEU']	[]
211	đoán ra các đặc trưng quan trọng nhất . Representation bias Các bias trong MTL được học và chia sẻ chung cho các tác vụ vì thế nó không quá lệch	[[75, 78]]	[]	['MTL']	[]
212	dụng Dropout và ALV giúp cho EWC có được sự ổn định hơn trong việc học chuỗi tác vụ liên tục dài . Về tổng thể quá trình , Dropout và ALV đạt hiệu năng ngang	[[16, 19], [29, 32], [134, 137]]	[]	['ALV', 'EWC', 'ALV']	[]
213	0.3 Số subword CNN 2	[[15, 18]]	[]	['CNN']	[]
214	đo đánh giá là backward transfer ( BWT ) và độ chính xác trung bình trên tất cả các task ( ACC ) . Backward transfer là một công cụ tốt để đo mức độ ảnh hưởng của việc học task mới	[[35, 38], [91, 94]]	[[44, 88]]	['BWT', 'ACC']	['độ chính xác trung bình trên tất cả các task']
215	Tuy nhiên trong công việc này , LRT sẽ không được sử dụng trên các lớp tích chập mà chỉ sử dụng trên các tầng tuyến tính .	[[32, 35]]	[]	['LRT']	[]
216	Số tầng L sẽ được điều chỉnh trong quá trình tinh chỉnh tham số của mô hình như một siêu tham số . Tầng mã hóa ( Embedding Layer )	[]	[]	[]	[]
217	không đổi . Khi đó phân phối hậu nghiệm p ( z , Θ | X ) là một hàm ngẫu nhiên của dữ liệu , và giá trị kỳ vọng của phân phối này được gọi là phân bố hậu nghiệm	[]	[]	[]	[]
218	Mô hình dự đoán của NCF có thể viết thành công thức như sau : y ̂u i = f (P T u , Q T i | P , Q , Θ f )	[[20, 23]]	[]	['NCF']	[]
219	Theo cách tiếp cận Bayes , giả sử phân phối của các điểm dữ liệu giữa các tác vụ là độc lập với nhau , 𝑝 ( 𝜃 ) là xác suất tiên nghiệm của tham số θ , xác suất hậu nghiệm sau khi quan sát được dữ liệu từ T tác vụ là :	[]	[]	[]	[]
220	khoảng 3 - 4 % so với SVB - PP và PVB . Trong thử nghiệm này , với bộ dữ liệu được Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[22, 30], [34, 37], [143, 154]]	[]	['SVB - PP', 'PVB', 'KSTN - CNTT']	[]
221	Hình 4 . 2 Bộ dữ liệu CIFAR10 . Nguồn [ 16 ] . CIFAR10 là bộ dữ liệu gồm các ảnh có kích thước 32 × 32 cùng với ba kênh	[[22, 29], [47, 54]]	[]	['CIFAR10', 'CIFAR10']	[]
222	Độ đo đánh giá cho mô hình đề xuất và các mô hình còn lại là Hit Ratio ( HR ) và Normalized Discounted Cumulative Gain ( NDCG ) . Độ đo HR : Với mỗi người dùng , nếu sản phẩm test xuất hiện trong K sản phẩm	[[73, 75], [121, 125], [136, 138]]	[]	['HR', 'NDCG', 'HR']	[]
223	Bảng 3 : Kết quả của biểu diễn ẩn tìm được khi cố gắng ngăn chặn các điểm yếu ( Nguồn : [ NF16 ] ) 3.2.5 Một framework đơn giản cho việc tìm biểu diễn ẩn bằng học tương phản	[]	[]	[]	[]
224	tác vụ thứ t sẽ được sử dụng để đánh giá cho tác vụ đó . Học liên tục dựa trên tính không chắc chắn Theo [ 3 ] , trong phương pháp VCL nêu trên , để tính toán được đại lượng khả	[[131, 134]]	[]	['VCL']	[]
225	Khi so sánh giữa các mô hình cho thấy mô hình gợi ý đề xuất dựa trên BERT cho kết quả trên cả bốn độ đo cao hơn NAML đến 4 - 6 % , cao hơn LSTUR 2 - 2,5 % , kém mô hình NRMS 0.5 % trên độ đo AUC và MRR , cao hơn 0.5 % trên	[[112, 116], [139, 144], [169, 173], [191, 194], [198, 201]]	[]	['NAML', 'LSTUR', 'NRMS', 'AUC', 'MRR']	[]
226	λt = λ̃t + λ̃t−1 + ... + λ̃1 + λ̃0 Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT ( 18 )	[[95, 106]]	[]	['KSTN - CNTT']	[]
227	Như chúng ta thấy , trong hướng nghiên cứu CL nói chung , cách làm tạo ra các cặp view positive và negative rồi encode về biểu diễn ẩn và cố gắng đưa các biểu diễn ẩn của cặp positive về gần nhau đang gặp phải một vấn đề ,	[[43, 45]]	[]	['CL']	[]
228	Số lượng chủ đề Topics coherence ( NPMI )	[[35, 39]]	[]	['NPMI']	[]
229	sâu trong kiến trúc của mạng nơ-ron , còn mô hình NMTR tăng về chiều ngang . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[50, 54], [140, 151]]	[]	['NMTR', 'KSTN - CNTT']	[]
230	Các thành phần chính của SSL Từ trong định nghĩa của SSL , ta cũng có thể mường tượng ra , thành phần cơ bản chính của SSL chính là task mà ta định nghĩa , hay còn gọi là một pretext task .	[[25, 28], [53, 56], [119, 122]]	[]	['SSL', 'SSL', 'SSL']	[]
231	Lớp : CNTT 2.03 K59 Hệ đào tạo : Kĩ sư	[[6, 10]]	[]	['CNTT']	[]
232	• Vec - tơ hóa dữ liệu : Chọn những từ xuất hiện nhiều hơn 5 lần để xây dựng bộ từ vựng và thay những từ không có trong bộ từ vựng bằng token ‘ UNK ’ . • Mô hình HAN :	[[162, 165]]	[]	['HAN']	[]
233	Một phần [ X ] ij của đầu vào sẽ tạo ra giá trị lớn với bộ lọc F k nếu chúng tương đồng với nhau . Sau lớp tích chập , chiều không gian d1 × d2 của đầu vào X giảm về	[]	[]	[]	[]
234	Cuối cùng , rất hi vọng bạn đọc sẽ yêu thích những kiến thức mà đồ án mang lại và trong tương lai , mình mong rằng sẽ có nhiều người thích thú và đi theo hướng nghiên cứu SSL để có thể cùng chia sẻ , tìm tòi những kiến thức thú vị về lĩnh này .	[[171, 174]]	[]	['SSL']	[]
235	Từ đó ta thu được t giá trị độ chính xác , chỉ số đánh giá sẽ là trung bình của t giá trị này hay độ đo trong công thức PT 4 . 1 . Các phương pháp sẽ được so sánh qua độ chính xác trung bình sau khi đã học	[[120, 122]]	[]	['PT']	[]
236	+ KL ( qφ (θ ) | p (θ ) ) ( 2.14 ) Như vậy tối thiểu hóa KL ( qφ (θ ) | p (θ|D) ) tương đương với tối đa hóa Eqφ ( θ ) [ log p(D|θ ) ] −	[]	[]	[]	[]
237	Ví dụ cả hai mô hình đều dự đoán đúng : Câu 1 : We ' ve been spending a lot of time in Los Angeles talking to TV production people	[]	[]	[]	[]
238	• Lấy X = α − β ∗ log ( − log ( Y ) ) • Trong trường hợp X tuân theo phân phổi Gumbel chuẩn tắc . X = − log ( − log ( Y ) )	[]	[]	[]	[]
239	các lớp trong decoder trước khi tính điểm attention . Cuối cùng , phân phối xác suất của T ứng cử viên cho vị trí thứ đầu ra yj+1 sinh bởi decoder được tính như sau :	[]	[]	[]	[]
240	Y − là tập rỗng hoặc lấy ngẫu nhiên từ tập các cặp ( u , i ) chưa có tương tác rõ ràng . Hàm regular R ( u , i ) được sử dụng là tổng các chuẩn 2 của các vec-tơ tầng	[]	[]	[]	[]
241	. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03	[[269, 278]]	[]	['CNTT – TT']	[]
242	kích thước vector embedding dmodel , kích thước lớp ẩn df f của khối FFN , kích thước của dk và dv . Chi tiết về các lựa chọn này được miêu tả trong Bảng 2 . 4 , ngoài ra kích thước của dk	[[69, 72]]	[]	['FFN']	[]
243	sử dụng mô hình phân loại , do đó đồ án sẽ tập trung trình bày VCL cho bài toán Học liên tục các tác vụ phân loại . Trong bài toán phân loại , đối với từng tác vụ , dữ liệu được biểu diễn dưới dạng	[[63, 66]]	[]	['VCL']	[]
244	còn được gọi là hàm Digamma . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 24	[[90, 101]]	[]	['KSTN - CNTT']	[]
245	Bảng 0.4 Tham số tốt nhất cho Split CIFAR10 / 100	[[36, 49]]	[]	['CIFAR10 / 100']	[]
246	có của người dùng như sở thích dài hạn và ngắn hạn . Hay trong mô hình LSTUR [ 4 ] có đề cập đến sở thích dài hạn của người dùng , tuy nhiên cách tiếp cận dựa	[[71, 76]]	[]	['LSTUR']	[]
247	hình 21 : ACD ca sử dụng đăng ký ứng viên hình 22 : ACD ca sử dụng đăng ký nhà tuyển dụng 43	[[10, 13], [52, 55]]	[]	['ACD', 'ACD']	[]
248	nào minh chứng cho những điều mà em đã lập luận ở Phần 4 . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 50	[[122, 133]]	[]	['KSTN - CNTT']	[]
249	bình để làm giá trị dự đoán cuối cùng . Trong Học liên tục , BNN thường được sử dụng trong các phương pháp tiếp cận theo suy diễn Bayes và xấp xỉ VI , ở đó phân phối xấp xỉ hậu nghiệm 𝑞𝜙 ( 𝜃 ) sẽ	[[61, 64], [146, 148]]	[]	['BNN', 'VI']	[]
250	Sinh viên thực hiện : Phan Tuấn Anh - 20150157 • DKL ( P ||Q ) > 0 ∀ P ( X ) , Q ( X ) • DKL ( P ||Q ) = 0 ⇔ P ( x) = Q ( x )	[]	[]	[]	[]
251	 E ( X ) = Var ( X ) = λ Hình 4 mô tả phân phối Poisson với các giá trị kỳ vọng khác nhau . Hình 4 : Phân phối Poisson với kỳ vọng 𝜆 khác nhau	[]	[]	[]	[]
252	PT 2.4 5 • Luồng suy diễn biến phân liên tục ( OVI ) : dựa trên suy diễn Bayes , ràng	[[0, 2], [47, 50]]	[[11, 44]]	['PT', 'OVI']	['Luồng suy diễn biến phân liên tục']
253	Current Context ( C2 ) : là tất cả các thông tin cung cấp ngữ cảnh cho sự kiện chính , giúp hiểu sự kiện chính trong mối quan	[]	[]	[]	[]
254	32.3 32.8 Bảng 1 : Kết quả của biểu diễn ẩn tìm được ở các không gian màu khác nhau ( Nguồn [ ZIE17 ] )	[]	[]	[]	[]
255	Gauss , và phân tách hoàn toàn : qφ ( E ) = QK	[]	[]	[]	[]
256	repeat t=t +1 ; Hai câu sA và sB tương ứng từ DA và DB ;	[]	[]	[]	[]
257	câu từ 𝑤𝑖𝑡 tới 𝑤𝑖1 : 𝑥𝑖𝑡 = 𝑊𝑒 𝑤𝑖𝑡 , 𝑡 ∈ [ 1, T ] ℎ⃗⃗𝑖𝑡 = ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗	[]	[]	[]	[]
258	Batchsize : Trong thiết lập gốc batch size được để là 512 . Với LCL , do tăng số view lên nên đòi hỏi dung lượng RAM GPU phải lớn hơn , do đó , với số view	[[64, 67], [113, 116], [117, 120]]	[]	['LCL', 'RAM', 'GPU']	[]
259	2 KIẾN THỨC CƠ SỞ Hình 2 : Minh họa gợi ý dùng lọc cộng tác độ quan tâm với các item đã được tương tác bởi cả u và v , gọi là Iuv , tương đồng	[]	[]	[]	[]
260	Factorization – GMF ) với mô hình Multi - layer Perceptron ( MLP ) để tạo thành mô hình Neural Matrix Factorization ( NeuMF ) mới . Mô hình GMF .	[[16, 18], [61, 64], [118, 123], [140, 143]]	[]	['GM', 'MLP', 'NeuMF', 'GMF']	[]
261	√𝑑 / ℎ ) 𝑉 query Q , key K , và value V là các bộ tham số mô hình , giá trị √𝑑 / ℎ được sử	[]	[]	[]	[]
262	được nhân bản ra thành ma trận 𝛼𝑓𝑢𝑙𝑙 có kích thước 𝑀 × 𝐷 , tương ứng với việc lấy mẫu riêng biệt 𝑀 lần cho từng điểm dữ liệu . ( 𝑙 )	[]	[]	[]	[]
263	log 𝑝 ( 𝑦𝑡𝑚 | 𝑠𝑡 , 𝜃 , 𝑥𝑡𝑚 ) là kỳ vọng của lô-ga-rít khả năng xảy ra của điểm dữ liệu 𝑚 , ta có thể biểu diễn 𝐿 ( 𝑚) = 𝐸𝛾𝑚 , 𝜀𝑚 log 𝑝 ( 𝑦𝑡𝑚 | 𝛼𝑡 , 𝜇𝑡 , 𝜎𝑡 , 𝛾 𝑚 , 𝜀 𝑚 , 𝑥𝑡𝑚 ) . Sau	[]	[]	[]	[]
264	Hình 27 : Kết quả các mô hình ITE ngày 18 / 4 ( a ) HR @ 10	[[30, 33], [52, 54]]	[]	['ITE', 'HR']	[]
265	mạch đơn nucleotide . Ở sinh vật nhân sơ ( ví dụ vi khuẩn ) , chuỗi RNA sau khi hình thành có thể đem đi tổng hợp protein được ngay , trong khi ở sinh vật nhân chuẩn ( như động ,	[[68, 71]]	[]	['RNA']	[]
266	Độ đo MRR được tính như sau [ 18 ] : 𝑄 1	[[6, 9]]	[]	['MRR']	[]
267	PT 3.1 Trong đó db ( database ) là 1 câu đã được chọn , gồm danh sách các từ , query là câu đang được xem xét , 𝑙𝑒𝑛 ( 𝑋 ) trả về số lượng phần tử trong X .	[[16, 18], [0, 2]]	[]	['db', 'PT']	[]
268	Tóm lại , có thể thấy được mô hình học biểu diễn đề xuất cho kết quả khả quan và có thể áp dụng vào các bài toán khác . Khi đánh giá mô hình gợi ý đề xuất dựa trên ý tưởng của mô hình BERT , do	[[184, 188]]	[]	['BERT']	[]
269	Lớp : CNTT 2.03 Email : hieu.lm161522@sis.hust.edu.vn Hệ đào tạo : Kỹ sư chính quy	[[6, 10]]	[]	['CNTT']	[]
270	Một tư tưởng ban đầu của tác giả đó là việc thiết kế lên một mạng CNN , nhận đầu vào là 9 patch của ảnh xếp chồng các kênh màu lên nhau . ( ví dụ ảnh có 3	[[66, 69]]	[]	['CNN']	[]
271	Chú ý rằng không gian thuộc tính LDA của người dùng và banner không nhất thiết phải dùng chung . Như vậy , người dùng	[[33, 36]]	[]	['LDA']	[]
272	Các yêu cầu ( AJAX ) sẽ được đưa đến Application . Dựa vào cài đặt các yêu cầu này mà	[[14, 18]]	[]	['AJAX']	[]
273	ta sẽ sử dụng mạng nơ-ron đơn giản là Multi-layer perceptron ( MLP ) . Ví dụ như ta sử dụng MLP với đầu ra tuân theo phân phối Gaussian cho bộ mã	[[63, 66], [92, 95]]	[]	['MLP', 'MLP']	[]
274	Độ đo AUC được sử dụng để tỷ lệ , sự tương quan giữa TPR ( True Positive Rate ) và FPR ( False Positive Rate ) khi thay đổi số các item gợi ý bằng cách đo tỷ lệ diện tích phần dưới của biểu đồ ROC [ 18 ] :	[[6, 9], [53, 56], [83, 86], [193, 196]]	[]	['AUC', 'TPR', 'FPR', 'ROC']	[]
275	biểu thị cho ma trận ratings với rui biểu thị cho rating của người dùng u dành cho bộ phim i. V ∈ RM ×N , W ∈ RM ×N tương ứng mà hai ma trận dữ liệu hành	[]	[]	[]	[]
276	Hình vẽ 2 sau đây thể hiện một cách cụ thể và rõ ràng hơn về các thành phần chính của SSL Đồ án tốt nghiệp	[[86, 89]]	[]	['SSL']	[]
277	diễn cuối cùng của nội dung bài báo được tính bằng tổng trọng số của các từ trong bài báo : 𝑃	[]	[]	[]	[]
278	Tác v 8 Permuted MNIST	[[17, 22]]	[]	['MNIST']	[]
279	Tôi – Vũ Hồng Phúc cam đoan những nội dung trong đồ án này là của tôi dưới sự hướng dẫn của PGS. TS. Thân Quang Khoát . Các kết quả nêu trong đồ án là trung thực , là thành	[[92, 96], [97, 100]]	[]	['PGS.', 'TS.']	[]
280	Autoencoding Variational Bayes ( AEVB ) là một sự lựa chọn đặc biệt tự nhiên cho mô hình chủ đề , bởi nó học một mạng suy diễn , một mạng nơ ron mà ánh xạ trực tiếp một văn bản tới một phân phối hậu nghiệm xấp xỉ .	[[33, 37]]	[]	['AEVB']	[]
281	Trong quá trình huấn luyện , ta cần cực tiểu hóa hàm mất mát : L = - log ( Pr[r]) – log ( Pc[c]) ( 6 )	[]	[]	[]	[]
282	Hình 2.1 Mô hình NR MS ....................................................................................... 7	[[17, 19], [20, 22]]	[]	['NR', 'MS']	[]
283	độ chính xác khi tính toán . t Công thức 1. 17 là quan trọng nhất đối với LSTM bởi vì nó đảm bảo giá trị đạo hàm dcdct−1	[[74, 78]]	[]	['LSTM']	[]
284	áp dụng ràng buộc L1 dạng chuẩn hóa và có trọng số lên các véc - tơ chú ý At . ≤t−1 t	[]	[]	[]	[]
285	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 32	[[267, 276]]	[]	['CNTT – TT']	[]
286	11 2 . CƠ SỞ LÝ THUYẾT	[]	[]	[]	[]
287	một phân phối mới , gọi là population distribution Fα , với α là số điểm dữ liệu được lấy mẫu . Ở phần này chúng ta sẽ trình bày chi tiết các phương pháp trên , bao gồm :	[]	[]	[]	[]
288	thuộc lớp a True positive ( TP )	[[28, 30]]	[]	['TP']	[]
289	cũng có thể viết dưới dạng là một biến ngẫu nhiên tuân theo phân phối Gauss : 𝑞𝜙 ( 𝑏𝑚ℎ | 𝐴 ) = 𝑁 ( 𝛾𝑚ℎ , 𝛿𝑚ℎ ) 𝐷	[]	[]	[]	[]
290	Với mỗi doc d , hàm mất mát được tính như sau : 𝐿 = ∑ ( 𝛾̅𝑑𝑣 − 𝛾𝑑𝑣 ) 2	[]	[]	[]	[]
291	Với mỗi ma trận tương tác , ta thiết kế một mô hình thuộc tính ẩn với hướng Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 25	[[139, 150]]	[]	['KSTN - CNTT']	[]
292	Ahn và các cộng sự [ 3 ] cho biết rằng nó được bắt nguồn từ hai mô hình biến thể của mô hình Autoencoder là VAE và DAE , mô hình Denoising Variational Autoencoder ( DVAE ) [ 3 ] khác với mô hình VAE ở việc thêm nhiễu trước khi tiến hành đưa vào	[[108, 111], [115, 118], [165, 169], [195, 198]]	[]	['VAE', 'DAE', 'DVAE', 'VAE']	[]
293	Trong đó A là ma trận đầu vào có kích thước 𝑀 × 𝐷 , B là ma trận preactivation có kích thước 𝑀 × 𝐻 , 𝜃 là ma trận trọng số của mạng kích thước 𝐷 × 𝐻 và 𝜉 là ma trận nhiễu cùng kích thước với đầu vào A và được nhân vào đầu vào đóng vai trò là thành phần Dropout .	[]	[]	[]	[]
294	. . . . 51 Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[74, 85]]	[]	['KSTN - CNTT']	[]
295	AI - Trí tuệ nhân tạo ( Artificial Intelligence ) là các kỹ thuật giúp cho máy tính thực hiện được những công việc của con người chúng ta . Những năm gần đây , AI là lĩnh vực đang nổi	[[0, 2], [160, 162]]	[[5, 21]]	['AI', 'AI']	['Trí tuệ nhân tạo']
296	phương giữa giá trị dự đoán và giá trị thực tế như sau : X Lsqr =	[]	[]	[]	[]
297	( 9 ) j=1 Kí hiệu Γ , Ψ theo thứ tự là hàm Gamma và đạo hàm logarit của hàm Gamma ,	[]	[]	[]	[]
298	3 . 3 ALV cho các phương pháp tiếp cận dựa trên ràng buộc trọng số Như đã được giới thiệu , ALV có khả năng áp dụng cho các phương pháp dựa trên hướng tiếp cận ràng buộc trọng số , trong phần này sẽ trình bày chi tiết việc áp	[[6, 9], [92, 95]]	[]	['ALV', 'ALV']	[]
299	~ N ( 0, I1I K ) ( * * ) Hình dưới mô tả phân rã ma trận Gauss với ràng buộc biến :	[]	[]	[]	[]
300	trên được tính theo công thức sau ( do đã nói từ trước , đồ án này tập trung vào SSL mà không tập trung quá nhiều vào các chứng minh toán học của thuật toán LLE , bạn đọc quan tâm có thể tham khảo bài báo gốc được trích dẫn ở	[[81, 84], [157, 160]]	[]	['SSL', 'LLE']	[]
301	được gọi là độ đo khoảng cách Kull - back – Leibler giữa hai phân phối q và p . Mặt khác độ đo KL là không âm , tức	[]	[]	[]	[]
302	Mô hình máy dịch dựa trên mạng nơ-ron sử dụng RNN Do mô hình dịch máy sử dụng RNN đã được thử nghiệm nhiều trên các bộ ngôn ngữ khác nhau [ 48 , 51 , 55 ] và đã không còn là mô hình đem lại kết quả cao nhất cho các bộ ngôn ngữ	[[46, 49], [78, 81]]	[]	['RNN', 'RNN']	[]
303	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 48	[[63, 74]]	[]	['KSTN - CNTT']	[]
304	Context-informing Content : cung cấp Previous Event ( C1 ) : là các sự kiện thực thông tin liên quan đến hoàn cảnh thực tế xảy ra ngay trước sự kiện chính , có thể tế mà trong đó sự kiện chính xảy ra .	[]	[]	[]	[]
305	Vì vậy trong công việc này tôi cũng sử dụng 𝛼 ( 𝑙 ) là một véc - tơ 𝐷 chiều . Khi đó số lượng tham số gốc của mô hình	[]	[]	[]	[]
306	Trong quá trình huấn luyện , CNN tự động học các giá trị filter . Ví dụ , trong phân lớp ảnh ở hình 2 , CNN sẽ cố gắng tìm ra các tham số tối ưu cho các filter tương ứng theo	[[29, 32], [104, 107]]	[]	['CNN', 'CNN']	[]
307	K-medoids HAC Không sử	[[10, 13]]	[]	['HAC']	[]
308	── ── ── ── * ─ ── ── ── ĐỒ ÁN TỐT NGHIỆP ĐẠI HỌC	[]	[]	[]	[]
309	Attention Network ( HAN ) . Mục đích thiết kế của nó là để : -	[[20, 23]]	[]	['HAN']	[]
310	Hồi quy Logistic Hồi quy Ridge K-means + GL	[[41, 43]]	[]	['GL']	[]
311	38.60 Bảng 5 : Kết quả thử nghiệm ( F1 ) cho binary classification Bảng kết quả 5 cho thấy bài nghiên cứu của em đạt kết quả tốt hơn tất cả các bài	[]	[]	[]	[]
312	Ở phần này , chúng tôi xin chỉ tập trung giới thiệu về multilayer perceptron không áp dụng chuẩn hóa . Một mạng nơ-ron lan truyền tiến bao gồm có 3 loại lớp nơ-ron	[]	[]	[]	[]
313	phương sai lớn , mà số lượng lấy mẫu ( 𝑀 ∗ 𝐻 mẫu ) cũng giảm bớt so với kĩ thuật đổi biến ( 𝑀 ∗ 𝐷 ∗ 𝐻 mẫu ) do số chiều của B ít hơn nhiều so với 𝜃 . Ta có :	[]	[]	[]	[]
314	( FN ) Dự đoán không thuộc lớp a	[[2, 4]]	[]	['FN']	[]
315	Trong mô hình ITE - 3 , em đề xuất thêm vào thông tin về zone . Trong hệ thống quảng cáo trực tuyến , zone đại diện cho	[[14, 21]]	[]	['ITE - 3']	[]
316	Thuật toán học cho LDA sử dụng suy diễn biến phân cụ thể như sau : Thuật toán 1 Thuật toán học LDA sử dụng suy diễn biến phân Đầu vào : Tập gồm D văn bản , các tham số tiên nghiệm η , α	[[19, 22], [95, 98]]	[]	['LDA', 'LDA']	[]
317	với nhau . Trong trường hợp chỉ có phân phối dữ liệu đầu vào p( 𝑥1: 𝑇 ) thay đổi theo	[]	[]	[]	[]
318	phương pháp trong SSL thực sự quan tâm . Do đó , các đánh giá của các thí nghiệm được trình bày trong đồ án này cũng chỉ so sánh về độ mạnh yếu của biểu diễn ẩn	[[18, 21]]	[]	['SSL']	[]
319	"φ φ λt , φt trong HPP như với một mô hình "" conjugate exponential "" thông"	[[18, 21]]	[]	['HPP']	[]
320	vecto λ và λ0 là một độ đo không hiệu quả để biểu diễn tính tương đồng của hai phân phối q ( Θ | λ ) và q ( Θ | λ0 ) . Trong khi đó , natural gradient chỉ hướng trong	[]	[]	[]	[]
321	ta xây dựng một hàm mất mát dựa trên những giá trị đã biết trong ma trận R N 1 X X	[]	[]	[]	[]
322	Bài toán có thể phát biểu lại như sau : Input : Dữ liệu tương tác user - item của hành vi mục tiêu YR , và dữ liệu tương tác của các loại hành vi khác { Y1 , Y2 , . . . , YR−1 }	[]	[]	[]	[]
323	Mà đó là một phân phối không có công thức tường minh nên được xấp xỉ bằng phân phối Gauss có kỳ vọng là tham số tối ưu của T1 và ma trận hiệp phương sai chéo ( tức các tham số độc lập với nhau ) :	[]	[]	[]	[]
324	Trong đó , 𝑤 [ 𝑖−𝑀: 𝑖+𝑀 ] là tổng hợp của embedding từ thứ i-M đến từ thứ i+M , C và b là các tham số của mạng CNN và M là kích thước cửa sổ trượt trong mạng CNN .	[[111, 114], [158, 161]]	[]	['CNN', 'CNN']	[]
325	2 CHƯƠNG 2 . CÁC NGHIÊN CỨU LIÊN QUAN VÀ CƠ SỞ LÝ THUYẾT	[]	[]	[]	[]
326	Tất cả những tham khảo trong ĐATN , bao gồm hình ảnh , bảng biểu , câu trích dẫn đều được ghi rõ ràng nguồn gốc trong danh mục tham khảo .	[[29, 33]]	[]	['ĐATN']	[]
327	Minh họa lời giải tạo bởi Lasso , GL và SGL … . … … … … … … … … … … ....	[[34, 36], [40, 43]]	[]	['GL', 'SGL']	[]
328	ĐỒ ÁN TỐT NGHIỆP Kết hợp phân nhóm gene đồng biểu hiện và một số biến thể của Lasso trong bài toán	[]	[]	[]	[]
329	• Công thức 1.13 thể hiện quá trình cập nhật thông tin trong cổng update . Tương tự như RNN chuẩn , quá trình cập nhật nhận véc - tơ đầu vào xt và trạng thái ẩn trước đó ht−1 ,	[[88, 91]]	[]	['RNN']	[]
330	mức độ thưa của lời giải của các mô hình Lasso , Group Lasso và Sparse Group Lasso . Hình 5 . Minh họa lời giải tạo bởi Lasso , GL và SGL	[[128, 130], [134, 137]]	[]	['GL', 'SGL']	[]
331	Nguồn [ 62 ] . Có hai cách xây dựng dendrogram là tổng hợp ( hay tích tụ - Hierarchical Agglomerative Clustering – HAC) và phân chia ( Hierarchical Divisive Clustering –	[[115, 118]]	[]	['HAC']	[]
332	nhiên ALV vẫn vượt trội hơn với kết quả độ chính xác trung bình đạt được là 92.96 % và cao hơn Dropout 0.14 % . Hình 4 . 6 Kết quả thử nghiệm PMNIST trên EWC	[[6, 9], [142, 148], [154, 157]]	[]	['ALV', 'PMNIST', 'EWC']	[]
333	Mô hình sử dụng hàm kích hoạt và h như trên là mô hình Generalized Matrix Factorization ( GMF ) . Multi - Layer Perceptron	[[90, 93]]	[]	['GMF']	[]
334	𝑏𝑚 = 𝑎𝑚 𝑊 với 𝑤𝑑 = 𝑠𝑑 𝜃𝑑 𝑣à 𝑠𝑑 ~ 𝑁 ( 1, 𝛼 ) PT 3.2	[[44, 46]]	[]	['PT']	[]
335	0.20 Bảng 4 . 4 và bảng 4 . 5 biểu thị các giá trị perplexity và độ gắn kết các chủ đề ( NPMI ) của 2 mô hình ProdLDA VAE và Online LDA với từng giá trị k ( số lượng topics )	[[89, 93], [118, 121], [132, 135], [110, 117]]	[]	['NPMI', 'VAE', 'LDA', 'ProdLDA']	[]
336	46 5 THỬ NGHIỆM VÀ ĐÁNH GIÁ	[]	[]	[]	[]
337	13 Main Content : mô tả nội dung chính của Main Event ( M1 ) : Là sự kiện quan trọng	[]	[]	[]	[]
338	Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT ( 47 ) 47	[[60, 71]]	[]	['KSTN - CNTT']	[]
339	Trong khi VB cổ điển xấp xỉ phân phối hậu nghiệm bằng cách tối ưu hoá hàm lower bound của hàm log-complete data , còn gọi là ELBO như đề cập ở phần lý thuyết suy diễn biến phân :	[[10, 12], [125, 129]]	[]	['VB', 'ELBO']	[]
340	tập từ điển các character ta biểu diễn mỗi character thành một vector k chiều được khởi - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03	[[355, 364]]	[]	['CNTT – TT']	[]
341	( 𝑙 ) ⨀ là phép nhân Hadamard trên ma trận , 𝑠𝑡 ở đây là một ma trận kích thước 𝑀 × 𝐷 ( 𝑙 )	[]	[]	[]	[]
342	Ví dụ , đầu vào mạng CNN là một ảnh kích thước 28x28 tương ứng một ma trận có 28x28 giá trị và mỗi giá trị mỗi điểm ảnh là một ô trong ma trận . Trong một mạng ANN	[[21, 24], [160, 163]]	[]	['CNN', 'ANN']	[]
343	Thí nghiệm thứ 2 mà đồ án tốt nghiệp này triển khai liên quan đến tham số temperature . Đây là một tham số điều chỉnh trong hàm mất mát , bằng cách thay đổi tham số này trong cả mô hình SimCLR gốc và các mô hình LCL , thì nghiệm muốn	[[186, 192], [212, 215]]	[]	['SimCLR', 'LCL']	[]
344	Mạng tập trung phân cấp Natural Language Processing NLP	[[52, 55]]	[]	['NLP']	[]
345	của một một chuỗi , có đầu ra phụ thuộc vào những tính toán trước đó . Hình 2 là một mạng nơ-ron hồi quy RNN cơ bản . 5	[[105, 108]]	[[85, 104]]	['RNN']	['mạng nơ-ron hồi quy']
346	xỉ . Tồn tại một lớp rộng các thuật toán suy diễn xấp xỉ có thể sử dụng cho mô hình LDA , bao gồm xấp xỉ Laplace , suy diễn biến phân và lấy mẫu Makov chain	[[84, 87]]	[]	['LDA']	[]
347	TÀI LIỆU THAM KHẢO ............................................................................................ 43 PHỤ LỤC ..................................................................................................................... 46	[]	[]	[]	[]
348	được xét thêm . Kết quả ở hình 5 . 8 một lần nữa khẳng định VBD - CL tốt hơn HAT	[[60, 68], [77, 80]]	[]	['VBD - CL', 'HAT']	[]
349	( 29 ) Điểm F1 là trung bình điều hòa của precision và recall : F1 =	[]	[]	[]	[]
350	20 - Newsgroups ( 20NG ) , Ohsumed , R52 và R8 của Reuters , Movie Review ( MR ) :	[[76, 78]]	[]	['MR']	[]
351	được gắn trên đó , gọi là đoạn mồi . Sau đó các đoạn mRNA của đối tượng cần quan tâm sẽ được lấy ra , sử dụng enzyme phiên mã ngược để tái tạo lại các mạch DNA bổ sung	[[53, 57], [156, 159]]	[]	['mRNA', 'DNA']	[]
352	dùng với thuộc tính đó cao . Nếu ta gọi R ∈ RM ×N là ma trận lịch sử tương tác , Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[144, 155]]	[]	['KSTN - CNTT']	[]
353	𝑖=1 PT 4.1 trong đó : R j , i là độ chính xác đánh giá trên tác vụ i sau khi	[[4, 6]]	[]	['PT']	[]
354	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT . .	[[63, 74]]	[]	['KSTN - CNTT']	[]
355	trong đó ℽ ∈ ℝ và w ∈ ℝ2 là các tham số được cập nhật trong quá trình huấn luyện , dc ’ là chiều của vector đầu ra của ELMo . Cuối cùng , ta đưa hi qua một mạng feed forward	[[119, 123]]	[]	['ELMo']	[]
356	Variational Inference ( VI ) là một phương pháp suy diễn trong học máy cho phép ta xấp xỉ các hàm mật độ xác suất mà không dễ để tính toán . VI được sử dụng	[[24, 26], [141, 143]]	[]	['VI', 'VI']	[]
357	Factorization ( DMF ) . Với đầu vào là một ma trận xếp hạng rõ ràng không có đặc trưng tiềm ẩn , kiến trúc là tìm hiểu một không gian chiều thấp , tiềm ẩn phổ biến	[[16, 19]]	[]	['DMF']	[]
358	dùng và item ; k là số chiều của không gian thuộc tính ẩn . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 18	[[123, 134]]	[]	['KSTN - CNTT']	[]
359	∇ ΘAB log P ( s|smid , k ; ΘAB ) ] K k =1 Cập nhật tham số :	[]	[]	[]	[]
360	24 3 BÀI TOÁN HỆ GỢI Ý VỚI DỮ LIỆU HÀNH VI TIỀM ẨN VÀ RÕ	[]	[]	[]	[]
361	Email : phuc.vh173305@sis.hust.edu.vn Lớp : KHMT. 01 – K62 . Hệ đào tạo : Đại học chính quy	[[44, 48]]	[]	['KHMT']	[]
362	trong Hình 16 . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 37	[[79, 90]]	[]	['KSTN - CNTT']	[]
363	Giảng viên hướng dẫn PGS. TS. Thân Quang Khoát 2	[[21, 25], [26, 29]]	[]	['PGS.', 'TS.']	[]
364	CHƯƠNG 6 . KẾT LUẬN 6.1	[]	[]	[]	[]
365	tri thức tiên nghiệm nhúng của từ - Poisson Matrix Factorization using Word Embedding Prior ( PFEP ) . Tương tự với mô hình CTMP , ý tưởng xây dựng mô hình này trên cơ sở	[[94, 98], [124, 128]]	[]	['PFEP', 'CTMP']	[]
366	Do đây là phương pháp cuối cùng trong tổ hợp các bài trình bày hướng nghiên cứu SSL , nên bảng 6 sau đây chính là kết quả của việc huấn luyện lớp giữ nguyên các tham số của mạng encoder học được từ	[[80, 83]]	[]	['SSL']	[]
367	30 4 CÁC MÔ HÌNH ĐỀ XUẤT	[]	[]	[]	[]
368	PGS. TS. Thân Quang Khoát - - - - - - - - - - - - - - - Chữ ký GVHD	[[63, 67], [0, 4], [5, 8]]	[]	['GVHD', 'PGS.', 'TS.']	[]
369	tập con khác nhau . 1 . 9 Manifold Learning và thuật toán Local Linear Embedding ( LLE ) Locally Linear Embedding ( LLE ) là một thuật toán giảm chiều dữ liệu phi tuyến	[[83, 86], [116, 119]]	[]	['LLE', 'LLE']	[]
370	Chúng ta gọi đây là vấn đề vanishing variance . Điều này đồng nghĩa với việc mô Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[140, 151]]	[]	['KSTN - CNTT']	[]
371	4 . 2 . 3 Kết quả cài đặt và so sánh với các nghiên cứu trước So sánh với các kết quả nghiên cứu gần đây cho thấy , kết quả trong bài nghiên cứu của em đạt kết quả tốt nhất trên cả PDTB - Lin và PDTB - Ji đối với 11 - way classification .	[[181, 185], [195, 199]]	[]	['PDTB', 'PDTB']	[]
372	Đại lượng KL này chính là đại lượng ràng buộc trong hướng tiếp cận dựa trên ràng buộc trọng số đã trình bày ở trên , với mục đích kiểm soát tính ổn định của các trọng số quan trọng và tính	[[10, 12]]	[]	['KL']	[]
373	4 CÁC MÔ HÌNH ĐỀ XUẤT Hình 12 : Mô hình ITE - user _ item _ pcat	[[40, 64]]	[]	['ITE - user _ item _ pcat']	[]
374	Khi đó chúng ta ước lượng biến ẩn β chính bằng kỳ vọng của phân phối Dirichlet với tham số λ trên , tức là : P	[]	[]	[]	[]
375	4 CÁC MÔ HÌNH ĐỀ XUẤT zone 1 . Kiến trúc của mô hình ITE - 3 ( thêm đầu vào zone so với ITE - 2 ) được	[[88, 95], [53, 60]]	[]	['ITE - 2', 'ITE - 3']	[]
376	Bảng 5 : Kết quả thử nghiệm ( F1 ) cho binary classification … … … … … … … … … … .. 36	[]	[]	[]	[]
377	đặc trưng và dự đoán bệnh Alzheimer . Xie và cộng sự ( 2018 ) [ 38 ] sử dụng ANN cùng	[[77, 80]]	[]	['ANN']	[]
378	sử dụng để vec-tơ hóa biểu diễn của các item khi nội dung các item ở dạng văn bản . Trong khuôn khổ đồ án , em sẽ không đi vào chi tiết mô hình LDA ,	[[144, 147]]	[]	['LDA']	[]
379	pháp đi theo hướng tiếp cận Bayesian là VCL và UCB . 3.2 Variational continual learning ( VCL )	[[40, 43], [47, 50], [90, 93]]	[]	['VCL', 'UCB', 'VCL']	[]
380	( các độ đo AUC , MRR , nDCG@ 5 và nDCG@ 10 sẽ được trình bày cụ thể ở chương 4 ) : 9	[[12, 15], [18, 21], [24, 28], [35, 39]]	[]	['AUC', 'MRR', 'nDCG', 'nDCG']	[]
381	= L ( q ) + KL ( q|| p ) ( 2 ) trong đó , chúng ta đang giả định tổng quát rằng các biến ẩn Z là biến ngẫu	[]	[]	[]	[]
382	V = M ( Arg1 , Arg2 ) trong đó M là mô hình encoder chứa toàn bộ ba module đã mô tả trên Arg1 , Arg2 là hai câu đầu vào ban đầu	[]	[]	[]	[]
383	hình 24 : ACD ca sử dụng sửa thông tin cá nhân 44	[[10, 13]]	[]	['ACD']	[]
384	2.3 . 1 Đồ thị 8 2 . 3 . 2 Graph Convolutional Networks ( GCN )	[[58, 61]]	[]	['GCN']	[]
385	Bên cạnh đó , khi so sánh và đánh giá độ chính xác giữa mô hình HAN với các mô hình khác ( được liệt kê ở phần tiếp theo ) , em sử dụng độ đo F1 . 4 . 2 . 4 Tinh chỉnh trên tập siêu tham số ( hyper parameters )	[[64, 67]]	[]	['HAN']	[]
386	HÀ NỘI , 06/2021 Phiếu giao nhiệm vụ đồ án tốt nghiệp 1 . Thông tin về sinh viên :	[]	[]	[]	[]
387	thể hiện bằng công thức sau : 𝐿 = − ∑ 𝑖∈𝑆	[]	[]	[]	[]
388	lưu thành các thư mục 2 - NTD chọn vào 1 thư mục bất kì , sẽ hiện ra danh sách các hồ sơ được lưu trong thư mục đó	[[26, 29]]	[]	['NTD']	[]
389	Tuy nhiên , ngoài những điều cải thiện được so với mô hình NRMS thì mô hình NAML vẫn chưa giải quyết được một số vấn đề đặt ra đối với mô hình NRMS đã nêu .	[[59, 63], [76, 80], [143, 147]]	[]	['NRMS', 'NAML', 'NRMS']	[]
390	𝐺𝑅𝑈 ( 𝑥𝑖𝑡 ) , 𝑡 ∈ [ 𝑇, 1 ] Với từ 𝑤𝑖𝑡 , ta xây dựng được một chuỗi mã hóa bằng cách ghép trạng thái ẩn tiến ℎ⃗⃗𝑖𝑡 với trạng thái ẩn lùi ℎ⃖⃗𝑖𝑡 .	[]	[]	[]	[]
391	) ( 35 ) trong đó LI , LE tương ứng là phần hàm lỗi của dữ liệu hành vi tiềm ẩn và hành	[]	[]	[]	[]
392	Tham số α được đặt là 0.1 . Bảng 3 . 1 : Điểm BLEU của các mô hình học đối ngẫu sử dụng mô hình Transformer , k là số mô hình ngôn ngữ sử dụng cho mỗi ngôn ngữ	[[46, 50]]	[]	['BLEU']	[]
393	thay đổi các siêu tham số sẽ ảnh hưởng như thế nào đến cả hai phương pháp này ( liệu việc thay đổi siêu tham số ảnh hưởng đến mô hình SimCLR gốc thì có ảnh hưởng đến phương pháp LCL hay không ) .	[[134, 140], [178, 181]]	[]	['SimCLR', 'LCL']	[]
394	vậy SVB là một trường hợp đặc biệt của HPP khi ρt = 1 ∀t . Khi đặt toàn bộ ρt cùng là một hằng số cố định trong khoảng ( 0, 1 ) thì HPP được gọi là SVB - PP	[[4, 7], [39, 42], [132, 135], [148, 156]]	[]	['SVB', 'HPP', 'HPP', 'SVB - PP']	[]
395	( Θ . T ( zi , Xi ) − A ( Θ ) ) Chúng ta có natural gradient của hàm F- ELBO trên toàn bộ dữ liệu { Xi } αi=1 là :	[[69, 76]]	[]	['F- ELBO']	[]
396	và negative mới , lúc này LCL sử dụng một hàm mất mát tương tự với InfoNCE . Xét một batch dữ liệu vào với N ảnh x1 , x2 , ... , xN . Gọi k là số view đang xét đến .	[[26, 29]]	[]	['LCL']	[]
397	ước P ( Y t ) , P ( Y t+1 ) lần lượt là phân phối không gian đầu ra của tác vụ t và t + 1 . Khi đó P ( Y t ) 6 = P ( Y t+1 ) trong kịch bản học từng tác vụ ; P ( Y t ) = P ( Y t+1 ) trong	[]	[]	[]	[]
398	chung giữa các tác vụ hơn . ALV khi đó cũng có tính chất này giống như phân tích trong [ 13 ] , nên đảm bảo được việc ràng buộc các tham số toàn cục trong miền tối	[[28, 31]]	[]	['ALV']	[]
399	gợi ý Neural Collaborative Filtering ( NCF ) . Sinh viên thực hiện : Bùi Văn Tài , 20143908 , K59 , KSCLC HTTT & TT	[[39, 42], [100, 105], [106, 110], [113, 115]]	[]	['NCF', 'KSCLC', 'HTTT', 'TT']	[]
400	1 . Tập các phép DA : • Các phép DA liên quan đến không gian : cắt ảnh , thay đổi kích thước của	[[17, 19], [33, 35]]	[]	['DA', 'DA']	[]
401	Mỗi chủ đề ẩn k ∈ { 1 , 2 , ... , K} lại là một phân phối xác xuất trên tất cả các từ của tập từ điển với kích thước V , ta biểu diễn phân phối này bởi một vector βk = ( βk1 , βk2 , ... , βkV ) ( còn gọi là topic distribution ), trong đó βkj là	[]	[]	[]	[]
402	𝑘=1 Thuật toán FCM cũng tương tự K-means , ngoại trừ việc ở bước xác định cụm cho các quan sát , ta cần tính xác suất 𝛾𝑛𝑘 theo công thức :	[[15, 18]]	[]	['FCM']	[]
403	DANH MỤC BẢNG 5.1 5.2	[]	[]	[]	[]
404	Thí nghiệm tiến hành so sánh việc tăng giảm hệ số C ảnh hưởng như thế nào đến kết quả và so sánh xem việc nới lỏng ràng buộc cho phép w âm thì kết quả có tốt hơn so với cách làm ban đầu không .	[]	[]	[]	[]
405	ALV cho UCL Như đã được trình bày trong 2 . 2 . 3 , quá trình suy diễn của UCL giống hệt so với VCL , điểm khác biệt là UCL tinh chỉnh lại đại lượng KL trong công thức hàm	[[0, 3], [8, 11], [75, 78], [96, 99], [120, 123], [149, 151]]	[]	['ALV', 'UCL', 'UCL', 'VCL', 'UCL', 'KL']	[]
406	tương ứng với từng bài toán . Trong bài toán gợi ý phim , phim là item ; trong Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[142, 153]]	[]	['KSTN - CNTT']	[]
407	for k = 1 , . . . , K do Đặt điểm thưởng bởi mô hình ngôn ngữ của cho quan sát thứ k là r1 , k = LMB ( smid , k ) ;	[]	[]	[]	[]
408	lượng MI giữa X và Y nhưng cũng đồng thời làm cho biểu diễn ẩn tìm ra không có nhiều ý nghĩa .. Đấy chính là lí do mà trong các phương pháp huấn luyện mạng	[[6, 8]]	[]	['MI']	[]
409	đoán càng gần bounding box thực tế . Độ chính xác trung bình ( AP ) là phần diện tích phía dưới được tạo bởi đường cong Precision - Recall khi IoU > = 0.5.	[[63, 65], [143, 146]]	[[37, 60]]	['AP', 'IoU']	['Độ chính xác trung bình']
410	đã cho thấy số tác vụ có thể lên tới 50 tác vụ . Do các phương pháp sử dụng mạng BNN nên thực tế số lượng tham số cần học sẽ gấp đôi số lượng trọng số của mạng .	[[81, 84]]	[]	['BNN']	[]
411	Hình 12 : Tổng quan về SimCLR ( Nguồn [ Che+20 ] ) Tầm ảnh hưởng của các phép DA : Một trong những đóng góp lớn trong bài	[[23, 29], [78, 80]]	[]	['SimCLR', 'DA']	[]
412	∂L i đã tính được ∂W	[]	[]	[]	[]
413	. . . . 2 . 3 . 3 Phân tách ma trận tổng quát ( GMF - Generalized Matrix	[[48, 51]]	[[18, 45]]	['GMF']	['Phân tách ma trận tổng quát']
414	Latent Semantic Indexing ( LSI ) [ 6 ] và probabilistic Latent Semantic Indexing	[[27, 30]]	[]	['LSI']	[]
415	Multi Layer Perceptron ĐATN	[[23, 27]]	[]	['ĐATN']	[]
416	Mô hình dual learning trong [ 19 ] được cài đặt dựa trên mô hình dịch chuỗi - chuỗisử dụng RNN đồng thời sử dụng mô hình ngôn ngữ RNN . 40	[[91, 94], [130, 133]]	[]	['RNN', 'RNN']	[]
417	vec - tơ pu và qi . Có thể thấy rằng , về mặt bản chất mô hình MF giả sử các chiều trong không gian thuộc tính ẩn là độc lập với nhau và tổ hợp tuyến tính chúng	[[63, 65]]	[]	['MF']	[]
418	Với câu cần dịch là X = x1 , x2 , . . . , xm và câu dịch là Y = y1 , y2 , . . . , yn . Encoder sẽ kết nối X	[]	[]	[]	[]
419	hai mô hình còn lại . Ở cả hai độ đo HR @ 10 và NDCG @ 10 , nhìn chung hai mô hình ITE - item_ pcat và ITE - user_item_pcat đều có kết quả cao hơn	[[37, 39], [48, 52], [83, 99], [103, 123]]	[]	['HR', 'NDCG', 'ITE - item_ pcat', 'ITE - user_item_pcat']	[]
420	Giải thuật tối ưu cho UCL cũng tương tự như VCL ( Hình 2. 3 ) nhưng sử dụng hàm lỗi được biểu diễn trong công thức PT 2.21 để tối ưu các tham số . 15	[[22, 25], [44, 47], [115, 117]]	[]	['UCL', 'VCL', 'PT']	[]
421	GMM ( đề xuất ) Số nhóm	[[0, 3]]	[]	['GMM']	[]
422	trọng số , UCL sử dụng tính không chắc chắn để xác định một nút trong các tầng mạng có quan trọng hay không . Để giảm số lượng tham số UCL sử dụng chung	[[11, 14], [135, 138]]	[]	['UCL', 'UCL']	[]
423	Tại thời điểm t ≥ 1 , thu được dữ liệu của t minibatches D1 , D2 , ... , Dt . • Giả sử khi minibatch Dt đến , đã học được mô hình dự đoán M t−1 , khi đó	[]	[]	[]	[]
424	Nén mạng được tiến hành sau khi quá trình tối ưu kết thúc . Tương tự việc sử dụng α để tính tỉ lệ drop trong VD , VBD sử dụng tỉ lệ tín hiệu trên nhiễu ( Signal to noise ,	[[109, 111], [114, 117]]	[]	['VD', 'VBD']	[]
425	trên một tập dữ liệu chuẩn , thực tế để đánh giá được tốt nhất các mô hình , tạo điều kiện thuận lợi cho việc nghiên cứu trong lĩnh vực gợi ý tin tức . Tập dữ liệu MIND thu thập khoảng 160 nghìn bài báo tiếng Anh và hơn 15 triệu	[[164, 168]]	[]	['MIND']	[]
426	𝐸𝑞𝑡( θ ) log 𝑝( 𝑦𝑡𝑖 | 𝑥𝑡𝑖 , θ ) ≈ ∑𝐾𝑘=1 log 𝑝( 𝑦𝑡𝑖 | 𝑥𝑡𝑖 , 𝜃 𝑘 ) trong đó 𝜃 𝑘 = 𝜇𝑡 + 𝜎𝑡 ⨀ 𝜖 𝑘 với 𝐾 𝜖 𝑘 là một biến ngẫu nhiên được lấy mẫu từ phân phối Gauss đơn vị 𝑁 ( 0,1 ) và	[]	[]	[]	[]
427	| 𝐷𝐴 ) . Từ đó ta có log 𝑝 ( 𝜃 |𝐷𝐴 ) ≈ − ( 𝜃 − 𝜃𝐴 ) 𝑑𝑖𝑎 ( 𝐹 ) ( 𝜃 −	[]	[]	[]	[]
428	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 47	[[63, 74]]	[]	['KSTN - CNTT']	[]
429	Trong các phương pháp tiếp cận theo suy diễn VI , công thức PT 2 . 3 yêu cầu phải tính toán hai đại lượng là lô -ga-rít của khả năng xảy ra và KL . Trong nhiều	[[45, 47], [60, 62], [143, 145]]	[]	['VI', 'PT', 'KL']	[]
430	Giáo viên hướng dẫn : PGS. TS. Đỗ Phan Thuận HÀ NỘI Ngày 30 tháng 5 năm 2019	[[22, 26], [27, 30]]	[]	['PGS.', 'TS.']	[]
431	• KNN : K - nearest neighbor s Đồ án tốt nghiệp 5	[[2, 5]]	[]	['KNN']	[]
432	được lý giải như sau : việc tối đa hóa ELBO bao gồm việc tối thiểu hóa KL - một hàm số tỉ lệ nghịch với αkd . Như vậy quá trình huấn luyện sẽ có xu hướng làm tăng	[[39, 43], [71, 73]]	[]	['ELBO', 'KL']	[]
433	Nhìn chung , bất kì một phương pháp SSL nào cũng sẽ gồm 2 bước : Bước 1 giải quyết một pretext task được định nghĩa từ trước và bước 2 đánh giá xem biểu diễn	[[36, 39]]	[]	['SSL']	[]
434	ngữ và các thư viện sau : STT Tên Phiên bản	[[26, 29]]	[]	['STT']	[]
435	k - NN : k = 7 Naïve Bayes : không có . 43	[]	[]	[]	[]
436	trước , với tư tưởng kết hợp manifold learning vào các phương pháp trong hướng nghiên cứu CL hiện tại , đây là một cách làm mà chưa có nơi nào trên thế giới đã Đồ án tốt nghiệp	[[90, 92]]	[]	['CL']	[]
437	NGÀNH CÔNG NGHỆ THÔNG TIN TÊN ĐỀ TÀI PHÂN LOẠI CẢM XÚC VĂN BẢN TIẾNG VIỆT	[]	[]	[]	[]
438	Đồng thời có điểm khác biệt ở tầng học chú ý đối với User Encoder so với mô hình NRMS . Có thể thấy , kết quả mô hình NAML so với mô hình NRMS là tốt hơn , do khai	[[81, 85], [118, 122], [138, 142]]	[]	['NRMS', 'NAML', 'NRMS']	[]
439	ThS. Ngô Văn Linh TRƯỜNG ĐẠI HỌC BÁCH KHOA HÀ NỘI VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG	[[0, 4]]	[]	['ThS.']	[]
440	3 - NTD xác nhận chọn xóa bài đăng . Các bước thay Không thế	[[4, 7]]	[]	['NTD']	[]
441	Với ba mô hình ITE- onehot , ITE- item_ pcat , ITE- user_item_ pcat cũng như hai mô hình MTMF , NMTR , em sẽ xác định trước lr , η và batch - size trên hai Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[219, 230], [15, 26], [29, 44], [47, 67], [89, 93], [96, 100]]	[]	['KSTN - CNTT', 'ITE- onehot', 'ITE- item_ pcat', 'ITE- user_item_ pcat', 'MTMF', 'NMTR']	[]
442	Convolutional Neural Network Convolutional Neural Network ( CNN ) là một trong những mô hình học sâu tiên tiến , được sử dụng để xây dựng trong các hệ thống thông minh với độ chính	[[60, 63]]	[]	['CNN']	[]
443	Đây chính là tư tưởng chung của VI . Cụ thể , để xấp xỉ được hai phân phối , VI tối thiểu hóa khoảng cách KL ( KL	[[32, 34], [77, 79], [106, 108], [111, 113]]	[]	['VI', 'VI', 'KL', 'KL']	[]
444	RÀNG ( IMPLICIT , EXPLICIT BEHAVIOR ) việc phản ánh sở thích hay mối quan tâm của người dùng , nhưng cũng có thể ngầm định rằng mức độ tin cậy của các loại hành vi tương ứng với thứ tự của	[]	[]	[]	[]
445	Kết quả mô hình NAML được so sánh với các mô hình Deep Learning khác dựa trên các độ đo : AUC , MRR , nDCG@ 5 và nDCG@ 10 ( các độ đo này sẽ được trình bày cụ thể ở chương 4 ) :	[[16, 20], [90, 93], [96, 99], [102, 106], [113, 117]]	[]	['NAML', 'AUC', 'MRR', 'nDCG', 'nDCG']	[]
446	thống là sẽ áp dụng mô hình phát hiện đối tượng ( YOLOv5 ) để phát hiện nhân viên , khách hàng và thuật toán theo dõi đối tượng ( SORT ) để theo dõi đối tượng . Sau khi	[[130, 134]]	[]	['SORT']	[]
447	Nguyên nhân là do tác vụ B tương tác với tập đặc trưng này một cách khó khăn hơn . Nhưng với MTL , khi hai tác vụ A và B được học cùng nhau thì	[[93, 96]]	[]	['MTL']	[]
448	hưởng đến tính chính xác của chương trình . Thư viện pyvi có độ chính xác F1 cho tách từ tiếng Việt là 0.978637686 Em sử dụng pyvi để tiến hành thực hiện tách từ trong phần tiền xử lý dữ liệu .	[]	[]	[]	[]
449	Ta xét hướng tiếp cận này trên một bộ dữ liệu 𝐷 . Mục tiêu là học ra một mô hình với bộ tham số 𝜃 để có thể đưa ra kết quả suy diễn từ dữ liệu này hay giá trị	[]	[]	[]	[]
450	MỘT MÔ HÌNH HỌC SÂU CHO BÀI TOÁN NHẬN DẠNG QUAN HỆ ẨN GIỮA HAI CÂU	[]	[]	[]	[]
451	HAC ( đề xuất ) N/A 277	[[0, 3]]	[]	['HAC']	[]
452	2 PT 2.20 Tổng hợp lại từ PT 2.18 , PT 2. 19 và PT 2. 20 ta thu được hàm lỗi sử dụng cho	[[2, 4], [26, 28], [36, 38], [48, 50]]	[]	['PT', 'PT', 'PT', 'PT']	[]
453	Ngữ cảnh của các từ trong câu có ý nghĩa trong việc biểu diễn câu . Vì thế , tầng CNN sẽ học biểu	[[82, 85]]	[]	['CNN']	[]
454	Hình 25 : Kết quả khi thay đổi tham số temperature Ta thấy , khi thay đổi temperature = 0.1 lên 0.5 , mặc dù lúc đầu kết quả có vẻ khác nhau song khi hội tụ , các mô hình LCL và SimCLR với temperature = 0.5 đều	[[171, 174], [178, 184]]	[]	['LCL', 'SimCLR']	[]
455	Lời đầu tiên , tôi xin bày tỏ lòng biết ơn sâu sắc tới thầy hướng dẫn PGS. TS. Thân Quang Khoát giảng viên tại bộ môn Hệ thống Thông tin - Viện Công nghệ thông tin và truyền thông - Trường Đại học Bách khoa Hà Nội đã định hướng tận tâm	[[70, 74], [75, 78]]	[]	['PGS.', 'TS.']	[]
456	Số hệ số khác 0 LGL	[[16, 19]]	[]	['LGL']	[]
457	trận . Tuy nhiên rank ( H ) = rank ( U V T ) ≤ min ( rank ( U ) , rank ( V T ) ) ≤ k	[]	[]	[]	[]
458	Sinh viên thực hiện : Phan Tuấn Anh - 20150157 Bayesian Neural Net ( BNN ) Mô hình mạng nơ ron như đã đề cập ở trên còn được gọi là deterministic neural network , có	[[69, 72]]	[]	['BNN']	[]
459	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 15 2 KIẾN THỨC CƠ SỞ	[[63, 74]]	[]	['KSTN - CNTT']	[]
460	CHƯƠNG 1 . GIỚI THIỆU ĐỀ TÀI 1 . 1 Đặt vấn đề	[]	[]	[]	[]
461	item cùng loại với nhau . Hình 11 : Mô hình ITE - item _ pcat Công thức biểu diễn và hàm mục tiêu cho các mô hình ITE được đề xuất sau	[[114, 117], [44, 61]]	[]	['ITE', 'ITE - item _ pcat']	[]
462	với khoảng cách càng lớn dần thì RNN bắt đầu không thể nhớ và học được nữa ( hiện tượng vanishing gradient ) [ 4 ] . Từ đó Long short term memory ( LSTM ) ra đời để giải quyết vấn đề này .	[[148, 152]]	[]	['LSTM']	[]
463	Bảng 2 . 1 Kết quả mô hình NRMS Kết quả mô hình NRMS cho thấy các độ đo tốt hơn các mô hình Deep Learning khác , các độ đo cao hơn mô hình tốt nhất trước đó 2 % trên độ đo nDCG @ 5 và	[[27, 31], [48, 52], [172, 176]]	[]	['NRMS', 'NRMS', 'nDCG']	[]
464	cũng đã phân tích rõ lý thuyết của các phương pháp cơ sở để lập luận những vấn đề trên , trong đó SVB không có cơ chế cân bằng thông tin và gặp phải vấn đề vanishing variance khi nhận được lượng dữ liệu đủ lớn , SVB - PP khắc	[[98, 101], [212, 220]]	[]	['SVB', 'SVB - PP']	[]
465	𝑝 ( 𝜃 | 𝐷 ) – xác suất có điều kiện của 𝜃 khi biết dữ liệu D . Công thức được khai triển theo suy diễn Bayes sau khi được lô-ga-rít hóa :	[]	[]	[]	[]
466	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 45 PHỤ LỤC	[[61, 70]]	[]	['CNTT – TT']	[]
467	Nhiều trường hợp các User không tương tác đủ số lượng bài Shortterm sẽ được thêm các giá trị padding 0 để thuận lợi cho quá trình tính toán . Mô hình LSTUR được đánh giá trên tập MIND và cho kết quả như sau : Methords	[[150, 155], [179, 183]]	[]	['LSTUR', 'MIND']	[]
468	Bên cạnh đó , mẫu số của ( 2.4 ) là p ( X ) hoàn toàn không phụ thuộc vào θ . Do đó , ta có ( 2.4 ) được viết lại thành :	[]	[]	[]	[]
469	phương pháp rất nổi tiếng của CL . Về mặt tư tưởng MOCO cũng tạo ra các cặp	[[30, 32], [51, 55]]	[]	['CL', 'MOCO']	[]
470	Tính stochastic gradient của ΘBA : K 1 X	[]	[]	[]	[]
471	– Mô hình vẫn chưa giải quyết được vấn đề cold start 4 . 3 Mô hình ITE - user _ item _ pcat Không nhiều hệ thống thực tế có được những thông tin mô tả về người dùng .	[[67, 91]]	[]	['ITE - user _ item _ pcat']	[]
472	Thứ hai , phương pháp học batch có nhược điểm Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 14	[[106, 117]]	[]	['KSTN - CNTT']	[]
473	Xét một tầng ẩn 𝑙 của mô hình mạng nơ- ron kết nối đầy đủ , có 𝐴 ( 𝑙 ) là ma trận đầu vào có kích thước 𝑀 × 𝐷 , 𝜃 ( 𝑙) là ma trận trọng số có kích thước 𝐷 × 𝐻 , trong 18	[]	[]	[]	[]
474	hoạt là hàm identity ( nghĩa là đầu ra của hàm chính bằng đầu vào của hàm ) và cố định h với tất cả các trọng số đều bằng 1 , ta nhận được chính xác mô hình MF .	[[157, 159]]	[]	['MF']	[]
475	Với 𝑧𝑖 , 𝑙 = 𝜇𝑖 + 𝜎𝑖 ⊙ 𝜀𝑙 và 𝜀𝑙 ~ 𝒩 ( 0, 𝐼 ) 11 Trên đây là những kiến thức cơ sở về mô hình chủ đề LDA và phương pháp học	[[100, 103]]	[]	['LDA']	[]
476	Continual Learning ( SCCL ) , dịch là học liên tục dựa trên nén cấu trúc mạng . Phương pháp sử dụng kỹ thuật nén cấu trúc mạng dựa trên Group Lasso regularization làm trọng tâm để	[[21, 25]]	[]	['SCCL']	[]
477	Đặt s = sA ; Sinh K câu smid , 1 , . . . , smid , K sử dụng beam search từ mô hình dịch P ( .| s ; ΘAB ) ; for k = 1 , . . . , K do	[]	[]	[]	[]
478	PT 2.14 Với t = 1 thì 𝑞0 ( θ ) = 𝑝 ( 𝜃 ) . 𝑍𝑡 là đại lượng hằng số chuẩn hóa trong phép chiếu	[[0, 2]]	[]	['PT']	[]
479	được sử dụng có dạng f L ( aL−1 ) = Act ( ( W L ) T aL−1 + bL ) 31	[]	[]	[]	[]
480	ĐỒ ÁN TỐT NGHIỆP Học tự giám sát NGUYỄN HỒNG QUỐC KHÁNH	[]	[]	[]	[]
481	| B | α , chúng ta xấp xỉ nhiễu cho natural gradient trên dữ liệu của minibatch α	[]	[]	[]	[]
482	Tư tưởng chính của CL đó là tạo ra nhiều cách nhìn khác nhau của cùng một dữ liệu , mà ta tạm gọi là các view . Các view này có thể được tạo ra bởi nhiều cách ,	[[19, 21]]	[]	['CL']	[]
483	dụng và kết quả đã được kiểm chứng . 2 . 1 Mô hình Gợi ý tin tức dựa trên cơ chế tự chú ý đa chiều ( NRMS )	[[101, 105]]	[[51, 98]]	['NRMS']	['Gợi ý tin tức dựa trên cơ chế tự chú ý đa chiều']
484	văn bản Consequence ( M2 ) : Là các sự kiện được sinh ra bởi sự kiện chính , nó có thể xảy	[]	[]	[]	[]
485	Các hàm kích hoạt trọng mô hình MLP ta sử dụng hàm ReLU như đã nói ở mục trước . Hàm aout sẽ dùng hàm sigmoid để phù hợp với dữ liệu tương tác dạng binary .	[[32, 35], [51, 55]]	[]	['MLP', 'ReLU']	[]
486	học γ1 , t và γ2 , t , N là số nhiễu áp dụng ; repeat	[]	[]	[]	[]
487	bài báo . Cấu trúc của mô-đun News Encoder được mô tả bằng hình : Hình 2. 4 Mô-dun News Encoder mô hình LSTUR	[[104, 109]]	[]	['LSTUR']	[]
488	0.7 Tác v 7 VBD - CL	[[12, 20]]	[]	['VBD - CL']	[]
489	ei = ∑𝑀 𝑖=1 𝛼𝑖 𝑐𝑖 Thành phần tiếp theo của News Encoder là Topic Encoder , được sử dụng để học	[]	[]	[]	[]
490	2 . 1 Mô hình Biterm Topic Model ( BTM ) 3 3	[[35, 38]]	[]	['BTM']	[]
491	Gán mỗi quan sát cho một cụm có tâm gần nó nhất . Tính toán giá trị hàm lỗi L trong công thức ( 16 )	[]	[]	[]	[]
492	Tương tự như vậy , trong CL , khi nhìn một bức ảnh đầu vào dưới nhiều view khác nhau , như qua các phép biến hình hay các kênh khác nhau của không gian màu , hay qua các phép DA khác nhau , thì về cơ bản ,	[[25, 27], [175, 177]]	[]	['CL', 'DA']	[]
493	Quá trình đọc dữ liệu và phân chia dữ liệu được thực hiện giống với thử nghiệm trong phương pháp UCL [ 3 ] , trong công việc này mã nguồn đọc và phân chia dữ 29	[[97, 100]]	[]	['UCL']	[]
494	thường 478K EWC	[[12, 15]]	[]	['EWC']	[]
495	Learning rate Topics coherence ( NPMI ) Hình 4 . 19 Đồ thị biểu diễn giá trị NPMI khi thay đổi tốc độ học mô hình trên tập	[[33, 37], [77, 81]]	[]	['NPMI', 'NPMI']	[]
496	xác suất dự đoán các Positive Samples là cao hơn . Cụ thể , xác suất mỗi bài báo được User quan tâm là 𝑦 + , xác suất dự đoán K bài không được User đó quan tâm [ 𝑦1− , 𝑦2 − , . . . , 𝑦𝐾− ] , điểm số được tính bởi công thức :	[]	[]	[]	[]
497	Sự cải thiện tương đối trong L ( w , φ , γ , λ ) > 10−6 do E step : Bước suy diễn biến cục bộ cho từng văn bản for mỗi văn bản d trong Dt do	[]	[]	[]	[]
498	3 . 2 . 3 Mô-đun dự đoán tương tác ( Click Prediction ) Cuối cùng , sau khi qua L block ( tầng ) , biểu diễn cuối cùng của User được học từ	[]	[]	[]	[]
499	như là điểm mạnh và điểm yếu của nó . Cuối cùng đề cập một vài ứng dụng của GCN trong xử lý ngôn ngữ tự nhiên ( NLP ) .	[[76, 79], [112, 115]]	[[86, 109]]	['GCN', 'NLP']	['xử lý ngôn ngữ tự nhiên']
500	có một số pretext task theo hướng dự đoán , ví dụ như trong mô hình colorization , người ta cố xây dựng lên một mạng AE có khả năng tô màu cho ảnh đen trắng . Đồ án tốt nghiệp	[[117, 119]]	[]	['AE']	[]
501	Hình A . 2 Số lượng người dùng theo số bài đọc trung bình trong tuần 42	[]	[]	[]	[]
502	• Sử dụng một tập các phép DA để tạo ra các view các nhau cho dữ liệu . Đây là một bước không mấy xa lạ trong các phương pháp CL từ trước . • Sử dụng một mạng encoder fθ ( x) để tìm các biểu diễn ẩn tương ứng zi của các	[[27, 29], [126, 128]]	[]	['DA', 'CL']	[]
503	cho 𝑠𝑡 , 𝑚 ( 𝑚 ∈ { 1 , … , 𝑀 } và 𝑠𝑡 ,𝑚 ∈ 𝑅1×𝐷 ) . Phân phối của biến ngẫu nhiên cục bộ khi đó sẽ phụ thuộc vào các tham số có thể học được trong quá trình tối ưu , ở đây	[]	[]	[]	[]
504	Trong công thức PT 2.17 , 𝐿 là số lớp trong mạng nơ-ron của mô hình , ( 𝑙 ) ( 𝑙 )	[[16, 18]]	[]	['PT']	[]
505	X C×W ×H ) được coi như một nơ-ron tại một tầng của mạng MLP . Mỗi tầng l , trừ tầng đầu ra , đều được nhân từng phần tử với một tầng nhiễu trước khi được nhân	[[57, 60]]	[]	['MLP']	[]
506	j=1 wi , j ∗ yj || 2 Tương tự như phần tìm w , việc tìm Y cũng được giải quyết bằng các thuật toán tối ưu thông thường và đây cũng chính là kết quả cuối cùng của thuật toán LLE .	[[173, 176]]	[]	['LLE']	[]
507	2 . 1 . 3 Suy diễn biến phân Xét một mô hình mạng Bayes có các biến quan sát được ký hiệu là X , và tập các biến ẩn ( tham số cần học của mô hình ) ký hiệu là Z. Như đã trình bày ở	[]	[]	[]	[]
508	Đây cũng là một nhược điểm mà mô hình LSTUR chưa giải quyết được . 21	[[38, 43]]	[]	['LSTUR']	[]
509	lượng xấp xỉ này . Tương quan trên pre - activation Trong công thức PT 3. 11 ta thấy mỗi phần tử trong đại lượng pre-activation	[[68, 70]]	[]	['PT']	[]
510	Bảng 5 . 5 : Phần trăm không gian mạng MLP sử dụng qua các tác vụ trên tập Split MNIST và Permuted MNIST Tác vụ	[[39, 42], [81, 86], [99, 104]]	[]	['MLP', 'MNIST', 'MNIST']	[]
511	2 i=1 N ( µi , σi ) . Khi đó :	[]	[]	[]	[]
512	14 2 KIẾN THỨC CƠ SỞ P ∈ Rk×M là ma trận toàn bộ biểu diễn của M người dùng , Q ∈ Rk×N là ma	[]	[]	[]	[]
513	Quá trình huấn luyện là quá trình tối ưu 2 tham số 𝜃 và 𝛽 , còn quá trình suy diễn là quá trình tối ưu 𝜃 dựa trên biểu diễn TF - IDF và 𝛽 đã được tối ưu .	[[124, 132]]	[]	['TF - IDF']	[]
514	Các mô hình khác nhau sẽ tìm ra các biểu diễn khác nhau . Trong mô hình LSI , chúng ta biến đổi các phần tử của ma trận DOC −	[[72, 75]]	[]	['LSI']	[]
515	Mục tiêu của em khi thực hiện đề tài là nghiên cứu , hiểu , xây dựng và đánh giá được mô hình cơ bản là Phân tích ma trận phân tử cấu trúc sâu ( DMF ) để dự đoán đánh giá của người dùng đối với sản phẩm mà họ chưa đánh giá , đưa ra xác	[[145, 148]]	[[104, 142]]	['DMF']	['Phân tích ma trận phân tử cấu trúc sâu']
516	Tác v AGS - CL EWC	[[6, 14], [15, 18]]	[]	['AGS - CL', 'EWC']	[]
517	quy hoá . Tuy nhiên , không như các phương pháp chính quy hóa khác ( chẳng hạn L2 ) , mỗi đại lượng βkj trong iDropout có một hệ số chính quy hoá khác	[]	[]	[]	[]
518	33 mà như đề cập ở trên thì λ̃i là thông tin học được ở minibatch thứ i nên với mô hình LDA ( NB tương tự ) , theo công thức ( 12 ) chúng ta có :	[[88, 91], [94, 96]]	[]	['LDA', 'NB']	[]
519	Dt = { X t , Y t } với X t , Y t = { xtn , ynt } N n=1 tuân theo	[]	[]	[]	[]
520	𝛼 : véc − tơ tỉ lệ trộn 𝑀 : là số lượng trộn PT 5.1	[[45, 47]]	[]	['PT']	[]
521	( 14 ) 𝐾 điều kiện 𝑦𝑛𝑘 ∈ { 0, 1 } ∀𝑛 , 𝑘 ; ∑ 𝑦𝑛𝑘 = 1 ∀𝑘	[]	[]	[]	[]
522	Số lần lặp BPE 1000 convolutional encoder	[[11, 14]]	[]	['BPE']	[]
523	Tương tự với các item . Như vậy , đối với những người dùng không Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[128, 139]]	[]	['KSTN - CNTT']	[]
524	Công thức thông thường của lớp kết nối đầy đủ ( Fully Connected , FC ) là một biến đổi tuyến tính : B = AW .	[[66, 68]]	[[31, 45]]	['FC']	['kết nối đầy đủ']
525	wk = { w1k , w2k , ... , wtk } gồm t từ có trọng số lớn nhất trong topic distribution βk tương ứng . NPMI của chủ đề k được tính như sau :	[[101, 105]]	[]	['NPMI']	[]
526	Ước lượng cực đại hóa hậu nghiệm - Maximum A Posterior Ước lượng cực đại hóa hậu nghiệm ( MAP ) được bắt nguồn từ công thức Bayes . Giống như cái tên MAP sẽ làm việc trên xác suất hậu nghiệm , mà không phải	[[90, 93], [150, 153]]	[[55, 87], [0, 32]]	['MAP', 'MAP']	['Ước lượng cực đại hóa hậu nghiệm', 'Ước lượng cực đại hóa hậu nghiệm']
527	giúp mô hình có khả năng tránh được hiện tượng học quá khớp . Hình 2 . 1 mô tả trực quan cho hai kiến trúc mạng ANN và BNN kết nối đầy đủ .	[[112, 115], [119, 122]]	[]	['ANN', 'BNN']	[]
528	− 𝐾𝐿 ( 𝑞𝑡 ( 𝜃 ) | | 𝑞𝑡−1 ( 𝜃 ) ) Trong công thức PT 3.7 , đại lượng 𝐾𝐿 ( 𝑞𝑡 ( 𝜃 ) | | 𝑞𝑡−1 ( 𝜃 ) ) giúp giảm vấn đề quên	[[49, 51]]	[]	['PT']	[]
529	MỘT SỐ THÔNG KÊ KHÁC TRÊN TẬP DỮ LIỆU PEGA Thống kê số lượng người dùng mới mỗi ngày Hình A . 1 Thống kê số lượng User mới trong tháng 10 theo ngày	[]	[]	[]	[]
530	• Tốc độ phát hiện ( FPS ) : Là thời gian phát hiện được đối tượng trong bao nhiêu frame , tính bằng khung hình trên giây . • Độ chính xác trung bình trên một đối tượng ( AP ) : Là độ chính xác được tính	[[21, 24], [171, 173]]	[[126, 168], [101, 121]]	['FPS', 'AP']	['Độ chính xác trung bình trên một đối tượng', 'khung hình trên giây']
531	𝐸 ( 𝑊 ) = 𝐸𝐷 ( 𝑊 ) + 𝜆 ∑ 𝑅 ( 𝑊𝑙 ) 𝑙=1 Công thức 8 : Hàm mục tiêu huấn luyện mạng nơ-ron với regularization	[]	[]	[]	[]
532	ĐỒ ÁN TỐT NGHIÊP ÁP DỤNG DROPOUT BAYES BIẾN PHÂN TRONG BÀI TOÁN HỌC LIÊN TỤC	[]	[]	[]	[]
533	4.1.1 VBD trong học liên tục Độ quan trọng của nơ-ron	[[6, 9]]	[]	['VBD']	[]
534	Hàm mất mát : Trong hàm mất mát mà bài báo đưa ra , có một thành phần chính là phần tái cấu trúc ( reconstruct ) gần giống với hàm mất mát của AE truyền thống ,	[[143, 145]]	[]	['AE']	[]
535	Lớp : CNTT - TT 2.03 - K59 Hệ đào tạo : Đại học chính quy	[[6, 15]]	[]	['CNTT - TT']	[]
536	Thử nghiệm , đánh giá và Phần 6 : Kết luận . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 9	[[108, 119]]	[]	['KSTN - CNTT']	[]
537	16 Mạng neural nhân tạo ( Artificial Neural Networks - ANN ) là một kiến trúc trong đó các tầng tuyến tính và phi tuyến được xếp xem kẽ nhau , cho phép dữ liệu đi và mô hình	[[55, 58]]	[[3, 23]]	['ANN']	['Mạng neural nhân tạo']
538	cứu trong CL hiện tại ) , chứ không tập trung nhiều vào các bản chất toán học sâu xa trong hướng nghiên cứu manifold learning . Bạn đọc có thể xem qua phần này	[[10, 12]]	[]	['CL']	[]
539	hàm softmax . 2 . 2 . 2 Mô hình CBOW Mô hình này tương tự với mô hình skip-gram , tuy nhiên thay vì đầu vào là từ mục tiêu	[[32, 36]]	[]	['CBOW']	[]
540	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 11 2 KIẾN THỨC CƠ SỞ	[[63, 74]]	[]	['KSTN - CNTT']	[]
541	t t Suy diễn các biến cục bộ z từ mô hình gốc B ( Θ̃ , z , Dt ) với Θ̃ , Dt đã	[]	[]	[]	[]
542	Vec - tơ pcat của zone sẽ đi qua một tầng embedding , rồi ghép vào các vec-tơ embedding MLP của người dùng và banner . Phần mô	[[88, 91]]	[]	['MLP']	[]
543	Cụ thể như sau : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 28	[[80, 91]]	[]	['KSTN - CNTT']	[]
544	ý tưởng từ kỹ thuật Dropout biến phân , cụ thể là VBD , và điều này đem lại thêm hai lợi ích cho phương pháp . Thứ nhất , mô hình có thể phân bổ các nơ -ron không	[[50, 53]]	[]	['VBD']	[]
545	Trong đồ án này sẽ gọi chung các bài toán mà con người muốn máy tính giải được với thuật ngữ pretext task . Bản thân thuật ngữ SSL cũng được sử dụng nhiều trong các lĩnh vực khác như	[[127, 130]]	[]	['SSL']	[]
546	mô hình , num parameters ( NP ) . Bộ nhớ sử dụng đánh giá xem phương pháp có khả thi để sử dụng trung thực tế hay không .	[[27, 29]]	[]	['NP']	[]
547	iDropout Infinite Dropout Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[86, 97]]	[]	['KSTN - CNTT']	[]
548	: Viện Công Nghệ Thông Tin và Truyền Thông Sinh viên thực hiện - Nguyễn Văn Chức - cam kết Đồ án nghiên cứu ( ĐANC )	[[110, 114]]	[[91, 107]]	['ĐANC']	['Đồ án nghiên cứu']
549	Trần Thị Hồng 5 . Xác nhận của giáo viên hướng dẫn về mức độ hoàn thành của ĐATN và cho phép bảo vệ : Hà Nội , ngày tháng năm	[[76, 80]]	[]	['ĐATN']	[]
550	Chúng tôi sử dụng mô hình LDA với K = 100 và α = 0.01 để phân tích chủ đề của bộ dữ liệu này và bỏ qua thông tin về nhãn . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[26, 29], [183, 194]]	[]	['LDA', 'KSTN - CNTT']	[]
551	Cũng ràng buộc sự thay đổi trên trọng số những Variational Continual Learning ( VCL ) ( Cuong V. Nguyen , 2017 ) lại dựa trên học Bayesian trực tuyến , sử dụng mạng nơ-ron Bayesian học ra phân phối của từng	[[80, 83]]	[]	['VCL']	[]
552	CHƯƠNG 2 . KIẾN THỨC NỀN TẢNG 2 . 1 Latent Dirichlet Allocation ( LDA )	[[66, 69]]	[]	['LDA']	[]
553	1 Mô tả 6 bộ dữ liệu dùng trong thử nghiệm học không giám sát . . 56 Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[129, 140]]	[]	['KSTN - CNTT']	[]
554	( RNN ) ra đời nhằm khắc phục nhược điểm trên . Đầu vào của mạng RNN là một chuỗi liên tục theo thời gian ( ví dụ : các khung hình liên tiếp nhau trong một video hay các các chuỗi từ trong một đoạn văn , .. )	[[2, 5], [65, 68]]	[]	['RNN', 'RNN']	[]
555	hàm likelihood có công thức như sau : Y Y	[]	[]	[]	[]
556	Các công việc ban đầu tập trung đưa ra đánh giá độ quan trọng của tham số . Một số ví dụ điển hình là EWC [ 5 ] , SI [ 7 ] , Bộ nhớ điểm tiếp hợp ( Memory Aware	[[102, 105], [114, 116]]	[]	['EWC', 'SI']	[]
557	Trung tâm Nghiên cứu phát triển công nghệ mạng Viettel - VTTEK . Thời gian một năm thực tập ở đây , team đã hỗ trợ em rất nhiều trong việc tiếp cận và	[[57, 62]]	[[0, 54]]	['VTTEK']	['Trung tâm Nghiên cứu phát triển công nghệ mạng Viettel']
558	4 CÁC MÔ HÌNH ĐỀ XUẤT ( 34 )	[]	[]	[]	[]
559	là trị riêng lớn nhất của nó thì : xT ∇2 L1 ( w1∗ ) x ≤ 1	[]	[]	[]	[]
560	So sánh hiệu năng mô hình HAN trên tập các thuật toán tối ưu hóa khác nhau Adagrad có learning rate là 0.01 cho độ chính xác trên tập huấn luyện bắt đầu cao	[[26, 29]]	[]	['HAN']	[]
561	W2 + b2 . ( 2.15 ) Tuy nhiên , thay vì sử dụng một khối self-attention duy nhất , Transformer sử dụng h khối	[]	[]	[]	[]
562	P∞ 2 P chúng ta có ∞	[]	[]	[]	[]
563	để gán phân phối xác suất trên các chủ đề cho một văn bản chưa biết trước , do đó chúng không được xem là một mô hình sinh ( generative model ) hiệu quả . Mô hình Latent Dirichlet Allocation ( LDA ) được đề xuất sau đó và giải quyết	[[193, 196]]	[]	['LDA']	[]
564	mô hình học biểu diễn đề xuất được thử nghiệm trên tập dữ liệu MIND có thể quan sát ở bảng dưới đây : Mô hình News Encoder	[[63, 67]]	[]	['MIND']	[]
565	3 . 2 Phương pháp Streaming Variational Bayes 3 . 2 . 1 Cơ sở lý thuyết Streaming Variational Bayes ( SVB ) được đề xuất bởi Broderick và cộng sự [ 4 ] ,	[[102, 105]]	[]	['SVB']	[]
566	Câu 1 : As the Dow average ground to its final 190.58 loss Friday , the S & P pit stay ed lock ed at its 30 - point trading limit . Câu 2 : 2,000 S& P contract s were for sale on the close , the equivalent of $ 330 million	[]	[]	[]	[]
567	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 34	[[267, 276]]	[]	['CNTT – TT']	[]
568	Lương [ 24 ] , vùng intron chiếm khoảng 5 % hệ gene , khoảng 1 % là vùng mã hóa protein , 0.5 % hệ gene là các vùng điều hòa và không mã hóa RNA nằm giữa các gene . 99.9 % hệ	[[141, 144]]	[]	['RNA']	[]
569	Lớp CNTT 2.03 – K59 Giảng viên hướng dẫn : HÀ NỘI 05/2019	[[4, 8]]	[]	['CNTT']	[]
570	• Đầu tiên , mô hình đề xuất một tập hợp các khu vực quan tâm bằng mạng tìm kiếm ( ROI ) hoặc mạng đề xuất khu vực ( RPN ) được chọn . Các vùng được đề	[[83, 86], [117, 120]]	[[94, 114], [45, 61]]	['ROI', 'RPN']	['mạng đề xuất khu vực', 'khu vực quan tâm']
571	( 42) Trong đồ án này , em chọn số K bằng 10 trong tất cả các độ đo để đánh giá kết quả .	[]	[]	[]	[]
572	Convolution Neural Network – Mạng tích chập NLP Natural Language Processing – Xử lý ngôn ngữ tự nhiên	[[44, 47]]	[[78, 101]]	['NLP']	['Xử lý ngôn ngữ tự nhiên']
573	Phương pháp đặt điều kiện lên 𝑞𝜙 ( 𝜃 ) như vừa trình bày còn được gọi là Suy diễn biến phân trên trường trung gian ( MFVI ) , nếu như mỗi 𝑞𝜙 ( 𝜃𝑗 ) là một phân phối Gauss thì phương	[[117, 121]]	[[73, 114]]	['MFVI']	['Suy diễn biến phân trên trường trung gian']
574	4 phân kỳ Kullback - Leibler ( KL ) và được tính theo công thức sau 𝐾𝐿 ( 𝑞||𝑝 ) = 𝑞 ( 𝑥 )	[[31, 33]]	[[2, 28]]	['KL']	['phân kỳ Kullback - Leibler']
575	thuật toán phân cụm có chồng lấn ( mỗi gene có thể thuộc về nhiều hơn một cụm ) : Fuzzy C-means ( FCM ) và Gaussian Mixture Model ( GMM ) . Kết quả của mô hình	[[98, 101], [132, 135]]	[]	['FCM', 'GMM']	[]
576	entropy càng cao càng thể hiện độ hỗn loạn và không chắc của tham số Θ , điều này là phù hợp và cần thiết với môi trường động của dữ liệu dòng . Entropy của	[]	[]	[]	[]
577	được mô hình hóa bởi phân phối p ( β t +1 k |β k ) ∼ N ( β k , σ I ) . Vì phương sai này	[]	[]	[]	[]
578	48 Thuật toán 5 Phương pháp học iDropout cho mô hình Bayesian Đầu vào : Dòng dữ liệu với các minibatch { D1 , D2 , ... , Dt , ... } , các tham số drop	[]	[]	[]	[]
579	chính xác trung bình Hình 5 . 1 : Độ chính xác trung bình khi kết thúc mỗi tác vụ trên tập Split MNIST và Permuted MNIST .	[[97, 102], [115, 120]]	[]	['MNIST', 'MNIST']	[]
580	không độc lập và đồng nhất ( non-iid ) từ một phân phối Q giả định nào đó . Chú ý rằng phân phối này có thể thay đổi dần dần hoặc đột ngột xuyên suốt quá trình	[]	[]	[]	[]
581	nghiệm ở tác vụ trước sẽ là tri thức tiên nghiệm cho việc học tác vụ hiện tại . ℒ 𝑡 ( 𝜃 ) = ℒ ( 𝐷𝑡 , 𝜃 ) + 𝐾𝐿 ( 𝑞𝑡 (𝜃 ) ||𝑞𝑡−1 (𝜃 ) ) PT 2.5	[[134, 136]]	[]	['PT']	[]
582	𝑔∈𝐺 Để làm rõ cách hoạt động của Latent Group Lasso , xét ví dụ : một tập dữ liệu X bao gồm các quan sát là vector bốn chiều x = ( x1 , x2 , x3 , x4 ) tương ứng với bốn hệ số	[]	[]	[]	[]
583	Trong trường hợp tổng quát với việc học ở tác vụ thứ t , hàm mục tiêu của EWC có thể viết lại như sau : 𝑡−1	[[74, 77]]	[]	['EWC']	[]
584	ĐỒ ÁN TỐT NGHIỆP Học liên tục dựa trên tham số cục bộ LÊ HẢI NAM	[]	[]	[]	[]
585	EWC [ 5 ] là phương pháp đầu tiên đề xuất phạt sự thay đổi của các tham số quan trọng của mạng đối với một tác vụ để giữ tri thức của tác vụ đó . Tư tưởng chính	[[0, 3]]	[]	['EWC']	[]
586	ITE - onehot và NMTR , tuy nhiên độ đo có ý nghĩa về tính xếp hạng hơn là NDCG @ K thì mô hình MTMF lại kém hơn khi k < = 32 . Nhưng có thể	[[0, 12], [16, 20], [74, 78], [95, 99]]	[]	['ITE - onehot', 'NMTR', 'NDCG', 'MTMF']	[]
587	Cấu hình máy thực hiện đồ án : • CPU : Intel ( R ) Xeon ( R ) CPU @ 2.30GHz	[[33, 36], [62, 65], [72, 75]]	[]	['CPU', 'CPU', 'GHz']	[]
588	Bảng 3 . 3 : Kết quả của mô hình sử dụng dual learning áp dụng mô hình T ransf ormer − base với beam size 5 và tham số phạt α = 2.0 RNN - base	[[132, 135]]	[]	['RNN']	[]
589	kế các hàm mất mát heuristic để ràng buộc sự thay đổi của trọng số và của nơ-ron . • AGS - CL : [ 15 ] tìm độ quan trọng của nơ-ron và sử dụng tính chất thưa của	[[85, 93]]	[]	['AGS - CL']	[]
590	1 ) Các khái niệm và kí hiệu Chúng ta định nghĩa một số thuật ngữ sau : • Các từ là phần tử cơ bản tạo thành văn bản , được đánh chỉ số bởi { 1 , 2 , ... , V }	[]	[]	[]	[]
591	11 12 Đầu vào : Hai bộ dữ liệu đơn ngữ DA và DB , trạng thái khởi tạo của hai mô hình	[]	[]	[]	[]
592	Khi quá trình học hội tụ , độ quan trọng của mỗi nơ-ron được định nghĩa thông qua giá trị SNR : SN R ( Ei ) = |µσii |	[[90, 93]]	[]	['SNR']	[]
593	khác nhau của những bức ảnh khác nhau thì đó chính là cặp negative . Pretext task chung của CL đó là phân biệt được cặp nào là cặp positive , cặp nào là cặp negative .	[[92, 94]]	[]	['CL']	[]
594	đó với khả năng xảy ra tại tác vụ T hiện tại , và nhân với hệ số chuẩn hóa . Trong hầu hết các trường hợp , phân phối hậu nghiệm thường là các phân phối rất phức	[]	[]	[]	[]
595	• Với mỗi văn bản d = 1 , ... , D : θ̂dk ∝ γdk ( chuẩn hóa vectơ K chiều γd ) • Với mỗi topic k = 1 , ... , K : β ̂kj ∝ λkj ( chuẩn hóa vectơ V chiều λk )	[]	[]	[]	[]
596	Trong một bài báo khác , bài BYOL ( ở [ Gri+20 ] ) , thay vì sử dụng cả cặp positive và negative như thông thường , bài này lược bỏ bớt cặp negative và giải quyết vấn đề các biểu diễn ẩn bị kéo gần về một điểm của biểu diễn ẩn	[[29, 33]]	[]	['BYOL']	[]
597	Theo định lý Bayes , phân phối tham số của mô hình khi có đã quan sát được tập dữ liệu ( hay còn gọi là phân phối hậu nghiệm ) : p ( θ | D ) =	[]	[]	[]	[]
598	learning , OCI ) [ 20 ] . Với kịch bản ODI thì mô hình có thể cần có cơ chế tìm ra biên giữa các tác vụ để biết khi nào dữ liệu đang đến là của tác vụ mới để có chiến lược	[[11, 14], [39, 42]]	[]	['OCI', 'ODI']	[]
599	3 BÀI TOÁN HỆ GỢI Ý VỚI DỮ LIỆU HÀNH VI TIỀM ẨN VÀ RÕ RÀNG ( IMPLICIT , EXPLICIT BEHAVIOR )	[]	[]	[]	[]
600	Thuật toán 3 Phương pháp học SVB - PP cho LDA Đầu vào : Dòng dữ liệu với các minibatch { D1 , D2 , ... , Dt , ... } , các tham số η , α , ρ Đầu ra : Tham số biến phân toàn cục λ	[[29, 37], [42, 45]]	[]	['SVB - PP', 'LDA']	[]
601	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 21	[[267, 276]]	[]	['CNTT – TT']	[]
602	trúc mạng bị đầy nhanh hơn ở các tập khác 5 . 7 , từ tác vụ thứ 6 trở đi là gần như đầy , tuy nhiên kết quả vẫn cho thấy độ hiệu quả của VBD - CL .	[[137, 145]]	[]	['VBD - CL']	[]
603	dòng Bayesian khác chưa giải quyết được . Phần thực nghiệm cũng sẽ chứng tỏ Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[136, 147]]	[]	['KSTN - CNTT']	[]
604	hình 30 : : ACD ca sử dụng đánh giá công việc theo số sao 3.4 . 10 . Đăng bài tuyển dụng	[[12, 15]]	[]	['ACD']	[]
605	Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 26 η	[[60, 71]]	[]	['KSTN - CNTT']	[]
606	HÀ NỘI , 6/2021 ĐỀ TÀI TỐT NGHIỆP 1 . Thông tin về sinh viên	[]	[]	[]	[]
607	1 Ở hính 1 . 1 Các từ có kích thước càng lớn thể hiện xác suất văn bản chứa từ đó càng lớn , do đó kích thước các từ trong ảnh thể hiện tính đại diện cho chủ đề .	[]	[]	[]	[]
608	của mô hình , và VBD cung cấp một cơ chế hiệu quả đánh giá độ quan trọng của nơ-ron , đưa ra một cơ chế trực tiếp giảm thiểu mức độ quên . Thứ nhất , Dropout nói chung và VBD nói riêng có khả năng tìm ra được vùng	[[17, 20], [171, 174]]	[]	['VBD', 'VBD']	[]
609	• Số điện thoại : 0919020291 • Hệ đào tạo : KSCLC - TN - TT - VN - • Lớp : Tài năng Khoa học máy tính	[[44, 64]]	[]	['KSCLC - TN - TT - VN']	[]
610	[ 11 ] giải quyết vấn đề của SVI bằng cách giả sử toàn bộ dòng dữ liệu được sinh ra từ Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[29, 32], [147, 158]]	[]	['SVI', 'KSTN - CNTT']	[]
611	Ta có các bước cơ bản để xây dựng các vec -tơ biểu diễn cho dữ liệu : • Xây dựng bộ từ vựng , gọi độ lớn bộ từ vựng là VOCABULARY _ SIZE . • Đóng khung dữ liệu : Dữ liệu ban đầu có thể có kích thước khác nhau nhưng đầu	[]	[]	[]	[]
612	đẹp thì nhiều khả năng tất cả các mẫu sinh ra đều trọc đầu hoặc mạng AE sẽ sinh ra những tấm hình như hình 3 Hình 3 : Kết quả sinh ảnh của mạng Auto Encoder	[[69, 71]]	[]	['AE']	[]
613	N M Hình 2 : Biểu diễn đồ thị xác suất của mô hình LDA	[[51, 54]]	[]	['LDA']	[]
614	McNicholas ( 2010 ) [ 71 ] giới thiệu EPGMM , một công cụ dựa trên GMM , cho phép mô hình hóa mối tương quan giữa các mức độ biểu hiện gen	[[38, 43], [67, 70]]	[]	['EPGMM', 'GMM']	[]
615	GM F và GMF và MLP . Tương tự với qG	[[8, 11], [15, 18], [0, 2], [34, 36]]	[]	['GMF', 'MLP', 'GM', 'qG']	[]
616	Đặt điểm thưởng toàn phần cho quan sát thứ k là rk = αr1,k + ( 1 − α ) r2 , k ; end Tính stochastic gradient của ΘAB :	[]	[]	[]	[]
617	Việc giới hạn lại số lượng tham số cho ALV sẽ được để lại cho công việc trong tương lai và đề xuất ý tưởng trong 5 . 2 . 2 . 40	[[39, 42]]	[]	['ALV']	[]
618	viết quy tắc chuỗi đạo hàm cho tensor . Với Y = g ( X ) và z = f ( Y ) thì : ∇X z =	[]	[]	[]	[]
619	. Hàm mục tiêu : Hàm mục tiêu chung cho các mô hình ITE được phát biểu như sau :	[[52, 55]]	[]	['ITE']	[]
620	𝑇 sao cho : 𝛽 ∗ = argmin ‖𝑦 − 𝑋𝛽 ‖ 22 ( 1)	[]	[]	[]	[]
621	W = { w1 , w2 , · · · , wM } tương ứng với tập nhãn lớp C = { c1 , c2 , · · · , cM } . 2 ) Mô hình sinh	[]	[]	[]	[]
622	tự K-means : 𝑁 𝐾	[]	[]	[]	[]
623	trong đó S là ma trận đường chéo gồm K giá trị riêng lớn nhất của ma trận document-term . Với các ma trận DOC [ D×K ] và T OP IC [ K×V ] tìm được , chúng ta	[]	[]	[]	[]
624	Khi đó chúng ta có thể tính toán được đạo hàm của L̂ đối với wt dưới dạng tường minh sau :	[]	[]	[]	[]
625	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 48 finally	[[61, 70]]	[]	['CNTT – TT']	[]
626	thấy RMS prop cho độ chính xác trên tập validation cao nhất tại epoch 3 là 0.8940 và giảm dần không có dấu hiệu tăng . Đồng thời độ chính xác trên tập huấn luyện tại	[[5, 8]]	[]	['RMS']	[]
627	được ký hiệu là L ( θ ) . Bài toán tối thiểu hàm mất mát để tìm tập thâm số của mô hình được 11	[]	[]	[]	[]
628	Split CIFAR100 : Tập dữ liệu gốc CIFAR100 [ 37 ] gồm 100 lớp với mỗi lớp là một vật thể ( ví dụ máy bay , tàu thủy , v.v) . Có tổng cộng 60000 ảnh trong tập huấn luyện	[[6, 14], [33, 41]]	[]	['CIFAR100', 'CIFAR100']	[]
629	Tầng Word Embedding cũng tương tự như Title Encoder , chuyển chuỗi các từ [ 𝑤1𝑏 , 𝑤2𝑏 , . . . , 𝑤𝑃𝑏 ] thành tập các vector từ tương ứng [ 𝑒1𝑏 , 𝑒2𝑏 , . . . , 𝑒𝑃𝑏 ] , trong đó P là số từ trong nội dung bài báo , giá trị P trong mô hình sẽ được điều chỉnh để lấy P	[]	[]	[]	[]
630	Ma trận đánh giá của người dùng và sản phẩm Ma trận nhân tố ẩn của toàn bộ người dùng P	[]	[]	[]	[]
631	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 27 Sử dụng ELMo đã được huấn luyện với đầu vào là một chuỗi các ký tự cho mỗi từ trong	[[61, 70], [87, 91]]	[]	['CNTT – TT', 'ELMo']	[]
632	trong đó Stochastic Variational Inference ( SVI ) [ 8 ] là một ví dụ điển hình . Tuy nhiên , SVI là một phương pháp học trực tuyến ( online ) , nó yêu cầu một bộ dữ	[[44, 47], [93, 96]]	[]	['SVI', 'SVI']	[]
633	Nhận xét Mô hình NRMS được xây dựng dựa trên Deep Learning đã khai thác khá tốt các đặc điểm của bài toán gợi ý tin tức , mô hình đã tập trung vào các đặc trưng cơ	[[17, 21]]	[]	['NRMS']	[]
634	đến các mô hình gặp hiện tượng overfitting sớm như vậy . ( a ) HR @ 10	[[63, 65]]	[]	['HR']	[]
635	75.3 Bảng 6 : Kết quả của biểu diễn ẩn khi so sánh các phương pháp SOTA hiện nay ( Nguồn [ Car+20 ] )	[[67, 71]]	[]	['SOTA']	[]
636	45 5 THỬ NGHIỆM VÀ ĐÁNH GIÁ	[]	[]	[]	[]
637	Hình 4 . 11 Độ chính xác trên từng tác vụ trong thử nghiệm với VCL 36	[[63, 66]]	[]	['VCL']	[]
638	Như vây với cách tiếp cận xác suất cho NCF , bài toán gợi ý giống như một bài toán phân loại nhị phân .	[[39, 42]]	[]	['NCF']	[]
639	SSL và semi-supervised learning , trước đây cũng đã có một bài người ta đã làm và rất nổi tiếng đó là bài Deep Clustering [ Car + 18 ] . Hình 17 : Mô hình Deep Clustering ( Nguồn [ Car + 18 ] )	[[0, 3]]	[]	['SSL']	[]
640	end for * Bước cập nhật tham số toàn cục * Chọn ρt = ( τ0 + t ) κ P	[]	[]	[]	[]
641	số lượng click trong top K tổng số click trong tập test	[]	[]	[]	[]
642	. . . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 16	[[66, 77]]	[]	['KSTN - CNTT']	[]
643	Vì vậy trong phạm vi đề tài này , em muốn đề xuất tìm hiểu , cài đặt , đánh giá mô hình HAN cho bài toán phân loại cảm xúc văn bản tiếng Việt .	[[88, 91]]	[]	['HAN']	[]
644	Phần này tiến hành hai thử nghiệm tương ứng với hai loại dữ liệu trên bài toán học không giám sát , cụ thể là học chủ đề ẩn của một tập các văn bản dựa trên mô hình LDA .	[[165, 168]]	[]	['LDA']	[]
645	2 . 2 Các hệ số a , b , C có thể chọn sao cho các hệ số w có thể âm và kiểm soát độ	[]	[]	[]	[]
646	trong các nhãn đúng được tính là một ví dụ đúng . 4 . 2 . 2 Tham số chi tiết cho mô hình Ở đây , em sử dụng pre-trained word embedding là 300-dim word 2vec ( Mikolov et al. ,	[]	[]	[]	[]
647	""" OccurrenceOffsets "" : [ 35 ] , "" SurfaceForms "" : [ "" PGA Tour "" ] } ] Bảng 4 . 2 Mô tả thuộc tính bài báo tập MIND"	[[113, 117]]	[]	['MIND']	[]
648	Như đã đề cập ở phần mục tiêu và phạm vi đề tài , mô hình mà em đề xuất để giải quyết bài toán phân loại cảm xúc văn bản là mô hình HAN ( Hierarchical Attention 2	[[132, 135]]	[]	['HAN']	[]
649	MỘT SỐ MÔ HÌNH GỢI Ý TIN TỨC Hệ thống gợi ý ( Recommender Systems ) : là một mảng quan trọng trong học	[]	[]	[]	[]
650	• Các từ là phần tử cơ bản tạo thành văn bản , được đánh chỉ số bởi { 1 , 2 , ... , V } trong một tập từ điển có kích thước V . • Mỗi văn bản được kí hiệu là d và có nhãn cd ∈ { 1 , 2 , ... , C } tương ứng , trong	[]	[]	[]	[]
651	( 𝜃 | 𝐷𝐴 , 𝐷𝐵 ) = log 𝑝 ( 𝐷𝐵 | 𝜃 ) + log 𝑝 ( 𝜃 | 𝐷𝐴 ) − log 𝑝 ( 𝐷𝐵 ) PT 2.9 Trong công thức PT 2.9 , đại lượng bên trái là xác suất hậu nghiệm của tham số	[[92, 94], [69, 71]]	[]	['PT', 'PT']	[]
652	Mạng xã hội ( MXH ) là một đồ thị mô tả sự tương tác giữa các cá thể có cùng mối quan tâm , có liên hệ trực tiếp hay gián tiếp . Theo định nghĩa [ 2 ] , Mạng xã	[[14, 17]]	[[0, 11]]	['MXH']	['Mạng xã hội']
653	4.2.5 Baseline Sau khi cấu hình cho mô hình HAN để đạt được kết quả tốt nhất trên tập validation , em tiến hành so sánh , đánh giá hiệu năng của mô hình HAN với những mô hình học	[[44, 47], [153, 156]]	[]	['HAN', 'HAN']	[]
654	Pmột tập dữ liệu . Tham số α sẽ được chia đều cho các mô hình ngôn ngữ , tức là LM (s) = K1 K	[]	[]	[]	[]
655	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 51	[[267, 276]]	[]	['CNTT – TT']	[]
656	Cũng giống như LSI , pLSI có số lượng tham số của mô hình là tuyến tính theo số lượng văn bản D . Điều này khiến mô hình có thể dễ gặp phải overfitting ,	[[15, 18], [21, 25]]	[]	['LSI', 'pLSI']	[]
657	kết quả của LCL vẫn tốt hơn ổn định so với SimCLR gốc . Kịch bản 3 : Quan sát sự thay đổi mô hình khi đưa thêm các tham số a , b , C vào để điều chỉnh w .	[[12, 15], [43, 49]]	[]	['LCL', 'SimCLR']	[]
658	Chúng ta sẽ chứng minh rằng đại lượng Eζ [ A( s̃k ) ] − A ( sk ) ) Vj=1 ukj tương đương với một phép chính quy hoá . Thật vậy , sử dụng xấp xỉ Taylor bậc hai , ta có :	[]	[]	[]	[]
659	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 15	[[267, 276]]	[]	['CNTT – TT']	[]
660	Bảng 5 . 6 : Phần trăm không gian mạng CNN sử dụng qua các tác vụ trên tập Split CIFAR100 và Split CIFAR10 - 100	[[39, 42], [81, 89], [99, 112]]	[]	['CNN', 'CIFAR100', 'CIFAR10 - 100']	[]
661	thực hiện quá trình sinh các từ đầu ra , kết quả biểu diễn của các zi sẽ được đưa trở lại decoder để tiếp tục sinh ra các từ phía sau . Đầu ra của lớp thứ i được ký hiệu là E i = ( ei1 , ei2 , . . . , ein )	[]	[]	[]	[]
662	Khởi tạo ngẫu nhiên Θ = Θ0 for t = 1,2 , ... , T , ... do Nhận dữ liệu ở minibatch thứ t là Dt	[]	[]	[]	[]
663	3.4 Sử dụng mạng Multi-layer perceptron ( MLP ) cho bộ mã hóa và bộ giải mã	[[42, 45]]	[]	['MLP']	[]
664	6 Hình 2 . 1 Mô hình NRMS	[[21, 25]]	[]	['NRMS']	[]
665	Trong các phương pháp của CL mà em được biết , em lựa chọn phương pháp SimCLR . Do đó , đồ án này xoay quanh bài toán kết hợp manifold learning vào	[[26, 28], [71, 77]]	[]	['CL', 'SimCLR']	[]
666	trình bày ở phần đầu . Tuy nhiên chúng ta cũng thấy rằng trên các bộ TMN , TMN - title , The Irish Times thì iDropout với dr = 0 cũng có hiện	[[69, 72], [75, 78]]	[]	['TMN', 'TMN']	[]
667	Cuối cùng , NPMI cho toàn bộ K chủ đề là : Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 58	[[12, 16], [103, 114]]	[]	['NPMI', 'KSTN - CNTT']	[]
668	Tương tự như đối với tác tử thứ nhất , kênh này sẽ chuyển thông điệp từ ngôn ngữ B sang ngôn ngữ A bằng một mô hình dịch .	[]	[]	[]	[]
669	Bảng 0.1 Tham số tốt nhất cho PMNIST EWC	[[30, 36], [37, 40]]	[]	['PMNIST', 'EWC']	[]
670	Đồng thời cũng xin gửi lời cảm ơn tới các em Nguyễn Đức Tùng , Nguyễn Bá Khải - sinh viên lớp CNTT2.04 Khoá 60 và anh Trương Giang Khang - cựu sinh viên lớp KSTN CNTT Khoá 58 , trường đại học Bách Khoa Hà Nội , cũng đều là	[[94, 98], [157, 166]]	[]	['CNTT', 'KSTN CNTT']	[]
671	( 21 ) 𝐾 điều kiện ∑ 𝜙𝑘 = 1	[]	[]	[]	[]
672	Sử ảnh hưởng của đại lượng σ 2 lên hiệu suất của iDropout là rất rõ ràng trong hình 9 Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[146, 157]]	[]	['KSTN - CNTT']	[]
673	Phương pháp Hierarchical Power Priors ( HPP ) được trình bày dưới đây có thể giải quyết tốt các vấn đề đó . Các tác giả của HPP có ý tưởng sử dụng một mô hình chuyển dịch ( transition	[[40, 43], [124, 127]]	[]	['HPP', 'HPP']	[]
674	những đặc điểm quan trọng trong hành vi người dùng và đưa ra gợi ý thích hợp . Thử nghiệm cho thấy mô hình học biểu diễn đề xuất cho kết quả tốt hơn so với LDA truyền thống , các văn bản có tính diễn giải theo chủ đề , đồng thời áp dụng	[[156, 159]]	[]	['LDA']	[]
675	Chữ ký của GVHD Bộ môn : Hệ thống thông tin	[[11, 15]]	[]	['GVHD']	[]
676	Độ đo HR@K mới chỉ quan tâm đến sự xuất hiện của test item trong top K . Độ đo NDCG quan tâm đến cả xếp hạng của test item trong	[[79, 83], [6, 10]]	[]	['NDCG', 'HR@K']	[]
677	45.82 48.6 Bảng 6 : Kết quả nghiên cứu sự ảnh hưởng của các hàm lỗi L1 , L2 và L3 lên kết quả .	[]	[]	[]	[]
678	- Thể loại thường là cụm từ . Từ những lập luận trên , tác giã đã đề xuất mô hình NAML nhằm khai thác được	[[82, 86]]	[]	['NAML']	[]
679	TÓM TẮT NỘI DUNG ĐỒ ÁN TỐT NGHIỆP Trong kỷ nguyên phát triển của internet , trong đó phải kể đến các trang mạng xã hội , các diễn đàn , thương mại điện tử hay trang tin	[]	[]	[]	[]
680	Hàm aout và h tương tự như trong mô hình GMF . Đối với các hàm kích hoạt ax , ta có thể tùy ý lựa	[[41, 44]]	[]	['GMF']	[]
681	G ( Θt ) và tối ưu nó sử dụng một thuật toán cập nhật dựa trên gradient . Giải thuật sau mô tả phương pháp học của iDropout cho mô hình Bayesian tổng quát .	[]	[]	[]	[]
682	hình : 1T ( ( σtl ) 2 − log ( σtl ) 2 ) . Ràng buộc này cùng với ( b ) đạt cực tiểu khi σtl = 2σt −1 .	[]	[]	[]	[]
683	Dirichlet tiên nghiệm của β t+1 , tức là : p ( β t+1 | D1 , D2 , ... , Dt , η ) ≈ q ( βkt +1 | λt ) = Dir ( βkt +1 | λt ) ( 20 )	[]	[]	[]	[]
684	KL ( q ( z , Θ ) | |p( z , Θ|x) ) là nhỏ nhất . Rõ ràng điều này là không phù hợp với dữ liệu dòng có kích thước không lường trước được .	[]	[]	[]	[]
685	Hình dưới là kết quả huấn luyện với những learning rate schedule s khác nhau . Hình 8 So sánh hiệu năng của mô hình HAN trên các learning rate schedule s khác nhau	[[116, 119]]	[]	['HAN']	[]
686	Phần này bao gồm các phương pháp từ rất sơ khai như Denoising auto encoder , split -brain autoencoders cho đến những phương pháp có thể xem là hiện đại nhất ( SOTA ) trong hướng nghiên cứu này trong một năm trờ lại đây như	[[159, 163]]	[]	['SOTA']	[]
687	Đối với MTL thì việc các tác vụ chia sẻ chung một tầng ẩn nó làm tăng dữ liệu học cho mô hình . Trong khi học một tác vụ , mô hình cố gắng tối ưu sao cho đạt được kết quả	[[8, 11]]	[]	['MTL']	[]
688	Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT . .	[[60, 71]]	[]	['KSTN - CNTT']	[]
689	Đối với tập dữ liệu MIND , các mô hình được thử nghiệm với các phiên bản News Encoder khác nhau , nhằm đánh giá tác động của News Encoder và độ phù hợp của các cách tiếp cận cụ thể :	[[20, 24]]	[]	['MIND']	[]
690	3.5 Mô hình học HAN........................................................................................... 15	[[16, 19]]	[]	['HAN']	[]
691	xúc văn bản , có thể áp dụng cho các bài toán phân tích đánh giá của người dùng về sản phẩm , đồ ăn , nhà hàng . Bên cạnh đó , giá trị nghiên cứu của mô hình học HAN	[[162, 165]]	[]	['HAN']	[]
692	p ( β | η ) p ( W | C , β ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[88, 99]]	[]	['KSTN - CNTT']	[]
693	pháp sau : Phương pháp 1 : PDTB - Lin ( Lin et al. , 2009 ) sử dụng các phần 2 - 21 , 22 , 23 tương ứng làm tập dữ liệu training , dev , test .	[[27, 37], [131, 134]]	[]	['PDTB - Lin', 'dev']	[]
694	đó nhóm các phương pháp tiếp cận theo hướng Bayesian Inference đã đạt được nhiều thành tựu đáng kể . Trong nhóm các phương pháp này , hai frame work variational continual learning ( VCL )	[[182, 185]]	[]	['VCL']	[]
695	0 trong trường hợp còn lại 1 Chọn ngẫu nhiên K quan sát trong tập dữ liệu làm tâm cụm .	[]	[]	[]	[]
696	Hình 2 . 3 Giải thuật cho VCL 11 VCL là một phương pháp tổng quát trong Học liên tục có khả năng giải quyết	[[26, 29], [33, 36]]	[]	['VCL', 'VCL']	[]
697	được cập nhật và các key cũng sẽ lưu trữ những thông tin khác so với các key trước ) . Cập nhật bằng momentum : Một điểm khác biệt nữa của mô hình MOCO với	[[147, 151]]	[]	['MOCO']	[]
698	Đồng thời , phân bố dữ liệu trên tập Pega và tập MIND khác nhau . Do đó , các độ đo trên tập Pega sẽ có sự khác biệt lớn hơn trên	[[49, 53]]	[]	['MIND']	[]
699	2.3. 4 Nhận xét Với cách khai thác vào sở thích ngắn hạn và sở thích dài hạn của User , mô hình LSTUR đã cho kết quả tốt hơn các mô hình Deep Learning đơn thuần khác . Tất	[[96, 101]]	[]	['LSTUR']	[]
700	A ( sk ) = log Vi =1 exp ( ski ) . Giả sử ma trận drop π được lấy ngẫu nhiên từ biến thể ξ của phân phối Bernoulli , t = 1 ) = 1 − dr , p ( π t = 0 ) = dr , khi đó ta có :	[]	[]	[]	[]
701	được gọi là ( variational ) cận dưới hay lower bound ( ELBO ) của xác suất biên khả năng xảy ra của điểm dữ liệu x ( i ) . Do đó , từ ( 3. 1 ) ta có thể suy ra :	[[55, 59]]	[]	['ELBO']	[]
702	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 29 4 CÁC MÔ HÌNH ĐỀ XUẤT	[[63, 74]]	[]	['KSTN - CNTT']	[]
703	Các quan điểm trên về cơ bản tương tự mô hình LSTUR , tuy nhiên cách tiếp cận để khai thác các quan điểm sẽ dựa trên mô hình BERT . Kiến trúc tổng quan của mô hình đề xuất được minh họa bằng hình dưới đây :	[[46, 51], [125, 129]]	[]	['LSTUR', 'BERT']	[]
704	pair ) . Trong bài báo người ta kết hợp 3 phép DA : randomcropping ( đưa ảnh về một kích cỡ cố định ) , random color distortions ( biến đổi màu của ảnh ) và	[[47, 49]]	[]	['DA']	[]
705	sánh với các phương pháp SSL khác hoặc các phương pháp học có giám sát tương ứng . Có nhiều cách để tính ra kết quả trong bước này , như đưa biểu diễn ẩn vào	[[25, 28]]	[]	['SSL']	[]
706	Sau mỗi normalization layer ta sử dụng một kết nối residual với đầu ra là LayerNorm ( X+ SubLayer ( X) )	[]	[]	[]	[]
707	K-means cùng với SOM và Hierarchical Clustering là ba thuật toán phân cụm được sử dụng phổ biến nhất được dùng cho loại dữ liệu này [ 51 ] .	[[17, 20]]	[]	['SOM']	[]
708	vẫn nhỉnh hơn đáng kể ở hầu hết các giá trị của k . Mô hình ITE -onehot Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[60, 71], [135, 146]]	[]	['ITE -onehot', 'KSTN - CNTT']	[]
709	Kịch bản gia tăng về miền dữ liệu Trong thử nghiệm này , bộ dữ liệu PMNIST sẽ được sử dụng như mô tả trong 4 . 1 . 1 và một kiến trúc đơn đầu ra sẽ được sử dụng trong quá trình thử nghiệm	[[68, 74]]	[]	['PMNIST']	[]
710	VD chỉ ra một lựa chọn duy nhất có thể thỏa mãn yêu cầu đối với phân phối tiên nghiệm đó là một bất biến log - uniform :	[[0, 2]]	[]	['VD']	[]
711	liệu , nhưng lại chưa được sử dụng với mục đích tạo ra các bài toán liên quan đến CL . Ở các mô hình trước đây , các bài toán cho việc học CL thường được định nghĩa từ việc cắt các biểu diễn ẩn từ mạng để dự đoán địa phương từ toàn cục ( trong bài	[[82, 84], [139, 141]]	[]	['CL', 'CL']	[]
712	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 6	[[63, 74]]	[]	['KSTN - CNTT']	[]
713	cỡ embedding , h là số đầu của self-attention , df f là kích thước tầng ẩn đầu tiên của lớp FFN , t là thời gian huấn luyện và params là số lượng tham số cần huấn luyện T ransf ormer − small	[[92, 95]]	[]	['FFN']	[]
714	đó ta định nghĩa lên hai thành phần của hàm mất mát là Reconstruction Loss và Adversarial Loss như sau : Lrec = ||M̃	[]	[]	[]	[]
715	4 . 4 Mô hình ITE - 2 Ba mô hình tiếp theo sẽ xây dựng riêng biệt hơn cho hệ thống quảng cáo trực tuyến , nhưng chúng hoàn toàn có thể áp dụng cho những hệ thống có đặc điểm	[[14, 21]]	[]	['ITE - 2']	[]
716	VCL và UCL cũng chứng kiến những cải thiện rõ rệt , độ chính xác trung bình đạt được lần lượt là 98.67 % và 99.73 % . 33	[[0, 3], [7, 10]]	[]	['VCL', 'UCL']	[]
717	Ta có : ( 1 , nếu như test item nằm trong top K .	[]	[]	[]	[]
718	thước batch - size : 500 cho { 20NewsGroups , Grolier , TMN , TMN - title } , 5000 cho { Yahoo - title , NYT- title } . Chúng tôi đặt số topic K = 50 cho { 20NewsGroups ,	[]	[]	[]	[]
719	Với các mô hình học sử dụng mạng nơ-ron nhân tạo như CNN , LSTM không sử dụng cơ chế Attention , mặc dù , thể hiện độ chính xác cao hơn so với các method học máy bình thường , nhưng vẫn thấp hơn 2 - 3 % so với mô hình HAN .	[[53, 56], [59, 63], [218, 221]]	[]	['CNN', 'LSTM', 'HAN']	[]
720	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 26	[[267, 276]]	[]	['CNTT – TT']	[]
721	Mà hàm mục tiêu 𝐸 ( 𝑊 ) gồm có hai phần đó là cực tiểu hóa hàm lỗi 𝐸𝐷 ( 𝑊 ) trên tập dữ liệu huấn luyện 𝐷 và cực tiểu hóa số lượng nơ-ron ở mỗi tầng . Vậy	[]	[]	[]	[]
722	DANH MỤC TỪ VIẾT TẮT VÀ THUẬT NGỮ Attention Cơ chế tập trung ; Cơ chế giám sát có trọng số	[]	[]	[]	[]
723	Từ kết quả trong bảng trên có thể thấy , mô hình NAML cho kết quả trên các độ đo vượt trội so với các mô hình còn lại , cụ thể : AUC vượt trội 3 % , MRR 3 % , nDCG @ 5 và nDCG @ 10 vượt trội 4 % .	[[49, 53], [129, 132], [149, 152], [159, 163], [171, 175]]	[]	['NAML', 'AUC', 'MRR', 'nDCG', 'nDCG']	[]
724	học máy , học sâu trên mạng Internet , và rất nhiều tài liệu , blog về các kiến thức cơ bản trong những lĩnh vực này , tuy nhiên , vẫn chưa có một tài liệu cụ thể nào viết sâu và mang tính giới thiệu tổng hợp về hướng nghiên cứu SSL bằng tiếng Việt .	[[229, 232]]	[]	['SSL']	[]
725	tăng cường sự ràng buộc dựa trên độ lớn của 𝐿 2	[]	[]	[]	[]
726	Ngoài ra , một độ đo toàn cục mở rộng mới cho trung tâm bậc có tên là Tendency to Make Hub ( TMH ) được định nghĩa như sau : 23	[[93, 96]]	[]	['TMH']	[]
727	Vấn đề vanishing variance : Theo cách học của SVB cho LDA , khi dữ liệu của minibatch t + 1 đến , λt sẽ được sử dụng làm tham số trong phân phối	[[46, 49], [54, 57]]	[]	['SVB', 'LDA']	[]
728	Để phân loại câu vào các loại sự kiện , biểu diễn Si được đưa qua một mạng liên kết đầy đủ ( FC ) hai lớp . 2.3.	[[93, 95]]	[[70, 90]]	['FC']	['mạng liên kết đầy đủ']
729	hơn , đầy đủ hơn so với kết quả mô hình LDA trong hình 1 . 1 . Các chủ đề đã rõ ràng hơn , chi tiết hơn và các từ tập trung vào cùng một khía cạnh cụ thể thay vì	[[40, 43]]	[]	['LDA']	[]
730	• Các mô hình ITE có thể mở rộng từ hai nhóm hành vi thành nhiều loại hành vi tương tự như mô hình NMTR . Như vậy , về mặt bản chất có thể	[[14, 17], [99, 103]]	[]	['ITE', 'NMTR']	[]
731	Topics coherence ( NPMI ) Hình 4 . 1 Đồ thị biểu diễn độ kết hợp các chủ đề khi thay đổi số lượng chủ đề trên tập dữ liệu Associated Press	[[19, 23]]	[]	['NPMI']	[]
732	• Email : honghaipvu@gmail.com • Điện thoại liên lạc : 035 854 7694 • Lớp : CNTT 2.4 K59	[[76, 80]]	[]	['CNTT']	[]
733	Trong đó T là số lượng tác vụ cho tới thời điểm hiện tại . Tuy nhiên , mô hình không 8	[]	[]	[]	[]
734	thể tạo ra được một mạng F có biểu diễn ẩn được lấy đầu vào từ toàn bộ ảnh . Từ đó , ta cũng có thể kết hợp việc huấn luyện bằng cách sử dụng một hàm mất mát	[]	[]	[]	[]
735	- Hibernate là một thư viện ORM ( Object Relational Mapping ) mã nguồn mở giúp lập trình viên viết ứng dụng có thể kết nối các đối tượng với hệ quản trị cơ sở dữ liệu quan hệ , và hỗ trợ thực hiện các khái niệm lập trình hướng	[[28, 31]]	[]	['ORM']	[]
736	Topics coherence ( NPMI ) Số lần lấy mẫu	[[19, 23]]	[]	['NPMI']	[]
737	Lời cam đoan của sinh viên Tôi – Trần Thị Hồng - cam kết ĐATN là công trình nghiên cứu của bản thân tôi dưới sự	[[57, 61]]	[]	['ĐATN']	[]
738	phân loại . Và hàm mất mát là hàm Negative Log- likelihood của tất cả các mẫu S , thể hiện bằng công thức sau :	[]	[]	[]	[]
739	CL như MOCO và BYOL , với hi vọng , hướng đi này có thể cải tiến toàn bộ các phương pháp trong CL và mang lại một tư duy mới cho hướng nghiên cứu CL . Đồ án tốt nghiệp	[[0, 2], [7, 11], [15, 19], [95, 97], [146, 148]]	[]	['CL', 'MOCO', 'BYOL', 'CL', 'CL']	[]
740	Giảng viên hướng dẫn : ThS. Ngô Văn Linh HÀ NỘI 05/2019	[[23, 27]]	[]	['ThS.']	[]
741	ràng buộc gradient để giữ lại tri thức cho tác vụ cũ . Nhưng hai hạn chế khác của HAT đó là yêu cầu biết trước số lượng tác vụ , và mạng có xu hướng bị đầy sớm .	[[82, 85]]	[]	['HAT']	[]
742	Đối với tập dữ liệu Pega của công ty VCcorp , các kết quả thử nghiệm có thể quan sát ở bảng sau : AUC	[[98, 101]]	[]	['AUC']	[]
743	Bây giờ ta xem xét so sánh giữa các mô hình . Trên bộ Retailrocket , dễ dàng thấy rằng ba mô hình ITE đều có kết quả tốt hơn ở tất cả giá trị	[[98, 101]]	[]	['ITE']	[]
744	Split CIFAR100 , Split Omniglot Mạng	[[6, 14]]	[]	['CIFAR100']	[]
745	β ma trận topic distribution K × V , với mỗi hàng là tỷ lệ của các từ trong một chủ đề	[]	[]	[]	[]
746	Học liên tục dựa trên sự không chắc chắn , UCL UCL [ 33 ] xây dựng độ quan trọng của nơ-ron dựa trên tính không chắc chắn ( uncertainty ) của mạng Bayesian , và điều chỉnh đại lượng KL trong ELBO để cân bằng tính ổn định và mềm dẻo của mô hình .	[[43, 46], [47, 50], [182, 184], [191, 195]]	[]	['UCL', 'UCL', 'KL', 'ELBO']	[]
747	SVM dựa trên ý tưởng tìm một siêu phẳng phân tách sao cho mức lề ( tức là khoảng cách gần nhất từ một điểm tới mặt phân cách ) là lớn nhất . SVM chỉ làm việc được với bài	[[0, 3], [141, 144]]	[]	['SVM', 'SVM']	[]
748	Masked Language Modeling ( MLM ) Mô hình dự đoán giá trị ban đầu của các từ bị che dấu dựa trên ngữ cảnh của các từ không bị che dấu trong chuỗi	[[27, 30]]	[]	['MLM']	[]
749	từ biểu diễn đặc trưng của từng tác vụ , từ đó can thiệp vào độ lớn gradient của trọng số để hạn chế sự thay đổi của nơ-ron quan trọng . • UCL : [ 33 ] định nghĩa độ quan trọng của nơ-ron trong mạng BNN và thiết	[[139, 142], [199, 202]]	[]	['UCL', 'BNN']	[]
750	Ví dụ , từ ‘ additionally ’ chỉ có quan hệ ‘ Expansion.Conjunction ’ . Từ bộ dữ liệu PDTB , em thống kê các quan hệ đi	[[85, 89]]	[]	['PDTB']	[]
751	10 Tác v 8 AGS - CL	[[11, 19]]	[]	['AGS - CL']	[]
752	Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 66 Chương 6 Kết luận	[[60, 71]]	[]	['KSTN - CNTT']	[]
753	TÓM TẮT NỘI DUNG ĐỒ ÁN Trong ĐATN này , ta thực hiện áp dụng mạng đồ thị tích chập ( GCN ) để phân loại văn bản theo các nhãn cho trước , kết hợp với mô hình chủ đề ( Topic	[[29, 33], [85, 88]]	[[61, 82]]	['ĐATN', 'GCN']	['mạng đồ thị tích chập']
754	của bài báo . Phép DA này nói một cách đơn giản là cho đầu ra với nhiều view hơn , các view này có thể có kích thước nhỏ hơn so với phép DA crop thông thường , do	[[19, 21], [137, 139]]	[]	['DA', 'DA']	[]
755	hình 23 : ACD ca sử dụng đăng nhập ứng viên 3 . 4 . 3 . Sửa thông tin cá nhân	[[10, 13]]	[]	['ACD']	[]
756	Khám phá ra các đặc trưng của bộ dữ liệu PDTB , đồng thời đề ra một số kỹ thuật để khai thác hiệu quả các đặc trưng đó giúp cải thiện chất lượng của mô hình . Kết quả , bài nghiên cứu đã được chấp nhận tại hội nghị Association for Computational	[[41, 45]]	[]	['PDTB']	[]
757	sát sau , còn ở thử nghiệm này , chúng ta sẽ bỏ qua thông tin về nhãn đó . với một số thống kê chi tiết ở Bảng 1 . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[175, 186]]	[]	['KSTN - CNTT']	[]
758	Khi đó ta có thể viết : V V	[]	[]	[]	[]
759	) D Đồ án tốt nghiệp	[]	[]	[]	[]
760	2 KIẾN THỨC CƠ SỞ • Θf là tập các tham số mô hình của hàm ánh xạ f Do hàm ánh xạ f được định nghĩa là một mạng nơ-ron nhiều tầng , công thức	[]	[]	[]	[]
761	Để minh họa cho vấn đề của LDA , hình 1 . 1 thể hiện các chủ đề con của chủ đề “ Thời sự ” : Hình 1 . 1 Các chủ đề con của chủ đề Thời sự	[[27, 30]]	[]	['LDA']	[]
762	− 𝐾𝐿 ( 𝑞 ( 𝑠 ) | | 𝑝( 𝑠) ) ≈ hằng số + 0.5 log 𝛼 + 𝑐1 𝛼 + 𝑐2 𝛼 2 + 𝑐3 𝛼 3 Trong đó : 𝑐1 = 1.1614512 , 𝑐2 = −1.50204118 ,	[]	[]	[]	[]
763	GCN , W ( l ) kích thước k x k là ma trận cho lớp biến đổi của thứ l của mạng . Sau khi đi qua GCN có L tầng , ta được ma trận H ( L ) có kích thước là ( n+m ) x k là ma trận biểu diễn của ( n+m ) nốt , mà trong đó , mỗi nốt là tổng hợp thông tin của	[[0, 3], [95, 98]]	[]	['GCN', 'GCN']	[]
764	19 Đây có thể coi là phương pháp phổ biến nhất của MTL trong học sâu . Ý tưởng của nó	[[51, 54]]	[]	['MTL']	[]
765	Giảng viên hướng dẫn : PGS. TS. Thân Quang Khoát Chữ ký của GVHD Bộ môn :	[[23, 27], [28, 31], [60, 64]]	[]	['PGS.', 'TS.', 'GVHD']	[]
766	đồng thời tâm cụm sẽ được tính là : 𝑘 𝑚 ∑𝑁	[]	[]	[]	[]
767	trong quá trình huấn luyện , 10000 ảnh còn lại được sử dụng trong quá trình kiểm thử . Một số mẫu dữ liệu trong bộ MNIST được mô tả trong Hình 4 . 1 .	[[115, 120]]	[]	['MNIST']	[]
768	[ logq ( X | Y ) ] θ Đồ án tốt nghiệp	[]	[]	[]	[]
769	• Mở rộng các chủ đề cho bài toán phân loại . • Tìm hiểu việc áp dụng mô hình HAN cho bài toán phân tích cảm xúc mức khía cạnh .	[[78, 81]]	[]	['HAN']	[]
770	và không ứng với key nào : exp ( q ∗ k+ / α ) Lq = − log PK	[]	[]	[]	[]
771	Với tham số mô hình là θ , hàm mục tiêu trong công thức PT 2. 15 có thể viết lại là : 𝑁	[[56, 58]]	[]	['PT']	[]
772	Hệ thống được cấu trúc theo mô hình MVC : - Model : Đây là thành phần chứa tất cả các nghiệp vụ logic , phương thức xử lý , truy xuất database , đối tượng mô tả dữ liệu như các Class , hàm xử lý .	[[36, 39]]	[]	['MVC']	[]
773	và SVB - PP , đặc biệt là trong môi trường dữ liệu tới vô hạn . 4 . 3 . 2 Cân bằng thông tin cũ và mới Trong mục này , chúng tôi sẽ thảo luận cách iDropout cân bằng giữa thông tin	[[3, 11]]	[]	['SVB - PP']	[]
774	Mạng nơ-ron nhân tạo là mô hình cơ bản , cung cấp nền tảng để xây dựng và phát triển những mạng nơ-ron nâng cao , trong đó có mạng nơ-ron hồi quy ( RNN ) . Ta sẽ	[[148, 151]]	[[126, 145]]	['RNN']	['mạng nơ-ron hồi quy']
775	• Công thức ( II ) : Closeness centrality được tính bằng bình quân của tổng số khoảng cách ngắn nhất từ một nút đến tất cả các nút còn lại . ∑𝑡𝜖𝑉 \ 𝑣 𝑑𝐺 ( 𝑣 , 𝑡)	[]	[]	[]	[]
776	tiên nghiệm của nó là p ( Θ | D1 , D2 , ... , Dt−1 , η ) . Mặt khác phân phối tiên nghiệm Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[150, 161]]	[]	['KSTN - CNTT']	[]
777	Training epoch s Topics coherence ( NPMI ) Hình 4 . 11 Đồ thị biểu diễn sự thay đổi giá trị NPMI khi thay đổi số lần học mô	[[36, 40], [92, 96]]	[]	['NPMI', 'NPMI']	[]
778	{ 𝑦1 , 𝑦2 , … , 𝑦𝑁 }; 𝑦𝑛 ∈ ℝ𝐾 , 𝑦𝑛𝑘 = { 1 nếu 𝑥𝑛 thuộc cụm 𝑘	[]	[]	[]	[]
779	dụng xấp xỉ VI trên đại lượng khả năng xảy ra này : 22 𝑁𝑡	[[12, 14]]	[]	['VI']	[]
780	VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG ─ ── ── ── * ─ ── ── ── ĐỒ ÁN TỐT NGHIỆP ĐẠI HỌC	[]	[]	[]	[]
781	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 33 4 . 2 11 - way classification	[[61, 70]]	[]	['CNTT – TT']	[]
782	𝐿𝛽 ( 𝛽 ) là đại lượng hiệu chỉnh , hàm lỗi mới được viết thành : 𝐿̃ ( 𝛽 ) = 𝐿𝐷 ( 𝛽 ) + 𝜆𝐿𝛽 ( 𝛽 ) ( 3 )	[]	[]	[]	[]
783	hệ thống gợi ý nói riêng .  Tìm hiểu sâu hơn về mô hình phân tích ma trận ( MF ) ứng dụng trong các hệ thống gợi ý	[[77, 79]]	[[49, 74]]	['MF']	['mô hình phân tích ma trận']
784	sử dụng hàm kích hoạt là hàm sigmoid và h được học từ dữ liệu . Mô hình này được gọi GMF - Generalized Matrix Factorization .	[[85, 88]]	[]	['GMF']	[]
785	k ∈ { 1 , 2 , ... , K} ( topic distributions ) : βk ∼ Dir ( η ) • Sinh ra Nd từ cho mỗi văn bản d :	[]	[]	[]	[]
786	Nguồn : genome.gov 2 . 2 . 5 Dữ liệu biểu hiện gene Toàn bộ quá trình truyền thông tin di truyền từ DNA tới protein gọi là quá trình biểu	[[100, 103]]	[]	['DNA']	[]
787	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 7 Danh mục bảng biểu	[[61, 70]]	[]	['CNTT – TT']	[]
788	bản . Ta giả sử một văn bản có L câu 𝑠𝑖 và mỗi câu có 𝑇𝑖 từ .	[]	[]	[]	[]
789	Học liên tục dựa trên suy diễn biến phân Học liên tục dựa trên suy diễn biến phân ( VCL ) [ 2 ] là một mô hình tổng quát	[[84, 87]]	[[41, 81], [0, 40]]	['VCL']	['Học liên tục dựa trên suy diễn biến phân', 'Học liên tục dựa trên suy diễn biến phân']
790	{ ( 𝑥 , 𝑦𝑖 ) , 𝑖 = 1 , 2 , … 𝑀 } , trong đó 𝑥 𝑖 = ( 𝑥1𝑖 , 𝑥2𝑖 , … , 𝑥𝑁𝑖 ) , y là đại lượng hồi quy tương ứng với quan sát i , N là số đặc trưng ( biến ) của mỗi quan sát . Mục tiêu của bài toán hồi quy	[]	[]	[]	[]
791	) . Phiên bản này đưa ra mối tục tuân theo phân phối Gauss : Emk ∼ N ( 1, α = 1−p	[]	[]	[]	[]
792	1 . Chọn w theo công thức tính của thuật toán LLE thông thường . Ta sử dụng	[[46, 49]]	[]	['LLE']	[]
793	trước được coi như phân phối tiên nghiệm cho tác vụ hiện tại . Khi đó ELBO cho tác vụ t có dạng :	[[70, 74]]	[]	['ELBO']	[]
794	trung của từ ) , Câu ( Bộ mã hóa câu , Độ tập trung của câu ) . Ở đây , độ tập trung có thể hiểu là giá trị thông tin mà từ và câu đóng góp trong việc phân loại văn bản tương	[]	[]	[]	[]
795	• Tốc độ xử lý để đảm bảo tính real -time và tính ứng dụng cao . Bài toán Object Tracking hiện nay có hai hướng tiếp cận chính đấy là Single Object Tracking ( SOT ) , tức là tập chung theo dõi một đối tượng cụ thể trong toàn bộ	[[159, 162]]	[]	['SOT']	[]
796	và θ , cụ thể hơn chúng đóng vai trò làm tham số cho phân phối tiên nghiệm Dirichlet của hai biến ẩn này . Điều này giúp mô hình LDA có có tổng quát hoá	[[129, 132]]	[]	['LDA']	[]
797	"để trọng số có thể cập nhật "" thoải mái "" hơn cho tác vụ mới . Trong quá trình tối ưu , HAT sử dụng một chiến lược heuristic tăng dần s qua các vòng lặp của một epoch , bắt đầu với s → 0 , atl , i → 21 , tức các nơ-ron đều được kích"	[[88, 91]]	[]	['HAT']	[]
798	Bài nghiên cứu 4 - way PDTB - Lin	[[23, 27]]	[]	['PDTB']	[]
799	NDCG @ 10 Hình 20 : So sánh các mô hình trên bộ Recobell qua từng vòng lặp với k = 32 Ở hình 19 , ta thấy sau những vòng lặp đầu tiên , hai mô hình ITE - item _ pcat	[[0, 4], [148, 165]]	[]	['NDCG', 'ITE - item _ pcat']	[]
800	Sau khi học trên một minibatch t , mô hình sẽ được đánh giá bằng độ đo LPP trên minibatch tiếp theo . 5 . 4 . 2 Kết quả thử nghiệm	[[71, 74]]	[]	['LPP']	[]
801	32 Thuật toán 2 Phương pháp học SVB cho LDA Đầu vào : Dòng dữ liệu với các minibatch { D1 , D2 , ... , Dt , ... } , các tham số tiên	[[32, 35], [40, 43]]	[]	['SVB', 'LDA']	[]
802	Để tính đại lượng khả năng xảy ra trong công thức PT 3.7 , ở đây ta sẽ sử dụng kĩ thuật LRT đã trình bày trong 2 . 1 . 3 . Xét một tầng 𝑙 trong mạng , ta có giá trị preactivation tại tầng 𝑙 với mà trận đầu vào 𝐴 ( 𝑙 ) là :	[[50, 52], [88, 91]]	[]	['PT', 'LRT']	[]
803	Trong các nhánh phát triển của hướng nghiên cứu SSL , có một nhánh rất phát triển và đang có rất nhiều mô hình đem lại các kết quả tốt ở nhánh này , thậm chí các phương pháp của hướng tiếp cận này có thể tạo ra các mô hình mạng nơ-ron	[[48, 51]]	[]	['SSL']	[]
804	Park và cộng sự ( 2015 ) [ 16 ] sử dụng Sparse Overlapping Group Lasso cho dữ liệu The Cancer Genome Atlas ( TCGA ) của nhiều loại bệnh ung thư	[[109, 113]]	[]	['TCGA']	[]
805	TỐT NGHIỆP ĐẠI HỌC NGÀNH CÔNG NGHỆ THÔNG TIN TÊN ĐỀ TÀI	[]	[]	[]	[]
806	Dựa theo [ 14 ] , chúng tôi sử dụng thuật toán tối ưu Nesterov ’ s accelerate gradient ( NAG ) [ 47 ] . Trong quá trình huấn luyện , tốc độ học sẽ được giảm từ epoch thứ 24 theo công thức	[[89, 92]]	[]	['NAG']	[]
807	Bài toán đó có công thức như sau : min T r ( QT C T Z ) + H ( Q )	[]	[]	[]	[]
808	Đối với LGL và SOGL , các hệ số 𝛽 cũng được tách thành các hệ số ẩn tương tự công thức ( 11 ) và ( 12 ) . Bài toán ( 13 ) có thể giải bằng một số biến	[[8, 11], [15, 19]]	[]	['LGL', 'SOGL']	[]
809	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 43	[[267, 276]]	[]	['CNTT – TT']	[]
810	Dropout tương ứng . Như vậy ta có thể nhận định rằng ALV có khả năng áp dụng vào các	[[53, 56]]	[]	['ALV']	[]
811	Viện Công nghệ Thông tin và Truyền thông các biểu diễn ẩn mà LCL muốn tìm ra :	[[61, 64]]	[]	['LCL']	[]
812	trình nào khác . Hà Nội , ngày tháng năm Tác giả ĐATN	[[49, 53]]	[]	['ĐATN']	[]
813	Trong công việc này , hai bộ dữ liệu mô phỏng có thể áp dụng cho môi trường học liên tục sẽ được xây dựng từ hai bộ dữ liệu gốc này như sau : • Split CIFAR100 : chia bộ dữ liệu CIFAR100 thành 10 bộ dữ liệu tương ứng	[[150, 158], [177, 185]]	[]	['CIFAR100', 'CIFAR100']	[]
814	× ( d2 − w+1 ) , trong đó O kij = h [ X] ij , F k i =	[]	[]	[]	[]
815	CIFAR10 / 100 Split Omniglot Mô hình gốc	[[0, 13]]	[]	['CIFAR10 / 100']	[]
816	Thêm vào đó tập trung vào việc cải tiến đại lượng KL trong hàm mục tiêu giúp cải thiện tính ổn định và mềm dẻo của mô hình , giải quyết tốt hơn cho vấn đề quên nhanh chóng của mô hình .	[[50, 52]]	[]	['KL']	[]
817	ẩn của cặp negative vẫn phải cách xa nhau giống như trong CL thông thường . Kết hợp 2 điều này một lần nữa giúp ta hình dung rõ hơn về hình dáng của không gian	[[58, 60]]	[]	['CL']	[]
818	và β = [ β1 β2 ... βV ] trong đó βj là cột thứ j của ma trận β , khi đó : sof tmax ( βk ) j = exp ( skj − A ( sk ) )	[]	[]	[]	[]
819	Hình 4 . 5 Kết quả thử nghiệm cho VCL và UCL trên PMNIST Kết quả thử nghiệm cho thấy việc áp dụng ALV lên các phương pháp tiếp cận theo OVI cũng giúp cải thiện hiệu năng của các phương pháp này trong kịch bản	[[34, 37], [41, 44], [50, 56], [98, 101], [136, 139]]	[]	['VCL', 'UCL', 'PMNIST', 'ALV', 'OVI']	[]
820	2.3 Mạng phát hiện khuôn mặt You only look once ( YOLO )	[[50, 54]]	[]	['YOLO']	[]
821	Để sử dụng bộ dữ liệu MNIST cho bài toán Học liên tục , hai bộ dữ liệu sẽ được tạo ra tương ứng với hai kịch bản thử nghiệm gia tăng số lượng tác vụ và miền dữ liệu , được gọi là Split MNIST và PMNIST .	[[22, 27], [194, 200], [185, 190]]	[]	['MNIST', 'PMNIST', 'MNIST']	[]
822	Học liên tục , giúp cải thiện các phương pháp tiếp cận theo hướng ràng buộc trọng số , còn được gọi là Học liên tục dựa trên tham số cục bộ ( ALV ) . Phương	[[142, 145]]	[[103, 139]]	['ALV']	['Học liên tục dựa trên tham số cục bộ']
823	quan trọng hay không , và một thành phần ràng buộc khác được đưa vào : L X	[]	[]	[]	[]
824	Mô hình T ransf ormer − small , T ransf ormer − small ,	[]	[]	[]	[]
825	Giả thiết mà em đặt ra ở đây có thể là với không gian ảnh thì việc tính w theo phương pháp LLE có thể chưa hẳn đã là một cách xấp xỉ tốt , trong khi đó sử dụng hai cách tính w kia mặc dù đơn giản hơn về mặt tính	[[91, 94]]	[]	['LLE']	[]
826	và dữ liệu quan sát được x mà thông qua một hàm chuyển đổi ( transformation function ) . Như vậy , biến ẩn toàn cục Θ trong Infinite Dropout không có vai trò	[]	[]	[]	[]
827	nhỏ , µlij , t càng bị ràng buộc để gần µlij , t−1 . Tuy nhiên , BNN có thể học ra những nơ-ron không quan trọng của mạng , cho nên những nơ-ron đó không nhất thiết cần ràng buộc mà có thể cho phép học ở tác vụ	[[65, 68]]	[]	['BNN']	[]
828	những kết quả tốt bất ngờ , đó cũng chính là lý do mô hình có tên Naive Bayes . Như vậy chúng ta có : p ( β | W , C , η ) ∝	[]	[]	[]	[]
829	ĐỀ TÀI TỐT NGHIỆP 1 . Thông tin sinh viên Họ và tên : Trần Tùng Lâm - MSSV : 20173226	[[70, 74]]	[]	['MSSV']	[]
830	cấp độ biểu diễn thông tin từ mức độ thấp đến cao và trừu tượng hơn thông qua convolution từ các filter . Đó là lý do CNN cho ra mô hình với độ chính xác cao .	[[118, 121]]	[]	['CNN']	[]
831	Ban và cộng sự ( 2010 ) [ 36 ] dùng SVM trong một nghiên cứu GWAS để tìm ra nhiều SNPs mới liên quan	[[36, 39], [61, 65], [82, 86]]	[]	['SVM', 'GWAS', 'SNPs']	[]
832	Trong công việc này một biến thể của LRT cũng sẽ được sử dụng để áp dụng vào phương pháp đề xuất cho các phương pháp tiếp cận theo OVI .	[[37, 40], [131, 134]]	[]	['LRT', 'OVI']	[]
833	Để khắc phục hạn chế về phụ thuộc xa , Long Short Term Memory ( LSTM ) dựa trên RNN ra đời để cải thiện vấn đề này . 2 . 3 Mạng bộ nhớ dài - ngắn ( Long Short Term Memory )	[[64, 68], [80, 83]]	[[123, 145]]	['LSTM', 'RNN']	['Mạng bộ nhớ dài - ngắn']
834	đánh giá trước đó . Kết quả mô hình học đề xuất dựa trên BERT ( MHDX ) với News Encoder là biểu diễn đề xuất có thể quan sát trong bảng sau đây :	[[57, 61], [64, 68]]	[]	['BERT', 'MHDX']	[]
835	𝑠𝑡 = 𝑡𝑎𝑛ℎ ( 𝑊 𝑠𝑡−1 + 𝑈 𝑥𝑡 ) 𝑦𝑡 = 𝑉 ℎ𝑡 RNN sử dụng 3 ma trận trọng số 𝑊 , 𝑈 , 𝑉 cho 2 quá trình tính toán .	[[38, 41]]	[]	['RNN']	[]
836	trong đó các giá trị kì vọng là : K X	[]	[]	[]	[]
837	Hình 5 . 2 : Sự thay đổi của độ chính xác của các tác vụ trong quá trình học Permuted MNIST .	[[86, 91]]	[]	['MNIST']	[]
838	26 CHƯƠNG 4 . PHƯƠNG PHÁP ĐỀ XUẤT	[]	[]	[]	[]
839	P ràng buộc là m j =1 wi , j = 1 , điều kiện này cho phép ta tịnh tiến toàn bộ dữ liệu thì	[]	[]	[]	[]
840	2K×V khả năng có thể của Θt khi bị drop thành một mô hình trung bình duy nhất . Đây chính là tính chất ensemble của Dropout mà chúng tôi đã đề cập .	[]	[]	[]	[]
841	∗ Trong công thức PT 2.10 , ta có log 𝑝 ( 𝜃𝐴 , 𝑖 | 𝐷𝐴 ) không phục thuộc vào tham số	[[18, 20]]	[]	['PT']	[]
842	giải trong hệ thống gợi ý tin tức LÊ MINH HIẾU hieu.lm161522@sis.hust.edu.vn	[]	[]	[]	[]
843	độ đo Jaccard làm giá trị thực tế ( ground-truth ) cho tính tương đồng giữa các người dùng , cái mà mô hình MF cần phải khôi phục lại được . • Độ tương đồng Jaccard : Gọi Ru là tập các item mà người dùng u đã	[[108, 110]]	[]	['MF']	[]
844	biến gradient , vấn đề phụ thuộc dài hạn nên cần một kiến trúc mới để giải quyết các vấn đề trên là Long short-term memory ( LSTM ) . Long short-term memory	[[125, 129]]	[]	['LSTM']	[]
845	bằng thông tin cũ và mới hiệu quả hơn SVB . • Có thể tránh được vấn đề vanishing variance vì theo công thức trên , khi t Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[38, 41], [181, 192]]	[]	['SVB', 'KSTN - CNTT']	[]
846	phương sai của 𝐿 được biểu diễn như sau : 23 𝑀	[]	[]	[]	[]
847	1 . GIỚI THIỆU ĐỀ TÀI 1 . 1 Đặt vấn đề .	[]	[]	[]	[]
848	KẾT LUẬN 6 Kết luận Phần này sẽ tóm tắt lại những kết quả , kinh nghiệm đã đạt được trong quá	[]	[]	[]	[]
849	Theo công thức PT 2.12 , khi đó ta có hàm mục tiêu cho EWC với ALV : 𝑁𝑡 𝐴𝐿𝑉	[[15, 17], [55, 58], [63, 66], [72, 75]]	[]	['PT', 'EWC', 'ALV', '𝐴𝐿𝑉']	[]
850	Cụ thể , các tham số của ANN được học dựa trên một tập dữ liệu cho trước . Khi quá trình học hội tụ , những	[[25, 28]]	[]	['ANN']	[]
851	Tuy nhiên , ta có thể xem xét một số đặc điểm của các hàm trên : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[128, 139]]	[]	['KSTN - CNTT']	[]
852	NTD NTD chọn vào mục hồ sơ ứng viên Nhà tuyển dụng đăng nhập vào hệ thống	[[0, 3], [4, 7]]	[]	['NTD', 'NTD']	[]
853	Giả sử phân phối biến phân được tham số hóa bởi φ : qφ ( θ ) , khi đó khoảng cách KL giữa qφ ( θ ) và 12	[[82, 84]]	[]	['KL']	[]
854	Khởi tạo ngẫu nhiên λ = λ0 for t = 1,2 , ... , T , ... do Nhận dữ liệu ở minibatch thứ t là Dt	[]	[]	[]	[]
855	trờ thành AE truyền thống . Mạng Split-Brain autoencoder : vận dụng cách làm dự đoán chéo các kênh màu : Như vậy từ mạng AE trong colorization ban đầu , bài báo đã tổng	[[10, 12], [121, 123]]	[]	['AE', 'AE']	[]
856	VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG ── ── ── ── * ─ ── ── ── ĐỒ ÁN	[]	[]	[]	[]
857	cân bằng được tốt tính chất này cũng sẽ giúp việc học được cải thiện . Điển hình có thể thấy trong trường hợp của EWC khi việc sử dụng Dropout cũng giúp cải thiện	[[114, 117]]	[]	['EWC']	[]
858	- Split MNIST : Bảng 0.2 Tham số tốt nhất cho Split MNIST	[[8, 13], [52, 57]]	[]	['MNIST', 'MNIST']	[]
859	HÀ NỘI , 6/2021 Đại học Bách Khoa Hà Nội	[]	[]	[]	[]
860	KEGG [ 81 ] , Biocarta [ 82 ] . Kết quả được trình bày trong Bảng 4 và Hình 11 . Có thể thấy	[[0, 4]]	[]	['KEGG']	[]
861	trong đó D là tập huấn luyện . Cơ chế Attention Hình 2 . 2 : Mô hình của cơ chế attention .	[]	[]	[]	[]
862	𝑠𝑡 = 𝑓𝑤 ( 𝑠𝑡 −1 , 𝑥𝑡 ) Hàm 𝑓𝑤 là hàm phi tuyến , ví dụ : 𝑡𝑎𝑛ℎ , 𝑅𝑒𝐿𝑈 . Ở đây , ta sử dụng 𝑡𝑎𝑛ℎ , công thức trên	[]	[]	[]	[]
863	nhiều thông tin từ bài báo một cách có chọn lọc , biểu diễn User bằng lịch sử đọc có trọng số . Hình 2 . 2 Cấu trúc mô hình NAML	[[124, 128]]	[]	['NAML']	[]
864	36 Tác động của tham số phạt α lên điểm BLEU của ( a ) ConvS2S- base và ( b ) Transformer - base . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .	[[40, 44], [55, 62]]	[]	['BLEU', 'ConvS2S']	[]
865	( sim ( z i , z j ) /τ ) ( 10 ) Trong đó , I [ k 6 = i ] là hàm nhận giá trị 0 khi k = i và nhận giá trị 1 khi k 6 = i ) ,	[]	[]	[]	[]
866	Để giúp dễ hiểu và nắm bắt , các nội dung trong mục này sẽ trình bày chi tiết việc áp dụng ALV cho lần lượt các phương pháp này .	[[91, 94]]	[]	['ALV']	[]
867	đó những nơ - ron có giá trị SNR nhỏ có thể được bỏ đi bằng cách đặt giá trị đầu ra của chúng bằng 0. Phần trình bày ở trên đang minh họa với mạng liên kết đầy đủ , tức đầu vào có dạng	[[29, 32]]	[]	['SNR']	[]
868	còn D là số chiều của dữ liệu . Mỗi hàng là một điểm dữ liệu trong không gian gốc .	[]	[]	[]	[]
869	Cực đại hàm mục tiêu trên bằng việc sử dụng một thuật toán cập nhật dạng gradient ascent . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[151, 162]]	[]	['KSTN - CNTT']	[]
870	thức cơ bản về CNN mà em tìm hiểu được , giúp ích trong việc giải quyết bài toán . Trước hết , Convolutional là một cửa sổ trượt trên một ma trận nhằm lấy ra được các thông tin hữu ích nhất từ ma trận đó thông qua các tham số ( kernel ) và cơ chế nhân tích	[[15, 18]]	[]	['CNN']	[]
871	. . . Kỹ thuật lấy mẫu đổi biến tham số - Reparameterization Trick . Sử dụng mạng Multi-layer perceptron ( MLP ) cho bộ mã hóa và	[[107, 110]]	[]	['MLP']	[]
872	chính là động lực để UCL cải tiến đại lượng này . Sau khi xấp xỉ đại lượng 𝑞𝑡 ( θ ) trong công thức PT 2.16 bằng GMF , ta có công thức tường minh của đại lượng KL :	[[21, 24], [100, 102], [113, 116], [160, 162]]	[]	['UCL', 'PT', 'GMF', 'KL']	[]
873	18 2 . 3 Multi- task learning Multi-task learning ( MTL ) là một phương pháp học cho phép ta học ra mô hình của	[[52, 55]]	[]	['MTL']	[]
874	N91737 - 0 N29160 - 0 Bảng 4 . 1 Mô tả hành vi người dùng tập MIND	[[62, 66]]	[]	['MIND']	[]
875	tập dữ liệu huấn luyện VOCABULARY _ SIZE .	[]	[]	[]	[]
876	à Phương pháp PCA sẽ cố gắng tìm phép biến đổi tuyến tính T thỏa mãn y = Tx sao cho trung bình bình phương lỗi ( MSE ) là bé nhất . Phương pháp tìm T :	[[14, 17], [113, 116]]	[[84, 110]]	['PCA', 'MSE']	['trung bình bình phương lỗi']
877	Một thư viện học sâu , là một API bậc cao có thể sử dụng chung với các thư viện deep learning nổi tiếng như tensorflow , CNTK , theano . Keras có một số	[[30, 33], [121, 125]]	[]	['API', 'CNTK']	[]
878	the 2015 SIAM International Conference on Data Mining , pages 451 – 459 . SIAM , 2015. Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[147, 158]]	[]	['KSTN - CNTT']	[]
879	+ Áp dụng các tính chất của định thức ma trận det ( AB ) = det ( A ) det ( B ) , det ( diag (A ) ) =	[]	[]	[]	[]
880	_ alpha = 0.5 , KL - weight = 1 VCL • w/o Dropout : Không cần	[[16, 18], [32, 35], [38, 41]]	[]	['KL', 'VCL', 'w/o']	[]
881	K là × 1000 Split MNIST PMNIST	[[18, 23], [24, 30]]	[]	['MNIST', 'PMNIST']	[]
882	Chúng ta xét một số kí hiệu toán học cơ bản để có thể mô hình hóa các công thức này . Xét một đầu vào x , mạng context Encoder F , xét M̃ là một	[]	[]	[]	[]
883	Hình 2 . 1 a . Mạng ANN kết nối đầy đủ , Mạng BNN kết nối đầy đủ . Nguồn [ 8 ] .	[[20, 23], [46, 49]]	[]	['ANN', 'BNN']	[]
884	Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 20	[[60, 71]]	[]	['KSTN - CNTT']	[]
885	( b ) NDCG @ 10 Hình 18 : So sánh các mô hình trên bộ Recobell khi k thay đổi	[[6, 10]]	[]	['NDCG']	[]
886	chiều và 1 kênh . F k ∈ R3 ×3 ×1 là bộ lọc thứ k với kích cỡ 3 × 3 . Mỗi phần có cỡ 3 × 3 của	[]	[]	[]	[]
887	ta có thể huấn luyện ra hai mạng F1 và F2 thỏa mãn hai hàm mất mát tương ứng : F1 ∗ = arg min l1 ( F1 ( X1 ) , X2 )	[]	[]	[]	[]
888	Đầu tiên , nhúng các từ vào các vec-tơ thông qua một ma trận nhúng 𝑊𝑒 , 𝑥𝑖𝑗 = 𝑊𝑒 𝑤𝑖𝑗 . Tiếp theo , sử dụng GRU 2 chiều để xây dựng chuỗi mã hóa của các từ bằng cách tổng hợp thông tin từ cả hai phía của từ , từ đó kết hợp thông tin theo ngữ cảnh trong chuỗi	[[107, 110]]	[]	['GRU']	[]
889	chuẩn hóa thõa mãn điều kiện 26 . Cuối cùng ta sử dụng thêm một hằng số C C. w	[]	[]	[]	[]
890	thấy được rõ rệt ưu điểm ALV . Các kết quả cho thấy ALV giúp các mô hình gốc đạt được hiệu năng cao trên hầu hết tất cả các tác vụ trong cả ba phương pháp	[[25, 28], [52, 55]]	[]	['ALV', 'ALV']	[]
891	translation - NMT ) và các vấn đề cơ bản của máy dịch dựa trên mạng nơ-ron . Mô hình ngôn ngữ là một khái niệm cơ bản trong xử lý ngôn ngữ tự nhiên , với mô hình ngôn ngữ ta	[[14, 17]]	[]	['NMT']	[]
892	NTD đã có bài đăng tuyển Các bước cơ 1 - NTD chọn vào một trong những mục : Việc	[[0, 3], [41, 44]]	[]	['NTD', 'NTD']	[]
893	Chương 2 : Trình bày về cơ sở lý thuyết được sử dụng làm nền tảng để xây dựng mô hình học HAN cho bài toán phân loại cảm xúc văn bản tiếng Việt . Chương 3 : Mô tả về kiến trúc , phương thức triển khai mô hình học , cách thức các	[[90, 93]]	[]	['HAN']	[]
894	dữ liệu . Đó là áp dụng thêm mô hình ngôn ngữ mới hiện nay đó là Bidirectional Encoder Representations from Transformers ( BERT ) , mô hình đem lại cải thiện chất lượng cho	[[123, 127]]	[]	['BERT']	[]
895	1 , nếu pi < thresh I [ pi < thresh ] =	[]	[]	[]	[]
896	Phần 3 : Deep Matrix Factorization ( DMF ) . Phần 4 : Thực nghiệm và đánh giá . Phần 5 : Kết luận và định hướng phát triển .	[[37, 40]]	[]	['DMF']	[]
897	‖ ) PT 3.12	[[4, 6]]	[]	['PT']	[]
898	NAML trên tất cả các độ đo . Ngoài ra , khi so với mô hình LSTUR , các độ đo cho kết quả tương đương .	[[0, 4], [59, 64]]	[]	['NAML', 'LSTUR']	[]
899	Tác vụ chính là tác vụ dự đoán ra quan hệ của hai câu cho bài toán IDRR . Tác vụ còn lại là một tác vụ sử dụng thêm các bộ dữ liệu từ bên	[[67, 71]]	[]	['IDRR']	[]
900	học được những biểu diễn ẩn có ý nghĩa và có những kết quả tốt hơn hẳn so với AE truyền thống , những phương pháp này vẫn tồn tại một số nhược điểm nhất định . Thứ nhất , mạng nơ-ron trong mô hình được trình bày ở bài [ Pat +16 ] được huấn	[[78, 80]]	[]	['AE']	[]
901	i=2 j=1 trong đó D là tổng số văn bản , D ( wik ) là số văn bản chưa từ wik , D ( wik , wjk ) là số văn bản chứa cả hai từ ( wik , wjk ) và 10−2 là đại lượng tránh zero cho hàm log .	[]	[]	[]	[]
902	ngắn và thưa ) , dẫn đến việc học những thông tin này khiến mô hình dễ bị Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 61	[[134, 145]]	[]	['KSTN - CNTT']	[]
903	N là số lượng bài trong lịch sử đọc của User . Cuối cùng , vector biểu diễn của User được	[]	[]	[]	[]
904	Tuy nhiên AGS - CL và UCL vẫn cần lưu lại bộ trọng số của mô hình cho tới tác vụ phía trước . Một	[[10, 18], [22, 25]]	[]	['AGS - CL', 'UCL']	[]
905	Gọi C , R tương ứng là tập các từ nối và tập các quan hệ . Với mỗi từ nối c i ∈ C , gọi Ri là tập con	[]	[]	[]	[]
906	với các thách thức đến từ bài toán khai phá dòng dữ liệu , cụ thể là cơ chế cân bằng các thông tin cũ và thông tin mới , khả năng thích ứng với sự thay đổi bất Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[220, 231]]	[]	['KSTN - CNTT']	[]
907	Do ta muốn dự đoán X2 từ X1 nên có thể xây dựng một hàm mất mát đơn giản như sau : l ( F ( X1 ) , X2 ) =	[]	[]	[]	[]
908	Sửa các bài đã đăng tuyển NTD NTD chọn vào 1 trong các mục Việc làm	[[26, 29], [30, 33]]	[]	['NTD', 'NTD']	[]
909	𝐿 = ∑ ( 𝛾̅𝑑𝑣 − 𝜃𝑑𝑘 ∗ 𝛽𝑘𝑣 𝑣 Trong đó , các giá trị khởi tạo của 𝜃 ở bước đầu tiên được khởi tạo ngẫu nhiên .	[]	[]	[]	[]
910	0.70 Tác v 7 VBD - CL	[[13, 21]]	[]	['VBD - CL']	[]
911	Để hiểu hình ảnh tài liệu , nhiều tác vụ yêu cầu mô hình tạo ra biểu diễn cấp tài liệu chất lượng cao . Tác giả đã sử dụng Multi-label Document Classification ( MDC )	[[161, 164]]	[]	['MDC']	[]
912	CIFAR100. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 . 5 Sự thay đổi của độ chính xác của các tác vụ trong quá trình học Split CIFAR10 - 100. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .	[[0, 8], [152, 165]]	[]	['CIFAR100', 'CIFAR10 - 100']	[]
913	các đặc tính không tương đồng với dữ liệu đến vào thời điểm sau , thậm Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 13	[]	[]	[]	[]
914	cứu SSL . Các phương pháp được trình bày sẽ gần như theo trình tự thời gian mà các bài báo tương ứng được công bố , với mục tiêu giúp cho người đọc có một cái	[[4, 7]]	[]	['SSL']	[]
915	Code của SimCLR gốc được tham khảo tại link github . Trong phần triển khai các ý tưởng của LCL , các tham số và phép DA sẽ được dùng giống với các	[[9, 15], [91, 94], [117, 119]]	[]	['SimCLR', 'LCL', 'DA']	[]
916	khác ( qua quá trình nguyên phân ) và từ thế hệ này sang thế hệ khác ( thông qua quá trình giảm phân và thụ tinh ) . Gene là một đoạn nằm dọc theo phân tử DNA , mang thông tin di truyền nhất định .	[[155, 158]]	[]	['DNA']	[]
917	= 𝑀 ∑𝑗 =1 𝑒𝑥𝑝 ( 𝑎𝑗𝑤 ) Trong đó , 𝑉𝑤 và 𝑣𝑤 là các tham số mô hình , và 𝒒𝑇𝑤 là vector truy vấn .	[]	[]	[]	[]
918	32 Bảng 2 . 2 : Kết quả huấn luyện của mô hình Sequence-to-sequence sử dụng LSTM , trong quá trình decode sử dụng beam search với kích thước 6 .	[[76, 80]]	[]	['LSTM']	[]
919	* Precision * Recall / ( Precision +Recall ) . F1 nằm trong nửa khoảng ( 0,1 ] . 13	[]	[]	[]	[]
920	phương pháp cực đại hóa kì vọng ( Expectation Maximization - EM ) . Có thể tóm tắt thuật toán như sau :	[[61, 63]]	[[0, 31]]	['EM']	['phương pháp cực đại hóa kì vọng']
921	được khả năng ghi nhớ tốt trên hầu hết các tác vụ . Tuy nhiên ALV vẫn chưa tốt hơn việc áp dụng Dropout trong trường hợp của VCL , nhưng cũng đạt được hiệu	[[62, 65], [125, 128]]	[]	['ALV', 'VCL']	[]
922	làm đồ án tốt nghiệp . Sinh viên : Nguyễn Trọng Nhật , 20143316 , Lớp KSTN - CNTT K59 Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[70, 81], [149, 160]]	[]	['KSTN - CNTT', 'KSTN - CNTT']	[]
923	với ndj là số lần xuất hiện từ wj của từ điển trong văn bản d . Chú ý rằng , cũng giống như LDA , biến đổi trên giả thiết các chiều của dữ liệu độc lập với nhau	[[92, 95]]	[]	['LDA']	[]
924	Recurrent Neural Network RNN Mạng nơ-ron hồi quy	[[25, 28]]	[[29, 48]]	['RNN']	['Mạng nơ-ron hồi quy']
925	Đồ án tốt nghiệp SGD Stochastic Gradient Descent	[[17, 20]]	[]	['SGD']	[]
926	learning ) [ 19 ] . Ở cả ba kịch bản , mô hình được học với tất cả dữ liệu của một tác vụ tại mỗi thời điểm ( có thể được coi như học liên tục ngoại tuyến - Offline learning ) .	[]	[]	[]	[]
927	công trong tương lai và có thể sẽ mở ra một cách nhìn mới , một hướng đi mới cho CL . Nếu suy nghĩ kĩ hơn , ta có thể hi vọng rằng trong tương lai sẽ có nhiều cách	[[81, 83]]	[]	['CL']	[]
928	Để ước lượng tính toán đại lượng khả năng xảy ra 𝐸𝑞𝑡 (θ) log 𝑝( 𝑦𝑡𝑖 | 𝑥𝑡𝑖 , θ ) , VCL sử dụng kĩ thuật xấp xỉ Monte Carlo và kĩ thuật đổi biến . Khi đó	[[82, 85]]	[]	['VCL']	[]
929	Xóa các công việc đã lưu 46 hình 29 : ACD ca sử dụng xóa các công việc đã lưu	[[38, 41]]	[]	['ACD']	[]
930	− 1 𝐴𝑇 𝑦 ( 2 ) trong đó A là ma trận của dữ liệu , 𝐴 ∈ ℝ𝑀 ∗ ( 𝑁+1 ) , cột thứ i của ma trận A là 𝐴𝑖 =	[]	[]	[]	[]
931	Các tham số C được sử dụng là 2, 1 , 0.5 và 0. 1 . Kết quả thu được : Khi so sánh việc nới lỏng cách lấy mẫu cho phép w âm và cách lấy w thuộc đoạn	[]	[]	[]	[]
932	mô hình đề xuất có kết quả gợi ý tốt hơn so với những mô hình khác trong các nghiên cứu gần đây . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[161, 172]]	[]	['KSTN - CNTT']	[]
933	2.2 Kết quả huấn luyện của hình Sequence-to-sequence sử dụng LSTM . . . . .	[[61, 65]]	[]	['LSTM']	[]
934	Tuy vậy , phần nghiên cứu chính của đồ án vẫn tập trung trọng tâm vào SSL . Do đó , phần	[[70, 73]]	[]	['SSL']	[]
935	Gọi { Y1 , Y2 , . . . , YR } là tập các ma trận tương tác cho cả R loại hành vi . Mỗi ma trận đều có kích thước M × N ,	[]	[]	[]	[]
936	tương tác ) . Ta ký hiệu tập các tương tác quan sát được là Y . Y − là tập dữ liệu	[]	[]	[]	[]
937	VŨ HỒNG PHÚC phuc.vh173305@sis.hust.edu.vn Ngành : Khoa học máy tính	[]	[]	[]	[]
938	KL Kullback - Leibler Divergence Phân kỳ Kullback - Leibler	[[0, 2]]	[[33, 59]]	['KL']	['Phân kỳ Kullback - Leibler']
939	các bài đăng phù hợp với nội dung . 2 - Các bài đăng sẽ được liệt kê dưới dạng danh sách các cột trong một bảng , NTD có thể kích	[[114, 117]]	[]	['NTD']	[]
940	vì mỗi trọng số nối tới một nơ-ron có độ lệch chuẩn σ khác nhau , UCL xét các trọng số đó có cùng một σ , và đó chính là độ không chắc chắn của nơ -ron : σil . UCL định nghĩa hai nguyên nhân dẫn tới việc quên tri thức cũ xảy ra trên một	[[66, 69], [160, 163]]	[]	['UCL', 'UCL']	[]
941	khi áp dụng ALV lần lượt là 74.23 % , 61.64 % và 73.13 % , cải thiện từ 2 – 3 % so với Dropout . Thêm vào đó để quan sát kĩ hơn khả năng của ALV trong việc hạn	[[12, 15], [141, 144]]	[]	['ALV', 'ALV']	[]
942	gợi ý đề xuất cũng cho kết quả tốt khi so sánh với các mô hình khác qua đánh giá trên các tập dữ liệu khác nhau . MỤC LỤC	[]	[]	[]	[]
943	ICT • Đồ án tốt nghiệp được thực hiện tại : Bộ môn Hệ thống Thông tin - Viện	[[0, 3]]	[]	['ICT']	[]
944	Cụ thể , mô hình có thể viết dưới dạng công thức như sau : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[122, 133]]	[]	['KSTN - CNTT']	[]
945	False Negative ( FN ) là số lượng những điểm dữ liệu thực tế có nhãn là positive nhưng được dự đoán là negative . False Positive ( FP ) là số lượng	[[17, 19], [131, 133]]	[]	['FN', 'FP']	[]
946	viết dưới dạng : θ ∗ = argmin L ( θ ) ( 1.1 )	[]	[]	[]	[]
947	Kiến trúc của mô hình ITE - item _ pcat được cho như Hình 11 . Có thể thấy thay đổi của mô hình ITE- item _ pcat so với mô hình ITE- onehot là thay vì	[[22, 39], [96, 112], [128, 139]]	[]	['ITE - item _ pcat', 'ITE- item _ pcat', 'ITE- onehot']	[]
948	. Với ba mô hình ITE - 2 , ITE - 3 và ITE - 4 , em tiến hành tune các siêu tham số lr [ 0.0005, 0.001, 0.01 ] , k [ 32 , 64 , 128 ] , batch-size [ 1024, 2048 ] và η [ 0.01, 0.05,	[[17, 24], [27, 34], [38, 45]]	[]	['ITE - 2', 'ITE - 3', 'ITE - 4']	[]
949	Thứ hai , một mạng AE truyền thống chính là ví dụ điển hình của việc tìm ra một biểu diễn ẩn ít chiều hơn . Mạng nơ-ron này được huấn luyện thông qua việc biểu	[[19, 21]]	[]	['AE']	[]
950	Một số ưu điểm của AJAX : • Cải thiện tốc độ của trang web , tăng hiệu suất và khả năng sử dụng .	[[19, 23]]	[]	['AJAX']	[]
951	hiệu và biểu diễn toán học cơ bản , tác giả chỉ ra rằng việc tối thiểu hóa hàm mất mát trong mạng AE truyền thống về mặt bản chất cũng chính là cực đại hóa MI giữa input X và biểu diễn ẩn Y , hay nói cách khác cũng tương đương với công thức	[[98, 100], [156, 158]]	[]	['AE', 'MI']	[]
952	Trước hết chúng ta biểu diễn dữ liệu quan sát được là tập văn bản dưới dạng một ma trận document - term DOC − T ERM [ D×V ] , trong đó D là số văn bản , V là kích thước từ điển và phần tử ( d , j ) là số lần xuất hiện từ thứ j của tập từ	[]	[]	[]	[]
953	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 3 TÓM TẮT NỘI DUNG ĐỒ ÁN	[[61, 70]]	[]	['CNTT – TT']	[]
954	minh họa cho điều đó . Ràng buộc phép chiếu bộ phân loại ( Classifier - Projection Regularization , CPR ) [ 4 ] là một ví dụ về công việc khai thác tính chất này để giải	[[100, 103]]	[[23, 56]]	['CPR']	['Ràng buộc phép chiếu bộ phân loại']
955	Hiệu chỉnh thành phần ( 𝒃 ) : Các ràng buộc cho các tham số quan trọng giúp tăng khả năng ghi nhớ của mô hình đã được UCL cải tiến mạnh mẽ , tuy vậy sẽ làm	[[118, 121]]	[]	['UCL']	[]
956	K-medoids cho kết quả tốt nhất khi xét cả hai yếu tố là số gene được chọn và hiệu năng . Nguyên nhân có thể do nó ít nhạy cảm với nhiễu hơn so với K-means và HAC nên	[[158, 161]]	[]	['HAC']	[]
957	"Tập Split CIFAR10 - 100 sau khi học xong , Hard - VBD - CL có kết quả nhỉnh hơn VBD - CL có thể do có những tác vụ "" nhạy cảm "" nên VBD - CL bị quên nhiều tác vụ đó , còn Hard - VBD - CL giữ được độ chính"	[[10, 23], [80, 88], [132, 140], [43, 58], [171, 186]]	[]	['CIFAR10 - 100', 'VBD - CL', 'VBD - CL', 'Hard - VBD - CL', 'Hard - VBD - CL']	[]
958	chia ra gồm lần lượt 50000 và 10000 ảnh tương ứng với mỗi lớp là 5000 và 1000 . Hình 4 . 2 mô tả dữ liệu của CIFAR10 . CIFAR100 cũng tương tự như CIFAR10 , ngoại trừ bộ dữ liệu này bao gồm 100	[[109, 116], [119, 127], [146, 153]]	[]	['CIFAR10', 'CIFAR100', 'CIFAR10']	[]
959	2 LỜI CẢM ƠN Lời đầu tiên , em xin được gửi lời cảm ơn chân thành đến các thầy cô trường Đại học	[]	[]	[]	[]
960	kj đều là các hàm lõm , trong khi hàm log-sum- exp là một hàm lồi khá quen thuộc . Do đó , F ( β tk ) là hàm lõm đối với β tk , nên có thể tìm cực đại bằng việc sử dụng	[]	[]	[]	[]
961	Thay ngược γ ∗ trở lại 2.16 , ta thu được hàm mục tiêu cuối cùng cho VBD : maxw , µ , σ	[[69, 72]]	[]	['VBD']	[]
962	Và như vậy , RNN ra đời với ý tưởng chính lá sử dụng một bộ nhớ để lưu lại thông tin từ những bước tính toán xử lý trước để dựa vào nó có thể đưa ra dự đoán chính xác nhất cho bước dự đoán hiện tại .	[[13, 16]]	[]	['RNN']	[]
963	Cực đại hoá hàm ELBO này tương đương với cực tiểu khoảng cách KL ( q ( z , Θ ) | |p( z , Θ |x) ) . PVB cũng đi tối ưu một hàm mục tiêu tương tự , gọi	[[16, 20], [99, 102]]	[]	['ELBO', 'PVB']	[]
964	33 CHƯƠNG 4 . THỬ NGHIỆM VÀ ĐÁNH GIÁ	[]	[]	[]	[]
965	Danh mục viết tắt • SSL : Self-supervised learning • DAE : Denoising Auto Encoder	[[20, 23], [53, 56]]	[]	['SSL', 'DAE']	[]
966	mạng nơ-ron từ pretext task có tốt không . Một trong những cách kinh điển và cũng được xem là tiêu chuẩn để đánh giá chung cho hầu hết các phương pháp SSL đó chính là đưa mạng nơ-ron đã học được này	[[151, 154]]	[]	['SSL']	[]
967	Với mô hình LDA , đẳng thức trên có thể viết dưới dạng tường minh : N XX	[[12, 15]]	[]	['LDA']	[]
968	Sinh viên thực hiện : Trần Thị Hồng Lớp CNTT 2.03 – K59	[[40, 44]]	[]	['CNTT']	[]
969	document- term ) để phép truncated SVD có độ chính xác cao , hơn nữa quá trình Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 18	[[35, 38], [139, 150]]	[]	['SVD', 'KSTN - CNTT']	[]
970	Đầu ra : Tham số biến phân toàn cục λ Khởi tạo ngẫu nhiên λ = λ0 for t = 1,2 , ... , T , ... do	[]	[]	[]	[]
971	PHIẾU GIAO NHIỆM VỤ ĐỒ ÁN TỐT NGHIỆP ( ĐATN ) 1 . Thông tin về sinh viên : •	[[39, 43]]	[[20, 36]]	['ĐATN']	['ĐỒ ÁN TỐT NGHIỆP']
972	( 48 ) Chú ý rằng , với mỗi khả năng có thể của π t , thì việc sử dụng Dropout tương đương với việc lấy mẫu một mô hình đơn ( single learner ) từ 2K×V mô hình có	[]	[]	[]	[]
973	log ( 𝑝𝑖 ) Trong đó , S là tập các mẫu Positive trong tập training . Tương tự mô hình NRMS , mô hình NAML cũng được thử nghiệm trên tập dữ	[[86, 90], [101, 105]]	[]	['NRMS', 'NAML']	[]
974	𝑖=1 log 𝑝 ( 𝑦𝑡 | 𝑥𝑡 ) . Sử dụng phương pháp MFVI với hai phân phối biến phân cho tham số toàn cục và cục bộ lần lượt là 𝑞𝑡 ( 𝜃 ) , 𝑞𝑡 ( 𝑠) ta có :	[[44, 48]]	[]	['MFVI']	[]
975	18 / 4 và 19 / 4 , dễ thấy rằng hiệu năng của hai mô hình ITE - 3 và ITE - 4 cao hơn Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 49	[[58, 65], [69, 76], [148, 159]]	[]	['ITE - 3', 'ITE - 4', 'KSTN - CNTT']	[]
976	định hướng nghiên cứu trong tương lai . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 16	[[100, 111]]	[]	['KSTN - CNTT']	[]
977	Hình 4 . 6 : Cải tiến trên JacReg và FrobReg ( 3FC ) Trên 3FC , với JacReg và A_JacReg , A_JacReg chống quá khớp tốt hơn . So với	[[47, 50], [58, 61], [27, 33], [37, 44], [68, 74], [78, 86], [89, 97]]	[]	['3FC', '3FC', 'JacReg', 'FrobReg', 'JacReg', 'A_JacReg', 'A_JacReg']	[]
978	- • Tập trung xử lý theo hướng đối tượng , phù hợp sử dụng trong mô hình MVC và cung cấp nhiều API truy vấn cơ sở dữ liệu .	[[73, 76], [95, 98]]	[]	['MVC', 'API']	[]
979	Ngoài ra , ta cũng có thể thấy mô hình ITE - 4 có thể đạt được trạng thái tối ưu hơn so với mô hình ITE - 3 sau chừng hơn 34 vòng lặp . Những kết quả này phần	[[39, 46], [100, 107]]	[]	['ITE - 4', 'ITE - 3']	[]
980	𝐵 (𝑙) = ( 𝐴(𝑙) ⨀ 𝑠 (𝑙) ) 𝜃 (𝑙) trong đó : 𝑏𝑚ℎ = ∑ ( 𝑙 )	[]	[]	[]	[]
981	Tổng quan : Locally Linear Embedding ( nguồn [ SR03 ] ) là một thuật toán giảm chiều dữ liệu phi tuyến thuộc nhóm Manifold Learning . Ý tưởng chính của thuật	[]	[]	[]	[]
982	Và tiến hành test trên tập này để đưa ra thứ hạng theo dự đoán . Tiếp theo đó là sử dụng 2 độ đo là Hit Ratio ( HR ) và Normalized Discounted	[[112, 114]]	[]	['HR']	[]
983	Hình 7 : Ví dụ về mạng CNN Cách chọn tham số cho CNN Số lượng layer được chọn thường là 3 hoặc 4 .	[[23, 26], [49, 52]]	[]	['CNN', 'CNN']	[]
984	. Các kết quả nêu trong ĐATN là trung thực , không phải là sao chép toàn văn của bất kỳ công trình nào khác .	[[24, 28]]	[]	['ĐATN']	[]
985	• L̂HP P là một lower bound của LH P P : L̂HP P ( λt , φt , wt | D1 : t , η ) ≤ LHP P ( λt , φt , wt | D1 : t , η )	[]	[]	[]	[]
986	KHOA HỌC MÁY TÍNH Đề tài : Dịch máy sử dụng phương pháp học đối ngẫu Sinh viên thực hiện :	[]	[]	[]	[]
987	3.2.4 Học biểu diễn ẩn thông qua giải bài toán xếp hình Tổ hợp ba bài ở trên liên quan đến các pretext task tạo một mạng AE nhằm khôi	[[121, 123]]	[]	['AE']	[]
988	Parkinson [ 28 ] [ 29 ] , một số loại ung thư [ 30 ] [ 31 ] , tự kỉ [ 32 ] … Nghiên cứu tương quan toàn bộ hệ gene ( Genome-wide association study – GWAS ) và nghiên cứu tương quan toàn bộ hệ phiên mã ( Transcriptome - wide association study –	[[149, 153]]	[[77, 114]]	['GWAS']	['Nghiên cứu tương quan toàn bộ hệ gene']
989	CIFAR100 và Split CIFAR10 - 100.	[[0, 8], [18, 31]]	[]	['CIFAR100', 'CIFAR10 - 100']	[]
990	của văn bản đầu vào . Từ đây , ta có thể chia mô hình thành 4 công đoạn chính : Tiền xử lý dữ liệu , vec-tơ hóa dữ liệu , Mô hình HAN , Module phân loại .	[[130, 133]]	[]	['HAN']	[]
991	7.1 TB Bảng 3 : Cấu hình phần cứng sử dụng Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[4, 6], [106, 117]]	[]	['TB', 'KSTN - CNTT']	[]
992	hoá hàm lower bound của hàm log complete- data log p ( w | α, η ) như sau : Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 23	[[136, 147]]	[]	['KSTN - CNTT']	[]
993	4 . 1 Dữ liệu . Penn Discourse TreeBank ( PDTB ) : Trong bài toán này , ta sử dụng bộ dữ liệu PDTB	[[42, 46], [94, 98]]	[]	['PDTB', 'PDTB']	[]
994	so sánh với phương pháp cũ sử dụng bởi SGL … … … … … … … … … … … … … … … 39	[[39, 42]]	[]	['SGL']	[]
995	với item i trong hành vi mục tiêu . Từ đó , nhóm tác giả đề xuất mô hình NMTR với kiến trúc mạng nơ-ron ( Hình 9 ) , có khả năng mô tả tính thứ tự của các hành vi .	[[73, 77]]	[]	['NMTR']	[]
996	LSTM là một mô hình mạng nơ-ron hồi quy có khả năng học các thông tin phụ thuộc dài hạn ( long-term dependencies ) . LSTM xuất	[[0, 4], [117, 121]]	[]	['LSTM', 'LSTM']	[]
997	Hà nội ngày … tháng … năm 2021 Giảng viên hướng dẫn PGS. TS. Thân Quang Khoát	[[52, 56], [57, 60]]	[]	['PGS.', 'TS.']	[]
998	Câu 1 : Shares of UAL , the parent of United Airlines , were extremely active all day Friday , react ing to news and rumor s about the proposed $ 6.79 billion buy - out of the airline by an employee - management group .	[]	[]	[]	[]
999	FID giữa hai tập S1 và S2 là khoảng cách Frechet giữa hai tập I ( S1 ) = { I ( x 11 ) , I ( x 21 ) , ... , I ( x n1 ) } và I ( S2 ) = { I ( x 12 ) , I ( x 22 ) , ... , I ( x n2 ) } . Gọi ( m1 , C1 ) , ( m2 , C2 ) lần lượt là trung bình và ma trận	[[0, 3]]	[[29, 48]]	['FID']	['khoảng cách Frechet']
1000	lên 48.03 % . Tương tự , UCL cũng quan sát một sự cải thiện nhẹ về độ chính xác khi đạt được 64.84 % và cao hơn Dropout 0.52 % .	[[25, 28]]	[]	['UCL']	[]
1001	chuyển p ( Θt | Θt−1 ) sao cho khoảng cách KL giữa p ( Θt | D1 : t−1 ) và pδ ( Θt | D1 : t−1 ) ) là không vượt quá κ . Tuy nhiên đây chỉ mới là một điều kiện ràng buộc , chúng	[[43, 45]]	[]	['KL']	[]
1002	và giữa các hệ số trong cùng nhóm . Ý tưởng của tác giả khá đơn giản : đại lượng hiệu chỉnh sẽ là tổng của chuẩn L2 và chuẩn L1 :	[]	[]	[]	[]
1003	dụng LSTM cho kết quả khá tốt ngay cả khi không thực hiện quá trình tinh chỉnh phức tạp . Kết quả huấn luyện của các mô hình sử dụng ConvS2S và Transformer được miêu tả trong Bảng 2 . 5 và 2 .6 .	[[5, 9], [133, 140]]	[]	['LSTM', 'ConvS2S']	[]
1004	( 𝑙 ) Với 𝐿 ̅ là hàm mất mát không phụ thuộc vào 𝛼𝑡 và 𝛼𝑡 là phương sai của phân ( 𝑙 )	[]	[]	[]	[]
1005	Learning ( LCL ) . Hình vẽ dưới đây thể hiện rõ sự khác biệt ở không gian biểu diễn ẩn giữa LCL và CL :	[[11, 14], [92, 95], [99, 101]]	[]	['LCL', 'LCL', 'CL']	[]
1006	PHỤ LỤC ............................................................................................................ 46 DANH MỤC HÌNH VẼ	[]	[]	[]	[]
1007	Khi đó đầu ra của lớp tích chập sẽ là X̃ với [ X̃ ] kij = ReLU ( O kij ) , ∀i ∈ [ d1 − w + 1 ] , j ∈ [ d2 − w + 1 ] , k ∈ [ dˆ3 ]	[]	[]	[]	[]
1008	phân phối xác suất V chiều của các từ trong tập từ điển này , trong đó những từ có xác suất càng cao sẽ phản ảnh nội dung càng gần với chủ đề đó . Và rõ ràng	[]	[]	[]	[]
1009	Sau khi đã có được thông tin về cấu trúc cục bộ của không gian các view , việc tiếp theo mà LCL cần làm đó là đưa thông tin đó vào mạng để mạng có thể học được . Để làm được điều đó , cặp positive mà LCL xét đến là một biểu diễn ẩn của một	[[92, 95], [200, 203]]	[]	['LCL', 'LCL']	[]
1010	của VBD - CL khi học tác vụ mới . 34 5.3	[[4, 12]]	[]	['VBD - CL']	[]
1011	THỬ NGHIỆM VÀ ĐÁNH GIÁ Hình 16 : Mô hình ITE - 4 Điểm mạnh của mô hình :	[[41, 48]]	[]	['ITE - 4']	[]
1012	có tương tác , và 0 ngược lại . Vì mô hình chỉ tập trung vào những dữ liệu quan sát được , ta sử dụng R∗ để biểu thị tập các cặp người dùng - bộ phim quan sát được trong R , và V ∗ để biểu	[]	[]	[]	[]
1013	dung bài báo trực tiếp từ trang web của Microsoft . 4 . 2 . 2 Tập dữ liệu Pega ( VC corp ) Để thử nghiệm và đánh giá các mô hình đối với dữ liệu thực tế của Việt Nam , đồ	[]	[]	[]	[]
1014	Điểm mạnh : • Kết hợp hai mô hình GMF và MLP , mang tính tổng quát hóa cao • Nhờ tính chất tuyến tính của mô hình GMF và tính chất phi tuyến của mô	[[34, 37], [41, 44], [114, 117]]	[]	['GMF', 'MLP', 'GMF']	[]
1015	phần phối population FD như trên và giá trị n sao cho FD ( n ) lớn nhất chính là quan sát được sinh ra . Quay trở lại định nghĩa cho population posterior , hay phân bố hậu nghiệm	[]	[]	[]	[]
1016	Em cũng xin gửi lời cảm ơn tới anh Nguyễn Hữu Thiện – Assistant Professor – University of Oregon , TS. Thân Quang Khoát đã hướng dẫn và giúp đỡ em , cung cấp cho em kiến thức cũng như thiết bị tính toán hiệu năng cao trong suốt quá trình em làm	[[99, 102]]	[]	['TS.']	[]
1017	Giả sử sau khi học mô hình trên tập dữ liệu training Dtrain , chúng ta thu được tham số mô hình là Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[159, 170]]	[]	['KSTN - CNTT']	[]
1018	ℎ𝑖𝑡 tổng hợp thông tin của cả câu lấy trung tâm là từ 𝑤𝑖𝑡 . Ở đây ta sử dụng trực tiếp ma trận nhúng mức từ . Cuối cùng , ta thu được chuỗi mã hóa của từ ℎ𝑖𝑡 , ℎ𝑖𝑡 sẽ trở thành đầu vào của tầng tiếp	[]	[]	[]	[]
1019	Trong [ 29 ] , các tác giả đã đề xuất phương pháp tìm sự tương đồng giữa các ký tự OOV trong câu nguồn và câu đích trong quá trình hậu xử lý sau khi dịch . Một phương pháp phổ	[[83, 86]]	[]	['OOV']	[]
1020	= log 𝑝 ( 𝐷 | 𝜃 ) + log 𝑝 ( 𝜃 ) − log 𝑝 ( 𝐷 ) Đối với trường hợp hai tác vụ A và B tương ứng với bộ dữ liệu 𝐷𝐴 và 𝐷𝐵 , ta viết lại công thức PT 2.8 :	[[141, 143]]	[]	['PT']	[]
1021	Trong trường hợp của mô hình ITE - item _ pcat , sự khác nhau là ở vec-tơ đầu vào của item : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[156, 167], [29, 46]]	[]	['KSTN - CNTT', 'ITE - item _ pcat']	[]
1022	Ở đây các loại bệnh có hệ số di truyền ( heritability ) cao được sử dụng để đánh giá . Hệ số di truyền là một chỉ số thống kê được sử dụng để ước tính mức độ biến đổi của tính trạng kiểu hình trong một quần thể do sự biến đổi di truyền của các cá thể trong quần thể	[]	[]	[]	[]
1023	Khi đó chúng tôi đạt được hàm mục tiêu đối với β tk như sau : F ( β tk ) = log p( β tk | β kt−1 ) + G ( β tk ) V	[]	[]	[]	[]
1024	hay : λt = λt−1 + ( λ̂t − λ0 ) = λt−1 + λ̃t = λ0 + ti=1 ( λ̂i − λ0 ) với λ̃t được xem là thông tin học được từ dữ liệu ở minibatch thứ t . Phương pháp học SVB cho LDA được trình bày trong thuật toán 2 .	[[155, 158], [163, 166]]	[]	['SVB', 'LDA']	[]
1025	Ngoài max - pooling còn có các loại average- pooling và L2 pooling . Cuối cùng , ta đặt tất cả các lớp với nhau tạo thành CNN với đầu ra gồm các nơ -ron có số lượng tùy	[[122, 125]]	[]	['CNN']	[]
1026	nghiên cứu vẫn đang triển khai song song việc đánh giá trên các phương pháp như MOCO và BYOL , tuy nhiên , kết quả không hoàn toàn tốt như trên SimCLR , cần phải có thêm thời gian để điều chỉnh các tham số một cách phù hợp cũng như quan	[[80, 84], [88, 92], [144, 150]]	[]	['MOCO', 'BYOL', 'SimCLR']	[]
1027	Mô hình được phát triển từ mô hình MF , tập trung vào việc dự đoán chính xác ratings người dùng dành cho các item , bằng việc sử dụng thêm những tập dữ liệu hành vi tiềm ẩn ( item trong dữ liệu mà phương pháp này sử dụng để	[[35, 37]]	[]	['MF']	[]
1028	Một số dạng của mô hình NCF tổng quát là GMF , MLP , NeuMF , đây cũng là các mô hình cơ sở cho mô hình ITE mà em áp dụng để cải tiến mô hình của mình . Generalized Matrix Factorization ( GMF )	[[24, 27], [41, 44], [47, 50], [53, 58], [103, 106], [187, 190]]	[]	['NCF', 'GMF', 'MLP', 'NeuMF', 'ITE', 'GMF']	[]
1029	Kết quả của biểu diễn ẩn khi thêm vào lớp cuối ( Nguồn [ Che+20 ] ) So sánh kiến trúc MOCO với các kĩ thuật trước đây ( Nguồn [ He+20 ] )	[[86, 90]]	[]	['MOCO']	[]
1030	Có một chú ý khi xây dựng ELBO ở hai trường hợp này là phải đảm bảo giá trị W tối ưu bằng huấn luyện 14	[[26, 30]]	[]	['ELBO']	[]
1031	19 trong một tập từ điển có kích thước V . • Mỗi văn bản được kí hiệu là d , bao gồm Nd từ , được biểu diễn dưới dạng	[]	[]	[]	[]
1032	kết quả tốt nhất trên bộ dữ liệu Split MNIST và giúp EWC đạt được độ chính xác trung bình cao nhất trên cả ba phương pháp . Bảng 4 . 4 tổng hợp lại kết quả của các thử nghiệm sau quá trình học trên 5 tác	[[39, 44], [53, 56]]	[]	['MNIST', 'EWC']	[]
1033	trong đó : p ( β|η ) = Dir ( β|η ) ∝ C Y	[]	[]	[]	[]
1034	} cho SVB - PP ; population size α ∈ { 103 , 104 , 105 , 106 , 5.103 , 5.104 , 5.105 , 5.106 } , dimming factor κ ∈ { 0.5, 0.6 , 0.7 , 0.8 , 0.9 } và τ0 = 1 cho PVB .	[[6, 14], [161, 164]]	[]	['SVB - PP', 'PVB']	[]
1035	làm việc và nghiên cứu tại đây . Đặc biệt em xin cảm ơn GS. TS. Thân Quang Khoát và anh Phạm Văn Hoàng – hiện đang là sinh viên K61 , thầy và anh là những	[[56, 59], [60, 63]]	[]	['GS.', 'TS.']	[]
1036	 tổng các giá trị rời rạc của biến đó .  phân bằng q ( Z )	[]	[]	[]	[]
1037	và tầng embedding . Trong trường hợp vec-tơ đầu vào là one-hot , P và Q tương ứng chính là các ma trận thuộc tính ẩn của người dùng và item , với	[]	[]	[]	[]
1038	tham số của phân phối biến phân ứng với β Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 10	[[102, 113]]	[]	['KSTN - CNTT']	[]
1039	Và phân phối này sẽ được sử dụng làm phân phối tiên nghiệm cho tham số Θ ở minibatch thứ t . Để ý rằng một điều thú vị là nếu tại minibatch thứ t , phương pháp SVB sử	[[160, 163]]	[]	['SVB']	[]
1040	ta tổng quát cho trường hợp task thứ T : p ( θ | D1:T ) ≈ qT ( θ ) = proj ( q _ T - 1 ( θ ) p ( DT | θ ) ) Variational Continual Learning ( VCL ) cố gắng đi xây dựng một phép chiếu được định nghĩa thông qua bài toán tối thiểu hóa KL - Divergence Reverse trên không gian phân phối <	[[140, 143], [230, 232]]	[]	['VCL', 'KL']	[]
1041	NDCG @ 10 Hình 22 : So sánh các mô hình trên bộ Retailrocket qua từng vòng lặp với k = 16 Hình 21 , 22 tương ứng là kết quả của các mô hình trên bộ dữ liệu Retailrocket qua từng vòng lặp với các giá trị k = 8, 16 .	[[0, 4]]	[]	['NDCG']	[]
1042	Chú ý rằng , mặc dù theo 3 . 2 . 2 ta lựa chọn 𝛼 (𝑙 ) là một véc-tơ D chiều , tuy nhiên để cho việc viết công thức được mạch lạc và tường minh ở đây ta giả sử 𝛼 ( 𝑙 ) ( 𝑙 )	[]	[]	[]	[]
1043	nghiệm ta sẽ đặt thêm một hệ số cân bằng 𝛽 trước thành phần KL của tham số cục bộ , để cân bằng giữa việc tham số cục bộ 𝑠 sẽ học khớp với dữ liệu và bảo đảm tri thức tiên nghiệm cho tham số này .	[]	[]	[]	[]
1044	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 50	[[267, 276]]	[]	['CNTT – TT']	[]
1045	* Bước cập nhật tham P số toàn PNdcục *	[]	[]	[]	[]
1046	Kết quả của mô hình ITE - onehot và mô hình NMTR vẫn xấp xỉ nhau , và cao hơn so với mô hình MTMF .	[[20, 32], [44, 48], [93, 97]]	[]	['ITE - onehot', 'NMTR', 'MTMF']	[]
1047	thường được gọi đầy đủ là hàm Evidence Lower BOund ( ELBO ) . Rõ ràng để tìm được phân phối q xấp xỉ tốt nhất cho phân phối hậu nghiệm p , chúng ta mong muốn giá trị KL ( q|| p ) càng gần 0 càng tốt , tức là cần tối	[[53, 57]]	[]	['ELBO']	[]
1048	* — — — — — — — ĐỒ ÁN TỐT NGHIỆP	[]	[]	[]	[]
1049	trang đăng bài tuyển dụng . Từ đây , NTD có thể sửa nội dung bài đăng của mình .	[[37, 40]]	[]	['NTD']	[]
1050	Ký hiệu w1∗ , w2∗ là nghiệm tối ưu sau khi huấn luyện lần lượt hai tác vụ , L1 , L2 là hàm mất mát của hai tác vụ . Độ	[]	[]	[]	[]
1051	collaborative filtering layer s ) , với đầu ra cuối cùng là điểm số dự đoán . Mỗi tầng trong NCF có thể tùy biến để phát hiện các cấu trúc ẩn quan trọng trong tương	[[93, 96]]	[]	['NCF']	[]
1052	thuật LRT : 21 ( 𝑙 )	[[6, 9]]	[]	['LRT']	[]
1053	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 47	[[267, 276]]	[]	['CNTT – TT']	[]
1054	Kết hợp với công thức ( 14 ) ta có : q ( Θ | λt ) ≈ q ( Θ | λ̂t )	[]	[]	[]	[]
1055	EWC đề xuất xấp xỉ ma trận hiệp phương sai chéo Σ bằng ma trận Fisher Information chéo F : 1	[[0, 3]]	[]	['EWC']	[]
1056	thế Kết quả NTD xem được thông tin hồ sơ về 1 nhân viên	[[12, 15]]	[]	['NTD']	[]
1057	Sinh ra từ wdn ∼ M ultinomial ( β̃zdn ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 49	[[100, 111]]	[]	['KSTN - CNTT']	[]
1058	NDCG @ 10 Hình 28 : Kết quả các mô hình ITE ngày 19 / 4 Nhìn vào kết quả của ba mô hình ITE - 2 , ITE - 3 và ITE - 4 trong cả hai ngày	[[0, 4], [40, 43], [88, 95], [98, 105], [109, 116]]	[]	['NDCG', 'ITE', 'ITE - 2', 'ITE - 3', 'ITE - 4']	[]
1059	tham số γ . Phân phối Gauss kỳ vọng 0 và phân tách hoàn toàn ( fully factorized ) Q	[]	[]	[]	[]
1060	1 GIỚI THIỆU ĐỀ TÀI 1 Giới thiệu đề tài Dạo gần đây , các hệ thống thương mại điện tử ở Việt Nam đang nổi lên nhanh	[]	[]	[]	[]
1061	Mô hình được tối ưu sử dụng kĩ thuật stochastic gradient descent ( SGD ) . Mô hình Neural Multi-Task Recommendation ( NMTR )	[[67, 70], [118, 122]]	[]	['SGD', 'NMTR']	[]
1062	giúp tránh hiện tượng quên nhanh chóng ) sẽ được kết hợp lại để suy diễn cho dữ liệu tương ứng ở tác vụ này . Trong công việc này , phương pháp xấp xỉ VI sẽ được áp dụng lên phân phối	[[151, 153]]	[]	['VI']	[]
1063	với ảnh thật nhất có thể . Từ đó ta có hàm mất mát tổng hợp như sau : L = λrec Lrec + λadv Ladv	[]	[]	[]	[]
1064	dụng Dropout lại làm giảm hiệu năng của hai mô hình OVI là VCL và UCL . Điều này có thể giải thích là bởi vì bộ dữ liệu Split MNIST có phân phối dữ liệu khá tương đồng nhau , VCL và UCL vốn dĩ đã có tính chất không chắc chắn và việc áp	[[52, 55], [59, 62], [66, 69], [126, 131], [175, 178], [182, 185]]	[]	['OVI', 'VCL', 'UCL', 'MNIST', 'VCL', 'UCL']	[]
1065	Trong khuôn khổ ĐATN , em tiến hành triển khai mô hình HAN cho bài toán phân loại cảm xúc văn bản tiếng Việt đồng thời đánh giá mô hình này với những mô hình học khác cho tiếng Việt .	[[16, 20], [55, 58]]	[]	['ĐATN', 'HAN']	[]
1066	Sử dụng công thức ( 7 ) ta có thể thấy s23 ( 0.66 ) > s12 ( 0.5 ) > s13 ( 0.4 ) . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 16	[[145, 156]]	[]	['KSTN - CNTT']	[]
1067	HAT ở ba tác vụ đầu có độ chính xác cao hơn AGS - CL , nhưng sau đó do quên nhiều tri thức cũ và thậm chí ở những tác vụ sau không đạt độ chính xác 39	[[0, 3], [44, 52]]	[]	['HAT', 'AGS - CL']	[]
1068	ngẫu nhiên hỗ trợ khác  ∼ p() , trong đó p() không phụ thuộc vào φ . Ví dụ biến ngẫu nhiên tuân theo phân phối Gauss một chiều x ∼ N ( µ , σ 2 ) , φ = ( µ , σ ) có thể được biểu diễn như sau : x = µ + σ  = g ( φ , ) ,  ∼ N ( 0, 1 ) .	[]	[]	[]	[]
1069	Kết quả của việc thử nghiệm so sánh một phép DA và nhiều phép DA được thể hiện trong ma trận ở hình 14 ( đường chéo chính là việc chỉ dùng một phép DA duy nhất ) .	[[45, 47], [62, 64], [148, 150]]	[]	['DA', 'DA', 'DA']	[]
1070	Lời động viên , khích lệ tinh thần đến từ gia đình và bạn bè luôn là động lực để em phấn đấu hết mình và hoàn thành những mục tiêu mà bản thân đề ra . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[211, 222]]	[]	['KSTN - CNTT']	[]
1071	Nghiệm của bài toán ( 1 ) được tính bằng công thức : 𝛽 ∗ = ( 𝐴𝑇 𝐴 ) − 1 𝐴𝑇 𝑦	[]	[]	[]	[]
1072	trong khi học MTL với nhiều tác vụ , mô hình có thể học tập trung được các đặc trưng thực sự tốt cho các tác vụ . Vì vậy , nó làm tăng khả năng nắm bắt được các đặc trưng	[[14, 17]]	[]	['MTL']	[]
1073	"điều này , trước hết chúng ta biểu diễn logarit của hàm phân phối biên tại X, còn gọi là "" log complete -data "" hoặc evidence , dưới dạng như sau : Z"	[]	[]	[]	[]
1074	( TRAIN _ SET _ LENGTH , MAX _ SENTS , MAX _ SENT _ LENGTH ) , mỗi đơn vị trong ma trận là số nguyên tương ứng với vị trí của token trong bộ từ vựng . Kết thúc , ta biểu diễn dữ liệu ( văn bản ) huấn luyện ban đầu thành một ma trận dữ liệu	[]	[]	[]	[]
1075	Với thiết lập trong mô hình gốc , tác giả đang để temperature = 0.1 , trong phần thí nghiệm này , em thử đổi temperature = 0.5 và thực hiện quan sát trên cả LCL và SimCLR , kết quả thu được :	[[157, 160], [164, 170]]	[]	['LCL', 'SimCLR']	[]
1076	skip - gram cho độ chính xác cao hơn . Vì vậy mà hiện nay nó được sử dụng nhiều hơn so với mô hình CBOW .	[[99, 103]]	[]	['CBOW']	[]
1077	. . . . Hiệu suất LPP của các phương pháp trên mô hình LDA với bộ	[[18, 21], [55, 58]]	[]	['LPP', 'LDA']	[]
1078	luyện , tên là Stochastic Gradient Descent ( SGD ) . Khi đó thay vì cập nhật trọng J ( W )	[[45, 48]]	[]	['SGD']	[]
1079	sử dụng kết hợp các phép DA có thể khiến cho pretext task trở nên khó hơn nhưng Đồ án tốt nghiệp 29	[[25, 27]]	[]	['DA']	[]
1080	như HAT . Trong các phương pháp , EWC cần một lượng bộ nhớ nhiều nhất vì phải lưu lại cả độ quan trọng của tham số và bộ tham số tối ưu tác vụ phía trước , còn	[[4, 7], [34, 37]]	[]	['HAT', 'EWC']	[]
1081	9 do vậy EWC xấp xỉ đại lượng này trong biểu thức PT 2.9 qua phép xấp xỉ Taylor bậc 2 tại 𝜃 = 𝜃𝐴 ∗ , khi đó :	[[9, 12], [50, 52]]	[]	['EWC', 'PT']	[]
1082	[ 3 ] có thể coi là một biến thể của mạng nơ-ron tích chập ( CNN ) , là một mạng nơ-ron nhiều tầng hoạt động trực tiếp trên đồ thị , trong đó biểu diễn của mỗi nút được cập nhật lặp lại từ các nút lân cận của	[[61, 64]]	[[37, 58]]	['CNN']	['mạng nơ-ron tích chập']
1083	DANH MỤC HÌNH VẼ 3 . 1 Minh họa tư tưởng của nhóm phương pháp dựa trên tri thức . Nguồn [ 5 ] .	[]	[]	[]	[]
1084	KẾT LUẬN VÀ HƯỚNG PHÁT TRIỂN TƯƠNG LAI Phần này sẽ tóm tắt những kết quả cũng như những kinh nghiệm đã đạt được trong quá trình làm đồ án và định hướng phát triển nghiên cứu tiếp theo trong	[]	[]	[]	[]
1085	ở phần sau . Vì vậy em sẽ giới thiệu chi tiết cách xây dựng từ mô hình GMF , MLP cho đến NeuMF , và những ưu , nhược điểm của mô hình NeuMF .	[[71, 74], [77, 80], [89, 94], [134, 139]]	[]	['GMF', 'MLP', 'NeuMF', 'NeuMF']	[]
1086	dụng gated linear units ( GLU ) ( Dauphin et al. , 2016 ) ta được : zi = Ai ʘ sigmoid ( Bi ) ∈ ℝde Áp dụng residual connection ( Res1 ) ta thu được đầu ra dùng cho đầu vào của tầng tiếp	[[26, 29]]	[]	['GLU']	[]
1087	Nhìn chung , có thể nói các phương pháp trong CL nói riêng và SSL nói chung đang rất phát triển . Song , dù có đạt được các kết quả SOTA , thì trong bản thân mỗi	[[46, 48], [62, 65], [132, 136]]	[]	['CL', 'SSL', 'SOTA']	[]
1088	• Trong quá trình dữ liệu đến liên tục theo từng minibatch , SVB sử dụng phân phối hậu nghiệm trên dữ liệu quá khứ làm phân phối tiên nghiệm ( prior distribution ) cho dữ liệu hiện tại .	[[61, 64]]	[]	['SVB']	[]
1089	Nhà tuyển dụng đăng nhập vào hệ thống 1 - NTD chọn vào thanh tìm kiếm sẽ được dẫn tới 1 trang web đã hiển thị rất nhiều hồ sơ theo	[[42, 45]]	[]	['NTD']	[]
1090	Giáo viên hướng dẫn : ThS. Ngô Văn Linh HÀ NỘI 5 /2019 PHIẾU GIAO NHIỆM VỤ ĐỒ ÁN TỐT NGHIỆP	[[22, 26]]	[]	['ThS.']	[]
1091	Kế hoạch này có thể sử dụng phương pháp : time - based decay , step decay , exponential decay . Ở	[]	[]	[]	[]
1092	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 22 thông tin từ các bộ dữ liệu bên ngoài như hướng thứ hai thì các nghiên cứu này sử dụng	[[61, 70]]	[]	['CNTT – TT']	[]
1093	3.2 Mô hình phân loại cảm xúc Amend Representation Module ( ARM ) 3 . 2 . 1 Đặc điểm bạch tạng ( Albino feature )	[[60, 63]]	[]	['ARM']	[]
1094	– So với mô hình ITE- onehot , mô hình có thêm vec-tơ pcat đầu vào cho item , giúp mô hình nhận biết các item tương đồng ngay từ đầu vào • Hạn chế	[[17, 28]]	[]	['ITE- onehot']	[]
1095	thời gian . Sau đó họ giới hạn không gian các khả năng có thể của p ( Θt | D1 : t−1 ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[146, 157]]	[]	['KSTN - CNTT']	[]
1096	Kết quả được thể hiện trên Hình 12 . SVB không hoạt động tốt khi đối mặt với concept drift : hiệu suất giảm mạnh khi có sự chuyển tiếp giữa hai lớp và khôi	[[37, 40]]	[]	['SVB']	[]
1097	trong đó : s̃kj = ( βi πi ) T xk , A( s̃k ) = log Vi =1 exp ( s̃ki ) . Sử dụng các biến đổi trên , ta có thể viết hàm F dưới dạng tương đương sau :	[]	[]	[]	[]
1098	Ở đây tạo ra một nội dung bộ nhớ mới sử dụng cổng reset gate để lưu trữ thông tin phù hợp từ những trạng thái trước đó . Nếu 𝑟𝑡 có giá trị là 0 , tức là không sử dụng bất kì trạng thái nào trước đó .	[]	[]	[]	[]
1099	rằng bài toán ( 2 ) có thể tách thành N bài toán nhỏ , mỗi bài toán ứng với việc đi tối ưu một cột của ma trận Q : L ( qi ) =	[]	[]	[]	[]
1100	Khi đó mô hình có tính linh hoạt nhờ vào hàm phi tuyến để biểu diễn tương tác giữa người dùng và item , vì vậy có thể học một cách tốt hơn . Các công thức của mô hình MLP được định nghĩa cụ thể như sau :	[[167, 170]]	[]	['MLP']	[]
1101	Bảng 4 . 5 Giá trị trung bình độ gắn kết các chủ đề ( NPMI ) cho tập dữ liệu 20Newsgroups . Cao hơn thì tốt hơn	[[54, 58]]	[]	['NPMI']	[]
1102	Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 23 3 . CÁC PHƯƠNG PHÁP GIẢI QUYẾT BÀI TOÁN	[[61, 70]]	[]	['CNTT – TT']	[]
1103	Ước lượng cực đại hậu nghiệm ( Maximum a Posterior , MAP ) cực đại hóa phân phối hậu nghiệm này : θM AP = argmaxθ p ( θ |D )	[[53, 56]]	[[0, 28]]	['MAP']	['Ước lượng cực đại hậu nghiệm']
1104	- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Sinh viên thực hiện : Nguyễn Thế Linh – 20142585 – K59 – Lớp CNTT – TT 2.03 31	[[267, 276]]	[]	['CNTT – TT']	[]
1105	contest này . 4.1.4 Mysql MySQL là một hệ thống quản lý cơ sở dữ liệu quan hệ mã nguồn mở ( RDBMS ) dựa	[[92, 97]]	[[39, 89]]	['RDBMS']	['hệ thống quản lý cơ sở dữ liệu quan hệ mã nguồn mở']
1106	( dữ liệu ) E với tác vụ T và được đánh giá bởi độ đo P nếu máy tính khiến tác vụ T này cải thiện được độ chính xác P thông qua dữ liệu E cho trước . Học máy có liên hệ mật thiết	[]	[]	[]	[]
1107	Hình 9 : Hiệu suất LPP của các iDroppout trên 2 bộ 20NewsGroups và TMN title khi thay đổi phương sai σ 2 Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 62	[[19, 22], [67, 70], [165, 176]]	[]	['LPP', 'TMN', 'KSTN - CNTT']	[]
1108	trình học chuỗi tương tác của User cũng chưa tối ưu . Hơn nữa , mô hình LSTUR gặp vấn đề với chuỗi tương tác dài , các hành vi của người dùng càng xa trong	[[72, 77]]	[]	['LSTUR']	[]
1109	Trí tuệ nhân tạo : ( AI - Artificial Intelligence ) là các kỹ thuật giúp cho máy tính có thể thực hiện những công việc thường ngày của con người chúng ta . Các	[[21, 23]]	[[0, 16]]	['AI']	['Trí tuệ nhân tạo']
1110	ˆ t L̂ . Do đó có thể cập nhật các tham số biến phân • ∇	[]	[]	[]	[]
1111	CIFAR100 và Split CIFAR10 / 100 . Hình 4 . 7 mô tả chi tiết kiến trúc của phương pháp đề xuất được sử dụng trong thử nghiệm này .	[[0, 8], [18, 31]]	[]	['CIFAR100', 'CIFAR10 / 100']	[]
1112	kịch bản học từng miền ; P ( Y t ) = P ( Y t+1 ) và Y t 6 = Y t+1 trong kịch bản học từng lớp . 2.2	[]	[]	[]	[]
1113	Xét sự thay đổi của từng tác vụ ở hình 5 . 4 , AGS - CL duy trì ổn định độ chính xác của các tác vụ , VBD - CL cũng tương tự nhưng trên một số tác vụ như tác vụ đầu tiên có quên nhiều hơn hay thứ năm có cải thiện được độ	[[47, 55], [102, 110]]	[]	['AGS - CL', 'VBD - CL']	[]
1114	có thể khiến cho số lượng tham số của mô hình tăng lên đáng kể khi số lượng tác vụ ngày càng nhiều . Biến hỗ trợ 𝑠 ( 𝑙 ) là ma trận có kích thước 𝑀 × 𝐷 , tuy nhiên ở	[]	[]	[]	[]
1115	Q : tổng số item gợi ý rank i : Vị trí thứ i Độ đo nDCG @ k	[[51, 55]]	[]	['nDCG']	[]
1116	Mã nguồn của hai phương pháp EWC và UCL được sử dụng và kế thừa từ mã nguồn công bố trong bài báo gốc UCL [ 3 ] . Phương pháp VCL được cài đặt lại dựa	[[29, 32], [36, 39], [102, 105], [126, 129]]	[]	['EWC', 'UCL', 'UCL', 'VCL']	[]
1117	Tìm hiểu về phương pháp Auto-encoding variational bayes ( VAE ) . Áp dụng phương pháp VAE vào mô hình LDA . Thực nghiệm xây dựng mô hình và so sánh với mô hình khác là Online LDA	[[58, 61], [86, 89], [102, 105], [175, 178]]	[]	['VAE', 'VAE', 'LDA', 'LDA']	[]
1118	liên tục . Đây chính là động lực cho tác giả của EWC và các phương pháp tiếp cận dựa trên ràng buộc trọng số khác đề ra giải pháp có thể đánh giá được tầm quan	[[49, 52]]	[]	['EWC']	[]
1119	Ở đây , ta sẽ quan tâm đến số lượng click mà mô hình có thể dự đoán đúng ở trong top K . Ta gọi độ	[]	[]	[]	[]
1120	tác vụ t được viết như sau : | Dt | X	[]	[]	[]	[]
1121	Mô hình học biểu diễn văn bản dựa trên chủ đề tiềm ẩn của văn bản ( LDA ) [ 1 ] dù biểu diễn văn bản theo	[[68, 71]]	[]	['LDA']	[]
1122	Từ kết quả trên , ta có thể thấy biểu diễn từ mô hình học biểu diễn đề xuất khi so với biểu diễn CNN luôn cho kết quả vượt trội , mô hình NRMS cho kết quả tốt hơn 1 % trên các độ đo , mô hình NAML cho kết quả tốt hơn 6 - 8 % và mô hình	[[97, 100], [138, 142], [192, 196]]	[]	['CNN', 'NRMS', 'NAML']	[]
1123	Mô hình được gọi là ITE- onehot bởi vì đầu vào của mô hình là các vec - tơ one - hot mã hóa định danh của người dùng và item . Sử dụng các ký hiệu tương tự như trong mô hình NeuMF ( mục 2 . 3 . 5 ) , mô hình	[[174, 179], [20, 31]]	[]	['NeuMF', 'ITE- onehot']	[]
1124	False negative ( FN ) Dự đoán	[[17, 19]]	[]	['FN']	[]
1125	. Từ đó VD có thể biểu diễn hàm mục tiêu dưới dạng giống như VI trong công thức PT 2. 3 , ta viết lại như sau :	[[8, 10], [61, 63], [80, 82]]	[]	['VD', 'VI', 'PT']	[]
1126	− η − P̂i ( r̂i − P̂i qi ) + λqi s Tương tự , công thức cập nhật cho mỗi cột của P là	[]	[]	[]	[]
1127	Mỗi cột trong ma trận tương ứng với một vec -tơ biểu diễn thuộc tính ẩn cho bộ phim . Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[149, 160]]	[]	['KSTN - CNTT']	[]
1128	10 2 KIẾN THỨC CƠ SỞ bài toán gợi ý bán hàng , các sản phẩm được bán là item ; trong bài toán gợi ý	[]	[]	[]	[]
1129	2008 ) . Bộ dữ liệu này gồm các cặp câu được trích rút ra từ các bài báo của Wall Street Journal ( WSJ ) được tạo ra từ năm 2008 và được đánh giá là một tập dữ liệu chuẩn cho	[[99, 102]]	[]	['WSJ']	[]
1130	Trong mô hình ITE - onehot , các đầu ra thực tế nhận các giá trị 0 hoặc 1 . Do đó ta sử dụng hàm lỗi log-loss : X	[[14, 26]]	[]	['ITE - onehot']	[]
1131	mới , một tư duy mới cho hướng nghiên cứu CL . Đồ án tốt nghiệp	[[42, 44]]	[]	['CL']	[]
1132	Trong cả hai trường hợp nêu trên trọng số của mạng 𝜃 đều có thể diễn giải lại qua một ma trận trọng số mới là W , trong đó các thành phần sẽ là các biến ngẫu 17	[]	[]	[]	[]
1133	index nhỏ hơn – chuỗi lambda được sắp xếp từ lớn đến bé ) , đồng nghĩa với ít biến được chọn hơn . FCM có kích thước các nhóm lớn ( do chỉ có 25 nhóm ) nên số gene được chọn	[[99, 102]]	[]	['FCM']	[]
1134	• Giả sử dữ liệu được sinh độc lập , đồng nhất từ một phân phối với tham số Θ và tri thức tiên nghiệm η , tức là p ( x | Θ , η ) . Đồng thời cũng giả sử phân	[]	[]	[]	[]
1135	được cho là sẽ nắm bắt được cấu trúc phức tạp trong tương tác giữa người dùng với item , từ đó cải thiện chất lượng dự đoán . Đầu vào cho mô hình NCF là các vec - tơ u và i tương ứng mô tả cho người	[[146, 151]]	[]	['NCF l']	[]
1136	là : alpha =0.7, fit _ prior = True • Máy vec - tơ hỗ trợ ( SVM ) : Thư viện sklearn hỗ trợ thuật toán SVM với nhiều nhân khác nhau : SVC với	[[60, 63], [103, 106], [134, 137]]	[[38, 57]]	['SVM', 'SVM', 'SVC']	['Máy vec - tơ hỗ trợ']
1137	ta thường sử dụng các phiên bản cải tiến của giảm gradient như Stochastic gradient descent ( SGD ) [ 23 ] , AdaGrad [ 9 ] , Adam [ 24 ] , Nesterov ’ s accelerated gradient descent [ 47 ] . . . 1.2.2	[[93, 96]]	[]	['SGD']	[]
1138	của LDA và có tính diễn giải cao , gần gũi với con người , từ đó áp dụng vào các mô hình hệ gợi ý để so sánh và đánh giá kết quả . Ngoài ra , đồ án sẽ giới thiệu	[[4, 7]]	[]	['LDA']	[]
1139	( 38 ) Ở đây thay x trong VB cố điển bởi X được sinh ra từ phân phối population Fα . Tính chất :	[[26, 28]]	[]	['VB']	[]
1140	hợp với sở thích của User hơn . Cấu trúc mô hình NRMS có thể được quan sát trong hình sau đây [ 2 ] :	[[49, 53]]	[]	['NRMS']	[]
1141	M ( b ) iDropout cho mô hình LDA	[[29, 32]]	[]	['LDA']	[]
1142	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 19 2 KIẾN THỨC CƠ SỞ	[[63, 74]]	[]	['KSTN - CNTT']	[]
1143	12GB ( 16GB ) giờ / ngày	[[2, 4], [9, 11]]	[]	['GB', 'GB']	[]
1144	mức văn bản đang là phương pháp cho kết quả tốt nhất tại thời điểm đề xuất . 2.1. Phương pháp tiếp cận dựa trên Support Vector Machine ( SVM )	[[137, 140]]	[]	['SVM']	[]
1145	vec -tơ xi , yi , zi biểu thị cho các vec -tơ thuộc tính ẩn của bộ phim i trong các ma trận R , V , W .	[]	[]	[]	[]
1146	𝑊 kết hợp với bộ nhớ trước 𝑠𝑡−1 U kết hợp với 𝑥𝑡 để tính ra bộ nhớ của bước hiện tại 𝑠𝑡 Sau cùng kết hợp với V để tính 𝑦𝑡 .	[]	[]	[]	[]
1147	đầu làm tăng kết quả dịch . Bảng 3 . 2 : Điểm BLEU của các mô hình học đối ngẫu sử dụng nhiễu với 3 mô hình ngôn ngữ trên tập test 2013 .	[[46, 50]]	[]	['BLEU']	[]
1148	mà người dùng chưa từng tương tác để xếp hạng thay vì lấy toàn bộ . Ta sử dụng hai độ đo Hit Ratio ( HR ) và Normalized Discounted Cumulative Gain ( NDCG ) để đánh giá kết quả vị trí của test item trong danh sách đã được	[[101, 103], [149, 153]]	[]	['HR', 'NDCG']	[]
1149	lan truyền tiến hay suy diễn . Trọng số của mạng được học dựa trên tập dữ liệu huấn luyện 𝐷 = { ( 𝑥𝑖 , 𝑦𝑖 ) ⁡ |⁡𝑖 = 1, 2, … , 𝑛} bằng cách cực tiểu hóa hàm lỗi 𝐸𝐷 ( 𝑊 ) =	[]	[]	[]	[]
1150	2 . Mục đích nội dung của ĐATN Tìm hiểu , triển khai mô hình Hierarchical Attention Networks cho bài toán phân loại cảm xúc văn bản tiếng Việt .	[[26, 30]]	[]	['ĐATN']	[]
1151	giá trị riêng để tìm ra các biểu diễn DOC [ D×K ] và T OP IC [ K×V ] . Nhắc lại giả sử cơ bản của các mô hình chủ đề , trong đó mỗi văn bản được trộn bởi các chủ đề	[]	[]	[]	[]
1152	bằng thực nghiệm những phương pháp đó . Để hiểu rõ hơn về CL , trước tiên chúng ta sẽ tìm hiểu chi tiết mô hình SimCLR , một trong những mô hình có kết quả SOTA và đại diện cho nhóm các phương pháp	[[58, 60], [112, 118], [156, 160]]	[]	['CL', 'SimCLR', 'SOTA']	[]
1153	ρ tỷ lệ học với hai tham số κ và τ K	[]	[]	[]	[]
1154	Đặc biệt , khi áp dụng phép DA này vào những mô hình Đồ án tốt nghiệp 34	[[28, 30]]	[]	['DA']	[]
1155	này sẽ được sử dụng để xác định thứ tự các amino axid trong protein ( sẽ trình bày ở phần dưới ) . Sợi DNA có khả năng nhân đôi bằng cách tách mạch xoắn kép thành hai mạch đơn	[[103, 106]]	[]	['DNA']	[]
1156	Hình 5 . 8 : Độ chính xác trung bình khi kết thúc mỗi tác vụ trên tập Permuted MNIST với VBD - CL và HAT . 45	[[79, 84], [89, 97], [101, 104]]	[]	['MNIST', 'VBD - CL', 'HAT']	[]
1157	𝜃 ở tác vụ hiện tại B , và ∇ log 𝑝 ( 𝜃𝐴 , 𝑖 | 𝐷𝐴 ) = 0 do 𝜃𝐴 , 𝑖 là giá trị tối ưu ở tác vụ A .	[]	[]	[]	[]
1158	13 Với L là số lần lấy mẫu để xấp xỉ kỳ vọng . 2.3.4	[]	[]	[]	[]
1159	số cần học của mạng là các ma trận trọng số W , M và giá trị bias b . 19 h1	[]	[]	[]	[]
1160	Từ đây chúng tôi thấy đại lượng ρ này rất có ý nghĩa trong việc giúp SVB - PP khắc phục những hạn chế của SVB như sau : • Quên đi tri thức cũ với tốc độ hàm mũ , giúp phương pháp có cơ chế cân	[[69, 77], [106, 109]]	[]	['SVB - PP', 'SVB']	[]
1161	postgresql , AJAX , hibernate và Node.js đây là những công nghệ rất phù hợp với đồ án của em . - Framework spring boot là thư viện sử dụng ngôn ngữ java .	[[13, 17]]	[]	['AJAX']	[]
1162	tương đương , được gọi là Cận dưới chắc chắn ( ELBO ) : 𝐸𝐿𝐵𝑂 = 𝐸𝑞𝜙 (𝜃 ) log 𝑝 ( 𝐷 |𝜃 ) − 𝐾𝐿 ( 𝑞𝜙 ( 𝜃 ) | |𝑝(𝜃 ) )	[[47, 51], [26, 44]]	[]	['ELBO', 'Cận dưới chắc chắn']	[]
1163	cách tương tự . Ta sử dụng GRU hai chiều để mã hóa các câu trong văn bản ℎ⃗⃗𝑖 = ⃗⃗⃗⃗⃗⃗⃗⃗⃗⃗	[[27, 30]]	[]	['GRU']	[]
1164	end for Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 44	[[68, 79]]	[]	['KSTN - CNTT']	[]
1165	thành một bộ dữ liệu gọi là Split CIFAR10 / 100 và sử dụng cho kịch bản gia tăng số lượng tác vụ trong Học liên tục . Mỗi tác vụ sẽ tương ứng với việc	[[34, 47]]	[]	['CIFAR10 / 100']	[]
1166	Hard parameter sharing Hình dưới đây mô tả phương pháp hard parameter sharing trong MTL . Hình 10 : Hard parameter sharing	[[84, 87]]	[]	['MTL']	[]
1167	và Permuted MNIST cùng chung một kiến trúc MLP hai tầng , Split CIFAR100 và Split CIFAR10 - 100 cùng chung một kiến trúc mạng CNN ( chỉ khác nhau ở số tác	[[12, 17], [43, 46], [64, 72], [82, 95], [126, 129]]	[]	['MNIST', 'MLP', 'CIFAR100', 'CIFAR10 - 100', 'CNN']	[]
1168	phương pháp học dòng cho các mô hình Bayesian . Một số công việc như Streaming Variational Bayes ( SVB ) [ 4 ] , Hierarchical Power Priors ( HPP ) [ 10 ] đề xuất các thuật toán cập nhật đệ quy phân tham số biến phân biến toàn cục của	[[99, 102], [141, 144]]	[]	['SVB', 'HPP']	[]
1169	, log ( i+1 ) nếu như test item ở vị trí i trong top K .	[]	[]	[]	[]
1170	− log ( 𝜎𝑡 ) } Việc lựa chọn phân phối , cũng như quá trình lấy mẫu và áp dụng LRT sẽ được	[[79, 82]]	[]	['LRT']	[]
1171	Bảng 2 . 2 : Hiệu năng của các mô hình trước đó với bài toán PKD . Soft-LR ( EM ) tỏ ra vượt trội hơn hẳn khi sử dụng các đặc trưng thủ công làm biểu diễn cho dữ liệu .	[[61, 64], [67, 74], [77, 79]]	[]	['PKD', 'Soft-LR', 'EM']	[]
1172	Evaluation ( D3 ) : là đánh giá của người có ảnh hưởng lớn , nổi tiếng . Thường	[]	[]	[]	[]
1173	[ wdn = wj ] end while Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[83, 94]]	[]	['KSTN - CNTT']	[]
1174	tại là t có tập dữ liệu là Dt = { xti , yit } | D i=1 , độ quan trọng của tham số θj đối với mô hình cho tới tác vụ t − 1 là Ωt−1	[]	[]	[]	[]
1175	AGS - CL được xây dựng trên tư tưởng giảm thiểu sự thay đổi của luồng thông tin truyền qua các nơ-ron của mạng . AGS - CL định nghĩa hai nguyên nhân chính của	[[0, 8], [113, 121]]	[]	['AGS - CL', 'AGS - CL']	[]
1176	Thứ nhất , cả hai đều tìm độ quan trọng của nơ-ron trong từng tác vụ . Tuy nhiên , HAT tiếp cận theo cơ	[[83, 86]]	[]	['HAT']	[]
1177	( SGVB ) là một phương pháp tối ưu hiệu quả cho hàm mục tiêu này , bằng việc ước lượng đại lượng khả năng xảy ra trong quá trình học trên từng tập con có M điểm dữ liệu trong toàn bộ dữ liệu 𝐷 bằng kĩ thuật Monte Carlo [ 9 ] và kĩ thuật đổi biến	[[2, 6]]	[]	['SGVB']	[]
1178	tới hiện tại : R= Nt	[]	[]	[]	[]
1179	như huấn luyện của kỹ thuật VBD . 2.1 Tổng quan về học liên tục	[[28, 31]]	[]	['VBD']	[]
1180	1.2 Mạng nơ- ron Mạng nơ-ron nhân tạo ( Artificial neural network - ANN ) là một phương thức xử lý thông	[[68, 71]]	[[17, 37]]	['ANN']	['Mạng nơ-ron nhân tạo']
1181	bảo rằng các vector embeddings của từ nối có cùng quan hệ phải khác nhau . Cụ thể , với mỗi quan hệ ri ∈ R , gọi tập Ci là tập con của tập từ nối C gồm các từ nối liên kết với	[]	[]	[]	[]
1182	− KL ( q ( θ | ν ) | p ( θ | D1 : T −1 , α ) ) Việc sử dụng Gaussian Mixture Model ( GMM ) cho mô hình là thay vì sử dụng q ( θ | ν ) là một phần	[[85, 88]]	[]	['GMM']	[]
1183	VBD - CL được huấn luyện với 150 vòng lặp , và các phương pháp còn lại là 100 vòng lặp .	[[0, 8]]	[]	['VBD - CL']	[]
1184	P nếu như dùng P ta đo thấy năng lực thực thi của chương trình có tiến bộ sau khi trải qua E ” ( máy đã học ) [ 32 ] . Một nhánh nhỏ trong học máy gần đây rất được ưu chuộng là học sâu ( deep learning ) .	[]	[]	[]	[]
1185	( 2.8 ) trong đó A , B ∈ Rs , ⊗ là phép nhân từng phân tử . Khi đó , đầu ra v ( [ A B ] ) sẽ là một vector	[]	[]	[]	[]
1186	t bằng cách trích xuất thành phần G ( Θt ) từ đại lượng likelihood log p ( Dt | Θ ̃ ) . Việc này có thể thực hiện bằng cách sử dụng suy diễn biến phân chẳng hạn .	[]	[]	[]	[]
1187	Hình 2 . 3 : Cấu trúc khái quát của mô hình Transformer . Trong đó Q , K và V lần lượt là ma trận của các query , key và value . Trong mô hình	[]	[]	[]	[]
1188	PV Eξ [ A( s̃k )] − A( sk ) ) j=1 ukj tương đương với một phép chính	[]	[]	[]	[]
1189	k , σ I) ( 46 ) trong đó k là chỉ số hàng của Θ ( có kích thước K × V ) và I là một ma trận kích	[]	[]	[]	[]
1190	đổi quá nhiều . ( a ) HR @ 10	[[22, 24]]	[]	['HR']	[]
1191	Pr = softmax ( ErVr ) ∈ ℝn Pc = softmax ( EcVc ) ∈ ℝk trong đó Pr , Pc là phân phối xác suất trên tập quan hệ R và tập các từ nối C .	[]	[]	[]	[]
1192	∇ELBO . ( 4.2 ) Chú ý rằng mô hình cần giữ lại được tri thức của tất cả các tác vụ phía trước , vì	[[1, 5]]	[]	['ELBO']	[]
1193	min C ( G ) = log 4 , khi và chỉ khi ⇔ p g = pdat a p	[]	[]	[]	[]
1194	thấy sở thích ngắn hạn của người dùng . Và ý tưởng của mô hình BERT được áp dụng để học ra các bài báo quan trọng hơn với người dùng , đồng thời cũng thể	[[63, 67]]	[]	['BERT']	[]
1195	RNN hai chiều kết hợp hai RNN với nhau . Trình tự đầu vào được cung cấp theo thứ tự thời gian bình thường cho một mạng và theo thứ tự thời gian ngược lại cho mạng	[[0, 3], [26, 29]]	[]	['RNN', 'RNN']	[]
1196	74.3 Bảng 5 : Kết quả khi kết hợp phép augment Multi-crop ( Nguồn [ Car +20 ] ) thức số ( 16 ) , song trong lúc training để hạn chê về mặt thời gian , mã Q được lấy	[]	[]	[]	[]
1197	quan giữa người dùng và item được tính bằng : Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 31	[[109, 120]]	[]	['KSTN - CNTT']	[]
1198	ĐỒ ÁN TỐT NGHIỆP HỆ THỐNG GIẢI PHÁP VIỆC LÀM Nguyễn Đại Dương	[]	[]	[]	[]
1199	Củng cố trọng số mềm dẻo ( Elastic Weight Consolidation , EWC ) [ 5 ] sử dụng ma trận thông	[[58, 61]]	[[0, 24]]	['EWC']	['Củng cố trọng số mềm dẻo']
1200	3.2 Chống quên sử dụng cơ chế chú ý cứng cho tác vụ , HAT HAT [ 16 ] dựa trên biểu diễn riêng biệt của từng tác vụ , xây dựng cơ chế chú ý lên	[[54, 57], [58, 61]]	[]	['HAT', 'HAT']	[]
1201	Có thể thấy , ConvS2S được lợi ích nhiều nhất từ việc ensemble , điều này làm cho kết quả tăng lên tới 2.6 điểm BLEU ở mô hình base trên tập tst2013 . Trong khi đó , mô hình	[[14, 21], [112, 116]]	[]	['ConvS2S', 'BLEU']	[]
1202	Mô hình dịch máy sử dụng CNN Mô hình dịch máy sử dụng mạng CNN kết hợp RNN được đề xuất lần đầu bởi Gehring [ 13 ]	[[25, 28], [59, 62], [71, 74]]	[]	['CNN', 'CNN', 'RNN']	[]
1203	được viết dưới dạng : 𝑁 𝐾	[]	[]	[]	[]
1204	Bước đầu tiên là tối ưu hàm mục tiêu 𝐸 ( 𝑊 ) cho đến khi kích thước của mạng hội tụ . Bước thứ hai là tinh	[]	[]	[]	[]
1205	( 2.19 ) Trong trường hợp tổng quát với X là một biến ngẫu nhiên nhiều chiều X ∈ RK , biến ngẫu nhiên Z = g ( X ) : RK → RK , khi đó , hàm mật độ xác suất của biến ngẫu nhiên Z nhiều chiều có công	[]	[]	[]	[]
1206	Phạm vi đề tài tập trung nghiên cứu giải quyết bài toán nhận dạng quan hệ ẩn giữa hai câu trên tập dữ liệu PDTB . 1 . 3 Định hướng giải pháp .	[[107, 111]]	[]	['PDTB']	[]
1207	....................................................................................................... 40 5. KẾT LUẬN .............................................................................................................. 42	[]	[]	[]	[]
1208	Bảng 4 . 4 : Kết quả cải tiến JacReg chỉ bằng 1 giá trị a duy nhất ( 3FC ) Phương pháp JacReg	[[30, 36], [69, 72], [87, 93]]	[]	['JacReg', '3FC', 'JacReg']	[]
1209	lượng vi phân . Ở đây , 𝑞 ( 𝜃 | 𝜙 ) còn được gọi là phân phối biến phân . Mục tiêu của VI là cố gắng tìm ra được 𝜙 sao cho 𝑞 ( 𝜃 | 𝜙 ) càng gần với 𝑝 ( 𝜃 |𝐷 )	[[87, 89]]	[]	['VI']	[]
1210	KL ( q ( z , Θ ) | |p ( z , Θ |x) ) , thì đối với PVB , cực đại hoá hàm F- ELBO là không đảm Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 41	[[50, 53], [153, 164], [72, 79]]	[]	['PVB', 'KSTN - CNTT', 'F- ELBO']	[]
1211	vector embeddings của các quan hệ trong tập Ri , đồng nghĩa với việc ta phải tối thiểu hàm lỗi sau : L1 = ∑𝑘𝑖=1 ∑𝑟𝑗 ∈ 𝑅𝑖 ‖ 𝐸𝑐 [𝑐𝑖 ] − 𝐸𝑟 [ 𝑟𝑗 ] ‖	[]	[]	[]	[]
1212	Extensive experiment s show the significant improvement s of iDropout in comparision to other state-of-the-art methods . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT 6	[[181, 192]]	[]	['KSTN - CNTT']	[]
1213	tham số cục bộ 𝑠𝑡 : 𝐾𝐿 ( 𝑞𝑡 ( 𝑠𝑡 ) | | 𝑝 ( 𝑠𝑡 ) ) . Trong công việc này , việc lựa chọn phân phối tiên nghiệm cho 𝑠𝑡 và thành phần 𝐾𝐿 ( 𝑞𝑡 ( 𝑠𝑡 ) | | 𝑝 ( 𝑠𝑡 ) ) sẽ được xấp xỉ giống	[]	[]	[]	[]
1214	5 .7 Phần trăm không gian mạng CNN sử dụng qua các tác vụ trên tập Split . 35	[[31, 34]]	[]	['CNN']	[]
1215	Câu 1 : but ABC Sports also use s the call s as a sales tool Câu 2 : After thanking callers for voting , Frank Gifford offers a football videotape for $ 19.95	[]	[]	[]	[]
1216	0 = a−K Γ (K ) = a−K ( K − 1 ) ! Thế ( 2.25 ) vào ( 2.24 ) , ta được	[]	[]	[]	[]
1217	Tức là đi tìm hàm số yi ≈ f ( xi ) , ∀i = 1, 2 , . . . , N trong đó xi là những điểm dữ liệu trong tập huấn luyện và yi là nhãn tương ứng . Một cách tự	[]	[]	[]	[]
1218	giải trong biểu diễn đối với mô hình học biểu diễn dựa trên Deep Learning , đồng thời , vấn đề trùng lặp , thiếu chủ đề của mô hình LDA cũng được giải quyết . Trong mô hình đề xuất , bằng thực nghiệm có thể thấy các chủ đề con khá đầy đủ ,	[[132, 135]]	[]	['LDA']	[]
1219	được hiển thị trong ( H18 ) , trên tập dữ liệu ML 1m lớn , mô hình này với 2 lớp ( DMF - 2layer ) đạt hiệu suất tốt nhất . Các lớp sâu hơn dường như không hữu ích ,	[[47, 49], [83, 86]]	[]	['ML', 'DMF']	[]
1220	Topics coherence ( NPMI ) Hình 4 . 9 Đồ thị giá trị NPMI khi thay đổi số lượng dữ liệu đưa vào mỗi lần lặp trên tập dữ liệu Associated Press	[[19, 23], [52, 56]]	[]	['NPMI', 'NPMI']	[]
1221	Mức 1 có bốn nhãn quan hệ là : Comparison ( Comp ) , Contigency ( Cont ) , Expansion ( Exp ) , Temporal ( Temp ) . Ở mức 2 , mỗi quan hệ trong mức 1 được	[[44, 48], [66, 70], [87, 90], [106, 110]]	[]	['Comp', 'Cont', 'Exp', 'Temp']	[]
1222	sẽ được áp dụng . Tuy nhiên trong các trường hợp p ( 𝑦1 : 𝑇 ) thay đổi theo từng tác vụ ,	[]	[]	[]	[]
1223	- Có thể chạy trên cả CPU và GPU -	[[22, 25], [29, 32]]	[]	['CPU', 'GPU']	[]
1224	Trong đó 𝐽𝑥 ( 𝐹 ) là ma trận Jacobian của F theo 𝑥 , 𝐻𝑥 ( 𝐹 ) là ma trận Hessian của F theo 𝑥 . Chú ý rằng 𝐸𝛿 [ 𝐽ℎ(𝑙) ( 𝑥) ( 𝐿̅) 𝛿 ] = 0 do 𝛿 tuân theo phân phối Gauss có kỳ	[]	[]	[]	[]
1225	Tổng tham số lớn hơn số lượng tham số của CNN rất nhiều . Vì vậy mà CNN chạy nhanh hơn nhiều so với mạng ANN thông thường .	[[42, 45], [68, 71], [105, 108]]	[]	['CNN', 'CNN', 'ANN']	[]
1226	Collaborative Filtering ( NCF ) [ 6 ] . Theo lý thuyết , NCF sẽ sử dụng một mạng neural để nắm bắt các cấu trúc đặc trưng ẩn của người dùng và sản phẩm để đưa	[[26, 29], [57, 60]]	[]	['NCF', 'NCF']	[]
1227	Phan Vũ Hồng Hải MSSV : 20141394	[[17, 21]]	[]	['MSSV']	[]
1228	Biểu diễn của trạng thái cuối cùng sau khi qua các mạng GRU được dùng làm biểu diễn Short-term của User :	[[56, 59]]	[]	['GRU']	[]
1229	bản testing Dtest là : P	[]	[]	[]	[]
1230	𝑁 ∑𝑗 =1 𝑒𝑥𝑝 ( 𝑎𝑗 ) Trong đó , v và 𝑣𝑏 là tham số mô hình .	[]	[]	[]	[]
1231	vậy , để tối ưu CTR ( hay nói cách khác là tăng CTR càng cao càng tốt ) , ta cần tăng số lượng click và giảm số lượng view . Hay nói cách khác , ta cần gợi ý hiển	[[16, 19], [48, 51]]	[]	['CTR', 'CTR']	[]
1232	Trong bài nghiên cứu này , em có đề cập đến ba hàm lỗi L1 , L2 , L3 trong các phương trình ( 7 ) , ( 8 ) và ( 9 ) . Nhằm đánh giá mức độ quan trọng của các hàm lỗi này , dưới đây	[]	[]	[]	[]
1233	rộng của VD , khắc phục được nhược điểm phân phối tiên nghiệm không hợp lệ ( improper prior ) của VD. Một cách tổng quát , thay vì chọn log uniform làm phân	[[9, 11], [98, 100]]	[]	['VD', 'VD']	[]
1234	q ( Θ | λ ) q ( Θ | λ0 ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[85, 96]]	[]	['KSTN - CNTT']	[]
1235	mang đậm màu sắc cá nhân Expectation ( D4 ) : suy đoán về hậu quả của sự kiện chính hoặc theo ngữ cảnh .	[]	[]	[]	[]
1236	Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 52	[[63, 74]]	[]	['KSTN - CNTT']	[]
1237	trong đó W i và bi lần lượt là ma trận trọng số và bias của lớp ẩn thứ i. Mỗi phần tử wjk trong ma trận trọng số Wi thể hiện sự kết nối từ đơn thứ k của lớp i − 1 tới đơn vị thứ j của	[]	[]	[]	[]
1238	( 𝑙 ) phân phối cho từng phần tử 𝑠𝑡 , 𝑚𝑑 được chọn là 𝑁 ( 𝜇 , 𝛼𝑡, 𝑚𝑑 ) , trong đó 𝜇 là hằng số ( 𝑙 )	[]	[]	[]	[]
1239	= KL ( qφ (E ) | p (E |γ ) ) − Eqφ (E ) [ log p ( D |E ) ] + log p ( D |γ ) . Ta cần tối đa hóa ELBO với tham số { γ , φ } . Chú ý rằng mô hình còn tham số W	[[96, 100]]	[]	['ELBO']	[]
1240	nhớ T1 mà không học được T2 . Còn với ràng buộc hợp lý như trong 3 . 1 , mô hình có thể tìm được tham số thuộc giao của hai vùng hiệu quả của T1 và T2 .	[]	[]	[]	[]
1241	T ∇2 A ( sk ) ( s̃k − sk ) 2 mà Eξ [ s̃kj ] = skj nên :	[]	[]	[]	[]
1242	Learning cho IDRR Mô hình multi-task learning này nhằm mục đích dự đoán ra các quan hệ và quá trình huẩn luyện cho việc dự đoán các từ nối nhằm bổ sung tri thức cho việc dự đoán các	[[13, 17]]	[]	['IDRR']	[]
1243	Information. Synaptic Intelligence ( SI ) ( Friedemann Zenke , 2017 ) tính toán độ quan trọng của trọng số một cách trức tuyến bằng cách lưu lại sự thay đổi của hàm lỗi đối với từng trọng	[[37, 39]]	[]	['SI']	[]
1244	T ( Θt ) ) Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT ( 28 )	[]	[[71, 82]]	[]	['KSTN - CNTT']
1245	kĩ thuật DA . Láng giềng của một điểm trong không gian gốc mà nhóm đang định nghĩa chính là các view khác của cùng một dữ liệu gốc ban đầu .	[[9, 11]]	[]	['DA']	[]
1246	4 có được nhờ xấp xỉ Taylor bậc 2 của L1 ( w2 ∗ ) tại w1 ∗ ; 4 . 5 có được vì	[]	[]	[]	[]
1247	phải tình trạng tiêu biến gradient và do đó LSTM có thể nắm bắt được thông tin có sự phụ thuộc dài hạn hiệu quả hơn phiên bản mạng nơ-ron hồi quy đơn giản . Kiến trúc của LSTM	[[44, 48], [171, 175]]	[]	['LSTM', 'LSTM']	[]
1248	công việc 3 - Ngoài ra , NTD có thể chọn thêm loại ngành nghề , hoặc chọn thêm địa điểm	[[25, 28]]	[]	['NTD']	[]
1249	phối tiên nghiệm 𝑝 ( 𝜃 (𝑙) ) . Giá trị pre-activation khi đó : 𝐵 (𝑙 ) = 𝐴( 𝑙) 𝜃 ( 𝑙 ) . ( 𝑙 )	[]	[]	[]	[]
1250	34 c . Kết quả thử nghiệm trên Split CIFAR10 / 100 :	[[37, 50]]	[]	['CIFAR10 / 100']	[]
1251	men , tế bào huyết tương và tế bào ung thư ở người . FCM còn được kết hợp với nhiều phương pháp như SVM [ 67 ] hay học bán giám sát [ 68 ] để nâng cao hiệu quả trong bài	[[53, 56], [100, 103]]	[]	['FCM', 'SVM']	[]
1252	cùng , thông tin về xâu X sẽ được mã hóa vào hm . Decoder được mô tả bởi công thức dưới đây ( d )	[]	[]	[]	[]
1253	năng vượt trội của ALV so với hai trường hợp còn lại qua kết quả được đưa ra trong Hình 4 . 15 . Kết quả trên hai phương pháp này cho thấy , ALV giúp tăng khả	[[19, 22], [141, 144]]	[]	['ALV', 'ALV']	[]
1254	như sau : 𝑀𝐻 ( 𝐻 𝑙 ) = [ ℎ𝑒𝑎𝑑1 , ℎ𝑒𝑎𝑑2 , . . . , ℎ𝑒𝑎𝑑ℎ ] 𝑊 𝑂	[]	[]	[]	[]
1255	VIỆN CÔNG NGHỆ THÔNG TIN VÀ TRUYỀN THÔNG ĐỒ ÁN TỐT NGHIỆP Xây dựng mô hình học sâu giàu tính diễn	[]	[]	[]	[]
1256	Multiview Coding , PIRL , ... Trong khuôn khổ đồ án này , phần chi tiết sẽ chỉ trình bày mô hình SimCLR , MOCO .	[[19, 23], [97, 103], [106, 110]]	[]	['PIRL', 'SimCLR', 'MOCO']	[]
1257	Trước hết , do MF ánh xạ các người dùng và item vào cùng một không gian thuộc tính ẩn , mức độ tương đồng giữa hai người dùng cũng có thể được tính bằng tích vô hướng , hay tương đương với việc	[[15, 17]]	[]	['MF']	[]
1258	hạn chế trên của SVB . 3 . 3 Phương pháp học Hierarchical Power Priors	[[17, 20]]	[]	['SVB']	[]
1259	Chúng tôi tổng hợp quá trình học của iDropout cho LDA trong giải thuật sau . Sinh viên thực hiện : Nguyễn Văn Sơn , 20143863 , K59 , Lớp KSTN - CNTT	[[50, 53], [137, 148]]	[]	['LDA', 'KSTN - CNTT']	[]
1260	Quan hệ dự đoán ( Bai and Zhao , 2018 ) : Temporal Quan hệ đúng : Expansion Câu 1 : NBC has been able to charge premium rates for this ad time	[]	[]	[]	[]
1261	tử đầu vào liên tiếp , mỗi phần tử là một vector thuộc không gian s chiều , đầu ra của kernel là một vector Y ∈ R2s . Mỗi nhóm k phần tử đầu ra của lớp trước sẽ được xử lý tiếp theo	[]	[]	[]	[]
1262	VCcorp . 4 . 2 . 1 Tập dữ liệu MIND Tập dữ liệu MIcrosoft News Dataset ( MIND ) là một tập dữ liệu quy mô lớn về	[[31, 35], [73, 77]]	[]	['MIND', 'MIND']	[]
1263	CHƯƠNG 3 . PHƯƠNG PHÁP ĐỀ XUẤT 3 . 1 . Lasso và các biến thể của Lasso	[]	[]	[]	[]
1264	khả năng mô hình hoá hiệu quả dữ liệu có cấu trúc rời rạc , đặc biệt là dữ liệu văn bản chữ . LDA được ứng dụng rất thành công nhiều nhiều lĩnh vực bao gồm	[[94, 97]]	[]	['LDA']	[]
1265	các tham số của mô hình bằng cách cực đại hóa hàm phân phối hậu nghiệm p ( z , Θ | D1 , D2 , ... , Dt , η ) , với η là một tri thức tiên nghiệm khởi tạo của mô hình . Tuy nhiên , như đã trình bày , việc tính toán trực tiếp phân phối này là bất khả	[]	[]	[]	[]
1266	. Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT 39	[[65, 76]]	[]	['KSTN - CNTT']	[]
1267	Dù được định nghĩa theo nhiều cách khác nhau , nhưng từ hai khái niệm trên thì về cơ bản ta có thể hiểu được , ý tưởng chính của SSL chính là tự khai phá các tri thức trong dữ liệu bằng việc định nghĩa một vài bài toán mà con người tự đặt ra ,	[[129, 132]]	[]	['SSL']	[]
1268	M ( a ) iDropout cho mô hình tổng quát B ( β , z , x )	[]	[]	[]	[]
1269	tham số quan trọng đối với tác vụ A và EWC mong muốn trích xuất được thông tin từ thành phần này . Tuy nhiên xác suất hậu nghiệm log 𝑝 ( 𝜃 | 𝐷𝐴 ) là khó tính toán	[[39, 42]]	[]	['EWC']	[]
1270	t ( giờ ) BLEU	[[10, 14]]	[]	['BLEU']	[]
1271	DA này đều có kết quả tốt hơn đáng kể , được thể hiện ở thực nghiệm trong bảng 5 . Kết quả cuối cùng : Như đã trình bày từ phần giới thiệu về SSL , có thể nói pretext task không phải là kết quả thực sự mà các phương pháp SSL hướng đến .	[[0, 2], [142, 145], [221, 224]]	[]	['DA', 'SSL', 'SSL']	[]
1272	• Kịch bản gia tăng về miền dữ liệu : bộ dữ liệu PMNIST . • Kịch bản gia tăng về số lượng tác vụ với số nhãn cần phân loại là như nhau trên mọi tác vụ , ta gọi ngắn gọn là “ kịch bản gia tăng tác vụ đồng số nhãn ” :	[[49, 55]]	[]	['PMNIST']	[]
1273	, φM LP Sinh viên thực hiện : Nguyễn Trọng Nhật , 20143316 , K59 , Lớp KSTN - CNTT	[[71, 82], [5, 7]]	[]	['KSTN - CNTT', 'LP']	[]
1274	Ở đây , biểu diễn đầu vào của doc là vec-tơ LDA của nội dung các bài báo , cùng không gian thuộc tính	[[44, 47]]	[]	['LDA']	[]
